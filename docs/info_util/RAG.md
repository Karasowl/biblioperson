[00:00] a la hora de implementar proyectos de Inteligencia artificial una de las técnicas más populares es el rag o retrieval
[00:06] augmented generation la cual es una técnica de la cual hemos hablado muchas veces en distintos videos y en capítulos
[00:13] del podcast sin embargo hoy día la veremos más desde su aspecto técnico haciendo una demostración de código para poder
[00:20] ver cómo se implementa en un caso sencillo y así poder entender cómo se puede llevar a la práctica Esta
[00:27] técnica Pero antes de ir a la parte técnica vamos a hablar un poco de qué se trata y vamos
[00:32] a despejar dudas respecto a sus fundamentos ahora voy a ir a chat gpt y les voy a mostrar algunas
[00:37] preguntas que quizás no van a funcionar como uno esperaría la primera es qué es Evo Academy para comenzar abriremos
[00:45] chat gpt Classic la cual es una versión de chat gpt que no tiene ningun adicional y así nos permite
[00:52] ver cómo el modelo reacciona sin ninguna ayuda Entonces vamos a ir y vamos a poner que es Evo Academy
[00:58] lo cual nos responde que tiene que ver con una plataforma educativa en línea lo cual es correcto que ofrece
[01:04] cursos que también es correcto pero después ya se desvía a que tiene que ver con evolución biología y otros
[01:10] temas relacionados lo cual no es el caso porque nosotros tenemos que ver con cursos relacionados a Inteligencia artificial y
[01:16] tecnología ya y se dieron cuenta acá sucedió algo que se le conoce como una alucinación puesto que esto es
[01:24] impreciso pero hagamos otra pregunta porque puede ser que la pregunta de Academy sea muy específica Así que ahora vamos
[01:31] a ir y vamos a copiar otra pregunta con respecto a Quién es el presidente de Argentina y vemos que
[01:37] nos responde algo que tiene que ver con la actualización de sus datos que es que en mi última actualización
[01:43] en octubre del 2023 el presidente de Argentina es Alberto Fernández lo cual sabemos que actualmente no es así pues
[01:50] es Javier Miley Qué pasó acá pasó un tema de actualización pues no tiene acceso a estos datos más actualizados
[02:00] y finalmente veamos otra pregunta que es la que vamos a utilizar para el ejercicio de hoy que vamos a
[02:07] preguntarle en una nueva ventana donde se transmitirá la WWE es una empresa conocida por la lucha libre en 2025
[02:16] lo cual tiene relación a una noticia que salió recientemente de que va a cambiar durante ese año sin embargo
[02:23] vemos que ahora nos habla de la situación particular actual que es que se transmite en la cadena Fox y
[02:28] usa en Estados Unidos qué es lo que tienen en común estas preguntas de que acá chat gpt si bien
[02:34] pudo entregar una respuesta esta respuesta estaba limitada por su base de conocimiento ya sea porque no tenía conocimiento de
[02:42] la empresa en el caso de la pregunta de Academy o porque no tenía información actualizada como en el caso
[02:47] de la pregunta del presidente de Argentina o la que vamos a ocupar ahora para el ejemplo para poder entregarle
[02:53] estos datos muchas veces se cree que hay que actualizar el modelo y eso implicaría reentrenar lo y ocupar estos
[02:59] servidores gigantes que tienen empresas como Open Ai y correr procesos que son demasiado caros sin embargo eso puede aún
[03:06] así excluir datos que sean importantes para ti para poder personalizar el modelo para tu caso de uso para que
[03:12] tengas estos documentos cerca es que se utiliza Esta técnica que se llama rack el rack formalmente combina la generación
[03:20] de texto de un modelo de lenguaje enorme como chat gpt como Cloud como gemini entre otros con la recuperación
[03:27] de información desde una base de datos o una base de conocimiento mezclando Así un poco como estas facultades conversacionales
[03:35] que tiene la Inteligencia artificial con la capacidad de ir a buscar información que normalmente tienen bases de datos por
[03:42] lo mismo es ideal para aquellas aplicaciones en las cuales nos queremos asegurar que el modelo de lenguaje no esté
[03:48] alucinando pues pueda ir a buscar la última versión de la información respecto a la consulta que estamos haciendo Entonces
[03:57] cómo pasa esto cuando ya lo vemos en la práctica acá tenemos un siguiente esquema en el cual el usuario
[04:03] conversa con el robot pero a su vez el robot antes de contestarla el usuario va a una base de
[04:10] datos a una base de documentos y a partir de esos documentos se eligen algunos y se le entregan de
[04:17] vuelta al modelo de lenguaje antes de que le pueda entregar esa información al usuario es decir de cierta manera
[04:26] como que el modelo de lenguaje tiene ciertos conocimientos adicionales su entrenamiento pero que dependen en este caso de lo
[04:35] que el usuario le pregunte vamos a entrar un poquito más en detalle Cómo funciona esto primero tenemos que preparar
[04:41] esta base de datos Y para preparar esa base de datos lo que vamos a hacer es tomar documentos y
[04:47] los vamos a dividir en pedacitos Esta división en pedacitos se hace por varios motivos una tiene que ver con
[04:54] el hecho de que cada uno de estos pedacitos Quizás es más relevante que otro respecto a la duda del
[05:00] usuario por ejemplo supongamos que estamos viendo una noticia larga son respecto a un hecho y ese hecho por ejemplo
[05:07] puede contar qué es lo que sucedió dónde sucedió Y por qué sucedió puede ser que para lo que esté
[05:14] preguntando el usuario sea muy importante el dónde decidió y otras piezas de información no sean tan importantes por lo
[05:22] tanto a la hora de dividir esto en pedacitos haces que esa información pueda ser un poco más relevante para
[05:30] el usuario además también otra razón es el tema técnico muchos de los modelos de lenguaje tienen una capacidad limitada
[05:38] de información que pueden absorber al mismo tiempo que se le conoce como ventana de contexto al dividir la información
[05:45] en pedacitos Esto hace de que pueda ser administrada por un modelo de lenguaje aún cuando su ventana de contexto
[05:54] esté limitada luego cada uno de estos pedacitos se pasan por un modelo que se le llama el modelo de
[06:00] embeddings Si no han escuchado esa palabra no se preocupen lo vamos a explicar a continuación y el resultante de
[06:05] pasar estos pedacitos a través de los modelos de eding se guarda en lo que se conoce una base de
[06:11] datos vectorial también se puede almacenar en otras cosas pero hace el proceso todo más lento para efectos del ejemplo
[06:18] técnico que vamos a ver hoy día vamos a almacenarlo Sencillamente en una tabla pero en la práctica se ocupan
[06:25] bases de datos vectorial lo cual no es más ni menos que un autor de base de datos como otros
[06:31] como los que funcionan normalmente con sql como podría ser postgress my sequel etcétera Pero basado en en vez de
[06:39] que sean textos son los embeddings que están a través de este modelo Entonces si es que nunca han escuchado
[06:45] el concepto de embeddings No se preocupen ahora lo vamos a ver un poco más en detalle los embeddings son
[06:51] representaciones numéricas de cierta información como texto imágenes etcétera y tratan de cierta manera capturar el el significado de la
[07:01] palabra ya entonces acá les voy a dar un pequeño ejemplo que está obviamente sobres simplificado en este caso por
[07:08] ejemplo vemos una palabra que es rey que uno podría tratar de por ejemplo dividirla en que no sé la
[07:16] r está presente que la e está presente que la i está presente pero eso No necesariamente refleja su significado
[07:23] cierto Entonces los modelos de embeddings Qué son son un vector formalmente como se le llama en matemáticas pero si
[07:31] no si no conocen ese término no se preocupen pueden pensar que esto es una serie de números por ejemplo
[07:36] los que están acá 01 08 men 02 - 03 que de alguna manera representan esta palabra Y por qué
[07:44] digo representan porque una vez que tenemos esto en un espacio vectorial vamos a suponer que esto es un espacio
[07:49] de tres dimensiones pueden resultar cosas interesantes como esta por ejemplo que dado los números que tiene Rey quede más
[07:58] cerca de re que lo que queda de manzana lo cual tiene sentido porque nosotros sabemos del lenguaje normal que
[08:06] Rey obviamente se parece mucho más a Reina de lo que se parece a manzana pero a la vez vemos
[08:13] otras cosas interesantes por ejemplo en este gráfico de que de alguna manera pareciera que hombre y mujer tienen la
[08:19] misma relación están paralelos en este espacio vectorial a rey y reina obviamente Esto está un poco sobres simplificado con
[08:27] respecto a lo que se ve en la vida real pero uno pudiera llegar a encontrar cosas en este tipo
[08:31] de universos vectoriales que son tan interesantes como esta de que matemáticamente y no incluyendo las palabras ni entendiendo realmente
[08:40] Qué significan las palabras pua llegar a encontrar por ejemplo que Rey y sus números al restarles lo que es
[08:49] hombre y quedarme de alguna manera con este concepto abstracto de realeza se lo sumo a mujer y me dan
[08:56] los mismos números que reina ya esto Esto no es nuevo viene desarrollándose hace un tiempo atrás y obviamente esto
[09:02] es una sobres simplificación de lo que se encuentran pero muchas veces pasan cosas como estas de que de cierta
[09:07] manera logramos que palabras pasen a poder ser representadas en números lo cual obviamente es un avance tremendo en la
[09:16] disciplina del estudio del lenguaje natural pero eso qué tiene que ver con el rag bueno resulta de que esta
[09:24] información que hablamos en el rack estos documentos son transformados a números para qué Para que así podamos compararlos rápidamente
[09:34] ocupando ciertas funciones matemáticas y vamos a ver cómo se hace esto en un ejemplo pero por ahora quédense con
[09:41] la idea de que cada uno de estas piezas estos chuns estos pedacitos de los documentos van a ser convertidos
[09:47] a números y eso nos va a permitir hacer ciertas operaciones Matemáticas para poder entregarle la información a nuestro modelo
[09:54] de lenguaje de una manera mucho más fácil Entonces ahora que ya tengo mis os están cargados como numeritos en
[10:01] esta base de datos como embeddings qué es lo que hago a continuación tomo la query o pregunta o petición
[10:08] del usuario y esa petición que puede ser por ejemplo dónde se va a transmitir w rw o w en
[10:16] general en 2025 se transforma a números por qué porque así puedo comparar números con números puedo hacer operaciones Matemáticas
[10:25] entre los números que están en la base de datos entre los números que me pasó sol usuario y así
[10:30] voy a poder calcular cosas que me permitirán elegir el documento más relevante con ello con estos números vas y
[10:37] comparas en la base de datos todos los chunks o pedacitos que están cargados y eliges aquel que matemáticamente se
[10:46] parezca más una vez que tengo ese chunk que en este caso vamos a asumir Por un instante que es
[10:52] este rojo que está acá arriba se lo paso a un modelo de lenguaje y ese modelo de lenguaje lo
[10:59] que va a hacer es tratar de contestar la pregunta o el requerimiento que me haya hecho el usuario con
[11:05] esa pieza de información que es la más relevante según la comparación matemática que hicimos en el proceso de retrial
[11:14] Entonces si se dan cuenta Acá hay varios procesos que están ocurriendo está ocurriendo el indexing para poder dejar estos
[11:21] documentos en la base de datos Y luego a la base de datos Yo la voy a ir a consultar
[11:26] y traigo alguna parte de esa información no toda la parte que matemáticamente luzca más similar la traigo aso se
[11:34] le llama retrieval y luego con esa información que traje más la pregunta del usuario recién ahí interactúo de nuevo
[11:41] con un modelo de lenguaje y le entrego una respuesta al usuario a ese proceso se le llama generación Entonces
[11:48] tenemos estas tres macroprocesos y todo esto converge en un flujo que vamos a entrar a ver más en el
[11:55] demo técnico Pero antes de pasar a el demo técnico me gustaría también contarles de que hay algo que se
[12:02] le llama l Chain y que no vamos a ver en detalle en este video Quizás lo vamos a ver
[12:06] en un siguiente si que les interesa Así que si les interesa efectivamente saber un poco más de l Chain
[12:11] déjenlo en los comentarios pero lo que deben saber por ahora de l Chain porque lo vamos a ocupar en
[12:17] el código es que es un framework como otros frameworks o como otras librerías que uno ocupa para poder simplificar
[12:23] procesos a la hora de programar pero que este que qué es lo que hace Cuál es la gracia es
[12:29] que tiene ciertas herramientas que son muy utilizadas en el universo del rac por ejemplo hablamos de estos pedacitos l
[12:40] Chain tiene procesos automáticos y librerías para poder hacer esa división en pedacitos Y esa es la que vamos a
[12:47] ocupar en este demot técnico pero también tiene una serie de otros elementos que son bastante interesantes y que hacen
[12:56] la implementación del rack mucho más rápida Como por ejemplo conexión con modelos de lenguaje como openi Google gemini etcétera
[13:05] conexión con base de datos vectoriales algunas que corren en memoria y otras que corren en en un sitio web
[13:12] como podría ser chroma pinecone wiate entre otras también tiene maneras de procesar documentos y textos de extraer información por
[13:21] ejemplo de un Word y traerla algo que se pueda ser leído por un modelo de lenguaje entre otras cosas
[13:28] que efectivamente permiten que el proceso de generar un rack o de construir un agente que es algo un poquito
[13:35] más complejo diría yo se haga más fácil a través de tener una librería que de alguna manera ya programó
[13:43] alguno de los procesos repetitivos de este proceso Entonces ahora sí que vamos a entrar a la demo una cosa
[13:49] importante que debes saber es que esta implementación de rack recibe el nombre de naive rag que es como un
[13:56] rag que es s simple que es es la versión más básica Por ende después de haber analizado el demo
[14:03] y antes de finalizar el video entraremos en algunos detalles de cómo esto se puede complejizar pero ahora que estamos
[14:09] aprendiendo los fundamentos de esto es importante aprender esta versión el archivo que voy a ocupar ahora quedará referenciado en
[14:17] la descripción del video y es un archivo que está en Google Cab si es que no han utilizado alguna
[14:24] vez Google Cab pueden ver este video que ahora está en pantalla y que está referenciado también en la descripción
[14:32] de este video en ese video les cuento de qué se trata esa plataforma y cómo la pueden ocupar pero
[14:38] en este caso ahora que vamos a entrar a verlo en este ejemplo quédense con la idea de que es
[14:44] un Notebook Entonces si es que han ocupado antes lo que es Jupiter Notebook lo van a encontrar muy familiar
[14:51] y si es que no conocen esa aplicación quédense con la idea de que es una manera de poder correr
[14:56] python y r pero en este caso vamos a ocupar pyth en la nube básicamente vamos a ocupar los recursos
[15:02] de Google para disponibilizar una máquina que nos permita ver los aspectos técnicos de esto sin embargo la mayoría de
[15:09] este código va a ser compatible por si tú lo quieres implementar en tu propio computador Así que vamos allá
[15:15] ahora vamos a ver el documento que está abierto y en este documento también he colocado algunas cosas que les
[15:20] pueden servir el flujo algunas definiciones etcétera Pero vamos a pasar rápidamente a lo primero y les voy a explicar
[15:28] qué es lo que está pasando aquí ahora voy a correr esta celda y esta otra celda y lo que
[15:34] simplemente están haciendo es preparar algunas cosas del ambiente están instalando librerías y están configurando algunas cosas que vamos a
[15:42] utilizar la primera línea simplemente instala l Chain y un módulo de Open Ai que vamos a ocupar actualmente l
[15:50] Chain tiene también módulos para otros modelos Así que si ustedes les gusta Google gemini o Cloud pueden revisar la
[15:57] documentación de l Chain y para ver cómo se llaman esas otras librerías Pero hay para casi todos los modelos
[16:03] que se les puedan ocurrir inclusive si lo quieren correr en local en su propio computador En esta segunda Zelda
[16:10] van a poder ver que hay ciertas configuraciones vamos a importar algunas librerías que vamos a ocupar y vamos a
[16:16] importar ciertos módulos de l Chain que vamos a utilizar también además estamos configurando nuestra Api de Open Ai que
[16:24] en el caso que ustedes no ocupen Google Cab este plazo puede ser levemente distinto porque yo tengo esto en
[16:31] algo que acá se llama secrets donde tengo efectivamente la Api de Open Ai que estoy ocupando en mi cuenta
[16:39] si ustedes no tienen una apik de Open Ai también les dejaré referenciado en la descripción de este video un
[16:46] artículo donde comentamos Cómo se puede obtener una dicho eso lo último que hago en esta Zelda es configurar un
[16:55] modelo de lenguaje utilizando un método de la Chain que simplemente le digo que ahora voy a ocupar un modelo
[17:02] de Open Ai que es gpt 4 o mini vamos a ocupar el mini porque es un modelo rápido que
[17:10] nos permite hacer estos ejercicios sin mayor complejidad sin embargo para tu caso de uso puede ser más necesario o
[17:17] menos necesario un modelo sobre otro Entonces lo primero que vamos a hacer es Llamar acá y esto obviamente cuando
[17:25] veamos el video de lch vamos a entenderlo un poquito más en profundidad para poder llamar los distintos modelos de
[17:31] lenguaje ocupa esto que es punto invoke entonces voy a correr esta celda y lo que va a hacer es
[17:37] llamar el modelo que ya configuré en El Paso anterior y me va a decir eh la respuesta que me
[17:43] diría el modelo lenguaje en este caso me dice que el modelo lenguaje me respondió que no tiene información específica
[17:49] sobre dónde se va a transmitir W raw en 2025 que es este programa de lucha libre ya que esto
[17:56] podría depender de bla bla bla bla básicamente no sabe Okay entonces cómo vamos a reparar esto utilizando rack para
[18:04] que sí sepa esta actualización y sí pueda tener algún conocimiento que no tiene eso es lo que vamos a
[18:10] ver en los siguientes pasos lo primero del rack es toda esta parte de indexing eso quiere decir que tenemos
[18:17] que cargarle información en la base de datos Entonces le voy a cargar la noticia que habla del acontecimiento que
[18:25] está pasando ahora de que finalmente la W se va a mover a Netflix Okay entonces le estoy cargando una
[18:32] noticia que la tengo escrita en formato texto pero también para no hacerle tan fácil la vida y que esto
[18:38] sea lo único que sabe y que Por ende el ejercicio sea muy tramposo Por decirlo deuna manera puesto que
[18:44] solamente esa información la que tiene cargada le he añadido una noticia super distinta para que también pueda discriminar entre
[18:53] estas dos piezas de información y cuál es Es que hay un millonario en Chile que dirigía muchas empresas Y
[18:59] que ahora ya no está dirigiendo esas empresas una noticia que también es noticia y por es relevante cargarla en
[19:06] un rack que pueda administrar actualidad pero que no es necesariamente relacionado a este acontecimiento y nuestro código deberá ser
[19:15] suficientemente bueno para poder discriminar de que este tipo de cosas no le sirven ocupando la librería de l Chain
[19:22] generamos documentos en base a esto l Chain maneja todo esto de información en base a algo que se le
[19:28] llaman documentos y hemos generado una lista con dos documentos el de la noticia uno y el de la noticia
[19:36] dos dado que ya tenemos nuestros documentos ahora lo que hay que hacer es dividirlo en partes Entonces vamos a
[19:44] ocupar algo que se le llama recursive character text splitter que básicamente lo que hace es tratar de dividir la
[19:53] información en párrafos es decir que no corta un documento justo en la mitad de una palabra o justo mitad
[20:00] de una frase va a tratar de privilegiar dividir un documento cuando haya un punto seguido este es uno de
[20:05] los métodos más utilizados para dividir documentos Pero vamos a hablar un poco más adelante de esto no siempre es
[20:13] necesariamente el mejor pero en este caso es algo s sencillo que se puede implementar rápidamente y que la verdad
[20:20] es que para la inmensa mayoría de los casos es suficiente entonces vamos a correr este proceso y vamos a
[20:27] ver que el resultado ante es que agarró estos documentos y los dividió en partes y acá yo lo que
[20:34] hice fue imprimir esas partes para que las podamos visualizar es decir esa noticia que antes era un gran bloque
[20:41] de texto fue dividida en en este caso la noticia de la do fue dividida en seis partes distintas y
[20:49] la otra fue dividida en en ocho partes distintas Por qué de nuevo como hablamos antes Esta división ayuda a
[20:59] aumentar la relevancia de la información y filtrar exactamente lo que necesitamos y dejar de lado aquellas cosas que no
[21:06] necesitamos y ahora ya tengo en el fondo mis documentos cargados y estos están divididos en partes la siguiente parte
[21:14] de este ejercicio es volver esto en números es decir vamos a utilizar un modelo de embeddings hay varios modelos
[21:23] de embeddings en el mercado Y ahora yo voy a ocupar el de Open Ai sin embargo podrías ocupar alguno
[21:29] distinto Por ejemplo si estás haciendo alguna aplicación en local hay algunos modelos que te permiten generar esos embeddings de
[21:36] manera local un detalle importante es que el modelo de embeddings se ocupa para comparar números con números por lo
[21:44] tanto puede ser distinto al del modelo de lenguaje todo el proceso de retrieval se basa en los embeddings y
[21:53] puede estar separado en su tecnología al de generación en este caso en específico vamos a ocupar Open Ai para
[22:00] ambos procesos sin embargo puede ser que tú tengas por ejemplo la Api de Open Ai para embeddings y la
[22:08] Api de Google para traer la información Eso ya va a depender un poco de lo que te funcione mejor
[22:13] para tu proyecto entonces para poder avanzar con el ejercicio vamos a tomar estos chunks y los vamos a guardar
[22:22] en lo que yo voy a llamarle como una base de datos temporal en el fondo lo que estoy haciendo
[22:27] es ocupar algo que se llama un dataframe de pandas simplemente para guardarlo acá como si fuera un Excel Esto
[22:34] me sirve ahora para este ejercicio simple puesto que estamos manipulando una cantidad menor de información estamos manejando si se
[22:42] dan cuenta 14 chunks ya no estamos manejando muchísimos chunks en el caso que estuvieramos manejando muchísima información o varios
[22:52] miles de documentos Eh Esto obviamente no va a resistir y en ese caso es mejor hacerlo con una base
[22:58] de datos vectorial si le interesa que abordemos mucho más el tema de base de datos vectorial también podrían dejarnos
[23:05] un comentario porque así sabemos si les interesan estos temas o no pero por ahora vamos a avanzar simplemente creando
[23:12] una tabla como si fuera una tabla Excel ya es un en un dataframe ahora con estos datos que son
[23:19] 14 desde el 0 al 13 vamos a convertirlos en embeddings y para eso vamos a crear una pequeña función
[23:27] que simplemente va a procesarlos ocupando la librería de l Chain en particular su implementación de la Api de Open
[23:37] Ai para embeddings y ahora para cada una de esas filas vamos a calcular los embeddings y va a lucir
[23:44] algo así y ahora que estamos viendo esto en pantalla vamos a reflexionar un poco sobre lo que pasó qué
[23:50] es lo que pasó cada una de estas líneas por ejemplo Netflix en la nueva casa de la dolee fue
[23:56] transformada a una serie de números esto que está acá es una lista de números y cada uno de esos
[24:03] números van entre el -1 y el 1 y tratan de alguna manera reflejar lo que significa esa oración si
[24:10] se dan cuenta esto rápidamente se corta dándote luz de que al menos vemos dos números aquí alcanzamos a ver
[24:17] el comienzo de un tercero pero de que hay muchísimos más números vamos a correr este pequeña línea de código
[24:24] simplemente para saber cuántos hay hay 153 seis números o dimensiones que son básicamente los números que utiliza Este modelo
[24:34] para poder tratar de alguna manera reflejar el significado de esa oración es una buena aproximación un poco de lo
[24:43] que pasa en la vida real donde hay algo que de cierta manera siempre la podemos describir de múltiples puntos
[24:48] por ejemplo tomen una persona esa persona se puede describir por ejemplo por donde nació por el género por su
[24:56] gusto por lo que hace por Cómo luce etcétera etcétera etcétera O sea hay múltiples maneras en las cuales Yo
[25:04] podría caracterizar una persona así como ahora en estos modelos de embeddings hay múltiples dimensiones que pueden tratar de alguna
[25:13] manera definir lo que significa algo Este es un modelo en particular que tiene 100 dimensiones Pero hay modelos que
[25:21] tienen menos y hay modelos que tienen más pero por ahora Este modelo está bastante bien y es uno de
[25:27] los que más se utiliza en el en el mercado entonces recapitulando qué es lo que hemos hecho hasta el
[25:32] momento técnicamente hablando tomamos los documentos los dividimos en pedacitos y a cada uno de esos pedacitos les obtuvimos números
[25:41] que están correlacionados a su significado de alguna manera reflejan de qué se trata y ahora para poder encontrar Cuál
[25:48] de esos pedacitos o chunks son los más relevantes vamos a crear una función y esa función lo que va
[25:54] a hacer es tomar cualquier pregunta que me haga el usuario la va a transformar a números y luego va
[26:03] a calcular algo que se le llama el producto punto que es una manera de calcular la distancia entre dos
[26:09] vectores Cuál es el supuesto aquí de que mientras más se parezcan dos cosas es más relevante para la pregunta
[26:19] es decir por ejemplo si yo te pregunto dónde está el perro de Isabel y uno de los documentos es
[26:26] el perro de Isabel está en el parque que al parecerse mucho la pregunta con la respuesta yo tengo que
[26:33] de alguna manera asumir que esa pieza de información es más relevante ese es un supuesto clave que hacen las
[26:39] distintas bases de datos vectoriales y distintas implementaciones del proceso de retrieval en el caso de este ejemplo de noticias
[26:46] si el usuario hace una pregunta y la comparamos con las cosas que están diciendo los distintos medios Mientras más
[26:52] similar sea la pregunta a lo que ha dicho un medio vamos a considerar que eso es más relevante esa
[26:58] cercanía cuando tienes algo tan complejo como 15 dimensiones se tiene que medir de alguna manera matemática y es aquí
[27:06] es donde ocupamos esta fórmula que es el producto punto si ustedes utilizan para sus proyectos base de datos vectoriales
[27:13] muchas base de datos vectoriales ya tienen esto calculado es decir tienen una fórmula para poder calcular la cercanía de
[27:23] la pregunta Respecto a los documentos que están en su base de datos Entonces esta Ahora la vamos a ocupar
[27:30] con esta pregunta dónde se va a transmitir W r en 2025 y vamos a pedirle en el fondo de
[27:37] que compare aquellos números que tiene la query con los números de la base de datos Y ha encontrado de
[27:46] que lo que hay en la base de datos que es más relevante para esa pregunta es Netflix anunciado un
[27:52] histórico acuerdo con la empresa de entretenimiento deportivo WWE a partir de 2025 la n roja será el lugar exclusivo
[28:01] del popular programa Mountain raw en Estados Unidos Canadá Reino Unido y latinoamérica esa información no fue generada por un
[28:09] modelo de lenguaje venía de la noticia de la noticia que le cargamos al inicio y es una parte de
[28:14] la noticia es decir es solamente un chunk en implementaciones reales muchas veces se obtienen más de un chunk para
[28:21] poder contestar una pregunta pero en este caso como es una implementación sencilla avanzaremos solamente con uno en la siguiente
[28:30] parte del código lo que vamos a hacer es pasarle este resultado a un modelo de lenguaje y vamos a
[28:36] llamar a Open Ai en este caso y vamos a darle el siguiente prompt eres un Bot servicial e informativo
[28:43] que responde preguntas utilizando el texto de referencia incluido a continuación asegúrate de responder en una oración completa siendo exhaustivo
[28:51] y proporcionando toda la información de fondo relevante si se dan cuenta al tener este prompt Eso quiere decir que
[28:57] si nosotros queremos modificar este proceso se puede hacer por ejemplo si tú quisieras que la respuesta sea en un
[29:02] tono más amable lo podrías incluir acá si quieres que sea en un idioma particular también lo puedes incluir acá
[29:09] pero si se dan cuenta este prompt lo único que hace es decirle al modelo de lenguaje Ey esta fue
[29:16] la pregunta que me entregó el usuario Este es el texto que está en tu bibliografía o texto de referencia
[29:21] trata de responder en base a estas dos piezas de información lo que te preguntó el usuario lo cual Debería
[29:27] ser bastante natural para cualquier modelo y ahora vamos a pedirle a este modelo lenguaje que haga esto con este
[29:37] prompt la query que me pasó el usuario y el texto de referencia y me dice a partir de 2025
[29:45] WWE raw se transmitirá exclusivamente en Netflix en Estados Unidos Canadá Reino Unido y latinoamérica lo que significa de los
[29:53] que los fanáticos de la lucha libre como yo podrán disfrutar de su programa favorito a través de esta plataforma
[29:59] de streaming y como se podrán dar cuenta Esto sí A diferencia de la respuesta inicial de chat gpt Esto
[30:05] sí es correcto Y sí se basa en la realidad se basa en esta noticia que le hemos cargado cosa
[30:12] importante que pasó también es que se dieron cuenta en ningún momento más volvió a salir la otra noticia Que
[30:17] le cargué puesto que todo este proceso que acabamos de hacer le permitió discriminar que eso efectivamente no era tan
[30:25] relevante Entonces vamos a recapitular un poco lo que ha pasado en nuestra demo qué es lo que hicimos tomamos
[30:32] esta noticia que venía desde un medio y la dividimos en pedacitos utilizando una técnica que estaba en la librería
[30:39] l Chain que se llama recursive text splitter cada uno de estos pedacitos fue transformado a números como los que
[30:46] ven en pantalla que son por cada uno de estos párrafos más de 15 números luego tomamos una pregunta del
[30:55] usuario y esa pregunta también bien la transformamos a números y ahora que tenemos estos dos tipos de números el
[31:02] de la pregunta y el de la información los comparamos y al compararlos pasó un proceso matemático que no vimos
[31:09] en detalle pero que pasó y Calcula estos números que están acá a cada uno de estos vectores y elige
[31:17] el que tenga el número más alto en este caso es el párrafo que ahora está señalizado en pantalla al
[31:24] ver qué decía ese párrafo vemos que decía la la pieza de información que efectivamente respondía a la pregunta y
[31:31] para finalizar y proceder al paso de generación lo que hicimos fue pasarlo estos modelos de lenguaje que en este
[31:37] caso fue gpt 4 o Mini y le dimos un prompt el cual incluía una pregunta que era la pregunta
[31:44] que me había pasado el usuario y un texto de referencia que fue la pieza más relevante que encontró de
[31:49] toda la noticia y con esto si se dan cuenta y ustedes Lo leen en voz alta podrían efectivamente contestar
[31:56] la pregunta ahora en la slide tengo una de las posibles respuestas pero como saben los modelos de lenguaje podrían
[32:03] generar distintas respuestas cada vez que lo corro Entonces en este caso la respuesta que está ahora en pantalla es
[32:09] levemente distinta a la que vimos en la implementación del código que de hecho si simplemente lo corro de nuevo
[32:16] por la naturaleza generativa es levemente distinta nuevamente Entonces como pudieron ver en este ejemplo lo que hicimos fue tomar
[32:25] documentos e indexar luego con esos documentos que estaban divididos en partes y cargados como vectores numéricos lo que hicimos
[32:35] fue compararlos con una pregunta del usuario que se le llama retrieval y trajimos la información que era más parecida
[32:43] a esa pregunta del usuario luego con esa información que trajimos más la pregunta del usuario se lo pasamos a
[32:49] un modelo de lenguaje para que respondiera nuestra pregunta lo cual se le llama generación este proceso que vimos ahora
[32:57] que es la versión más sencilla del rack que se llama naive rack es fundamental para poder entender esto pero
[33:03] la disciplina puede llegar a mucho más se puede sofisticar más y con ello poder aumentar la precisión de Esta
[33:09] técnica a través de poder aumentar la precisión o aumentar la sofisticación en distintas partes del proceso que hablaremos a
[33:17] continuación pero por mientras si es que ustedes están trabajando en una empresa que les interesa implementar esto Les comento
[33:24] que nosotros tenemos un curso corporativo que se llama fundamento to de las aplicaciones de Inteligencia artificial con modelos de
[33:30] lenguaje donde en una de las clases explicamos precisamente esto y luego en otr clase explicamos otras técnicas para poder
[33:38] personalizar tus modelos de lenguaje si estás en una empresa y estás interesado en adquirir alguno de estos cursos puedes
[33:44] ir al link que está ahora en pantalla pero que también estará en la descripción del video dicho eso quería
[33:50] detenerme un poco a ver este modelo donde está el flujo completo de lo que es rack y contarles un
[33:56] poco de cómo esto se va sofisticando en distintos casos por ejemplo hablamos del proceso de indexación que c está
[34:05] ahora acá arriba ya donde yo tomo los documentos los rompo en pedacitos y los meto en una base de
[34:12] datos vectorial transformándolos en números eso también puede ser sofisticado de muchas maneras una de las cosas que pasa es
[34:21] que uno puede tener distintas técnicas para poder romper esto en pedacitos ahora Vimos una que se utilizaba important que
[34:28] es el recursive tex splitter pero también existen otras técnicas para poder entender bien Cómo es la mejor manera de
[34:36] poder dividir la información que estoy cargando también la transformación a números nosotros ocupamos el modelo Open Ai que es
[34:42] uno de los que más utilizan Pero en algunas ocasiones hay modelos de eding especializados para ciertas industrias que pueden
[34:51] hacer incluso que la información Se entienda de mejor manera porque por ejemplo si administras solamente documentos legales y solamente
[34:59] documentos legales probablemente ocupar un modelo de embeddings especializado en eso puede capturar más los significados específicos de ciertas palabras
[35:08] Asimismo hay para otras industrias y por ejemplo para programación donde ciertas palabras en código Como por ejemplo el Tab
[35:17] ya que es una palabra invisible Por decirlo así es un carácter invisible puede tener un significado que no tiene
[35:23] en la generalidad de un modelo tan amplio como el de open luego obviamente esta información se carga en base
[35:30] de datos vectoriales Y hay muchos proveedores actualmente los cuales tienen cada uno sus PR y sus contras otra cosa
[35:38] que también se ocupa para poder mejorar es cuando vamos a ver el proceso de retrieval que parte con la
[35:44] pregunta del usuario esa pregunta del usuario se transforma en beddings y ya hablamos las posibles mejoras de los modelos
[35:51] de embeddings pero también esa pregunta del usuario depende de que el usuario exactamente te pregunte cómo está la información
[35:59] en tu base de datos por lo tanto hay distintas técnicas en las cuales uno por ejemplo interviene esa pregunta
[36:07] para poder hacerla más compatible o le quita información que pueda ser irrelevante para tu base de datos de manera
[36:14] tal de poder mejorar la probabilidad de que la pregunta o el requerimiento inicial sea algo que sea cercano con
[36:23] la base de datos que está cargada luego esa información se obtiene más de una pieza de información en nuestro
[36:30] ejemplo ocupamos una pero no es raro ver que en la vida real se ocupen 10 o más pero qué
[36:36] se hace con eso en la vida real Generalmente actualmente las empresas más sofisticadas toman esas 10 20 piezas de
[36:43] información y ocupan un ranker externo que es otro modelo que Define cuál de estas 20 piezas de información podría
[36:52] ser más relevante para la pregunta añadiéndole más complejidad más latencia al proceso pero también más precisión y finalmente que
[37:00] esto también lo conversamos en nuestro ejemplo es que el prompt que utiliza el llm en este caso es bastante
[37:07] estándar pero también podría ser sofisticado