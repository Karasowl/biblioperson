This video is sponsored by Hedcon. One of the... Best open source platform for logging, monitoring, and debugging your large-numeral model applications. So large-language remodel. At default. Don't have real time. Data about what's going on in the world. I also don't have access to a comedy. Private data. But for a majority of use case. You kinda want to bring those external resources to the client model and let it work. Beyond just the training data. And traditionally, the most popular method has been RAC. Which represent for retrieval augmented generation. That basically means. You're a creative, b**** big and large base. That contain everything you want the large language model to know. And when there's a user question or user query, where do we search in your database to retrieve the most relevant information? Which is a portion of the whole database. And then send bozos retrieve data. As well as user question. Into the context window of large language model. But recently, there's one term that became increasingly popular. Called a keg. And CAGR Rep. for CAge Augmented generation. This represents for an alternative. Instead of doing a search and put only portion of the random data, as context. The key difference of CAC is that it preloads the whole database. Into Model's Contacts window. All of it. This method has totally changed. How do I build agents and large language model applications? That's why today I want to take you through. What exactly does this method mean, when to use rack and CAG? And most importantly, let's try to build an MCP using this KEG method. To pass any external talks and retrieve the most remnant code example. From the whole Doc website. Without further ado, let's firstly dive a bit deeper into CAT. This approach of preload all the data into context window. Didn't really make sense 24 months ago. Because back then, state of art large-language model, context window is only 4,000 tokens. So you can't really feed anything more than a small PDF. But this number has dramatically increased for the past 24 months. For now. Most of the flagship model support around 100 to 200,000. Tokens Context Window and Google's Gemini model is support even up to 2 million tokens. That is 488 times. More than what we had 24 months ago. It's roughly translated to around 1.5 million words. And put this into context for a normal novel. It has about 90,000 words, and even book like War and Peace only have 587,000 words. So it was Gemini 2.0 Pro. You can almost feel the street of water. Piece into the context window. And if you're building, shadow is PDF type of use case. For a simple research PDF paper like this, it only has around 5,000 tokens. That's why this app. Approach of preload the whole database into the context window. Started making more and more sense. But still. There are pros and cons of each approach. I will quickly demonstrate the difference. When and how to use which one. So for all the rack pipeline, it starts with a data preparation. Process like this to turn your data into a vector database set contains small chunks of data. And then, when there's a new user question, we'll also turn users questions. Into embedding so that we can use this embedding to search inside the vector database and return a list of chunks. That is most relevant to what the user is asking. And this is typically what we call top key results. And we will send both those chunks as well as user question. To the large-language model prompt and get it to generate accurate answer. But there are few challenges about building such rack system. On one hand, it is a bit complex to set up. It does require some setup to turn your knowledge into a vector database. In this pipeline, it also introduced quite a bit of latencies. For example, there will be time spent on actually turning user question into embedding. And retrieve the most relevant chunks from the vector database. If your use case is, user can upload a few PDF and start chatting with PDF. It will need to wait for turning all the PDF docs into a vector database. But most challenging problem is that retrieval accuracy. How can you making sure the chunks you return here contain all the information. The large learning model will need to answer the user's question. For example, for the first step of splitting your large doc into small chunks, if you just do, a very basic and simple implementation of each chunk is like 500 characters. You might have situation like This. API doc website. But the actual complete code example will be breaking down into different chunks. So, when large large model receive just one part of chunks, it won't have the full picture of what the code example actually is. On the other side, if your knowledge base contains financial report across different years and user, just ask a question, what's the latest revenue? It might return information of revenue from past years as well. Which can also confuse the large-range model. And sometimes, users' questions can also be quite complex if you just simply turn the question into embedding. It might not return all the information that is needed. So there's a whole bunch of techniques that people are using. To resolve those problems, like metadata filtering, query transformation to break down the question into multiple different small queries, re-ranking and bunch of other things. But Keg on the other side, because we're feeding the whole documents into the large-language model. We don't worry about whether we retrieve the right information. To feed the large range model. Because it must be somewhere. And because we don't need to do all those vectorizing retrieval implementation, the code implementation is going to be extremely simple. Here is a quick example of how can we build a chat with PDF use case with a Gemini model. All you need is just these 10 lines of code. Passing on the doc URL. As well as a user message. And we just work out of box. But you might have concern about if we feed a big document. To the large range model. Where you'll be able to retrieve information. Accurately as we want. And how about the cost and speed? Since every time, we are feeling such a big prompt. To the model. Well, those three aspects are exactly three parts. That has changed dramatically for the past few months, especially with Google's Gemini model. So for the first one, the retrieval accuracy, typically, we're tasked about something called the needle in the haystack. Which basically means you are feeding a huge amount of text to large language. Model and ask to retrieve certain information that exists in specific parts of this text. And see if it retrieves things properly. Now this research from Google, even with Gemini 1.5 Pro, it already... Demonstrate near perfect recall of specific information in a vast context. Of up to 1 million tokens. And with the recently release of Gem Knight, 2.0, according to this test from Vectara. They are comparing hallucination ray of top large lender models. Google Gemini 2.0 is achieving an extraordinary result. And we can expect this number to keep getting better with new models came out. On the other side, how about the cost and speed? These two things also changed dramatically thanks to Gemini. So, even though some models like GPT-4O, the cost can be a bit higher with $2.5. Per million import token. But with a model like Gemini 2.0 flash. The input price for million token is only $0.10. Which is... Almost 96% cheaper. And with this price point, you can really feed a whole bunch of data to the model context window. Without worrying about the build your get. Meanwhile, the speed also improved a lot. In some of the tests I did and logged on Helicon, I can feed the whole developer. Docs from Firecrow. To Gemini 2.0. And for that amount of context window. The cost is only $0.006, and you get results within 3.4 seconds. Which is very impressive. And if you haven't heard about Helicon before, Helicon is one of the best open source platform for logging, monitoring, and debugging large-range model application in productions. It gives us ability to see exactly how people are interacting with our large-range model applications, track cost, arrow, and latencies. So we can optimize for performance. And I can also do a bunch of very advanced and interesting things, like auto caching. The response. If the problem is same, to save cost and improve speed, set up custom properties so I can segment different type of requests. And the best part is that is extremely easy to set up, all you need to do just adding this part of HTTP options to your existing. Jim Nykov. Then, all the requests will automatically be logged on the platform. I can see exactly what type of request and prompt users are actually giving our applications. Track which part is creating most of the latency. So, if you're building and launching your large-language model applications, I highly recommend you set up Headacombs. To capture all the user requests. So that you can optimize for the cost speed aim permit. And now let's get back to building your Cag pipeline. So, in general, my current go-to approach is I would try to avoid a rack and just feed all the context. To the large-range model and see what kind of results do I get? And based on that, I can start doing some optimizations. Barak as a message to makes sense. And we can often use that. To handle really big data set. Because, even though the model's context window keep getting... Bigger and bigger. So as company, data is also ever growing. So if your use case required dealing with database that is really diverse, Really, really big. Their rack is still pretty good. approach. Even though. There are also approaches you can take with CAC as well. Assuming your database is very, very big. That can't be fit into just one large, large model context window. What you can do is that you can just use traditional search. To search for which data and most likely contain the information. That is related to the query. 比上 either the metadata, file name, and other stuff. And only feed those filtered data. To larger than your model. Or you can even do something more fancier. For this data, you can do parallel large-range model call. And in the end, have another large range model called to summarize the result together, best information out of all those data points. So what I want to do is I will take you through a quick example of. How do we apply those keg methods to some of the really... Big database use case. And the example I want to show you is, how can you build MCP to enable Cursor or Windsurf to deal with external docs much better? Where this MCP can receive the doc URL. And return only the most relevant code example. Based on this talk. So first, I will install all the packages that we need, import all of them. And also set up a environment keys. So we'll be using Gemini 2.0 Flash model. So I will get an API key from Google AI studio. And we'll also use FireCrow. As a service to scripting, the API docs. As well as headcount. To log and track the latency and cost of every single large-range motor call. Before we dive into that, I want to give you a quick example of how. How can you build this basic shadowless PDF use case with Gemini 2.0? As I mentioned before, it is extremely, extremely simple. All we need to do is just create a client. Of Gemini model. And here, I swapped HTTP options to Hadacons URL. 所以 all those large range motor call will be automatically logged there. And then I would download this research paper here. Feed both this PDF file. As well as a prompt here. To the model. And you can see here it returned the results, which looks correct. Based on what we see. On the paper here. And if we go to Hadacom, you can see that this that we just answered. Cost only $0.0001. And we got a result within three seconds. Which is quite impressive. Next, we're going to start building this. External Doc MCP pipeline. The first thing I will do is I want to give a quick. test. So I record Gemini directly. With this prompt, help me generate request to script this website using FilePro. If I run this one. You can see the API endpoint in return here. Is incorrect, and that's because Gemini didn't really have the context and knowledge about the latest documentations of Ficor. So this is a great example we can use. Test. Quickly build a proof concept here. Firstly, I will use FileCrop's map, URL, and point. Which will return all the separate pages under a certain domain. And if I check the length, it returned 153 pages in total. So, of course, we can try to script. All the pages here and send to Large Line remodel. As context. But it will create quite a bit of latency. Not just from the model generation. But also scraping takes quite a long time. So what I plan to do... Thank you is that I will use Gemini. To filter out. Only URL. That is relevant to the API reference. So I only need to script. Rather than the page. And from this point, you can see a future down. To only 27 pages. That is relevant. Still, substantial amount of contacts, but much smaller than 153. Then I can pass on all these 27 links. To the file code. Batch script URL. End point. And download the markdown of every single page. So this process does take a while. But what we will do eventually is save the stocks locally. So the next time we're going to script again. It'll be much faster. Cool. So you can see that we get all the markdown content for all the pages. Which is quite a big context. I'm going to feed all the markdowns up. Buff along with the theme prompt that we sent to Gymnabi 4. And let it generate results. So with this one, you can see it get a result. properly. It pick up the right end point. As well as a request body format, and if I go to Helicon, you can see for this request where we download the whole API doc website. Okay. Across 27 page. It costs around $10, $0.006. And get a result within two seconds, which is really, really fast. So this is how... Simple and easy it is now. To give large range models knowledge. And Gemini also have this context cation feature. Which means for those large contacts, we can actually create a cache. Then the next time when user try to query the same doc, we can just pass that cache to the model class. So it will be even faster in terms of generation. But unfortunately, at the moment, this context cation functionality didn't support Gemini 2.0 model. So I will leave this part later when it is available. And I also turn this pipeline into an MCP server. Where it has this function called Retrieve API doc. Users can just give the request and share the doc URL. And this MCP will handle all the work in terms of retrieving the most relevant code example. From the dock itself. So my cursor and wing server can work with external docs much better. And there are also optimizations I did. Where it will automatically save all the script markdown file locally, so we don't need to script again and again. If you are interested in using this MCP, I have put MCP server repo in the AI Builder Club Commodian building. With the step-by-step process of how to set up. As well as the notebook example that I took you through in the video today. So if you're interested, you can click on the link below to join the community. I will continuously post interesting learnings and tips about building production-ready large large model application, as well as advanced AI coding tips. But more importantly, you have this community of top AI builders. Who are launching their own AI sets. So you can come and ask any question you have. Both me and other community members can just come and share any insights we have. I hope you enjoy this video. Thank you and I see you next time.