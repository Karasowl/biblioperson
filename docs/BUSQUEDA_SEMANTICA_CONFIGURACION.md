# ConfiguraciÃ³n de BÃºsqueda SemÃ¡ntica en Biblioperson

## **Estado Actual de ImplementaciÃ³n** âœ…

Biblioperson ya tiene **bÃºsqueda semÃ¡ntica completamente implementada** con la siguiente infraestructura:

- âœ… **MeiliSearch** con soporte vectorial
- âœ… **SQLite** para almacenar embeddings
- âœ… **Scripts de procesamiento** automÃ¡tico de embeddings
- âœ… **API endpoint** `/api/search/semantic`
- âœ… **Frontend integrado** con bÃºsqueda hÃ­brida (literal + semÃ¡ntica)

## **Modelos de Embeddings Recomendados**

### **ğŸ¥‡ RECOMENDACIÃ“N PRINCIPAL: Para Contenido en EspaÃ±ol**

```yaml
Modelo: paraphrase-multilingual-MiniLM-L12-v2
Dimensiones: 384
Proveedor: sentence-transformers
Ventajas:
  - Optimizado para espaÃ±ol
  - Balance perfecto rendimiento/calidad
  - Soporte multilingÃ¼e
  - TamaÃ±o moderado (420MB)
```

### **ğŸ¥ˆ ALTERNATIVA ESPECIALIZADA (Ya implementado)**

```yaml
Modelo: hiiamsid/sentence_similarity_spanish_es
Dimensiones: 384
Proveedor: sentence-transformers
Ventajas:
  - EspecÃ­ficamente entrenado para espaÃ±ol
  - Excelente para similitud semÃ¡ntica
  - Ya configurado en el sistema
```

### **ğŸ¥‰ PARA MÃXIMA CALIDAD**

```yaml
Modelo: all-mpnet-base-v2
Dimensiones: 768
Proveedor: sentence-transformers
Ventajas:
  - Mejor calidad general
  - Soporta mÃºltiples idiomas
  - TamaÃ±o mayor (1.1GB)
```

## **ConfiguraciÃ³n de Dimensiones**

### **384 Dimensiones (RECOMENDADO)**
- âœ… **Balance Ã³ptimo** rendimiento/calidad
- âœ… **Memoria moderada**: ~1.5GB RAM para 100K segmentos
- âœ… **Velocidad alta**: bÃºsquedas en <100ms
- âœ… **Almacenamiento eficiente**: ~150MB por 100K segmentos

### **768 Dimensiones (Para mayor precisiÃ³n)**
- âš¡ **Mayor precisiÃ³n** semÃ¡ntica
- âš ï¸ **MÃ¡s memoria**: ~3GB RAM para 100K segmentos
- âš ï¸ **BÃºsquedas mÃ¡s lentas**: 200-300ms
- âš ï¸ **MÃ¡s almacenamiento**: ~300MB por 100K segmentos

### **1536 Dimensiones (Solo OpenAI)**
- ğŸ’° **Costoso**: $0.0001 por 1K tokens
- ğŸŒ **Requiere conexiÃ³n** a internet
- âš ï¸ **No recomendado** para uso local

## **ConfiguraciÃ³n Actual del Sistema**

### **Archivos de ConfiguraciÃ³n**

```bash
# Scripts de procesamiento
scripts/backend/procesar_semantica.py    # GeneraciÃ³n de embeddings
scripts/backend/indexar_meilisearch.py   # IndexaciÃ³n vectorial
scripts/process_and_import.py            # Pipeline completo

# API endpoints
app/src/app/api/search/route.ts          # BÃºsqueda hÃ­brida
scripts/api_conexion.py                  # Backend con /api/search/semantic
```

### **Modelos Soportados Actualmente**

```python
# Sentence Transformers (Local)
"all-MiniLM-L6-v2"                      # 384 dims - Por defecto
"all-mpnet-base-v2"                     # 768 dims - Mejor calidad
"hiiamsid/sentence_similarity_spanish_es" # 384 dims - EspaÃ±ol

# APIs Externas
"text-embedding-ada-002"                 # 1536 dims - OpenAI
"text-embedding-3-small"                 # 1536 dims - OpenAI v3
```

## **CÃ³mo Cambiar el Modelo de Embeddings**

### **1. Cambiar Modelo en el Script de Procesamiento**

```bash
# Cambiar a modelo optimizado para espaÃ±ol
python scripts/backend/procesar_semantica.py \
  --model="paraphrase-multilingual-MiniLM-L12-v2" \
  --provider="sentence-transformers" \
  --batch-size=32
```

### **2. Regenerar Embeddings para Contenido Existente**

```bash
# Regenerar todos los embeddings
python scripts/backend/procesar_semantica.py \
  --model="paraphrase-multilingual-MiniLM-L12-v2" \
  --regenerate-all
```

### **3. Actualizar ConfiguraciÃ³n de MeiliSearch**

```python
# En scripts/backend/indexar_meilisearch.py
embedder_settings = {
    "source": "userProvided",
    "dimensions": 384  # Cambiar segÃºn el modelo
}
```

## **ComparaciÃ³n de Rendimiento**

| Modelo | Dimensiones | TamaÃ±o | RAM (100K) | Velocidad | Calidad EspaÃ±ol |
|--------|-------------|--------|------------|-----------|-----------------|
| MiniLM-L6-v2 | 384 | 80MB | 1.5GB | âš¡âš¡âš¡ | â­â­â­ |
| **MiniLM-L12-v2** | **384** | **420MB** | **1.5GB** | **âš¡âš¡âš¡** | **â­â­â­â­** |
| spanish-similarity | 384 | 420MB | 1.5GB | âš¡âš¡âš¡ | â­â­â­â­â­ |
| mpnet-base-v2 | 768 | 1.1GB | 3GB | âš¡âš¡ | â­â­â­â­â­ |
| OpenAI ada-002 | 1536 | API | - | âš¡ | â­â­â­â­ |

## **ConfiguraciÃ³n Recomendada para ProducciÃ³n**

### **Para Bibliotecas PequeÃ±as (<50K segmentos)**

```yaml
Modelo: paraphrase-multilingual-MiniLM-L12-v2
Dimensiones: 384
Batch Size: 64
Hardware MÃ­nimo: 8GB RAM, 4 cores
```

### **Para Bibliotecas Medianas (50K-200K segmentos)**

```yaml
Modelo: paraphrase-multilingual-MiniLM-L12-v2
Dimensiones: 384
Batch Size: 32
Hardware Recomendado: 16GB RAM, 8 cores
```

### **Para Bibliotecas Grandes (>200K segmentos)**

```yaml
Modelo: all-mpnet-base-v2
Dimensiones: 768
Batch Size: 16
Hardware Recomendado: 32GB RAM, 16 cores
GPU: Opcional (acelera 3-5x)
```

## **BÃºsqueda HÃ­brida Implementada**

El sistema actual soporta **tres tipos de bÃºsqueda**:

### **1. BÃºsqueda Literal** ğŸ“
- Operadores: `AND`, `OR`, `NOT`, `"frases exactas"`
- Velocidad: âš¡âš¡âš¡ InstantÃ¡nea
- PrecisiÃ³n: â­â­â­â­â­ Para tÃ©rminos exactos

### **2. BÃºsqueda SemÃ¡ntica** ğŸ§ 
- Conceptos: "cristianismo perseguido" â†’ "seguidores de JesÃºs crucificados"
- Velocidad: âš¡âš¡ 100-300ms
- PrecisiÃ³n: â­â­â­â­ Para ideas y conceptos

### **3. BÃºsqueda HÃ­brida** ğŸ”„
- Combina ambos tipos
- Resultados ordenados por relevancia
- Deduplica automÃ¡ticamente

## **Ejemplos de Uso**

### **BÃºsqueda Literal**
```
Query: "amor eterno" AND (Neruda OR DarÃ­o)
Resultado: Textos que contienen exactamente "amor eterno" de esos autores
```

### **BÃºsqueda SemÃ¡ntica**
```
Query: cristianismo perseguido
Resultado: Segmentos sobre persecuciÃ³n religiosa, mÃ¡rtires, etc.
```

### **BÃºsqueda HÃ­brida**
```
Query: revoluciÃ³n social
Resultado: 
- Textos literales con "revoluciÃ³n social" (score: 1.0)
- Textos sobre cambios sociales, levantamientos (score: 0.8-0.95)
```

## **Indicadores de Rendimiento**

### **MÃ©tricas de Calidad**
- **PrecisiÃ³n@10**: >85% para bÃºsquedas semÃ¡nticas
- **Recall**: >90% para conceptos relacionados
- **Latencia**: <200ms para 100K segmentos

### **MÃ©tricas de Sistema**
- **Uso de RAM**: 1.5GB base + 15MB por 10K segmentos
- **Almacenamiento**: 1.5KB por segmento (384 dims)
- **Tiempo de indexaciÃ³n**: 1000 segmentos/minuto

## **Monitoreo y Mantenimiento**

### **Logs de BÃºsqueda**
```bash
# Ver logs de bÃºsqueda semÃ¡ntica
tail -f logs/search.log | grep "semantic"
```

### **EstadÃ­sticas de MeiliSearch**
```bash
# Ver estadÃ­sticas del Ã­ndice
curl http://localhost:7700/stats
```

### **RegeneraciÃ³n Incremental**
```bash
# Solo procesar nuevos contenidos
python scripts/backend/procesar_semantica.py --only-new
```

## **Troubleshooting ComÃºn**

### **Problema: BÃºsqueda semÃ¡ntica devuelve pocos resultados**
```bash
# Verificar embeddings generados
python -c "
import sqlite3
conn = sqlite3.connect('data.ms/documents.db')
result = conn.execute('SELECT COUNT(*) FROM embeddings').fetchone()
print(f'Embeddings en BD: {result[0]}')
"
```

### **Problema: BÃºsquedas lentas**
- âœ… Verificar dimensiones del modelo (usar 384 en lugar de 768)
- âœ… Reducir batch_size en bÃºsquedas
- âœ… Considerar usar GPU para aceleraciÃ³n

### **Problema: Resultados irrelevantes**
- âœ… Cambiar a modelo mÃ¡s especializado en espaÃ±ol
- âœ… Ajustar threshold de similitud (>0.7)
- âœ… Combinar con bÃºsqueda literal

## **PrÃ³ximas Mejoras Recomendadas**

1. **ğŸ¯ Fine-tuning**: Entrenar modelo especÃ­fico para literatura hispana
2. **ğŸ“Š Analytics**: Dashboard de mÃ©tricas de bÃºsqueda
3. **ğŸ”„ A/B Testing**: Comparar diferentes modelos automÃ¡ticamente
4. **ğŸŒ Multi-modal**: Soporte para imÃ¡genes y metadatos
5. **âš¡ GPU Acceleration**: Para bibliotecas muy grandes

---

**ConclusiÃ³n**: El sistema actual con `paraphrase-multilingual-MiniLM-L12-v2` (384 dimensiones) ofrece el mejor balance para contenido en espaÃ±ol, con excelente rendimiento y calidad semÃ¡ntica. 