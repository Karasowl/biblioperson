# Gu√≠a de Procesamiento de Datos - Biblioperson

## Objetivo

Esta gu√≠a detalla el proceso completo para convertir m√∫ltiples fuentes textuales (mensajes, escritos, poemas, canciones, etc.) en una base de datos depurada y estandarizada que alimente el backend de Biblioperson como base de un "gemelo digital" consultable por IA.

## üèóÔ∏è Arquitectura del Procesamiento

```
Archivos Originales ‚Üí Conversi√≥n ‚Üí Segmentaci√≥n ‚Üí Enriquecimiento ‚Üí NDJSON ‚Üí Base de Datos
     (M√∫ltiples         (Loaders)    (Reglas)      (Metadatos)     (Est√°ndar)   (SQLite +
      formatos)                                                                   Meilisearch)
```

## üìÅ Estructura de Directorios

### Estructura Recomendada

```
dataset/
‚îú‚îÄ‚îÄ raw_data/                    # Archivos originales organizados por autor
‚îÇ   ‚îú‚îÄ‚îÄ autor1/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ libro1.pdf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ articulo1.docx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ poemas/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ poema1.txt
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ poema2.md
‚îÇ   ‚îî‚îÄ‚îÄ autor2/
‚îÇ       ‚îú‚îÄ‚îÄ ensayo1.txt
‚îÇ       ‚îî‚îÄ‚îÄ redes_sociales/
‚îÇ           ‚îî‚îÄ‚îÄ telegram_export.ndjson
‚îú‚îÄ‚îÄ processed_data/              # Archivos NDJSON procesados
‚îÇ   ‚îú‚îÄ‚îÄ autor1_libro1.ndjson
‚îÇ   ‚îú‚îÄ‚îÄ autor1_articulo1.ndjson
‚îÇ   ‚îú‚îÄ‚îÄ autor1_poemas.ndjson
‚îÇ   ‚îî‚îÄ‚îÄ autor2_ensayo1.ndjson
‚îú‚îÄ‚îÄ output/                      # Resultados finales y unificados
‚îÇ   ‚îî‚îÄ‚îÄ unified/
‚îÇ       ‚îî‚îÄ‚îÄ dataset_final.ndjson
‚îî‚îÄ‚îÄ scripts/                     # Scripts de procesamiento
    ‚îî‚îÄ‚îÄ app_depuracion.py
```

### Organizaci√≥n por Autor

**Principio fundamental**: Cada autor tiene su propio directorio en `raw_data/`

```
raw_data/
‚îú‚îÄ‚îÄ garcia_marquez/              # Nombre del autor como directorio
‚îÇ   ‚îú‚îÄ‚îÄ cien_anos_soledad.pdf    # Libros principales
‚îÇ   ‚îú‚îÄ‚îÄ cronica_muerte.docx      # Otros trabajos
‚îÇ   ‚îî‚îÄ‚îÄ entrevistas/             # Subdirectorios por tipo
‚îÇ       ‚îî‚îÄ‚îÄ entrevista_1982.txt
‚îú‚îÄ‚îÄ borges/
‚îÇ   ‚îú‚îÄ‚îÄ ficciones.pdf
‚îÇ   ‚îú‚îÄ‚îÄ laberintos.txt
‚îÇ   ‚îî‚îÄ‚îÄ poemas/
‚îÇ       ‚îú‚îÄ‚îÄ fervor.md
‚îÇ       ‚îî‚îÄ‚îÄ luna_enfrente.txt
‚îî‚îÄ‚îÄ usuario_personal/            # Para contenido personal
    ‚îú‚îÄ‚îÄ escritos/
    ‚îÇ   ‚îú‚îÄ‚îÄ ensayo1.md
    ‚îÇ   ‚îî‚îÄ‚îÄ reflexiones.txt
    ‚îú‚îÄ‚îÄ poemas/
    ‚îÇ   ‚îî‚îÄ‚îÄ mis_poemas.txt
    ‚îî‚îÄ‚îÄ redes_sociales/
        ‚îú‚îÄ‚îÄ telegram_export.ndjson
        ‚îî‚îÄ‚îÄ twitter_backup.json
```

## üìã Formato Est√°ndar NDJSON

### Estructura de Campos

Cada l√≠nea del archivo NDJSON es un objeto JSON con estos campos:

```json
{
  "id_unico": "autor_documento_001",
  "texto_segmento": "Contenido textual del segmento",
  "autor_documento": "Nombre del Autor",
  "titulo_documento": "T√≠tulo del Documento",
  "orden_segmento_documento": 1,
  "tipo_segmento": "parrafo",
  "jerarquia_contextual": {"capitulo": "1", "seccion": "A"},
  "idioma_documento": "es",
  "fecha_publicacion_documento": "2023-12-01",
  "fuente_original": "escritos",
  "contexto_archivo": "raw_data/autor/libro.pdf",
  "hash_documento_original": "abc123def456"
}
```

### Campos Explicados

| Campo | Descripci√≥n | Ejemplo | Obligatorio |
|-------|-------------|---------|-------------|
| `id_unico` | Identificador √∫nico del segmento | `"garcia_marquez_cien_anos_001"` | ‚úÖ |
| `texto_segmento` | Contenido textual | `"Muchos a√±os despu√©s..."` | ‚úÖ |
| `autor_documento` | Nombre del autor | `"Gabriel Garc√≠a M√°rquez"` | ‚úÖ |
| `titulo_documento` | T√≠tulo del documento | `"Cien a√±os de soledad"` | ‚úÖ |
| `orden_segmento_documento` | Posici√≥n en el documento | `1, 2, 3...` | ‚úÖ |
| `tipo_segmento` | Tipo de contenido | `"parrafo", "titulo", "lista"` | ‚úÖ |
| `jerarquia_contextual` | Estructura del documento | `{"capitulo": "1"}` | ‚úÖ |
| `idioma_documento` | C√≥digo ISO del idioma | `"es", "en", "fr"` | ‚úÖ |
| `fecha_publicacion_documento` | Fecha de publicaci√≥n | `"1967-05-30"` | ‚ö†Ô∏è |
| `fuente_original` | Tipo de fuente | `"escritos", "poemas", "redes_sociales"` | ‚úÖ |
| `contexto_archivo` | Ruta del archivo original | `"raw_data/autor/archivo.pdf"` | ‚úÖ |
| `hash_documento_original` | Hash del documento | `"abc123def456"` | ‚úÖ |

### Ejemplos por Tipo de Contenido

#### Libro/Ensayo
```json
{"id_unico":"garcia_marquez_cien_anos_001","texto_segmento":"Muchos a√±os despu√©s, frente al pelot√≥n de fusilamiento, el coronel Aureliano Buend√≠a hab√≠a de recordar aquella tarde remota en que su padre lo llev√≥ a conocer el hielo.","autor_documento":"Gabriel Garc√≠a M√°rquez","titulo_documento":"Cien a√±os de soledad","orden_segmento_documento":1,"tipo_segmento":"parrafo","jerarquia_contextual":{"capitulo":"1"},"idioma_documento":"es","fecha_publicacion_documento":"1967-05-30","fuente_original":"escritos","contexto_archivo":"raw_data/garcia_marquez/cien_anos_soledad.pdf","hash_documento_original":"abc123def456"}
```

#### Poema
```json
{"id_unico":"borges_fervor_001","texto_segmento":"Las calles de Buenos Aires\nya son mi entra√±a.\nNo las √°vidas calles,\ninc√≥modas de turba y ajetreo,\nsino las calles desganadas del barrio,\ncasi invisibles de habituales,\nenternecidas de penumbra y de ocaso","autor_documento":"Jorge Luis Borges","titulo_documento":"Fervor de Buenos Aires","orden_segmento_documento":1,"tipo_segmento":"poema_completo","jerarquia_contextual":{"seccion":"Calles"},"idioma_documento":"es","fecha_publicacion_documento":"1923","fuente_original":"poemas","contexto_archivo":"raw_data/borges/fervor_buenos_aires.txt","hash_documento_original":"def456ghi789"}
```

#### Mensaje de Red Social
```json
{"id_unico":"usuario_telegram_001","texto_segmento":"Interesante reflexi√≥n sobre la naturaleza del tiempo en la f√≠sica cu√°ntica. ¬øSer√° que el tiempo es realmente una ilusi√≥n como sugiere Einstein?","autor_documento":"Usuario Personal","titulo_documento":"Conversaciones Telegram","orden_segmento_documento":1,"tipo_segmento":"mensaje","jerarquia_contextual":{"chat":"Debates Filosof√≠a","fecha_mensaje":"2023-11-15"},"idioma_documento":"es","fecha_publicacion_documento":"2023-11-15","fuente_original":"redes_sociales","contexto_archivo":"raw_data/usuario_personal/telegram_export.ndjson","hash_documento_original":"ghi789jkl012"}
```

## üîß Reglas de Segmentaci√≥n

### 1. Poemas y Canciones

**Estrategia**: Archivo completo como una sola entrada

**Caracter√≠sticas**:
- **Un segmento por archivo**: Todo el poema/canci√≥n en un solo registro
- **Preservaci√≥n de estructura**: Mantiene saltos de l√≠nea y estrofas
- **Tipo de segmento**: `"poema_completo"` o `"cancion_completa"`
- **Jerarqu√≠a contextual**: Puede incluir secci√≥n, libro, √°lbum

**Ejemplo de procesamiento**:
```
Archivo: raw_data/poeta/mi_poema.txt
‚Üì
Un solo registro NDJSON con todo el contenido
```

### 2. Mensajes y Redes Sociales

**Estrategia**: Cada mensaje es una entrada independiente

**Caracter√≠sticas**:
- **Un segmento por mensaje**: Cada mensaje/comentario/post individual
- **Preservaci√≥n de metadatos**: Fecha, hora, contexto de conversaci√≥n
- **Tipo de segmento**: `"mensaje"`, `"comentario"`, `"post"`
- **Jerarqu√≠a contextual**: Chat, hilo, conversaci√≥n

**Ejemplo de procesamiento**:
```
Archivo NDJSON con 100 mensajes
‚Üì
100 registros NDJSON individuales
```

### 3. Escritos y Libros

**Estrategia**: Segmentaci√≥n por p√°rrafos (doble salto de l√≠nea `\n\n`)

**Caracter√≠sticas**:
- **Un segmento por p√°rrafo**: Divisi√≥n natural del texto
- **Detecci√≥n de t√≠tulos**: P√°rrafos cortos y aislados como t√≠tulos
- **Tipo de segmento**: `"parrafo"`, `"titulo"`, `"subtitulo"`
- **Jerarqu√≠a contextual**: Cap√≠tulo, secci√≥n, subsecci√≥n

**Ejemplo de procesamiento**:
```
Archivo con 50 p√°rrafos
‚Üì
50 registros NDJSON (p√°rrafos + t√≠tulos)
```

### 4. Documentos Estructurados (PDF, DOCX)

**Estrategia**: Segmentaci√≥n basada en estructura del documento

**Caracter√≠sticas**:
- **Respeta jerarqu√≠a**: T√≠tulos, subt√≠tulos, p√°rrafos
- **Preserva formato**: Listas, tablas, citas
- **Tipo de segmento**: `"titulo"`, `"parrafo"`, `"lista"`, `"tabla"`, `"cita"`
- **Jerarqu√≠a contextual**: Estructura completa del documento

### 5. Datos Tabulares (CSV, Excel)

**Estrategia**: Conversi√≥n a formato narrativo

**Caracter√≠sticas**:
- **Filas como registros**: Cada fila se convierte en texto descriptivo
- **Encabezados como contexto**: Nombres de columnas proporcionan estructura
- **Tipo de segmento**: `"registro_datos"`, `"encabezado_tabla"`
- **Jerarqu√≠a contextual**: Hoja, tabla, secci√≥n

## üîÑ Proceso de Depuraci√≥n y Estandarizaci√≥n

### Fase 1: Conversi√≥n de Formatos

**Objetivo**: Convertir todos los archivos a formatos procesables

**Acciones**:
1. **Detectar formato** del archivo de entrada
2. **Seleccionar loader** apropiado autom√°ticamente
3. **Convertir contenido** a estructura com√∫n
4. **Validar extracci√≥n** de contenido

**Herramientas**:
- **Loaders espec√≠ficos**: Para cada formato de archivo
- **ProfileManager**: Selecci√≥n autom√°tica de loader
- **Validaci√≥n**: Verificaci√≥n de contenido extra√≠do

```python
# Ejemplo de conversi√≥n autom√°tica
from dataset.processing.profile_manager import ProfileManager

manager = ProfileManager()
resultados = manager.process_file("documento.pdf", profile_name="book_structure")
```

### Fase 2: Segmentaci√≥n Inteligente

**Objetivo**: Dividir contenido seg√∫n reglas espec√≠ficas por tipo

**Acciones**:
1. **Identificar tipo** de contenido (libro, poema, mensaje)
2. **Aplicar reglas** de segmentaci√≥n correspondientes
3. **Generar segmentos** con metadatos apropiados
4. **Validar coherencia** de segmentaci√≥n

**Algoritmos**:
- **Detecci√≥n de p√°rrafos**: Por doble salto de l√≠nea
- **Identificaci√≥n de t√≠tulos**: Por longitud y posici√≥n
- **Preservaci√≥n de estructura**: Para poemas y c√≥digo
- **Extracci√≥n de listas**: Para contenido enumerado

### Fase 3: Enriquecimiento de Metadatos

**Objetivo**: A√±adir informaci√≥n contextual y estructural

**Acciones**:
1. **Extraer fecha** del archivo o contenido
2. **Inferir autor** del directorio padre
3. **Detectar idioma** del contenido
4. **Generar jerarqu√≠a** contextual
5. **Calcular hash** del documento original

**Fuentes de metadatos**:
- **Nombre de archivo**: Fechas, t√≠tulos, versiones
- **Propiedades de archivo**: Fecha de creaci√≥n/modificaci√≥n
- **Estructura de directorios**: Autor, categor√≠a, tipo
- **Contenido del documento**: T√≠tulos, fechas internas
- **Metadatos embebidos**: PDF, DOCX properties

### Fase 4: Eliminaci√≥n de Duplicados

**Objetivo**: Detectar y eliminar contenido duplicado

**Estrategias**:
1. **Hash exacto**: Para duplicados id√©nticos
2. **Similaridad textual**: Para duplicados aproximados
3. **An√°lisis sem√°ntico**: Para contenido similar
4. **Validaci√≥n manual**: Para casos ambiguos

**Algoritmos**:
```python
# Detecci√≥n de duplicados por hash
import hashlib

def detect_exact_duplicates(segments):
    seen_hashes = set()
    duplicates = []
    
    for segment in segments:
        text_hash = hashlib.md5(segment['texto_segmento'].encode()).hexdigest()
        if text_hash in seen_hashes:
            duplicates.append(segment)
        else:
            seen_hashes.add(text_hash)
    
    return duplicates

# Detecci√≥n por similaridad
from difflib import SequenceMatcher

def detect_similar_duplicates(segments, threshold=0.9):
    duplicates = []
    
    for i, seg1 in enumerate(segments):
        for j, seg2 in enumerate(segments[i+1:], i+1):
            similarity = SequenceMatcher(None, seg1['texto_segmento'], seg2['texto_segmento']).ratio()
            if similarity > threshold:
                duplicates.append((seg1, seg2, similarity))
    
    return duplicates
```

### Fase 5: Estandarizaci√≥n de Campos

**Objetivo**: Asegurar consistencia en formato y contenido

**Validaciones**:
1. **Campos obligatorios**: Verificar presencia de todos los campos requeridos
2. **Formato de fechas**: Normalizar a ISO 8601
3. **C√≥digos de idioma**: Validar c√≥digos ISO 639-1
4. **Tipos de segmento**: Verificar vocabulario controlado
5. **Jerarqu√≠a contextual**: Validar estructura JSON

**Normalizaciones**:
```python
# Normalizaci√≥n de fechas
from datetime import datetime
import re

def normalize_date(date_string):
    """Normaliza diferentes formatos de fecha a ISO 8601."""
    if not date_string:
        return None
    
    # Patrones comunes
    patterns = [
        r'(\d{4})-(\d{2})-(\d{2})',  # YYYY-MM-DD
        r'(\d{4})-(\d{2})',         # YYYY-MM
        r'(\d{4})',                 # YYYY
        r'(\d{2})/(\d{2})/(\d{4})', # MM/DD/YYYY
        r'(\d{2})-(\d{2})-(\d{4})', # DD-MM-YYYY
    ]
    
    for pattern in patterns:
        match = re.match(pattern, date_string.strip())
        if match:
            # Procesar seg√∫n el patr√≥n
            return format_date(match.groups())
    
    return None

# Normalizaci√≥n de tipos de segmento
VOCABULARIO_TIPOS = {
    'parrafo', 'titulo', 'subtitulo', 'lista', 'cita', 'nota',
    'tabla', 'codigo', 'formula', 'referencia', 'poema_completo',
    'cancion_completa', 'mensaje', 'comentario', 'post'
}

def validate_segment_type(tipo):
    """Valida que el tipo de segmento est√© en el vocabulario controlado."""
    return tipo.lower() in VOCABULARIO_TIPOS
```

### Fase 6: Unificaci√≥n y Exportaci√≥n

**Objetivo**: Combinar todos los archivos procesados en un dataset final

**Acciones**:
1. **Recopilar archivos** NDJSON procesados
2. **Asignar IDs √∫nicos** globales
3. **Validar integridad** del dataset completo
4. **Generar estad√≠sticas** de procesamiento
5. **Exportar dataset final** unificado

**Estructura final**:
```
output/
‚îú‚îÄ‚îÄ unified/
‚îÇ   ‚îú‚îÄ‚îÄ dataset_final.ndjson      # Dataset completo
‚îÇ   ‚îú‚îÄ‚îÄ estadisticas.json         # M√©tricas del procesamiento
‚îÇ   ‚îú‚îÄ‚îÄ errores.log              # Log de errores encontrados
‚îÇ   ‚îî‚îÄ‚îÄ metadatos.json           # Informaci√≥n del dataset
‚îî‚îÄ‚îÄ por_autor/
    ‚îú‚îÄ‚îÄ garcia_marquez.ndjson    # Dataset por autor
    ‚îú‚îÄ‚îÄ borges.ndjson
    ‚îî‚îÄ‚îÄ usuario_personal.ndjson
```

## üîç Divisi√≥n de Responsabilidades

### üë§ Responsabilidades del Usuario

1. **Organizaci√≥n inicial**:
   - Colocar archivos en estructura de directorios correcta
   - Nombrar directorios con nombres de autores
   - Organizar archivos por tipo cuando sea necesario

2. **Configuraci√≥n de procesamiento**:
   - Seleccionar perfil de procesamiento apropiado
   - Indicar tipo de contenido cuando no sea obvio
   - Configurar par√°metros espec√≠ficos si es necesario

3. **Revisi√≥n y validaci√≥n**:
   - Revisar sugerencias de duplicados antes de eliminar
   - Corregir fechas o metadatos cuando el script no pueda inferirlos
   - Validar resultados de segmentaci√≥n en casos complejos

4. **Conversi√≥n de formatos**:
   - Convertir archivos muy espec√≠ficos a formatos est√°ndar
   - Preparar archivos corruptos o con problemas de encoding

### ü§ñ Responsabilidades del Script

1. **Procesamiento autom√°tico**:
   - Detectar formato de archivo autom√°ticamente
   - Seleccionar loader y estrategia de segmentaci√≥n apropiados
   - Extraer contenido y metadatos disponibles

2. **Segmentaci√≥n inteligente**:
   - Aplicar reglas de segmentaci√≥n seg√∫n tipo de contenido
   - Detectar t√≠tulos, p√°rrafos, listas autom√°ticamente
   - Preservar estructura y formato cuando sea relevante

3. **Enriquecimiento de datos**:
   - Inferir autor del directorio padre
   - Extraer fechas de nombres de archivo y metadatos
   - Generar jerarqu√≠a contextual autom√°ticamente
   - Calcular hashes y IDs √∫nicos

4. **Control de calidad**:
   - Detectar duplicados exactos y similares
   - Validar formato y consistencia de datos
   - Generar reportes de errores y estad√≠sticas
   - Sugerir correcciones cuando sea posible

## üìä Flujo de Trabajo Completo

### Preparaci√≥n

```bash
# 1. Organizar archivos
mkdir -p raw_data/mi_autor
cp mis_documentos/* raw_data/mi_autor/

# 2. Verificar estructura
ls -la raw_data/mi_autor/
```

### Procesamiento

```bash
# 3. Ejecutar procesamiento
cd dataset
python scripts/app_depuracion.py --autor="mi_autor" --profile="book_structure"

# 4. Verificar resultados
ls -la processed_data/
head -n 5 processed_data/mi_autor_*.ndjson
```

### Validaci√≥n

```bash
# 5. Validar formato NDJSON
python -c "
import json
with open('processed_data/mi_autor_libro.ndjson') as f:
    for i, line in enumerate(f, 1):
        try:
            json.loads(line)
        except json.JSONDecodeError as e:
            print(f'Error en l√≠nea {i}: {e}')
"

# 6. Estad√≠sticas b√°sicas
wc -l processed_data/*.ndjson
grep -c '"tipo_segmento":"titulo"' processed_data/*.ndjson
```

### Unificaci√≥n

```bash
# 7. Unificar dataset
python scripts/unificar_dataset.py --input="processed_data/" --output="output/unified/"

# 8. Verificar dataset final
wc -l output/unified/dataset_final.ndjson
cat output/unified/estadisticas.json
```

### Importaci√≥n

```bash
# 9. Importar a base de datos
cd ../backend/scripts
python importar_completo.py --source="../../dataset/output/unified/dataset_final.ndjson"

# 10. Verificar importaci√≥n
python -c "
import sqlite3
conn = sqlite3.connect('../data/biblioteca.db')
print('Segmentos importados:', conn.execute('SELECT COUNT(*) FROM segmentos').fetchone()[0])
print('Autores √∫nicos:', conn.execute('SELECT COUNT(DISTINCT autor_documento) FROM segmentos').fetchone()[0])
"
```

## üîß Consideraciones T√©cnicas

### Rendimiento

- **Procesamiento por streaming**: Para archivos grandes
- **Paralelizaci√≥n**: Procesamiento simult√°neo de m√∫ltiples archivos
- **Cach√© inteligente**: Evitar reprocesar archivos sin cambios
- **Optimizaci√≥n de memoria**: Liberaci√≥n de recursos durante procesamiento

### Escalabilidad

- **Procesamiento incremental**: A√±adir nuevos documentos sin reprocesar todo
- **Versionado de datasets**: Mantener historial de cambios
- **Distribuci√≥n de carga**: Para bibliotecas muy grandes
- **Monitoreo de recursos**: Control de uso de CPU y memoria

### Robustez

- **Manejo de errores**: Continuar procesamiento aunque algunos archivos fallen
- **Validaci√≥n exhaustiva**: Verificar integridad en cada paso
- **Recuperaci√≥n de errores**: Reanudar procesamiento desde puntos de control
- **Logging detallado**: Trazabilidad completa del proceso

## üìö Pr√≥ximos Pasos

Despu√©s de completar el procesamiento de datos:

1. **Importaci√≥n**: [Gesti√≥n de Datos](GUIA_GESTION_DATOS.md) - Importar NDJSON a base de datos
2. **Configuraci√≥n**: [Meilisearch](GUIA_MEILISEARCH.md) - Configurar motor de b√∫squeda
3. **Uso**: [Inicio R√°pido](INICIO_RAPIDO.md) - Usar la aplicaci√≥n completa
4. **Extensi√≥n**: [Loaders](GUIA_LOADERS.md) - A√±adir soporte para nuevos formatos

## üÜò Soluci√≥n de Problemas Comunes

### Error: "Archivo no procesable"

**S√≠ntomas**: El script no puede leer ciertos archivos

**Causas comunes**:
- Archivo corrupto o formato no est√°ndar
- Encoding incorrecto
- Permisos de archivo insuficientes

**Soluciones**:
```bash
# Verificar encoding
file -i mi_archivo.txt

# Convertir encoding si es necesario
iconv -f ISO-8859-1 -t UTF-8 mi_archivo.txt > mi_archivo_utf8.txt

# Verificar permisos
ls -la mi_archivo.txt
chmod 644 mi_archivo.txt
```

### Error: "Segmentaci√≥n incorrecta"

**S√≠ntomas**: Los p√°rrafos se dividen mal o los t√≠tulos no se detectan

**Causas comunes**:
- Formato de texto inconsistente
- Reglas de segmentaci√≥n no apropiadas para el tipo de documento

**Soluciones**:
```python
# Usar perfil espec√≠fico
manager.process_file("archivo.txt", profile_name="poetry_structure")

# Configurar par√°metros manualmente
manager.configure_profile("custom", {
    "segmentation_strategy": "sentence_based",
    "min_segment_length": 30
})
```

### Error: "Metadatos faltantes"

**S√≠ntomas**: Fechas o t√≠tulos no se extraen correctamente

**Causas comunes**:
- Nombres de archivo sin informaci√≥n de fecha
- Metadatos no est√°ndar en documentos

**Soluciones**:
```python
# Proporcionar metadatos manualmente
resultados = manager.process_file(
    "archivo.pdf",
    override_metadata={
        "fecha_publicacion": "2023-01-01",
        "titulo_documento": "Mi T√≠tulo"
    }
)
```

Esta gu√≠a proporciona todo lo necesario para entender y ejecutar el proceso completo de conversi√≥n de documentos a NDJSON en Biblioperson. Para casos espec√≠ficos o problemas t√©cnicos, consulta las gu√≠as especializadas de loaders y especificaciones t√©cnicas.