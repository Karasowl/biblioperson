# Gu√≠a T√©cnica del Sistema de Deduplicaci√≥n de Biblioperson

## üìñ √çndice

1. [Arquitectura del Sistema](#arquitectura-del-sistema)
2. [Componentes Principales](#componentes-principales)
3. [Configuraci√≥n](#configuraci√≥n)
4. [API de Programaci√≥n](#api-de-programaci√≥n)
5. [Base de Datos](#base-de-datos)
6. [Modos de Salida](#modos-de-salida)
7. [Manejo de Errores](#manejo-de-errores)
8. [Rendimiento](#rendimiento)
9. [Troubleshooting](#troubleshooting)

## üèóÔ∏è Arquitectura del Sistema

### Visi√≥n General

El sistema de deduplicaci√≥n de Biblioperson est√° dise√±ado con una arquitectura modular que permite:

- **Integraci√≥n opcional**: Puede habilitarse/deshabilitarse sin afectar el pipeline existente
- **Configuraci√≥n granular**: Control fino sobre comportamiento y rendimiento
- **M√∫ltiples interfaces**: CLI, API REST, y UI web
- **Fallbacks robustos**: Contin√∫a funcionando aunque componentes fallen

### Principios de Dise√±o

1. **Opcional por Defecto**: No interrumpe flujos existentes
2. **Configuraci√≥n Externa**: Comportamiento controlado por archivos YAML
3. **Manejo de Errores Graceful**: Contin√∫a procesamiento en caso de fallos
4. **Performance Optimizada**: Hash incremental, cache en memoria
5. **Compatibilidad Retroactiva**: Funciona con c√≥digo existente

## üîß Componentes Principales

### 1. DeduplicationManager (`dataset/processing/deduplication.py`)

**Responsabilidades:**
- C√°lculo de hashes SHA-256
- Gesti√≥n de base de datos SQLite
- Detecci√≥n de duplicados
- Estad√≠sticas y reportes

**M√©todos Principales:**

```python
class DeduplicationManager:
    def compute_sha256(self, file_path: Union[str, Path]) -> str:
        """Calcula hash SHA-256 de un archivo usando chunks eficientes."""
        
    def check_and_register(self, file_path: Union[str, Path], title: str = None) -> Tuple[str, bool]:
        """Verifica si es duplicado y registra si es nuevo."""
        
    def get_duplicate_info(self, document_hash: str) -> Optional[Dict[str, Any]]:
        """Obtiene informaci√≥n de un documento duplicado."""
        
    def list_documents(self, search: str = None, before: str = None, after: str = None) -> List[Dict[str, Any]]:
        """Lista documentos con filtros opcionales."""
        
    def remove_by_hash(self, document_hash: str) -> bool:
        """Elimina documento por hash."""
        
    def get_stats(self) -> Dict[str, Any]:
        """Obtiene estad√≠sticas de la base de datos."""
```

### 2. OutputModeSerializer (`dataset/processing/output_modes.py`)

**Responsabilidades:**
- Serializaci√≥n diferenciada por modo
- Exportaci√≥n a JSON/NDJSON
- Filtrado de campos seg√∫n configuraci√≥n

**Modos Disponibles:**

```python
class OutputMode(Enum):
    GENERIC = "generic"      # NDJSON simple para IAs
    BIBLIOPERSON = "biblioperson"  # NDJSON enriquecido completo
```

### 3. DedupConfigManager (`dataset/processing/dedup_config.py`)

**Responsabilidades:**
- Carga de configuraci√≥n desde YAML
- Validaci√≥n de par√°metros
- Acceso centralizado a configuraci√≥n

**Configuraci√≥n Principal:**

```python
@dataclass
class DeduplicationConfig:
    enabled: bool = True
    default_output_mode: str = "biblioperson"
    database_path: str = "dataset/data/deduplication.db"
    continue_on_error: bool = True
    log_errors: bool = True
    warn_when_disabled: bool = False
```

### 4. API REST (`dataset/processing/dedup_api.py`)

**Endpoints Disponibles:**

| M√©todo | Endpoint | Descripci√≥n |
|--------|----------|-------------|
| GET | `/api/dedup` | Listar documentos con filtros |
| DELETE | `/api/dedup/{hash}` | Eliminar por hash |
| DELETE | `/api/dedup/path` | Eliminar por ruta |
| POST | `/api/dedup/clear` | Limpiar todos |
| POST | `/api/dedup/prune` | Eliminar por fecha |
| GET | `/api/dedup/stats` | Estad√≠sticas |
| DELETE | `/api/dedup/batch` | Eliminaci√≥n por lotes |

## ‚öôÔ∏è Configuraci√≥n

### Archivo Principal: `dataset/config/deduplication_config.yaml`

```yaml
# Configuraci√≥n principal
deduplication:
  enabled: true
  default_output_mode: "biblioperson"
  database_path: "dataset/data/deduplication.db"
  
  error_handling:
    continue_on_error: true
    log_errors: true
    warn_when_disabled: false

# Configuraci√≥n de modos de salida
output_modes:
  generic:
    description: "Salida NDJSON simple sin metadatos adicionales"
    enable_deduplication: false
    included_fields:
      - "segment_id"
      - "document_id"
      - "text"
      - "segment_type"
      - "segment_order"
      - "text_length"
      - "processing_timestamp"
      - "source_file_path"
      - "document_title"
      - "document_author"
      - "document_language"
      - "pipeline_version"
      - "segmenter_used"
  
  biblioperson:
    description: "Salida NDJSON enriquecida con metadatos completos"
    enable_deduplication: true
    included_fields: "all"
    include_document_hash: true

# Configuraci√≥n de compatibilidad
compatibility:
  supported_profiles:
    - "json"
    - "prosa"
    - "verso"
    - "autom√°tico"
  
  supported_file_formats:
    - ".pdf"
    - ".docx"
    - ".txt"
    - ".md"
    - ".json"
    - ".ndjson"

# Configuraci√≥n de rendimiento
performance:
  hash_chunk_size: 8192
  operation_timeout: 30
  enable_hash_cache: true
  max_cache_size: 1000
```

### Configuraci√≥n Program√°tica

```python
from dataset.processing.dedup_config import get_config_manager

# Obtener configuraci√≥n
config_manager = get_config_manager()

# Verificar si est√° habilitado
if config_manager.is_deduplication_enabled():
    print("Deduplicaci√≥n habilitada")

# Verificar para modo espec√≠fico
if config_manager.is_deduplication_enabled_for_mode("biblioperson"):
    print("Deduplicaci√≥n habilitada para modo biblioperson")

# Obtener configuraci√≥n detallada
dedup_config = config_manager.get_deduplication_config()
print(f"Base de datos: {config_manager.get_database_path()}")
```

## üóÑÔ∏è Base de Datos

### Esquema SQLite

```sql
CREATE TABLE IF NOT EXISTS documents (
    hash TEXT PRIMARY KEY,           -- SHA-256 del archivo
    file_path TEXT NOT NULL,         -- Ruta completa del archivo
    title TEXT,                      -- T√≠tulo extra√≠do o nombre base
    first_seen TEXT NOT NULL,        -- Timestamp ISO-8601 UTC
    file_size INTEGER,               -- Tama√±o en bytes
    last_modified TEXT               -- √öltima modificaci√≥n del archivo
);

CREATE INDEX IF NOT EXISTS idx_documents_first_seen ON documents(first_seen);
CREATE INDEX IF NOT EXISTS idx_documents_title ON documents(title);
CREATE INDEX IF NOT EXISTS idx_documents_file_path ON documents(file_path);
```

### Ubicaci√≥n por Defecto

- **Desarrollo**: `dataset/.cache/dedup_registry.sqlite`
- **Producci√≥n**: Configurable en `deduplication_config.yaml`

### Operaciones Principales

```python
# Inicializar gestor
from dataset.processing.deduplication import get_dedup_manager
dedup_manager = get_dedup_manager()

# Verificar duplicado
hash_value, is_new = dedup_manager.check_and_register("/path/to/file.pdf")

if not is_new:
    duplicate_info = dedup_manager.get_duplicate_info(hash_value)
    print(f"Duplicado detectado: {duplicate_info['first_seen']}")

# Listar con filtros
documents = dedup_manager.list_documents(
    search="novela",
    after="2025-01-01T00:00:00Z"
)

# Estad√≠sticas
stats = dedup_manager.get_stats()
print(f"Total documentos: {stats['total_documents']}")
```

## üì§ Modos de Salida

### Modo Generic

**Prop√≥sito**: Alimentar sistemas de IA y procesamiento simple

**Caracter√≠sticas:**
- Campos m√≠nimos esenciales
- Sin deduplicaci√≥n
- Estructura ligera
- Ideal para LLMs

**Ejemplo de Salida:**

```json
{
  "segment_id": "doc_001_seg_001",
  "document_id": "doc_001",
  "text": "Este es el contenido del segmento...",
  "segment_type": "paragraph",
  "segment_order": 1,
  "text_length": 156,
  "processing_timestamp": "2025-06-14T22:45:19.823Z",
  "source_file_path": "/path/to/document.pdf",
  "document_title": "Mi Documento",
  "document_author": "Autor Desconocido",
  "document_language": "es",
  "pipeline_version": "1.0.0",
  "segmenter_used": "HeadingSegmenter"
}
```

### Modo Biblioperson

**Prop√≥sito**: Sistema completo con trazabilidad y deduplicaci√≥n

**Caracter√≠sticas:**
- Todos los metadatos disponibles
- Deduplicaci√≥n activa
- Hash del documento incluido
- Trazabilidad completa

**Ejemplo de Salida:**

```json
{
  "segment_id": "doc_001_seg_001",
  "document_id": "doc_001",
  "document_language": "es",
  "text": "Este es el contenido del segmento...",
  "segment_type": "paragraph",
  "segment_order": 1,
  "text_length": 156,
  "processing_timestamp": "2025-06-14T22:45:19.823Z",
  "source_file_path": "/path/to/document.pdf",
  "document_title": "Mi Documento",
  "document_author": "Autor Desconocido",
  "additional_metadata": {
    "document_hash": "a1b2c3d4e5f6...",
    "file_size": 1024576,
    "extraction_method": "pymupdf",
    "corruption_detected": false,
    "author_detection_confidence": 0.85,
    "language_detection_confidence": 0.95
  },
  "pipeline_version": "1.0.0",
  "segmenter_used": "HeadingSegmenter"
}
```

## üö® Manejo de Errores

### Estrategias de Recuperaci√≥n

1. **Continue on Error**: Contin√∫a procesamiento sin deduplicaci√≥n
2. **Fallback Export**: Usa exportaci√≥n tradicional si falla sistema de modos
3. **Graceful Degradation**: Desactiva funciones problem√°ticas autom√°ticamente

### Configuraci√≥n de Errores

```yaml
deduplication:
  error_handling:
    continue_on_error: true      # Continuar si falla deduplicaci√≥n
    log_errors: true             # Registrar errores en logs
    warn_when_disabled: false    # Advertir cuando est√© deshabilitado
```

### C√≥digos de Error Comunes

| C√≥digo | Descripci√≥n | Acci√≥n Recomendada |
|--------|-------------|-------------------|
| `DEDUP_001` | Base de datos no accesible | Verificar permisos y ruta |
| `DEDUP_002` | Error calculando hash | Verificar archivo existe y es legible |
| `DEDUP_003` | Configuraci√≥n inv√°lida | Revisar `deduplication_config.yaml` |
| `DEDUP_004` | Timeout en operaci√≥n | Aumentar `operation_timeout` |

## ‚ö° Rendimiento

### Optimizaciones Implementadas

1. **Hash Incremental**: Lectura en chunks de 8KB
2. **Cache en Memoria**: Hashes recientes en RAM
3. **√çndices de Base de Datos**: B√∫squedas optimizadas
4. **Lazy Loading**: Carga bajo demanda

### Configuraci√≥n de Performance

```yaml
performance:
  hash_chunk_size: 8192          # Tama√±o de chunk para hash (bytes)
  operation_timeout: 30          # Timeout para operaciones (segundos)
  enable_hash_cache: true        # Cache de hashes en memoria
  max_cache_size: 1000          # M√°ximo de hashes en cache
```

### Benchmarks

| Operaci√≥n | Archivo 1MB | Archivo 10MB | Archivo 100MB |
|-----------|-------------|--------------|---------------|
| C√°lculo Hash | ~5ms | ~45ms | ~450ms |
| Consulta DB | ~1ms | ~1ms | ~1ms |
| Registro Nuevo | ~2ms | ~2ms | ~2ms |

## üîß Troubleshooting

### Problemas Comunes

#### 1. Deduplicaci√≥n No Funciona

**S√≠ntomas:**
- Documentos duplicados no se detectan
- No aparecen mensajes de duplicados en logs

**Soluciones:**
```python
# Verificar configuraci√≥n
from dataset.processing.dedup_config import get_config_manager
config = get_config_manager()

print(f"Habilitado: {config.is_deduplication_enabled()}")
print(f"Modo biblioperson: {config.is_deduplication_enabled_for_mode('biblioperson')}")

# Verificar base de datos
from dataset.processing.deduplication import get_dedup_manager
dedup = get_dedup_manager()
stats = dedup.get_stats()
print(f"Documentos registrados: {stats['total_documents']}")
```

#### 2. Error de Permisos en Base de Datos

**S√≠ntomas:**
- `PermissionError` al acceder SQLite
- Logs muestran errores de escritura

**Soluciones:**
```bash
# Verificar permisos del directorio
ls -la dataset/data/

# Crear directorio si no existe
mkdir -p dataset/data/

# Dar permisos de escritura
chmod 755 dataset/data/
```

#### 3. Performance Lenta

**S√≠ntomas:**
- Procesamiento muy lento con deduplicaci√≥n
- Timeouts frecuentes

**Soluciones:**
```yaml
# Optimizar configuraci√≥n
performance:
  hash_chunk_size: 16384    # Duplicar tama√±o de chunk
  operation_timeout: 60     # Aumentar timeout
  enable_hash_cache: true   # Asegurar cache habilitado
  max_cache_size: 2000     # Aumentar tama√±o de cache
```

### Herramientas de Diagn√≥stico

#### Validar Sistema Completo

```python
from dataset.processing.profile_manager import ProfileManager

pm = ProfileManager()

# Validar compatibilidad
compatibility = pm.validate_pipeline_compatibility()
print("Compatibilidad:", compatibility)

# Estado de deduplicaci√≥n
status = pm.get_deduplication_status()
print("Estado:", status)
```

#### Logs Detallados

```python
import logging

# Habilitar logs detallados
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger('dataset.processing')
logger.setLevel(logging.DEBUG)
```
 