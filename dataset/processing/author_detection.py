#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sistema Contextual de Detecci√≥n Autom√°tica de Autores

Este m√≥dulo implementa detecci√≥n inteligente de autores basada en:

üéØ AN√ÅLISIS CONTEXTUAL:
   - Patrones de atribuci√≥n ("Por:", "Autor:", etc.)
   - An√°lisis posicional (inicio/final vs medio del texto)
   - Validaci√≥n contextual (autor vs sujeto narrativo)
   - Morfolog√≠a hispana inteligente
   - Validaci√≥n cruzada

üìä Ejemplo pr√°ctico:
   - Biograf√≠a de Einstein: "Einstein" aparece 50 veces en el MEDIO ‚Üí Score bajo
   - Art√≠culo por Garc√≠a: "Garc√≠a" aparece 3 veces al INICIO/FINAL ‚Üí Score alto

El sistema distingue entre menciones narrativas y atribuciones autorales reales.

Autor: Sistema de IA
Fecha: 2024
"""

import re
import logging
import unicodedata
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from collections import defaultdict, Counter
from pathlib import Path

# Sistema de detecci√≥n mejorado con contexto de documento
from .enhanced_contextual_author_detection import EnhancedContextualAuthorDetector, DocumentContext

@dataclass
class AuthorCandidate:
    """Candidato a autor con informaci√≥n de confianza (sistema b√°sico)"""
    name: str
    confidence: float = 0.0
    sources: List[str] = field(default_factory=list)
    positions: List[int] = field(default_factory=list)
    extraction_method: str = ""
    context: List[str] = field(default_factory=list)

logger = logging.getLogger(__name__)

class AutorDetector:
    """
    Detector avanzado de autores para textos de verso y prosa.
    
    Implementa un algoritmo multi-nivel:
    1. Extracci√≥n de metadatos estructurados
    2. An√°lisis de patrones textuales espec√≠ficos
    3. An√°lisis de contexto y frecuencia
    4. Scoring y validaci√≥n final
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Inicializar el detector de autores.
        
        Args:
            config: Configuraci√≥n opcional del detector
        """
        self.config = config or {}
        self.confidence_threshold = self.config.get('confidence_threshold', 0.6)
        self.debug_mode = self.config.get('debug', False)
        
        # Configurar logging
        self.logger = logging.getLogger(f"{__name__}.AutorDetector")
        if self.debug_mode:
            self.logger.setLevel(logging.DEBUG)
            
        self.logger.info("üîç INICIALIZANDO AUTOR DETECTOR V1.0 - ALGORITMO AVANZADO")
        
        # === NIVEL 1: PATRONES DE METADATOS EXPL√çCITOS ===
        self.metadata_patterns = {
            'explicit_author': [
                r'(?:^|\n)\s*(?:Autor|Author|Por|By|Escrito por|Written by):\s*([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+){1,3})',
                r'(?:^|\n)\s*([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+){1,3})\s*\(autor\)',
                r'(?:^|\n)\s*¬©\s*([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+){1,3})',
            ],
            'byline_patterns': [
                r'(?:^|\n)\s*(?:Por|By)\s+([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+){1,3})\s*(?:\n|$)',
                r'(?:^|\n)\s*([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+){1,3})\s*[-‚Äì‚Äî]\s*\d{4}',
            ]
        }
        
        # === NIVEL 2A: PATRONES ESPEC√çFICOS PARA VERSO (POES√çA) ===
        self.verso_patterns = {
            'signature_end': [
                r'(?:^|\n)\s*‚Äî\s*([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+){1,3})\s*$',
                r'(?:^|\n)\s*[-‚Äì‚Äî]\s*([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+){1,3})\s*$',
            ],
            'attribution_verse': [
                r'(?:^|\n)\s*(?:De|Por)\s+([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+){1,3})\s*$',
                r'(?:^|\n)\s*([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+){1,3})\s*\([Aa]utor\)',
            ],
            'poem_title_author': [
                r'(?:Soneto|Canci√≥n|Eleg√≠a|Oda|Balada|Poema)\s+(?:de\s+)?([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+){1,3})',
                r'^([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+){1,3})\s*\(poem[as]?\)',
            ],
            'isolated_name': [
                r'(?:^|\n)\s*([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+){1,3})\s*(?:\n|$)',
            ]
        }
        
        # === NIVEL 2B: PATRONES ESPEC√çFICOS PARA PROSA ===
        self.prosa_patterns = {
            'academic_format': [
                r'([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+){1,3})\s*\(\d{4}\)',
                r'([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+),\s+([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+){0,2})\s*\(\d{4}\)',
            ],
            'article_header': [
                r'(?:^|\n)\s*([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+){1,3})\s*[-‚Äì‚Äî]\s*\d{1,2}/\d{1,2}/\d{4}',
                r'(?:^|\n)\s*\*\*([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+){1,3})\*\*',
            ],
            'book_author': [
                r'(?:^|\n)\s*([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+){1,3})\s*\n.*(?:ISBN|Edici√≥n|Editorial)',
                r'Editorial.*\n.*([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+){1,3})',
            ]
        }
        
        # === NIVEL 3: LISTA DE PALABRAS NO V√ÅLIDAS COMO NOMBRES ===
        self.invalid_names = {
            # Art√≠culos, preposiciones, conjunciones
            'el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas',
            'de', 'del', 'en', 'con', 'por', 'para', 'desde', 'hasta',
            'sobre', 'bajo', 'ante', 'tras', 'durante', 'mediante',
            'y', 'o', 'pero', 'sino', 'aunque', 'porque', 'como', 'cuando',
            # Palabras comunes que no son nombres
            'se√±or', 'se√±ora', 'don', 'do√±a', 'casa', 'vida', 'mundo',
            'tiempo', 'hombre', 'mujer', 'd√≠a', 'noche', 'amor', 'muerte',
            'dios', 'cristo', 'virgen', 'santo', 'santa', 'san',
            # Meses y d√≠as
            'enero', 'febrero', 'marzo', 'abril', 'mayo', 'junio',
            'julio', 'agosto', 'septiembre', 'octubre', 'noviembre', 'diciembre',
            'lunes', 'martes', 'mi√©rcoles', 'jueves', 'viernes', 's√°bado', 'domingo',
            # Palabras t√©cnicas
            'p√°gina', 'cap√≠tulo', 't√≠tulo', '√≠ndice', 'pr√≥logo', 'ep√≠logo',
            'introducci√≥n', 'conclusi√≥n', 'anexo', 'ap√©ndice', 'bibliograf√≠a'
        }
        
        # === COMPILAR PATRONES REGEX ===
        self._compile_patterns()
        
        self.logger.info(f"‚úÖ Detector configurado - umbral: {self.confidence_threshold}")
    
    def _compile_patterns(self) -> None:
        """Compilar todos los patrones regex para mayor eficiencia"""
        self.compiled_patterns = {}
        
        # Compilar patrones de metadatos
        for category, patterns in self.metadata_patterns.items():
            self.compiled_patterns[category] = [re.compile(p, re.MULTILINE | re.IGNORECASE) for p in patterns]
        
        # Compilar patrones de verso
        for category, patterns in self.verso_patterns.items():
            self.compiled_patterns[f"verso_{category}"] = [re.compile(p, re.MULTILINE | re.IGNORECASE) for p in patterns]
        
        # Compilar patrones de prosa
        for category, patterns in self.prosa_patterns.items():
            self.compiled_patterns[f"prosa_{category}"] = [re.compile(p, re.MULTILINE | re.IGNORECASE) for p in patterns]
    
    def detect_author(self, segments: List[Dict[str, Any]], profile_type: str, document_title: Optional[str] = None, source_file_path: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """
        Detectar autor usando sistema mejorado con contexto de documento.
        Esta versi√≥n prioriza EnhancedContextualAuthorDetector.
        
        Args:
            segments: Lista de segmentos de texto procesados
            profile_type: Tipo de perfil ('verso' o 'prosa')
            document_title: T√≠tulo del documento (opcional)
            source_file_path: Ruta del archivo fuente (opcional)
            
        Returns:
            Diccionario con informaci√≥n del autor detectado o None
        """
        self.logger.debug(f"Iniciando AutorDetector.detect_author para profile_type: {profile_type}")
        if not segments:
            self.logger.warning("No se proporcionaron segmentos para la detecci√≥n de autor.")
            return None

        self.logger.debug(f"Valor de document_title recibido en AutorDetector.detect_author: {document_title}")
        self.logger.debug(f"Valor de source_file_path recibido en AutorDetector.detect_author: {source_file_path}")

        filename = Path(source_file_path).name if source_file_path else None
        
        # Utilizar el metadata del primer segmento si est√° disponible y es un diccionario,
        # o un diccionario vac√≠o como fallback.
        metadata_for_context = {}
        if segments and isinstance(segments[0], dict):
            segment_metadata = segments[0].get('additional_metadata')  # O la clave correcta donde est√©n los metadatos globales
            if isinstance(segment_metadata, dict):
                metadata_for_context = segment_metadata.copy()
        # Si 'document_title' est√° en metadata_for_context, podr√≠a usarse como fallback si el par√°metro document_title es None.
        # Pero el par√°metro 'document_title' tiene precedencia.

        doc_context = DocumentContext(
            title=document_title,  # Usar el par√°metro directo
            filename=filename,     # Calculado del par√°metro directo source_file_path
            metadata=metadata_for_context
        )
        self.logger.debug(f"DocumentContext creado en AutorDetector.detect_author: title='{doc_context.title}', filename='{doc_context.filename}'")

        # Configurar detector mejorado.
        # self.config se inicializa en AutorDetector.__init__
        current_config = self.config if isinstance(self.config, dict) else {}
        enhanced_config = {
            'confidence_threshold': current_config.get('confidence_threshold', 0.6),
            'debug': current_config.get('debug', False),
            'strict_mode': current_config.get('strict_mode', True) # Default a True para priorizar known_authors
        }
        self.logger.debug(f"Configuraci√≥n para EnhancedContextualAuthorDetector: {enhanced_config}")
        
        enhanced_detector = EnhancedContextualAuthorDetector(enhanced_config)
        
        # Mapear profile_type a content_type esperado por el detector mejorado
        content_type_for_enhanced = 'poetry' if profile_type == 'verso' else 'prose'
        self.logger.debug(f"Llamando a enhanced_detector.detect_author_enhanced con content_type: {content_type_for_enhanced}")

        try:
            result = enhanced_detector.detect_author_enhanced(segments, content_type_for_enhanced, doc_context)
            
            if result and isinstance(result, dict) and result.get('name'):
                self.logger.info(f"‚úÖ Autor detectado por EnhancedDetector: {result.get('name')} (Confianza: {result.get('confidence', 0):.3f}, M√©todo: {result.get('method', 'N/A')})")
                if enhanced_detector.debug and result.get('details'): # Usar el debug flag del detector instanciado
                    self.logger.debug(f"Detalles de detecci√≥n (Enhanced): {result.get('details')}")
                return result
            else:
                self.logger.warning("EnhancedContextualAuthorDetector no devolvi√≥ un resultado de autor v√°lido.")
                if result is not None:
                    self.logger.debug(f"Resultado no v√°lido de EnhancedDetector: {result}")
                # NO HAY FALLBACK POR AHORA PARA AISLAR EL PROBLEMA
                return None
                
        except Exception as e:
            self.logger.error(f"Excepci√≥n durante enhanced_detector.detect_author_enhanced: {e}", exc_info=True)
            return None
    
    def _detect_author_basic(self, segments: List[Dict[str, Any]], profile_type: str) -> Optional[Dict[str, Any]]:
        """
        Detectar autom√°ticamente el autor usando an√°lisis b√°sico (respaldo).
        
        Args:
            segments: Lista de segmentos de texto procesados
            profile_type: Tipo de perfil ('verso' o 'prosa')
            
        Returns:
            Diccionario con informaci√≥n del autor detectado o None
        """
        # Unir todo el texto para an√°lisis global
        full_text = self._combine_segments_text(segments)
        if not full_text:
            return None
        
        candidates = []
        
        # === NIVEL 1: METADATOS EXPL√çCITOS ===
        metadata_candidates = self._extract_metadata_authors(full_text)
        candidates.extend(metadata_candidates)
        self.logger.debug(f"Nivel 1 - Metadatos: {len(metadata_candidates)} candidatos")
        
        # === NIVEL 2: PATRONES ESPEC√çFICOS POR TIPO ===
        if profile_type == 'verso':
            type_candidates = self._detect_verse_authors(full_text, segments)
        elif profile_type == 'prosa':
            type_candidates = self._detect_prose_authors(full_text, segments)
        else:
            type_candidates = []
        
        candidates.extend(type_candidates)
        self.logger.debug(f"Nivel 2 - Patrones espec√≠ficos: {len(type_candidates)} candidatos")
        
        # === NIVEL 3: SCORING Y VALIDACI√ìN ===
        if not candidates:
            self.logger.info("‚ùå No se encontraron candidatos de autor")
            return None
        
        scored_candidates = self._score_candidates(candidates, segments, full_text)
        
        # === NIVEL 4: SELECCI√ìN FINAL ===
        best_author = self._select_best_author(scored_candidates)
        
        if best_author:
            self.logger.info(f"‚úÖ AUTOR DETECTADO: '{best_author['name']}' (confianza: {best_author['confidence']:.2f})")
            return best_author
        else:
            self.logger.info("‚ùå No se pudo determinar autor con suficiente confianza")
            return None
    
    def _combine_segments_text(self, segments: List[Dict[str, Any]]) -> str:
        """Combinar el texto de todos los segmentos para an√°lisis global"""
        texts = []
        for segment in segments:
            text = segment.get('text', '') or segment.get('content', '')
            if text:
                texts.append(text.strip())
        return '\n\n'.join(texts)
    
    def _extract_metadata_authors(self, text: str) -> List[AuthorCandidate]:
        """NIVEL 1: Extraer autores de metadatos expl√≠citos"""
        candidates = []
        
        for category, compiled_patterns in self.compiled_patterns.items():
            if not category.startswith(('verso_', 'prosa_')):
                for pattern in compiled_patterns:
                    matches = pattern.finditer(text)
                    for match in matches:
                        name = self._clean_author_name(match.group(1))
                        if self._is_valid_name(name):
                            candidates.append(AuthorCandidate(
                                name=name,
                                confidence=0.0,  # Se calcular√° despu√©s
                                sources=[category],
                                positions=[match.start()],
                                extraction_method='metadata',
                                context=[self._get_context(text, match.start(), match.end())]
                            ))
        
        return candidates
    
    def _detect_verse_authors(self, text: str, segments: List[Dict[str, Any]]) -> List[AuthorCandidate]:
        """NIVEL 2A: Detectar autores espec√≠ficos en textos de verso"""
        candidates = []
        
        for category, compiled_patterns in self.compiled_patterns.items():
            if category.startswith('verso_'):
                for pattern in compiled_patterns:
                    matches = pattern.finditer(text)
                    for match in matches:
                        name = self._clean_author_name(match.group(1))
                        if self._is_valid_name(name):
                            candidates.append(AuthorCandidate(
                                name=name,
                                confidence=0.0,
                                sources=[category],
                                positions=[match.start()],
                                extraction_method='verse_pattern',
                                context=[self._get_context(text, match.start(), match.end())]
                            ))
        
        return candidates
    
    def _detect_prose_authors(self, text: str, segments: List[Dict[str, Any]]) -> List[AuthorCandidate]:
        """NIVEL 2B: Detectar autores espec√≠ficos en textos de prosa"""
        candidates = []
        
        for category, compiled_patterns in self.compiled_patterns.items():
            if category.startswith('prosa_'):
                for pattern in compiled_patterns:
                    matches = pattern.finditer(text)
                    for match in matches:
                        # Algunos patrones de prosa pueden capturar m√∫ltiples grupos
                        for i in range(1, len(match.groups()) + 1):
                            try:
                                name = self._clean_author_name(match.group(i))
                                if self._is_valid_name(name):
                                    candidates.append(AuthorCandidate(
                                        name=name,
                                        confidence=0.0,
                                        sources=[category],
                                        positions=[match.start()],
                                        extraction_method='prose_pattern',
                                        context=[self._get_context(text, match.start(), match.end())]
                                    ))
                                    break  # Solo tomar el primer nombre v√°lido del match
                            except IndexError:
                                break
        
        return candidates
    
    def _score_candidates(self, candidates: List[AuthorCandidate], 
                         segments: List[Dict[str, Any]], full_text: str) -> List[AuthorCandidate]:
        """NIVEL 3: Calcular score de confianza para cada candidato"""
        
        # Agrupar candidatos por nombre
        name_groups = defaultdict(list)
        for candidate in candidates:
            normalized_name = self._normalize_name(candidate.name)
            name_groups[normalized_name].append(candidate)
        
        scored_candidates = []
        
        for normalized_name, group in name_groups.items():
            # Combinar informaci√≥n de todos los candidatos del mismo nombre
            combined_candidate = self._combine_candidates(group)
            
            # Calcular score
            score = self._calculate_candidate_score(combined_candidate, segments, full_text)
            combined_candidate.confidence = score
            
            self.logger.debug(f"Candidato '{combined_candidate.name}' ‚Üí Score: {score:.3f}")
            
            scored_candidates.append(combined_candidate)
        
        # Ordenar por score descendente
        scored_candidates.sort(key=lambda c: c.confidence, reverse=True)
        
        return scored_candidates
    
    def _combine_candidates(self, candidates: List[AuthorCandidate]) -> AuthorCandidate:
        """Combinar m√∫ltiples candidatos del mismo nombre"""
        if len(candidates) == 1:
            return candidates[0]
        
        # Tomar el nombre m√°s com√∫n/completo
        names = [c.name for c in candidates]
        name_counter = Counter(names)
        best_name = name_counter.most_common(1)[0][0]
        
        # Combinar sources, positions y context
        all_sources = []
        all_positions = []
        all_context = []
        extraction_methods = []
        
        for candidate in candidates:
            all_sources.extend(candidate.sources)
            all_positions.extend(candidate.positions)
            all_context.extend(candidate.context)
            extraction_methods.append(candidate.extraction_method)
        
        return AuthorCandidate(
            name=best_name,
            confidence=0.0,  # Se calcular√° despu√©s
            sources=list(set(all_sources)),
            positions=all_positions,
            extraction_method='|'.join(set(extraction_methods)),
            context=all_context
        )
    
    def _calculate_candidate_score(self, candidate: AuthorCandidate, 
                                 segments: List[Dict[str, Any]], full_text: str) -> float:
        """Calcular score de confianza para un candidato"""
        score = 0.0
        
        # === FACTOR 1: M√âTODO DE EXTRACCI√ìN (0.0 - 0.4) ===
        if 'metadata' in candidate.extraction_method:
            score += 0.4  # Metadatos expl√≠citos son m√°s confiables
        elif 'verse_pattern' in candidate.extraction_method:
            if any('signature' in source for source in candidate.sources):
                score += 0.35  # Firmas en versos
            elif any('attribution' in source for source in candidate.sources):
                score += 0.3   # Atribuciones
            else:
                score += 0.25  # Otros patrones de verso
        elif 'prose_pattern' in candidate.extraction_method:
            if any('academic' in source for source in candidate.sources):
                score += 0.3   # Formato acad√©mico
            elif any('header' in source for source in candidate.sources):
                score += 0.25  # Headers de art√≠culos
            else:
                score += 0.2   # Otros patrones de prosa
        
        # === FACTOR 2: FRECUENCIA DE APARICI√ìN (0.0 - 0.3) ===
        frequency = len(candidate.positions)
        if frequency >= 3:
            score += 0.3
        elif frequency == 2:
            score += 0.2
        else:
            score += 0.1
        
        # === FACTOR 3: POSICI√ìN EN EL TEXTO (0.0 - 0.2) ===
        text_length = len(full_text)
        for position in candidate.positions:
            relative_position = position / text_length
            if relative_position <= 0.1 or relative_position >= 0.9:  # Inicio o final
                score += 0.2 / len(candidate.positions)  # Distribuir entre posiciones
            elif relative_position <= 0.2 or relative_position >= 0.8:  # Cerca del inicio/final
                score += 0.1 / len(candidate.positions)
        
        # === FACTOR 4: VALIDACI√ìN COMO NOMBRE PROPIO (0.0 - 0.1) ===
        if self._is_proper_name_format(candidate.name):
            score += 0.1
        
        # Asegurar que el score est√© entre 0 y 1
        return min(score, 1.0)
    
    def _select_best_author(self, scored_candidates: List[AuthorCandidate]) -> Optional[Dict[str, Any]]:
        """NIVEL 4: Seleccionar el mejor candidato basado en el score"""
        if not scored_candidates:
            return None
        
        best_candidate = scored_candidates[0]
        
        if best_candidate.confidence >= self.confidence_threshold:
            return {
                'name': best_candidate.name,
                'confidence': best_candidate.confidence,
                'extraction_method': best_candidate.extraction_method,
                'sources': best_candidate.sources,
                'frequency': len(best_candidate.positions),
                'positions': best_candidate.positions,
                'detection_details': {
                    'total_candidates': len(scored_candidates),
                    'threshold_used': self.confidence_threshold,
                    'context_samples': best_candidate.context[:3]  # Primeros 3 contextos
                }
            }
        
        return None
    
    def _clean_author_name(self, name: str) -> str:
        """Limpiar y normalizar nombre de autor"""
        if not name:
            return ""
        
        # Eliminar espacios extra y normalizar
        cleaned = re.sub(r'\s+', ' ', name.strip())
        
        # Eliminar caracteres especiales al inicio/final
        cleaned = re.sub(r'^[^\w\s]+|[^\w\s]+$', '', cleaned)
        
        # Capitalizar correctamente
        words = cleaned.split()
        capitalized_words = []
        for word in words:
            if len(word) > 1:
                capitalized_words.append(word.capitalize())
            else:
                capitalized_words.append(word.upper())
        
        return ' '.join(capitalized_words)
    
    def _is_valid_name(self, name: str) -> bool:
        """Validar si una cadena es un nombre v√°lido"""
        if not name or len(name) < 3:
            return False
        
        # Normalizar para comparaci√≥n
        normalized = self._normalize_name(name)
        words = normalized.lower().split()
        
        # Rechazar si contiene palabras inv√°lidas
        for word in words:
            if word in self.invalid_names:
                return False
        
        # Debe tener al menos una palabra que parezca nombre
        if len(words) < 2:
            return False
        
        # Verificar formato b√°sico de nombre
        if not re.match(r'^[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+)+$', name):
            return False
        
        return True
    
    def _is_proper_name_format(self, name: str) -> bool:
        """Verificar si tiene formato de nombre propio"""
        words = name.split()
        
        # Al menos 2 palabras
        if len(words) < 2:
            return False
        
        # Todas las palabras deben estar capitalizadas
        for word in words:
            if not word[0].isupper():
                return False
        
        # No m√°s de 4 palabras (nombres muy largos son sospechosos)
        if len(words) > 4:
            return False
        
        return True
    
    def _normalize_name(self, name: str) -> str:
        """Normalizar nombre para comparaci√≥n"""
        # Remover acentos
        normalized = unicodedata.normalize('NFD', name)
        normalized = ''.join(c for c in normalized if unicodedata.category(c) != 'Mn')
        
        # Convertir a min√∫sculas para comparaci√≥n
        return normalized.lower().strip()
    
    def _get_context(self, text: str, start: int, end: int, window: int = 50) -> str:
        """Obtener contexto alrededor de una coincidencia"""
        context_start = max(0, start - window)
        context_end = min(len(text), end + window)
        
        context = text[context_start:context_end]
        
        # Agregar indicadores si se trunc√≥
        if context_start > 0:
            context = "..." + context
        if context_end < len(text):
            context = context + "..."
        
        return context.replace('\n', ' ').strip()

# === FUNCIONES DE UTILIDAD PARA INTEGRACI√ìN ===

def detect_author_in_segments(segments: List[Dict[str, Any]], 
                            profile_type: str,
                            config: Optional[Dict[str, Any]] = None,
                            document_title: Optional[str] = None,
                            source_file_path: Optional[str] = None) -> Optional[Dict[str, Any]]:
    """
    Funci√≥n de conveniencia para detectar autor en segmentos.
    
    Args:
        segments: Lista de segmentos procesados
        profile_type: Tipo de perfil ('verso' o 'prosa')
        config: Configuraci√≥n opcional del detector
        document_title: T√≠tulo del documento (opcional)
        source_file_path: Ruta del archivo fuente (opcional)
        
    Returns:
        Informaci√≥n del autor detectado o None
    """
    # Usar detector h√≠brido si est√° disponible y habilitado
    use_hybrid = config.get('use_hybrid_detection', True) if config else True
    
    if use_hybrid:
        try:
            from .hybrid_author_detection import HybridAuthorDetector
            hybrid_detector = HybridAuthorDetector(config)
            result = hybrid_detector.detect_author(segments, profile_type)
            if result:
                return result
        except ImportError:
            # Si no est√° disponible el detector h√≠brido, usar el mejorado
            pass
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.warning(f"Error en detector h√≠brido, usando detector est√°ndar: {e}")
    
    # Fallback al detector mejorado
    try:
        enhanced_detector = EnhancedContextualAuthorDetector(config)
        return enhanced_detector.detect_author(segments, profile_type, document_title, source_file_path)
    except Exception:
        # Fallback final al detector b√°sico
        detector = AutorDetector(config)
        return detector.detect_author(segments, profile_type, document_title, source_file_path)

def get_author_detection_config(profile_type: str) -> Dict[str, Any]:
    """
    Obtener configuraci√≥n por defecto para detecci√≥n de autores.
    
    Args:
        profile_type: Tipo de perfil ('verso' o 'prosa')
        
    Returns:
        Configuraci√≥n recomendada
    """
    base_config = {
        'confidence_threshold': 0.6,
        'debug': False
    }
    
    if profile_type == 'verso':
        base_config.update({
            'confidence_threshold': 0.5,  # M√°s permisivo para poes√≠a
        })
    elif profile_type == 'prosa':
        base_config.update({
            'confidence_threshold': 0.7,  # M√°s estricto para prosa
        })
    
    return base_config