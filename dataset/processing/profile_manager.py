import os
import yaml
import logging
from typing import Dict, List, Any, Optional, Type, Tuple
from pathlib import Path
import dataclasses
from datetime import datetime, timezone
import uuid
import importlib
import re

from langdetect import detect, LangDetectException
from dataset.scripts.data_models import ProcessedContentItem, BatchContext
from .author_detection import detect_author_in_segments

# Importar detector de perfiles autom√°tico
try:
    from .profile_detector import ProfileDetector, detect_profile_for_file, get_profile_detection_config
    PROFILE_DETECTION_AVAILABLE = True
except ImportError:
    ProfileDetector = None
    detect_profile_for_file = None
    get_profile_detection_config = None
    PROFILE_DETECTION_AVAILABLE = False

# RECARGA FORZADA DE M√ìDULOS MODIFICADOS - V7.0
import dataset.processing.segmenters.heading_segmenter
importlib.reload(dataset.processing.segmenters.heading_segmenter)

import dataset.processing.loaders.pdf_loader
importlib.reload(dataset.processing.loaders.pdf_loader)

import dataset.processing.pre_processors.common_block_preprocessor
importlib.reload(dataset.processing.pre_processors.common_block_preprocessor)

from .segmenters.base import BaseSegmenter
from .segmenters.verse_segmenter import VerseSegmenter
from .segmenters.heading_segmenter import HeadingSegmenter
from .loaders import BaseLoader, MarkdownLoader, NDJSONLoader, JSONLoader, DocxLoader, txtLoader, PDFLoader, ExcelLoader, CSVLoader
from .pre_processors import CommonBlockPreprocessor

# Importar m√≥dulos de deduplicaci√≥n y modos de salida (opcional)
try:
    from .deduplication import get_dedup_manager
    from .output_modes import create_serializer, OutputMode
    from .dedup_config import get_config_manager, is_deduplication_enabled_for_mode
    DEDUPLICATION_AVAILABLE = True
except ImportError as e:
    get_dedup_manager = None
    create_serializer = None
    OutputMode = None
    get_config_manager = None
    is_deduplication_enabled_for_mode = None
    DEDUPLICATION_AVAILABLE = False

# A medida que se implementen, importar otros componentes:
# from .loaders.base import BaseLoader
# from .exporters.base import BaseExporter
# etc.

class ProfileManager:
    """
    Gestor del sistema de perfiles de procesamiento.
    
    Esta clase coordina todo el pipeline de procesamiento:
    - Carga de perfiles YAML
    - Selecci√≥n del loader adecuado
    - Inicializaci√≥n de segmentadores con sus configuraciones
    - Ejecuci√≥n de post-procesadores
    - Exportaci√≥n de resultados
    """
    
    def __init__(self, profiles_dir: str = None):
        """
        Inicializa el gestor de perfiles.
        
        Args:
            profiles_dir: Directorio donde se encuentran los perfiles YAML
        """
        self.logger = logging.getLogger(__name__)
        
        # Directorio de perfiles por defecto
        if profiles_dir is None:
            module_dir = os.path.dirname(os.path.abspath(__file__))
            profiles_dir = os.path.join(os.path.dirname(module_dir), 'config', 'profiles')
        
        self.profiles_dir = profiles_dir
        self.profiles = {}
        self._segmenter_registry = {}
        self._loader_registry = {}
        
        # Registrar componentes disponibles
        self.register_default_components()
        
        # Cargar perfiles desde directorio
        self.load_profiles()
    
    def reload_custom_segmenters(self):
        """Recarga los segmentadores personalizados. √ötil despu√©s de generar nuevos segmentadores."""
        self.logger.info("Recargando segmentadores personalizados...")
        self._load_custom_segmenters()
    
    def register_default_components(self):
        """Registra los componentes por defecto del sistema."""
        # Registrar segmentadores
        self.register_segmenter('verse', VerseSegmenter)
        self.register_segmenter('heading', HeadingSegmenter)
        
        # Registrar MarkdownSegmenter manualmente
        try:
            from .segmenters.markdown_segmenter import MarkdownSegmenter
            self.register_segmenter('markdown', MarkdownSegmenter)
            self.logger.info("[OK] MarkdownSegmenter registrado manualmente")
        except Exception as e:
            self.logger.warning(f"[WARN] No se pudo registrar MarkdownSegmenter: {e}")
        
        # Registrar MarkdownVerseSegmenter manualmente
        try:
            from .segmenters.markdown_verse_segmenter import MarkdownVerseSegmenter
            self.register_segmenter('markdown_verse', MarkdownVerseSegmenter)
            self.logger.info("[OK] MarkdownVerseSegmenter registrado manualmente")
        except Exception as e:
            self.logger.warning(f"[WARN] No se pudo registrar MarkdownVerseSegmenter: {e}")
        
        # Cargar segmentadores personalizados din√°micamente
        self._load_custom_segmenters()
        
        # Registrar loaders
        self.register_loader('.md', MarkdownLoader)
        self.register_loader('.markdown', MarkdownLoader)
        self.register_loader('.ndjson', NDJSONLoader)
        self.register_loader('.docx', DocxLoader)
        self.register_loader('.txt', txtLoader)
        self.register_loader('.pdf', PDFLoader)
        self.register_loader('.xls', ExcelLoader)
        self.register_loader('.xlsx', ExcelLoader)
        self.register_loader('.xlsm', ExcelLoader)
        self.register_loader('.csv', CSVLoader)  # Usando CSVLoader espec√≠fico para CSV
        self.register_loader('.tsv', CSVLoader)  # Tambi√©n para archivos TSV (valores separados por tabulaciones)
        self.register_loader('.json', JSONLoader)
        
        # Registrar MarkdownPDFLoader manualmente
        try:
            from .loaders.markdown_pdf_loader import MarkdownPDFLoader
            self.register_loader('.pdf_markdown', MarkdownPDFLoader)  # Extensi√≥n especial
            self.logger.info("[OK] MarkdownPDFLoader registrado manualmente")
        except Exception as e:
            self.logger.warning(f"[WARN] No se pudo registrar MarkdownPDFLoader: {e}")
    
    def register_segmenter(self, name: str, segmenter_class: Type[BaseSegmenter]):
        """
        Registra un segmentador en el sistema.
        
        Args:
            name: Nombre para referencia en perfiles
            segmenter_class: Clase del segmentador
        """
        self._segmenter_registry[name] = segmenter_class
        self.logger.debug(f"Registrado segmentador '{name}'")
    
    def register_loader(self, extension: str, loader_class: Type[BaseLoader]):
        """
        Registra un loader para una extensi√≥n de archivo.
        
        Args:
            extension: Extensi√≥n del archivo (con punto)
            loader_class: Clase del loader
        """
        self._loader_registry[extension.lower()] = loader_class
        self.logger.debug(f"Registrado loader para {extension}")
    
    def _load_custom_segmenters(self):
        """Carga din√°micamente segmentadores personalizados desde el directorio de segmentadores."""
        import importlib.util
        import inspect
        
        # Directorio donde se guardan los segmentadores personalizados
        segmenters_dir = os.path.join(os.path.dirname(__file__), 'segmenters')
        
        if not os.path.exists(segmenters_dir):
            return
        
        # Buscar archivos Python que terminen en '_segmenter.py'
        for filename in os.listdir(segmenters_dir):
            if filename.endswith('_segmenter.py') and filename not in ['base.py', 'verse_segmenter.py', 'heading_segmenter.py']:
                module_path = os.path.join(segmenters_dir, filename)
                module_name = filename[:-3]  # Remover .py
                
                try:
                    # Cargar el m√≥dulo din√°micamente
                    spec = importlib.util.spec_from_file_location(module_name, module_path)
                    if spec and spec.loader:
                        module = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(module)
                        
                        # Buscar clases que hereden de BaseSegmenter
                        for name, obj in inspect.getmembers(module, inspect.isclass):
                            if (obj != BaseSegmenter and 
                                issubclass(obj, BaseSegmenter) and 
                                hasattr(obj, '__module__') and 
                                obj.__module__ == module_name):
                                
                                # Registrar el segmentador usando el nombre del archivo sin '_segmenter'
                                segmenter_name = module_name.replace('_segmenter', '')
                                self.register_segmenter(segmenter_name, obj)
                                self.logger.info(f"Cargado segmentador personalizado: {segmenter_name} -> {name}")
                                
                except Exception as e:
                    self.logger.warning(f"No se pudo cargar segmentador personalizado {filename}: {str(e)}")
    
    def get_loader_for_file(self, file_path: str, profile_name: str = None) -> Optional[tuple]:
        """
        Obtiene el loader apropiado para un archivo y determina el tipo de contenido.
        
        Args:
            file_path: Ruta al archivo
            profile_name: Nombre del perfil (opcional)
            
        Returns:
            Tuple con (clase del loader, tipo de contenido) o None si no hay loader registrado
        """
        extension = Path(file_path).suffix.lower()
        
        # SPECIAL CASE: Si es perfil verso O prosa y archivo PDF, usar MarkdownPDFLoader
        if profile_name in ['verso', 'prosa'] and extension == '.pdf':
            try:
                from .loaders.markdown_pdf_loader import MarkdownPDFLoader
                self.logger.info(f"[PDF] Usando MarkdownPDFLoader para perfil {profile_name}")
                loader_class = MarkdownPDFLoader
            except ImportError:
                self.logger.warning("[WARN] MarkdownPDFLoader no disponible, usando PDFLoader tradicional")
                loader_class = self._loader_registry.get(extension)
        else:
            loader_class = self._loader_registry.get(extension)
        
        if not loader_class:
            self.logger.error(f"No hay loader registrado para extensi√≥n: {extension}")
            return None
        
        # Determinar el tipo de contenido basado en el perfil si est√° disponible
        content_type = 'escritos'  # Valor por defecto
        
        if profile_name and profile_name in self.profiles:
            profile = self.profiles[profile_name]
            
            # Si el perfil especifica un tipo de contenido por defecto, usarlo
            if 'default_content_type' in profile:
                content_type = profile['default_content_type']
            
            # Si el perfil tiene mapeo de extensiones a tipos, usarlo
            elif 'extension_types' in profile and extension in profile['extension_types']:
                content_type = profile['extension_types'][extension]
            
            # O inferir basado en el nombre del perfil
            elif 'poem' in profile_name or 'lyric' in profile_name:
                content_type = 'poemas' if 'poem' in profile_name else 'canciones'
        
        # Tambi√©n podemos intentar inferir del nombre del archivo
        filename = Path(file_path).stem.lower()
        if not profile_name or content_type == 'escritos':
            if any(word in filename for word in ['poema', 'poem', 'verso']):
                content_type = 'poemas'
            elif any(word in filename for word in ['cancion', 'song', 'lyric']):
                content_type = 'canciones'
        
        self.logger.debug(f"Loader para {extension}: {loader_class.__name__}, tipo: {content_type}")
        return (loader_class, content_type)
        
    def load_profiles(self):
        """Carga todos los perfiles YAML del directorio configurado y subcarpetas."""
        if not os.path.exists(self.profiles_dir):
            self.logger.warning(f"Directorio de perfiles no encontrado: {self.profiles_dir}")
            return

        # Cargar perfiles del directorio ra√≠z (compatibilidad)
        self._load_profiles_from_directory(self.profiles_dir)
        
        # Cargar perfiles de subcarpetas (core, special)
        for subdir in ['core', 'special']:
            subdir_path = os.path.join(self.profiles_dir, subdir)
            if os.path.exists(subdir_path):
                self._load_profiles_from_directory(subdir_path, category=subdir)
    
    def _load_profiles_from_directory(self, directory: str, category: str = None):
        """Carga perfiles de un directorio espec√≠fico."""
        for filename in os.listdir(directory):
            if filename.endswith(('.yaml', '.yml')):
                profile_path = os.path.join(directory, filename)
                try:
                    with open(profile_path, 'r', encoding='utf-8') as f:
                        profile = yaml.safe_load(f)
                    
                    if 'name' in profile:
                        # Agregar categor√≠a al perfil para organizaci√≥n
                        if category:
                            profile['_category'] = category
                        self.profiles[profile['name']] = profile
                        category_info = f" ({category})" if category else ""
                        self.logger.info(f"Cargado perfil: {profile['name']}{category_info}")
                    else:
                        self.logger.warning(f"Perfil sin nombre en {filename}")
                except Exception as e:
                    self.logger.error(f"Error al cargar perfil {filename}: {str(e)}")
    
    def get_profile(self, profile_name: str) -> Optional[Dict[str, Any]]:
        """
        Obtiene un perfil por su nombre.
        
        Args:
            profile_name: Nombre del perfil
            
        Returns:
            Diccionario con la configuraci√≥n del perfil o None si no existe
        """
        return self.profiles.get(profile_name)
    
    def list_profiles(self) -> List[Dict[str, Any]]:
        """
        Lista todos los perfiles disponibles.
        
        Returns:
            Lista con informaci√≥n b√°sica de cada perfil
        """
        return [
            {
                'name': profile['name'],
                'description': profile.get('description', ''),
                'segmenter': profile.get('segmenter', 'unknown'),
                'file_types': profile.get('file_types', [])
            }
            for profile in self.profiles.values()
        ]
    
    def create_segmenter(self, profile_name: str, file_path: str = None) -> Optional[BaseSegmenter]:
        """
        Crea una instancia de segmentador seg√∫n el perfil.
        
        Args:
            profile_name: Nombre del perfil a usar
            file_path: Ruta del archivo (para detectar si usar MarkdownSegmenter)
            
        Returns:
            Instancia del segmentador configurada o None si hay error
        """
        profile = self.get_profile(profile_name)
        if not profile:
            self.logger.error(f"Perfil no encontrado: {profile_name}")
            return None
        
        segmenter_type = profile.get('segmenter')
        if not segmenter_type:
            self.logger.error(f"Tipo de segmentador no especificado en perfil: {profile_name}")
            return None
        
        # [OK] CORREGIDO: El segmentador se determina por el TIPO DE CONTENIDO (prosa/verso)
        # NO por el formato de archivo. La conversi√≥n PDF ‚Üí Markdown es solo para preservar estructura visual.
        # Cada perfil debe especificar su segmentador correcto en su configuraci√≥n.
        
        if segmenter_type not in self._segmenter_registry:
            self.logger.error(f"Segmentador '{segmenter_type}' no registrado")
            return None
        
        # Configuraci√≥n para el segmentador
        config = {}
        
        # Usar configuraci√≥n espec√≠fica del segmentador si existe
        if 'segmenter_config' in profile:
            config.update(profile['segmenter_config'])
        
        # Mantener compatibilidad con configuraci√≥n legacy
        # Copiar thresholds del perfil (si no est√°n en segmenter_config)
        if 'thresholds' in profile and 'thresholds' not in config:
            config['thresholds'] = profile['thresholds']
        
        # Copiar patrones espec√≠ficos (si no est√°n en segmenter_config)
        for key in ['title_patterns', 'paragraph_patterns', 'section_patterns']:
            if key in profile and key not in config:
                config[key] = profile[key]
        
        # IMPORTANTE: Copiar configuraci√≥n de author_detection
        if 'author_detection' in profile:
            config['author_detection'] = profile['author_detection']
        
        # Crear instancia del segmentador
        try:
            segmenter_class = self._segmenter_registry[segmenter_type]
            return segmenter_class(config)
        except Exception as e:
            self.logger.error(f"Error al crear segmentador '{segmenter_type}': {str(e)}")
            return None
    
    def _create_processed_content_item(self,
                                      processed_document_metadata: Dict[str, Any],
                                      segment_data: Dict[str, Any],
                                      file_path: str,
                                      language_override: Optional[str] = None,
                                      author_override: Optional[str] = None,
                                      detected_lang: Optional[str] = None,
                                      segment_index: int = 0,
                                      job_config_dict: Optional[Dict[str, Any]] = None,
                                      segmenter_name: str = "unknown",
                                      main_document_author_name: Optional[str] = None,
                                      main_author_detection_info: Optional[Dict[str, Any]] = None,
                                      file_document_id: Optional[str] = None) -> ProcessedContentItem:
        """
        [CONFIG] FUNCI√ìN UNIFICADA SIMPLIFICADA: Crea ProcessedContentItem con estructura limpia en ingl√©s.
        
        Elimina duplicaciones y campos innecesarios. Metadatos consolidados sin redundancia.
        """
        current_timestamp_iso = datetime.now(timezone.utc).isoformat()
        
        # 1. DETERMINAR IDIOMA (con prioridad correcta)
        final_language = None
        if language_override and language_override.strip():
            # Validar c√≥digo de idioma
            lang_clean = language_override.strip().lower()
            if 2 <= len(lang_clean) <= 5 and lang_clean.replace('-', '').isalpha():
                final_language = lang_clean
                self.logger.info(f"Usando idioma forzado: {final_language}")
            else:
                self.logger.warning(f"C√≥digo de idioma inv√°lido: '{language_override}'. Usando detecci√≥n autom√°tica.")
        
        if not final_language:
            final_language = detected_lang or processed_document_metadata.get('language', 'unknown')
            if final_language and final_language != 'unknown':
                self.logger.debug(f"Usando idioma detectado: {final_language}")
        
        if not final_language or final_language == 'unknown':
            final_language = 'unknown'
        
        # 2. DETERMINAR AUTOR (con prioridad correcta: author_override > main_document_author_name > fallback)
        final_author = None
        if author_override and author_override.strip():
            author_clean = author_override.strip()
            if len(author_clean) <= 200:
                # Limpiar caracteres problem√°ticos
                import re
                final_author = re.sub(r'[<>|:*?"\\\/\n\r\t]', '', author_clean).strip()
                if final_author:
                    self.logger.info(f"Usando autor forzado: {final_author}")
                else:
                    self.logger.warning("Autor forzado qued√≥ vac√≠o despu√©s de limpieza")
            else:
                final_author = author_clean[:200].strip()
                self.logger.warning(f"Autor forzado truncado a 200 caracteres: {final_author}")
        
        if not final_author and main_document_author_name:
            final_author = main_document_author_name
            self.logger.debug(f"Usando autor principal del documento: {final_author}")
        
        if not final_author:
            final_author = processed_document_metadata.get('autor_documento') or processed_document_metadata.get('author')
            if final_author:
                self.logger.debug(f"Usando autor de fallback: {final_author}")
        
        # 3. CONSOLIDAR METADATOS ADICIONALES (sin duplicaciones)
        # Campos que YA est√°n como campos principales - NO incluir en additional_metadata
        main_fields = {
            'segment_id', 'document_id', 'document_language', 'text', 'segment_type', 
            'segment_order', 'text_length', 'processing_timestamp', 'source_file_path',
            'document_hash', 'document_title', 'document_author', 'publication_date',
            'publisher', 'isbn', 'pipeline_version', 'segmenter_used',
            # Tambi√©n excluir variantes en espa√±ol/ingl√©s para evitar duplicaci√≥n
            'ruta_archivo_original', 'source_file_path', 'ruta_archivo',
            'hash_documento_original', 'titulo_documento', 'author', 'autor_documento',
            'fecha_publicacion_documento', 'editorial_documento', 'isbn_documento', 
            'idioma_documento', 'language', 'file_format', 'extension_archivo',
            'nombre_archivo'
        }
        
        # Construir metadatos adicionales √öNICAMENTE con campos que no est√°n como principales
        additional_metadata_clean = {}
        
        # Agregar informaci√≥n de job si existe
        if job_config_dict:
            if job_config_dict.get("job_id"):
                additional_metadata_clean["job_id"] = job_config_dict["job_id"]
            if job_config_dict.get("origin_type_name"):
                additional_metadata_clean["job_origin_type"] = job_config_dict["origin_type_name"]
        
        # Agregar informaci√≥n de detecci√≥n de autor principal para trazabilidad
        if main_author_detection_info:
            additional_metadata_clean["main_author_detection_info"] = main_author_detection_info
        
        # Agregar solo campos verdaderamente adicionales del documento
        for key, value in processed_document_metadata.items():
            if key not in main_fields and key != 'metadatos_adicionales_fuente' and value is not None:
                # Usar nombres m√°s descriptivos para campos del documento
                if key not in ['author', 'language', 'file_format']:  # Skip common duplicates
                    additional_metadata_clean[f"doc_{key}"] = value
        
        # 4. EXTRAER DATOS DEL SEGMENTO (simplificado)
        if isinstance(segment_data, dict):
            text_content = segment_data.get('text', '') or segment_data.get('texto', '')
            segment_type = segment_data.get('type', 'text_block') 
            segment_order = segment_data.get('order_in_document', segment_index + 1)
            
            # Incluir metadata de detecci√≥n de autor a nivel de segmento (NO para document_author principal)
            segment_metadata = segment_data.get('metadata', {})
            if segment_metadata.get('detected_author'):
                additional_metadata_clean['segment_detected_author'] = segment_metadata['detected_author']
                additional_metadata_clean['segment_author_confidence'] = segment_metadata.get('author_confidence')
                additional_metadata_clean['segment_author_detection_method'] = segment_metadata.get('author_detection_method')
                if segment_metadata.get('author_detection_details'):
                    additional_metadata_clean['segment_author_detection_details'] = segment_metadata['author_detection_details']
            
            # NUEVO: Incluir informaci√≥n de p√°gina original si est√° disponible
            if segment_metadata.get('page'):
                additional_metadata_clean['originalPage'] = segment_metadata['page']
            elif segment_data.get('page'):
                additional_metadata_clean['originalPage'] = segment_data['page']
        else:
            text_content = str(segment_data) if segment_data else ''
            segment_type = 'text_block'
            segment_order = segment_index + 1
        
        # 5. CREAR ProcessedContentItem CON ESTRUCTURA LIMPIA
        # [CONFIG] CORREGIDO: Usar file_document_id consistente para todos los segmentos del mismo archivo
        final_document_id = file_document_id or processed_document_metadata.get('hash_documento_original') or processed_document_metadata.get('document_hash')
        if not final_document_id:
            # Generar ID consistente basado en la ruta del archivo (mismo ID para el mismo archivo)
            final_document_id = str(uuid.uuid5(uuid.NAMESPACE_URL, str(Path(file_path).absolute())))
        
        return ProcessedContentItem(
            segment_id=str(uuid.uuid4()),
            document_id=final_document_id,
            document_language=final_language,
            text=text_content,
            segment_type=segment_type,
            segment_order=segment_order,
            text_length=len(text_content),
            processing_timestamp=current_timestamp_iso,
            source_file_path=str(Path(file_path).absolute()),
            document_hash=processed_document_metadata.get('hash_documento_original') or processed_document_metadata.get('document_hash'),
            document_title=processed_document_metadata.get('titulo_documento') or processed_document_metadata.get('document_title') or Path(file_path).stem,
            document_author=final_author,
            publication_date=processed_document_metadata.get('fecha_publicacion_documento') or processed_document_metadata.get('publication_date'),
            publisher=processed_document_metadata.get('editorial_documento') or processed_document_metadata.get('publisher'),
            isbn=processed_document_metadata.get('isbn_documento') or processed_document_metadata.get('isbn'),
            additional_metadata=additional_metadata_clean,
            pipeline_version="profile_manager_v4.0_english_clean",
            segmenter_used=segmenter_name
        )

    def process_file(self, 
                    file_path: str, 
                    profile_name: str, 
                    output_file: Optional[str] = None,
                    encoding: str = 'utf-8',
                    force_content_type: Optional[str] = None,
                    confidence_threshold: float = 0.5,
                    job_config_dict: Optional[Dict[str, Any]] = None,
                    language_override: Optional[str] = None,
                    author_override: Optional[str] = None,
                    output_format: str = "ndjson",
                    folder_structure_info: Optional[Dict[str, Any]] = None,
                    output_mode: str = "biblioperson") -> tuple:
        """
        Procesa un archivo completo usando un perfil.
        
        Args:
            file_path: Ruta al archivo a procesar
            profile_name: Nombre del perfil a usar
            output_file: Archivo para guardar resultados (opcional)
            encoding: Codificaci√≥n para abrir el archivo (por defecto utf-8)
            force_content_type: Forzar un tipo espec√≠fico de contenido (ignora detecci√≥n autom√°tica)
            confidence_threshold: Umbral de confianza para detecci√≥n de poemas (0.0-1.0)
            job_config_dict: Diccionario con la configuraci√≥n del job actual (opcional)
            language_override: C√≥digo de idioma para override (opcional)
            author_override: Nombre del autor para override (opcional)
            output_format: Formato de salida ("ndjson" o "json")
            folder_structure_info: Informaci√≥n sobre la estructura de carpetas del archivo (opcional)
            output_mode: Modo de salida ("generic" o "biblioperson")
            
        Returns:
            Tuple con: (Lista de unidades procesadas, Estad√≠sticas del segmentador, Metadatos del documento)
        """
        
        # [DEBUG] LOGGING DETALLADO PARA DEBUG DE EXPORTACI√ìN
        self.logger.warning(f"[DEBUG] INICIO process_file - output_file recibido: '{output_file}'")
        self.logger.warning(f"[DEBUG] INICIO process_file - output_format: '{output_format}'")
        print(f"[DEBUG] INICIO process_file - output_file recibido: '{output_file}'")
        print(f"[DEBUG] INICIO process_file - output_format: '{output_format}'")
        
        if not os.path.exists(file_path):
            self.logger.error(f"Archivo no encontrado: {file_path}")
            # Devolver la estructura de tupla esperada por process_file.py
            return [], {}, {'error': f"Archivo no encontrado: {file_path}"}
        
        # [CONFIG] GENERAR DOCUMENT_ID √öNICO PARA TODO EL ARCHIVO
        # Este ID ser√° compartido por todos los segmentos del mismo archivo
        file_document_id = None
        
        # [CONFIG] SISTEMA DE DEDUPLICACI√ìN (opcional y configurable)
        document_hash = None
        if DEDUPLICATION_AVAILABLE and is_deduplication_enabled_for_mode(output_mode.lower()):
            try:
                config_manager = get_config_manager()
                dedup_config = config_manager.get_deduplication_config()
                
                # Verificar compatibilidad del perfil y formato de archivo
                if (config_manager.is_profile_supported(profile_name) and 
                    config_manager.is_file_format_supported(file_path)):
                    
                    dedup_manager = get_dedup_manager()
                    document_hash, is_new_document = dedup_manager.check_and_register(file_path)
                    
                    if not is_new_document:
                        # Documento duplicado detectado
                        duplicate_info = dedup_manager.get_duplicate_info(document_hash)
                        self.logger.warning(f"[RETRY] Documento duplicado detectado: {Path(file_path).name}")
                        self.logger.warning(f"   Original procesado: {duplicate_info['first_seen']}")
                        self.logger.warning(f"   Ruta original: {duplicate_info['file_path']}")
                        
                        # Devolver informaci√≥n del duplicado en lugar de procesar
                        return [], {}, {
                            'duplicate_detected': True,
                            'document_hash': document_hash,
                            'original_file_path': duplicate_info['file_path'],
                            'first_seen': duplicate_info['first_seen'],
                            'current_file_path': str(Path(file_path).absolute()),
                            'message': f"Documento duplicado. Original procesado el {duplicate_info['first_seen']}"
                        }
                    else:
                        self.logger.info(f"[OK] Documento nuevo registrado: {document_hash[:8]}...")
                        # Usar el hash de deduplicaci√≥n como document_id
                        file_document_id = document_hash
                else:
                    if dedup_config.warn_when_disabled:
                        self.logger.info(f"[INFO] Deduplicaci√≥n no aplicable para perfil '{profile_name}' o formato '{Path(file_path).suffix}'")
                    
            except Exception as e:
                config_manager = get_config_manager() if get_config_manager else None
                dedup_config = config_manager.get_deduplication_config() if config_manager else None
                
                if dedup_config and dedup_config.log_errors:
                    self.logger.error(f"Error en sistema de deduplicaci√≥n: {str(e)}")
                
                if not dedup_config or dedup_config.continue_on_error:
                    # Continuar procesamiento sin deduplicaci√≥n en caso de error
                    document_hash = None
                    if dedup_config and dedup_config.log_errors:
                        self.logger.info("Continuando procesamiento sin deduplicaci√≥n debido al error")
                else:
                    # Re-lanzar el error si la configuraci√≥n no permite continuar
                    raise
        elif not DEDUPLICATION_AVAILABLE:
            self.logger.debug("Sistema de deduplicaci√≥n no disponible (m√≥dulos no importados)")
        else:
            self.logger.debug(f"Deduplicaci√≥n deshabilitada para modo '{output_mode}'")
            config_manager = get_config_manager() if get_config_manager else None
            if config_manager:
                dedup_config = config_manager.get_deduplication_config()
                if dedup_config.warn_when_disabled:
                    self.logger.info(f"[INFO] Deduplicaci√≥n deshabilitada para modo '{output_mode}'")
        
        # [DEBUG] DETECCI√ìN AUTOM√ÅTICA DE PERFIL
        if profile_name == "autom√°tico":
            self.logger.info(f"[DEBUG] INICIANDO DETECCI√ìN AUTOM√ÅTICA DE PERFIL: {Path(file_path).name}")
            
            # Para PDFs, extraer contenido preservando estructura original con pymupdf
            content_sample = None
            if file_path.lower().endswith('.pdf'):
                try:
                    # Usar pymupdf para extraer markdown preservando estructura visual
                    import fitz  # pymupdf
                    
                    self.logger.debug(f"[DEBUG] Extrayendo contenido con pymupdf para preservar estructura original...")
                    
                    doc = fitz.open(file_path)
                    markdown_content = ""
                    
                    # Extraer las primeras p√°ginas para an√°lisis (suficiente para detecci√≥n)
                    max_pages = min(5, len(doc))
                    
                    for page_num in range(max_pages):
                        page = doc.load_page(page_num)
                        
                        # Extraer como markdown preservando estructura visual
                        page_markdown = page.get_text("markdown")
                        
                        if page_markdown.strip():
                            # Verificar corrupci√≥n en el contenido de la p√°gina
                            corruption_ratio = self._detect_text_corruption(page_markdown)
                            
                            if corruption_ratio > 0.3:
                                self.logger.debug(f"[SKIP] Saltando p√°gina {page_num + 1} (corrupci√≥n: {corruption_ratio:.1%})")
                                continue
                            
                            markdown_content += page_markdown + "\n\n"
                    
                    doc.close()
                    
                    # Usar el contenido markdown como muestra para detecci√≥n
                    content_sample = markdown_content.strip()
                    
                    self.logger.debug(f"[DEBUG] Contenido markdown extra√≠do: {len(content_sample)} caracteres")
                    
                    # DEBUG: Mostrar muestra del contenido markdown
                    if content_sample:
                        lines = content_sample.split('\n')
                        self.logger.debug(f"[DEBUG] DEBUG MARKDOWN: {len(lines)} l√≠neas totales")
                        self.logger.debug(f"[DEBUG] DEBUG PRIMERAS 3 L√çNEAS:")
                        for i, line in enumerate(lines[:3]):
                            self.logger.debug(f"[DEBUG]   [{i+1}]: '{line}'")
                    
                except Exception as e:
                    self.logger.warning(f"[WARN] Error extrayendo contenido markdown para detecci√≥n: {str(e)}")
            
            # Detectar perfil autom√°ticamente
            detected_profile = self.get_profile_for_file(file_path, content_sample)
            if detected_profile:
                profile_name = detected_profile
                self.logger.info(f"[OK] PERFIL AUTO-DETECTADO: '{profile_name}' para {Path(file_path).name}")
            else:
                # Fallback a prosa si no se puede detectar
                profile_name = "prosa"
                self.logger.warning(f"[WARN] No se pudo detectar perfil, usando fallback: '{profile_name}'")
        
        # 1. Obtener loader apropiado y tipo de contenido
        loader_result = self.get_loader_for_file(file_path, profile_name)
        if not loader_result:
            return [], {}, {'error': f"No se pudo obtener el loader para: {file_path}"}
            
        loader_class, content_type = loader_result
        
        # Si el usuario forz√≥ un tipo espec√≠fico, usarlo
        if force_content_type:
            self.logger.info(f"Forzando tipo de contenido: {force_content_type}")
            content_type = force_content_type
            
        # 2. Crear loader e intentar cargar el archivo
        raw_blocks: List[Dict[str, Any]] = []
        raw_document_metadata: Dict[str, Any] = {}
        try:
            self.logger.info(f"Usando loader: {loader_class.__name__}")
            
            # Si es JSONLoader y el perfil tiene configuraci√≥n JSON, usarla
            loader_kwargs = {'encoding': encoding}
            if loader_class.__name__ == 'JSONLoader':
                profile = self.get_profile(profile_name)
                
                # [CONFIG] PRIORIDAD: job_config_dict > perfil
                json_config = None
                
                # Primero, intentar obtener configuraci√≥n del job_config_dict (desde GUI)
                if job_config_dict and 'json_config' in job_config_dict:
                    json_config = job_config_dict['json_config']
                    self.logger.info(f"üì± Aplicando configuraci√≥n JSON desde GUI: {len(json_config.get('filter_rules', []))} reglas")
                
                # Si no hay configuraci√≥n desde GUI, usar la del perfil
                elif profile and 'json_config' in profile:
                    json_config = profile['json_config']
                    self.logger.info(f"‚öôÔ∏è Aplicando configuraci√≥n JSON del perfil: {profile_name}")
                
                # Aplicar la configuraci√≥n JSON encontrada
                if json_config:
                    loader_kwargs.update(json_config)
                    self.logger.debug(f"[CONFIG] Configuraci√≥n JSON aplicada: {json_config}")
                else:
                    self.logger.info(f"üìÑ JSONLoader sin configuraci√≥n espec√≠fica - usando valores por defecto")
            
            loader = loader_class(file_path, **loader_kwargs)
            loaded_data = loader.load()
            
            # Asegurar que las claves existan, incluso si est√°n vac√≠as
            raw_blocks = loaded_data.get('blocks', [])
            raw_document_metadata = loaded_data.get('document_metadata', {})
            raw_document_metadata.setdefault('source_file_path', str(Path(file_path).absolute()))
            raw_document_metadata.setdefault('file_format', Path(file_path).suffix.lower())
            raw_document_metadata.setdefault('profile_used', profile_name)
            
            # A√±adir hash del documento si est√° disponible (modo biblioperson)
            if document_hash:
                raw_document_metadata['document_hash'] = document_hash

            # === AGREGAR INFORMACI√ìN DE ESTRUCTURA DE CARPETAS ===
            if folder_structure_info:
                # Agregar informaci√≥n de estructura de carpetas a los metadatos del documento
                raw_document_metadata.update({
                    "folder_structure": folder_structure_info
                })
                self.logger.debug(f"Informaci√≥n de estructura de carpetas agregada a metadatos: {folder_structure_info}")

            # Si el loader report√≥ un error, lo propagamos antes del pre-procesamiento
            if raw_document_metadata.get('error'):
                self.logger.error(f"Error reportado por loader {loader_class.__name__} para {file_path}: {raw_document_metadata['error']}")
                return [], {}, raw_document_metadata

            if not raw_blocks and not raw_document_metadata.get('warning'): # Si no hay bloques y no es una advertencia de archivo vac√≠o
                self.logger.info(f"Loader {loader_class.__name__} no devolvi√≥ bloques para {file_path} y no hay advertencia de archivo vac√≠o.")

        except Exception as e:
            self.logger.error(f"Excepci√≥n al instanciar o usar loader {loader_class.__name__} para archivo {file_path}: {str(e)}", exc_info=True)
            error_metadata = {
                'source_file_path': str(Path(file_path).absolute()),
                'file_format': Path(file_path).suffix.lower(),
                'error': f"Excepci√≥n en ProfileManager durante la carga con loader: {str(e)}"
            }
            return [], {}, error_metadata
        
        # Para archivos JSON: aplicar pre-procesador para limpieza de caracteres de control
        if file_path.lower().endswith('.json'):
            # CORREGIDO: Los archivos JSON tambi√©n necesitan pre-procesamiento para limpieza de caracteres
            profile = self.get_profile(profile_name)
            preprocessor_config = profile.get('pre_processor_config') if profile else None
            
            # Crear pre-procesador con configuraci√≥n por defecto que incluye limpieza Unicode
            from dataset.processing.pre_processors.common_block_preprocessor import CommonBlockPreprocessor
            common_preprocessor = CommonBlockPreprocessor(config=preprocessor_config)
            
            try:
                self.logger.info(f"üßπ Aplicando limpieza de caracteres de control a archivo JSON: {file_path}")
                processed_blocks, processed_document_metadata = common_preprocessor.process(raw_blocks, raw_document_metadata)
                self.logger.info(f"[OK] Limpieza completada para JSON: {len(processed_blocks)} bloques procesados")
            except Exception as e:
                self.logger.error(f"Error durante limpieza de JSON {file_path}: {str(e)}")
                # En caso de error, usar bloques sin procesar pero registrar el problema
                processed_blocks = raw_blocks
                processed_document_metadata = raw_document_metadata
                processed_document_metadata['preprocessing_error'] = str(e)
            
            # [CONFIG] NUEVO: Detectar idioma tambi√©n para JSON
            detected_lang = None
            if language_override:
                # Validar c√≥digo de idioma antes de usarlo
                if len(language_override.strip()) < 2 or len(language_override.strip()) > 5:
                    self.logger.warning(f"C√≥digo de idioma inv√°lido: '{language_override}' (debe tener 2-5 caracteres). Usando detecci√≥n autom√°tica.")
                elif any(char.isdigit() or not char.isalnum() for char in language_override.strip() if char != '-'):
                    self.logger.warning(f"C√≥digo de idioma contiene caracteres inv√°lidos: '{language_override}'. Usando detecci√≥n autom√°tica.")
                else:
                    detected_lang = language_override.strip().lower()
                    self.logger.info(f"Usando idioma forzado para JSON {file_path}: {detected_lang}")
            
            if not detected_lang and processed_blocks:
                try:
                    # Concatenar texto de los primeros bloques para obtener una muestra representativa
                    sample_texts = []
                    total_chars = 0
                    max_chars = 1000
                    max_blocks = 5
                    
                    for i, block in enumerate(processed_blocks[:max_blocks]):
                        if total_chars >= max_chars:
                            break
                        
                        block_text = block.get('text', '') if isinstance(block, dict) else str(block)
                        if block_text.strip():
                            remaining_chars = max_chars - total_chars
                            if len(block_text) > remaining_chars:
                                block_text = block_text[:remaining_chars]
                            sample_texts.append(block_text.strip())
                            total_chars += len(block_text)
                    
                    sample_text = " ".join(sample_texts).strip()
                    
                    # Intentar detectar idioma si hay suficiente texto
                    if len(sample_text) >= 20:
                        detected_lang = detect(sample_text)
                        self.logger.info(f"Idioma detectado autom√°ticamente para JSON {file_path}: {detected_lang}")
                    else:
                        detected_lang = "und"
                        
                except LangDetectException as e:
                    self.logger.debug(f"No se pudo detectar idioma para JSON {file_path}: {str(e)}")
                    detected_lang = "und"
                except Exception as e:
                    self.logger.warning(f"Error inesperado durante detecci√≥n de idioma para JSON {file_path}: {str(e)}")
                    detected_lang = "und"
            else:
                detected_lang = detected_lang or "und"
            
            segments = []
            
            self.logger.warning(f"[DEBUG] DEBUG: Tenemos {len(processed_blocks)} bloques procesados")
            if processed_blocks:
                self.logger.warning(f"[DEBUG] DEBUG: Estructura del primer bloque: {processed_blocks[0]}")
            
            for i, block in enumerate(processed_blocks):
                # [CONFIG] USAR FUNCI√ìN UNIFICADA: Ahora con detecci√≥n de idioma
                block_with_type = block.copy() if isinstance(block, dict) else {'text': str(block)}
                block_with_type['type'] = 'json_element'
                
                segment = self._create_processed_content_item(
                    processed_document_metadata,
                    block_with_type,
                    file_path,
                    language_override,
                    author_override,
                    detected_lang,
                    i,
                    job_config_dict,
                    "json_direct_conversion",
                    None,  # main_document_author_name - no aplicable para JSON directo
                    None,  # main_author_detection_info - no aplicable para JSON directo
                    file_document_id  # [CONFIG] CORREGIDO: Pasar file_document_id consistente
                )
                segments.append(segment)
            
            segmenter_stats = {
                'json_elements_processed': len(segments),
                'processing_method': 'direct_conversion',
                'bypassed_segmenter': True
            }
            
            # Para JSON: retornar directamente sin procesamiento adicional
            self.logger.info(f"[OK] JSON procesado directamente: {len(segments)} segmentos creados")
            
            # [OK] CORREGIDO: Exportar si se especific√≥ ruta de salida
            self.logger.warning(f"[DEBUG] VERIFICANDO EXPORTACI√ìN - output_file: '{output_file}' (tipo: {type(output_file)})")
            print(f"[DEBUG] VERIFICANDO EXPORTACI√ìN - output_file: '{output_file}' (tipo: {type(output_file)})")
            
            if output_file:
                self.logger.warning(f"üì§ INICIANDO EXPORTACI√ìN - {len(segments)} segmentos JSON a: {output_file}")
                print(f"üì§ INICIANDO EXPORTACI√ìN - {len(segments)} segmentos JSON a: {output_file}")
                try:
                    self._export_results(segments, output_file, processed_document_metadata, output_format, output_mode)
                    self.logger.warning(f"[OK] EXPORTACI√ìN JSON COMPLETADA EXITOSAMENTE")
                    print(f"[OK] EXPORTACI√ìN JSON COMPLETADA EXITOSAMENTE")
                except Exception as e:
                    self.logger.error(f"‚ùå ERROR EN EXPORTACI√ìN JSON: {str(e)}")
                    print(f"‚ùå ERROR EN EXPORTACI√ìN JSON: {str(e)}")
                    self.logger.exception("Detalles del error de exportaci√≥n:")
            else:
                self.logger.warning(f"[WARN] NO SE EXPORTAR√Å - output_file es None o vac√≠o")
                print(f"[WARN] NO SE EXPORTAR√Å - output_file es None o vac√≠o")
            
            return segments, segmenter_stats, processed_document_metadata
            
        else:
            # Para otros archivos: usar CommonBlockPreprocessor normalmente
            profile = self.get_profile(profile_name)  # Obtener perfil para configuraci√≥n del preprocessor
            preprocessor_config = profile.get('pre_processor_config') if profile else None
            self.logger.warning(f"üí• CONFIG PARA CREAR PREPROCESSOR: {preprocessor_config}")
            print(f"üí• CONFIG PARA CREAR PREPROCESSOR: {preprocessor_config}")
            
            # FORZAR RECARGA DE COMMONBLOCKPREPROCESSOR PARA EVITAR CACHE
            import importlib
            import dataset.processing.pre_processors.common_block_preprocessor
            importlib.reload(dataset.processing.pre_processors.common_block_preprocessor)
            from dataset.processing.pre_processors.common_block_preprocessor import CommonBlockPreprocessor
            
            common_preprocessor = CommonBlockPreprocessor(config=preprocessor_config)
            
            try:
                # Debug: verificar que el hash est√© presente antes del preprocessor
                hash_before_preprocessor = raw_document_metadata.get("hash_documento_original")
                self.logger.debug(f"hash_documento_original antes del preprocessor: '{hash_before_preprocessor}' (tipo: {type(hash_before_preprocessor)})")
                self.logger.info(f"Aplicando CommonBlockPreprocessor a {len(raw_blocks)} bloques.")
                processed_blocks, processed_document_metadata = common_preprocessor.process(raw_blocks, raw_document_metadata)
                self.logger.debug(f"CommonBlockPreprocessor finalizado. Bloques resultantes: {len(processed_blocks)}.")
                # Debug: verificar que el hash se preserve despu√©s del preprocessor
                hash_after_preprocessor = processed_document_metadata.get("hash_documento_original")
                self.logger.debug(f"hash_documento_original despu√©s del preprocessor: '{hash_after_preprocessor}' (tipo: {type(hash_after_preprocessor)})")
            except Exception as e:
                self.logger.error(f"Excepci√≥n durante CommonBlockPreprocessor para archivo {file_path}: {str(e)}", exc_info=True)
                # Si el preprocesador falla, usamos los datos crudos del loader pero a√±adimos un error
                existing_error = raw_document_metadata.get('error', '') or ''
                raw_document_metadata['error'] = (existing_error + 
                                                 f"; Excepci√≥n en CommonBlockPreprocessor: {str(e)}").strip('; ')
                # Devolver datos crudos del loader con el error del preprocesador a√±adido
                return raw_blocks, {}, raw_document_metadata 
        
            # Si el preprocesador marc√≥ un error, lo propagamos.
            # Los errores del loader ya se manejaron, as√≠ que esto ser√≠a un error del preprocesador.
            if processed_document_metadata.get('error'):
                self.logger.error(f"Error reportado por CommonBlockPreprocessor para {file_path}: {processed_document_metadata['error']}")
                return [], {}, processed_document_metadata
                
            # 2.5. Detectar autor principal del documento usando EnhancedContextualAuthorDetector
            main_document_author_name = None
            main_author_detection_info = {}
            
            try:
                self.logger.info(f"Detectando autor principal del documento: {file_path}")
                
                # Determinar profile_type basado en profile_name
                profile_type = 'poetry' if 'verso' in profile_name.lower() else 'prose'
                
                # Llamar a la funci√≥n de detecci√≥n de autor
                author_detection_result = detect_author_in_segments(
                    segments=processed_blocks,
                    profile_type=profile_type,
                    document_title=processed_document_metadata.get('titulo_documento', ''),
                    source_file_path=str(file_path)
                )
                
                if author_detection_result and author_detection_result.get('name'):
                    main_document_author_name = author_detection_result['name']
                    main_author_detection_info = {
                        'confidence': author_detection_result.get('confidence', 0.0),
                        'method': author_detection_result.get('method', 'unknown'),
                        'source': author_detection_result.get('source', 'enhanced_contextual')
                    }
                    # Log prominente para mostrar autor detectado por documento
                    filename = Path(file_path).name
                    confidence_pct = main_author_detection_info['confidence'] * 100
                    self.logger.info(f"[TARGET] ===== AUTOR DETECTADO AUTOM√ÅTICAMENTE =====")
                    self.logger.info(f"üìÑ Documento: {filename}")
                    self.logger.info(f"‚úçÔ∏è  Autor: {main_document_author_name}")
                    self.logger.info(f"üìä Confianza: {confidence_pct:.1f}% ({main_author_detection_info['confidence']:.3f})")
                    self.logger.info(f"[DEBUG] M√©todo: {main_author_detection_info['method']}")
                    self.logger.info(f"[TARGET] ============================================")
                    self.logger.info(f"")
                    # --- COPIAR INMEDIATAMENTE A processed_document_metadata ---
                    processed_document_metadata['author'] = main_document_author_name
                    processed_document_metadata['author_confidence'] = main_author_detection_info.get('confidence', 0.0)
                    processed_document_metadata['author_detection_method'] = main_author_detection_info.get('method', 'unknown')
                    processed_document_metadata['author_detection_source'] = main_author_detection_info.get('source', 'enhanced_contextual')
                else:
                    filename = Path(file_path).name
                    self.logger.warning(f"‚ùå No se pudo detectar autor autom√°ticamente para: {filename}")
                    main_document_author_name = None
                    
            except Exception as e:
                filename = Path(file_path).name
                self.logger.error(f"‚ùå Error durante detecci√≥n de autor autom√°tico para {filename}: {str(e)}")
                main_document_author_name = None
                main_author_detection_info = {'error': str(e)}
                
            # 3. Crear segmentador seg√∫n perfil
            profile = self.get_profile(profile_name) # Recargar perfil por si se modific√≥
            segmenter = self.create_segmenter(profile_name, file_path)
            if not segmenter:
                # Si no se pudo crear el segmentador, devolver los bloques pre-procesados con un error.
                existing_error = processed_document_metadata.get('error', '') or ''
                processed_document_metadata['error'] = (existing_error + 
                                                     f"; No se pudo crear el segmentador '{profile.get('segmenter') if profile else 'desconocido'}' para el perfil '{profile_name}'").strip('; ')
                return processed_blocks, {}, processed_document_metadata
            
            # Configurar umbral de confianza si el segmentador lo soporta
            if hasattr(segmenter, 'set_confidence_threshold'):
                segmenter.set_confidence_threshold(confidence_threshold)
                self.logger.debug(f"Umbral de confianza establecido a: {confidence_threshold}")
            
            # 4. Segmentar contenido (usando los bloques pre-procesados)
            self.logger.info(f"Segmentando archivo: {file_path} con {len(processed_blocks)} bloques pre-procesados.")
            
            # Para otros archivos: usar segmentador normalmente
            segments = segmenter.segment(blocks=processed_blocks)
            segmenter_stats = segmenter.get_stats() if hasattr(segmenter, 'get_stats') else {}
            
            # NUEVO: Implementar fallback verso ‚Üí prosa si no hay segmentos
            if profile_name == 'verso' and len(segments) == 0 and len(processed_blocks) > 0:
                self.logger.warning(f"‚ö†Ô∏è VerseSegmenter produjo 0 segmentos para {file_path}")
                self.logger.warning(f"üîÑ Aplicando fallback: cambiando a perfil 'prosa'")
                
                # Crear segmentador de prosa
                prosa_segmenter = self.create_segmenter('prosa', file_path)
                if prosa_segmenter:
                    segments = prosa_segmenter.segment(blocks=processed_blocks)
                    segmenter_stats = prosa_segmenter.get_stats() if hasattr(prosa_segmenter, 'get_stats') else {}
                    
                    # Actualizar el nombre del segmentador usado
                    if profile:
                        profile['_actual_segmenter'] = 'prosa_fallback'
                    
                    self.logger.info(f"‚úÖ Fallback exitoso: {len(segments)} segmentos generados con perfil 'prosa'")
                else:
                    self.logger.error(f"‚ùå No se pudo crear segmentador de prosa para fallback")
        
        # 4.1. Detectar idioma del documento (o usar override)
        detected_lang = None
        if language_override:
            # Validar c√≥digo de idioma antes de usarlo
            if len(language_override.strip()) < 2 or len(language_override.strip()) > 5:
                self.logger.warning(f"C√≥digo de idioma inv√°lido: '{language_override}' (debe tener 2-5 caracteres). Usando detecci√≥n autom√°tica.")
                # No asignar detected_lang, continuar con detecci√≥n autom√°tica
            elif any(char.isdigit() or not char.isalnum() for char in language_override.strip() if char != '-'):
                self.logger.warning(f"C√≥digo de idioma contiene caracteres inv√°lidos: '{language_override}'. Usando detecci√≥n autom√°tica.")
                # No asignar detected_lang, continuar con detecci√≥n autom√°tica  
            else:
                detected_lang = language_override.strip().lower()
                self.logger.info(f"Usando idioma forzado para {file_path}: {detected_lang}")
        elif processed_blocks:
            try:
                # Concatenar texto de los primeros bloques para obtener una muestra representativa
                sample_texts = []
                total_chars = 0
                max_chars = 1000  # L√≠mite de caracteres para la muestra
                max_blocks = 5    # M√°ximo n√∫mero de bloques a usar
                
                for i, block in enumerate(processed_blocks[:max_blocks]):
                    if total_chars >= max_chars:
                        break
                    
                    block_text = ""
                    if isinstance(block, dict):
                        # Extraer texto del bloque seg√∫n su estructura
                        block_text = block.get('text', '') or block.get('content', '') or str(block.get('cleaned_text', ''))
                    else:
                        block_text = str(block)
                    
                    if block_text.strip():
                        remaining_chars = max_chars - total_chars
                        if len(block_text) > remaining_chars:
                            block_text = block_text[:remaining_chars]
                        sample_texts.append(block_text.strip())
                        total_chars += len(block_text)
                
                sample_text = " ".join(sample_texts).strip()
                
                # Intentar detectar idioma si hay suficiente texto
                if len(sample_text) >= 20:  # M√≠nimo de caracteres para detecci√≥n confiable
                    detected_lang = detect(sample_text)
                    self.logger.info(f"Idioma detectado autom√°ticamente para {file_path}: {detected_lang}")
                else:
                    self.logger.debug(f"Texto insuficiente para detecci√≥n de idioma en {file_path} (solo {len(sample_text)} caracteres)")
                    detected_lang = "und"
                    
            except LangDetectException as e:
                self.logger.debug(f"No se pudo detectar idioma para {file_path}: {str(e)}")
                detected_lang = "und"
            except Exception as e:
                self.logger.warning(f"Error inesperado durante detecci√≥n de idioma para {file_path}: {str(e)}")
                detected_lang = "und"
        else:
            self.logger.debug(f"No hay bloques procesados para detectar idioma en {file_path}")
            detected_lang = "und"
        
        # 4.5. Transformar segmentos (diccionarios) en instancias de ProcessedContentItem
        processed_content_items: List[ProcessedContentItem] = []
        if segments:
            # Log de debug para verificar el idioma detectado antes del bucle
            if detected_lang:
                self.logger.debug(f"Idioma detectado disponible para asignaci√≥n: '{detected_lang}'")
            else:
                self.logger.debug(f"No se detect√≥ idioma v√°lido, detected_lang = {detected_lang}")
            
            # Log de debug para verificar las claves disponibles en processed_document_metadata
            self.logger.debug(f"Claves disponibles en processed_document_metadata: {list(processed_document_metadata.keys())}")

            for i, segment_dict in enumerate(segments):
                # [CONFIG] USAR FUNCI√ìN UNIFICADA para archivos no-JSON tambi√©n
                item = self._create_processed_content_item(
                    processed_document_metadata,
                    segment_dict,
                    file_path,
                    language_override,
                    author_override,
                    detected_lang,
                    i,
                    job_config_dict,
                    profile.get('_actual_segmenter', profile.get('segmenter', 'desconocido')) if profile else 'desconocido',
                    main_document_author_name,
                    main_author_detection_info,
                    file_document_id  # [CONFIG] CORREGIDO: Pasar file_document_id consistente
                )
                
                processed_content_items.append(item)
        else: 
            # Si no hay segmentos del segmentador, processed_content_items quedar√° vac√≠a
            pass
        
        # 5. TODO: Aplicar post-procesador si est√° configurado
        
        # 6. Agregar informaci√≥n del autor detectado al processed_document_metadata para la UI
        if main_document_author_name:
            processed_document_metadata['author'] = main_document_author_name
            processed_document_metadata['author_confidence'] = main_author_detection_info.get('confidence', 0.0) if main_author_detection_info else 0.0
            processed_document_metadata['author_detection_method'] = main_author_detection_info.get('method', 'unknown') if main_author_detection_info else 'unknown'
            processed_document_metadata['author_detection_source'] = main_author_detection_info.get('source', 'enhanced_contextual') if main_author_detection_info else 'unknown'
        
        # 7. Exportar si se especific√≥ ruta de salida
        # Usar processed_document_metadata para la parte de metadatos del documento
        # y processed_content_items para los segmentos.
        if output_file: # [OK] CORREGIDO: output_file es la ruta del archivo de salida
            # El primer if es para si manager.process_file devolvi√≥ segmentos (ahora processed_content_items)
            if processed_content_items:
                self._export_results(processed_content_items, output_file, processed_document_metadata, output_format, output_mode)
            # El segundo elif es para cuando no hubo segmentos (lista vac√≠a) pero S√ç hay un output_path
            # y queremos exportar los metadatos del documento (que pueden contener un error o advertencia).
            elif not processed_content_items: 
                self.logger.info(f"No se generaron ProcessedContentItems para {file_path}, pero se exportar√°n metadatos a {output_file}")
                self._export_results([], output_file, processed_document_metadata, output_format, output_mode)

        # Devolver la tupla completa como espera process_file.py, usando la nueva lista de dataclasses
        return processed_content_items, segmenter_stats, processed_document_metadata
    
    def _detect_text_corruption(self, text: str) -> float:
        """
        Detecta la ratio de corrupci√≥n en un texto basado en caracteres duplicados consecutivos.
        
        Args:
            text: Texto a analizar
            
        Returns:
            float: Ratio de corrupci√≥n (0.0 = sin corrupci√≥n, 1.0 = completamente corrupto)
        """
        if not text or len(text) < 10:
            return 0.0
        
        # Contar caracteres duplicados consecutivos
        duplicated_chars = 0
        total_chars = len(text)
        
        i = 0
        while i < len(text) - 1:
            if text[i] == text[i + 1] and text[i].isalpha():
                # Encontrar cu√°ntos caracteres consecutivos son iguales
                consecutive_count = 1
                j = i + 1
                while j < len(text) and text[j] == text[i]:
                    consecutive_count += 1
                    j += 1
                
                # Si hay m√°s de 1 car√°cter consecutivo igual, es probable corrupci√≥n
                if consecutive_count > 1:
                    duplicated_chars += consecutive_count - 1  # Solo contar los extras
                
                i = j
            else:
                i += 1
        
        corruption_ratio = duplicated_chars / total_chars if total_chars > 0 else 0.0
        return corruption_ratio

    def _detect_extreme_corruption(self, text: str, corruption_threshold: float = 0.7) -> Tuple[bool, str]:
        """
        [CONFIG] NUEVA FUNCIONALIDAD - Detecta corrupci√≥n extrema en texto.
        
        Args:
            text: Texto a analizar
            corruption_threshold: Umbral de corrupci√≥n (0.7 = 70% de caracteres corruptos)
            
        Returns:
            Tuple[bool, str]: (is_corrupted, reason)
        """
        if not text or len(text.strip()) < 10:
            return False, ""
        
        text = text.strip()
        total_chars = len(text)
        
        # Contar caracteres problem√°ticos
        control_chars = sum(1 for c in text if ord(c) < 32 and c not in ['\n', '\r', '\t'])
        replacement_chars = text.count('ÔøΩ')  # Caracteres de reemplazo Unicode
        null_chars = text.count('\x00')
        
        # Contar secuencias repetitivas de caracteres extra√±os (solo contar si son significativas)
        weird_pattern_matches = re.findall(r'[^\w\s\.,;:!?¬°¬ø\-\(\)\[\]"\'√°√©√≠√≥√∫√±√º√Å√â√ç√ì√ö√ë√ú]+', text)
        weird_patterns = sum(len(match) for match in weird_pattern_matches if len(match) >= 3)
        
        # Calcular porcentaje de corrupci√≥n
        corrupted_chars = control_chars + replacement_chars + null_chars + weird_patterns
        corruption_ratio = corrupted_chars / total_chars if total_chars > 0 else 0
        
        # Detectar patrones espec√≠ficos de corrupci√≥n extrema
        extreme_patterns = [
            r'ÔøΩ{10,}',  # 10 o m√°s caracteres de reemplazo consecutivos
            r'[\x00-\x08\x0B\x0C\x0E-\x1F]{5,}',  # 5 o m√°s caracteres de control consecutivos
            r'^[\sÔøΩ\x00-\x1F]*$',  # Solo espacios, caracteres de reemplazo y control
        ]
        
        has_extreme_pattern = any(re.search(pattern, text) for pattern in extreme_patterns)
        
        # Determinar si est√° extremadamente corrupto (MUCHO M√ÅS RESTRICTIVO)
        is_extremely_corrupted = (
            replacement_chars >= 50 or  # Al menos 50 caracteres de reemplazo
            has_extreme_pattern or
            (replacement_chars >= 20 and replacement_chars > (total_chars * 0.5))  # 20+ caracteres Y m√°s del 50%
        )
        
        if is_extremely_corrupted:
            reason = f"Corrupci√≥n extrema detectada: {corruption_ratio:.1%} caracteres corruptos"
            if has_extreme_pattern:
                reason += " (patrones extremos detectados)"
            return True, reason
        
        return False, ""

    def _clean_for_json_serialization(self, obj):
        """
        Limpia recursivamente un objeto para asegurar serializaci√≥n JSON segura.
        Remueve o reemplaza caracteres de control que pueden causar errores de JSON.
        """
        if isinstance(obj, str):
            # Limpiar caracteres de control problem√°ticos para JSON
            import re
            # Remover caracteres de control ASCII (0x00-0x1F) excepto \n, \r, \t
            cleaned = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F]', ' ', obj)
            # Remover caracteres de control extendidos (0x7F-0x9F)
            cleaned = re.sub(r'[\x7F-\x9F]', ' ', cleaned)
            # Remover caracteres Unicode problem√°ticos para JSON
            cleaned = re.sub(r'[\uFEFF\u200B-\u200F\u2028\u2029]', ' ', cleaned)
            # Filtrar cualquier car√°cter que no sea seguro para JSON
            safe_chars = []
            for char in cleaned:
                if (ord(char) >= 32 and ord(char) != 127) or char in '\n\r\t':
                    safe_chars.append(char)
                else:
                    safe_chars.append(' ')
            return ''.join(safe_chars).strip()
        elif isinstance(obj, dict):
            return {key: self._clean_for_json_serialization(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._clean_for_json_serialization(item) for item in obj]
        else:
            return obj

    def _export_results(self, segments: List[Any], output_file: str, document_metadata: Optional[Dict[str, Any]] = None, output_format: str = "ndjson", output_mode: str = "biblioperson"):
        """
        Exporta los segmentos procesados usando el sistema de modos de salida.
        
        Args:
            segments: Lista de segmentos procesados
            output_file: Archivo de salida
            document_metadata: Metadatos del documento
            output_format: Formato de salida ("ndjson" o "json")
            output_mode: Modo de salida ("generic" o "biblioperson")
        """
        # [DEBUG] LOGGING DETALLADO PARA DEBUG DE EXPORTACI√ìN
        self.logger.warning(f"[DEBUG] _export_results INICIADO")
        self.logger.warning(f"[DEBUG] Par√°metros recibidos:")
        self.logger.warning(f"[DEBUG]   - segments: {len(segments)} elementos")
        self.logger.warning(f"[DEBUG]   - output_file: '{output_file}'")
        self.logger.warning(f"[DEBUG]   - output_format: '{output_format}'")
        self.logger.warning(f"[DEBUG]   - output_mode: '{output_mode}'")
        print(f"[DEBUG] _export_results INICIADO con {len(segments)} segmentos")
        print(f"[DEBUG] Exportando a: {output_file} en modo {output_mode}")
        
        try:
            # Verificar disponibilidad del sistema de modos de salida
            if not DEDUPLICATION_AVAILABLE or not create_serializer:
                # Fallback al m√©todo de exportaci√≥n original
                self.logger.warning("Sistema de modos de salida no disponible, usando exportaci√≥n tradicional")
                self._export_results_fallback(segments, output_file, document_metadata, output_format)
                return
            
            # Crear serializador seg√∫n el modo
            serializer = create_serializer(output_mode)
            
            # Procesar segmentos corruptos antes de la serializaci√≥n
            processed_segments = []
            corrupted_segments_count = 0
            
            self.logger.info("üßπ INICIANDO EXPORTACI√ìN CON LIMPIEZA MEJORADA DE CARACTERES DE CONTROL")
            
            for i, segment in enumerate(segments):
                # Extraer texto para verificaci√≥n de corrupci√≥n
                if hasattr(segment, 'text'):
                    segment_text = segment.text
                elif hasattr(segment, 'texto_segmento'):
                    segment_text = segment.texto_segmento
                else:
                    segment_text = segment.get('text', '') if isinstance(segment, dict) else str(segment)
                
                # [CONFIG] DETECTAR Y MANEJAR CORRUPCI√ìN EXTREMA
                is_corrupted, corruption_reason = self._detect_extreme_corruption(segment_text)
                
                if is_corrupted:
                    corrupted_segments_count += 1
                    
                    # Crear copia del segmento con texto corregido
                    if hasattr(segment, 'text'):
                        # ProcessedContentItem nuevo
                        corrected_segment = segment
                        corrected_segment.text = f"[CORRUPTED TEXT IN SOURCE FILE]\n\nSegment #{i+1} contains extremely corrupted text that cannot be processed correctly.\n\nReason: {corruption_reason}\n\nRecommendation: Review original PDF or try advanced OCR."
                        corrected_segment.text_length = len(corrected_segment.text)
                    
                        # A√±adir informaci√≥n de corrupci√≥n a metadatos
                        if not corrected_segment.additional_metadata:
                            corrected_segment.additional_metadata = {}
                        corrected_segment.additional_metadata["corruption_detected"] = True
                        corrected_segment.additional_metadata["corruption_reason"] = corruption_reason
                        corrected_segment.additional_metadata["original_text_length"] = len(segment_text)
                        
                    elif hasattr(segment, 'texto_segmento'):
                        # ProcessedContentItem viejo
                        corrected_segment = segment
                        corrected_segment.texto_segmento = f"[CORRUPTED TEXT IN SOURCE FILE]\n\nSegment #{i+1} contains extremely corrupted text that cannot be processed correctly.\n\nReason: {corruption_reason}\n\nRecommendation: Review original PDF or try advanced OCR."
                        corrected_segment.longitud_caracteres_segmento = len(corrected_segment.texto_segmento)
                    
                        # A√±adir informaci√≥n de corrupci√≥n a metadatos
                        if not corrected_segment.metadatos_adicionales_fuente:
                            corrected_segment.metadatos_adicionales_fuente = {}
                        corrected_segment.metadatos_adicionales_fuente["corruption_detected"] = True
                        corrected_segment.metadatos_adicionales_fuente["corruption_reason"] = corruption_reason
                        corrected_segment.metadatos_adicionales_fuente["original_text_length"] = len(segment_text)
                
                    else:
                        # Diccionario
                        corrected_segment = segment.copy() if isinstance(segment, dict) else {"text": str(segment)}
                        corrected_segment["text"] = f"[CORRUPTED TEXT IN SOURCE FILE]\n\nSegment #{i+1} contains extremely corrupted text that cannot be processed correctly.\n\nReason: {corruption_reason}\n\nRecommendation: Review original PDF or try advanced OCR."
                        
                        if "metadata" not in corrected_segment:
                            corrected_segment["metadata"] = {}
                        corrected_segment["metadata"]["corruption_detected"] = True
                        corrected_segment["metadata"]["corruption_reason"] = corruption_reason
                        corrected_segment["metadata"]["original_text_length"] = len(segment_text)
                    
                    # Log del problema
                    self.logger.warning(f"üö® Corrupci√≥n extrema en segmento #{i+1}: {corruption_reason}")
                    processed_segments.append(corrected_segment)
                else:
                    processed_segments.append(segment)
            
            # Log resumen de corrupci√≥n
            if corrupted_segments_count > 0:
                self.logger.warning(f"üö® RESUMEN DE CORRUPCI√ìN: {corrupted_segments_count}/{len(segments)} segmentos ten√≠an corrupci√≥n extrema y fueron reemplazados")
            
            # Usar el serializador para exportar
            serializer.export_segments(
                processed_segments, 
                output_file, 
                document_metadata, 
                output_format
            )
            
            self.logger.info(f"Resultados exportados en modo {output_mode.upper()} formato {output_format.upper()}: {output_file}")
            if corrupted_segments_count > 0:
                self.logger.info(f"üìä Estad√≠sticas de corrupci√≥n: {corrupted_segments_count} segmentos reemplazados por texto corrupto")
                
        except Exception as e:
            self.logger.error(f"Error al exportar resultados: {str(e)}")
            raise
        
        self.logger.warning(f"[OK] EXPORTACI√ìN COMPLETADA EXITOSAMENTE")
        print("[OK] EXPORTACI√ìN COMPLETADA EXITOSAMENTE")
    
    def _export_results_fallback(self, segments: List[Any], output_file: str, document_metadata: Optional[Dict[str, Any]] = None, output_format: str = "ndjson"):
        """
        M√©todo de fallback para exportaci√≥n cuando el sistema de modos no est√° disponible.
        
        Args:
            segments: Lista de segmentos procesados
            output_file: Archivo de salida
            document_metadata: Metadatos del documento
            output_format: Formato de salida ("ndjson" o "json")
        """
        self.logger.info("[RETRY] Usando exportaci√≥n tradicional (fallback)")
        
        try:
            # Asegurar que el directorio de salida existe
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Procesar segmentos para limpieza b√°sica
            processed_segments = []
            for segment in segments:
                # Limpiar caracteres de control b√°sicos
                if hasattr(segment, 'text'):
                    segment.text = self._clean_control_characters(segment.text)
                elif hasattr(segment, 'texto_segmento'):
                    segment.texto_segmento = self._clean_control_characters(segment.texto_segmento)
                elif isinstance(segment, dict) and 'text' in segment:
                    segment['text'] = self._clean_control_characters(segment['text'])
                
                processed_segments.append(segment)
            
            # Exportar seg√∫n el formato
            if output_format.lower() == "json":
                # Exportar como JSON array
                with open(output_file, 'w', encoding='utf-8') as f:
                    import json
                    json_data = []
                    for segment in processed_segments:
                        if hasattr(segment, '__dict__'):
                            json_data.append(self._clean_for_json_serialization(segment.__dict__))
                        else:
                            json_data.append(self._clean_for_json_serialization(segment))
                    json.dump(json_data, f, ensure_ascii=False, indent=2)
            else:
                # Exportar como NDJSON (por defecto)
                with open(output_file, 'w', encoding='utf-8') as f:
                    import json
                    for segment in processed_segments:
                        if hasattr(segment, '__dict__'):
                            segment_dict = self._clean_for_json_serialization(segment.__dict__)
                        else:
                            segment_dict = self._clean_for_json_serialization(segment)
                        f.write(json.dumps(segment_dict, ensure_ascii=False) + '\n')
            
            self.logger.info(f"Exportaci√≥n tradicional completada: {output_file}")
            
        except Exception as e:
            self.logger.error(f"Error en exportaci√≥n tradicional: {str(e)}")
            raise
    
    def _clean_control_characters(self, text: str) -> str:
        """
        Limpia caracteres de control b√°sicos del texto.
        
        Args:
            text: Texto a limpiar
            
        Returns:
            Texto limpio
        """
        if not isinstance(text, str):
            return str(text)
        
        # Remover caracteres de control comunes
        import re
        # Mantener saltos de l√≠nea y tabulaciones, remover otros caracteres de control
        cleaned = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
        return cleaned

    def get_profile_for_file(self, file_path: str, content_sample: Optional[str] = None) -> Optional[str]:
        """
        Sugiere el perfil m√°s adecuado para un archivo usando detecci√≥n autom√°tica avanzada.
        
        Args:
            file_path: Ruta al archivo
            content_sample: Contenido extra√≠do del archivo (opcional, para mejor detecci√≥n)
            
        Returns:
            Nombre del perfil sugerido o None si no hay sugerencia
        """
        # Intentar detecci√≥n autom√°tica primero
        detected_profile = self.auto_detect_profile(file_path, content_sample)
        if detected_profile:
            self.logger.info(f"[TARGET] Perfil detectado autom√°ticamente: {detected_profile}")
            return detected_profile
        
        # Fallback al m√©todo manual si la detecci√≥n autom√°tica falla
        self.logger.debug("[RETRY] Usando detecci√≥n manual como fallback")
        return self._get_manual_profile_fallback(file_path)

    def auto_detect_profile(self, file_path: str, content_sample: Optional[str] = None) -> Optional[str]:
        """
        [DEBUG] DETECCI√ìN AUTOM√ÅTICA DE PERFILES - ALGORITMO CONSERVADOR
        
        Detecta autom√°ticamente el perfil m√°s adecuado para un archivo usando an√°lisis estructural.
        
        ALGORITMO CONSERVADOR:
        - JSON: Detecci√≥n por extensi√≥n (f√°cil)
        - PROSA: Por defecto para todo contenido
        - VERSO: Solo cuando >80% del texto cumple criterios estructurales puros
        
        Args:
            file_path: Ruta al archivo
            content_sample: Muestra del contenido (opcional, se leer√° si no se proporciona)
            
        Returns:
            Nombre del perfil detectado o None si no se puede detectar
        """
        if not PROFILE_DETECTION_AVAILABLE:
            self.logger.warning("[WARN] Sistema de detecci√≥n autom√°tica de perfiles no disponible")
            return self.get_profile_for_file(file_path)  # Fallback al m√©todo manual
        
        try:
            self.logger.info(f"[DEBUG] INICIANDO DETECCI√ìN AUTOM√ÅTICA DE PERFIL: {Path(file_path).name}")
            
            # Configuraci√≥n conservadora
            config = get_profile_detection_config()
            config['debug'] = self.logger.isEnabledFor(logging.DEBUG)
            
            # Detectar perfil usando el algoritmo conservador
            candidate = detect_profile_for_file(file_path, config, content_sample)
            
            if candidate and candidate.confidence >= 0.35:  # Umbral m√≠nimo de confianza ajustado
                confidence_pct = candidate.confidence * 100
                self.logger.info(f"[OK] PERFIL AUTO-DETECTADO: '{candidate.profile_name}' "
                               f"(confianza: {confidence_pct:.1f}%)")
                
                # Log de las razones de la detecci√≥n
                for reason in candidate.reasons:
                    self.logger.debug(f"   üìã {reason}")
                
                # Verificar que el perfil detectado existe en el sistema
                if candidate.profile_name in self.profiles:
                    return candidate.profile_name
                else:
                    self.logger.warning(f"[WARN] Perfil detectado '{candidate.profile_name}' no existe en el sistema")
                    # Fallback al m√©todo manual si el perfil detectado no existe
                    self.logger.info("[RETRY] Usando m√©todo manual como fallback")
                    return self._get_manual_profile_fallback(file_path)
            else:
                confidence_pct = candidate.confidence * 100 if candidate else 0
                self.logger.warning(f"[WARN] Confianza insuficiente para detecci√≥n autom√°tica: {confidence_pct:.1f}%")
                # Fallback al m√©todo manual cuando la confianza es muy baja
                self.logger.info("[RETRY] Usando m√©todo manual como fallback")
                return self._get_manual_profile_fallback(file_path)
                
        except Exception as e:
            self.logger.error(f"‚ùå Error en detecci√≥n autom√°tica de perfil: {str(e)}")
            # Fallback al m√©todo manual en caso de error
            self.logger.info("[RETRY] Usando m√©todo manual como fallback por error")
            return self._get_manual_profile_fallback(file_path)

    def _get_manual_profile_fallback(self, file_path: str) -> Optional[str]:
        """
        M√©todo de fallback que usa detecci√≥n manual sin llamar a auto_detect_profile.
        Evita bucles infinitos.
        
        Args:
            file_path: Ruta al archivo
            
        Returns:
            Nombre del perfil sugerido o None si no hay sugerencia
        """
        self.logger.debug("[RETRY] Ejecutando detecci√≥n manual como fallback")
        
        file_path = Path(file_path)
        extension = file_path.suffix.lower()
        filename = file_path.stem.lower()
        
        # Detecci√≥n por extensi√≥n JSON
        if extension == '.json':
            if 'json' in self.profiles:
                self.logger.debug("Perfil json detectado por extensi√≥n")
                return 'json'
        
        # Palabras clave que sugieren tipos de contenido
        poem_keywords = ['poema', 'poemas', 'poes√≠a', 'poes√≠as', 'versos', 'verso', 'estrofa', 'poeta', 
                         'poem', 'poetry', 'lyric', 'lyrics', 'canci√≥n', 'canciones']
        book_keywords = ['libro', 'capitulo', 'cap√≠tulos', 'book', 'chapter', 'section', 'manual', 
                        'gu√≠a', 'documento', 'texto', 'escrito']
        
        # Comprobar primero en el nombre del archivo
        for keyword in poem_keywords:
            if keyword in filename:
                if 'verso' in self.profiles:
                    self.logger.debug(f"Perfil verso detectado por palabra clave en el nombre: {keyword}")
                    return 'verso'
                
        for keyword in book_keywords:
            if keyword in filename:
                if 'prosa' in self.profiles:
                    self.logger.debug(f"Perfil prosa detectado por palabra clave en el nombre: {keyword}")
                    return 'prosa'
                
        # Verificar si alg√∫n perfil tiene configuraci√≥n espec√≠fica para esta extensi√≥n
        for profile_name, profile in self.profiles.items():
            if 'file_types' in profile and extension in profile['file_types']:
                self.logger.debug(f"Perfil {profile_name} sugerido por tipo de archivo: {extension}")
                return profile_name
        
        # Fallback por defecto: prosa para la mayor√≠a de documentos de texto
        if extension in ['.pdf', '.txt', '.md', '.docx', '.doc']:
            if 'prosa' in self.profiles:
                self.logger.debug(f"Perfil prosa sugerido por defecto para: {extension}")
                return 'prosa'
        
        # Si todo lo dem√°s falla, devolver None
        self.logger.debug("No se pudo determinar perfil con m√©todo manual")
        return None

    def get_profile_detection_report(self, file_path: str, content_sample: Optional[str] = None) -> Dict[str, Any]:
        """
        Generar reporte detallado de detecci√≥n de perfil para debugging.
        
        Args:
            file_path: Ruta al archivo
            content_sample: Muestra del contenido (opcional)
            
        Returns:
            Reporte detallado con m√©tricas y an√°lisis
        """
        if not PROFILE_DETECTION_AVAILABLE:
            return {
                'error': 'Sistema de detecci√≥n autom√°tica no disponible',
                'fallback_used': True,
                'manual_suggestion': self.get_profile_for_file(file_path)
            }
        
        try:
            detector = ProfileDetector(get_profile_detection_config())
            return detector.get_detection_report(file_path, content_sample)
        except Exception as e:
            return {
                'error': f'Error generando reporte: {str(e)}',
                'file_path': str(file_path)
            }

    def validate_pipeline_compatibility(self) -> Dict[str, Any]:
        """
        Valida la compatibilidad del pipeline con el sistema de deduplicaci√≥n.
        
        Returns:
            Diccionario con informaci√≥n de compatibilidad
        """
        compatibility_info = {
            'deduplication_available': DEDUPLICATION_AVAILABLE,
            'config_loaded': False,
            'supported_profiles': [],
            'supported_formats': [],
            'errors': [],
            'warnings': []
        }
        
        if DEDUPLICATION_AVAILABLE:
            try:
                config_manager = get_config_manager()
                compatibility_info['config_loaded'] = True
                
                compat_config = config_manager.get_compatibility_config()
                compatibility_info['supported_profiles'] = compat_config.supported_profiles
                compatibility_info['supported_formats'] = compat_config.supported_file_formats
                
                # Validar que los perfiles existan
                available_profiles = list(self.profiles.keys())
                for profile in compat_config.supported_profiles:
                    if profile not in available_profiles and profile != 'autom√°tico':
                        compatibility_info['warnings'].append(f"Perfil configurado '{profile}' no est√° disponible")
                
                self.logger.info("[OK] Validaci√≥n de compatibilidad completada exitosamente")
                
            except Exception as e:
                compatibility_info['errors'].append(f"Error validando compatibilidad: {str(e)}")
                self.logger.error(f"Error en validaci√≥n de compatibilidad: {str(e)}")
        else:
            compatibility_info['warnings'].append("Sistema de deduplicaci√≥n no disponible")
        
        return compatibility_info
    
    def get_deduplication_status(self) -> Dict[str, Any]:
        """
        Obtiene el estado actual del sistema de deduplicaci√≥n.
        
        Returns:
            Diccionario con el estado del sistema
        """
        status = {
            'available': DEDUPLICATION_AVAILABLE,
            'enabled': False,
            'database_path': None,
            'modes': {},
            'stats': None
        }
        
        if DEDUPLICATION_AVAILABLE:
            try:
                config_manager = get_config_manager()
                dedup_config = config_manager.get_deduplication_config()
                
                status['enabled'] = dedup_config.enabled
                status['database_path'] = str(config_manager.get_database_path())
                
                # Estado de los modos
                for mode in ['generic', 'biblioperson']:
                    mode_config = config_manager.get_output_mode_config(mode)
                    if mode_config:
                        status['modes'][mode] = {
                            'description': mode_config.description,
                            'deduplication_enabled': mode_config.enable_deduplication,
                            'include_hash': mode_config.include_document_hash
                        }
                
                # Estad√≠sticas de la base de datos si est√° disponible
                if dedup_config.enabled:
                    try:
                        dedup_manager = get_dedup_manager()
                        status['stats'] = dedup_manager.get_stats()
                    except Exception as e:
                        status['stats'] = {'error': str(e)}
                
            except Exception as e:
                status['error'] = str(e)
        
        return status

    def create_pre_processor(self, pre_processor_type: str, profile: Optional[Dict] = None) -> 'BasePreProcessor':
        """
        Crea una instancia del pre-procesador especificado.
        
        Args:
            pre_processor_type: Tipo de pre-procesador a crear
            profile: Perfil opcional para configuraci√≥n espec√≠fica
            
        Returns:
            Instancia del pre-procesador
        """
        # LOGGING DETALLADO PARA DEBUG
        print(f"[CONFIG][CONFIG][CONFIG] CREANDO PRE-PROCESADOR: {pre_processor_type} [CONFIG][CONFIG][CONFIG]")
        print(f"[CONFIG][CONFIG][CONFIG] PERFIL RECIBIDO: {profile.get('name') if profile else 'None'} [CONFIG][CONFIG][CONFIG]")
        
        preprocessor_config = profile.get('pre_processor_config') if profile else None
        print(f"[CONFIG][CONFIG][CONFIG] CONFIG DEL PRE-PROCESADOR: {preprocessor_config} [CONFIG][CONFIG][CONFIG]")
        
        # Obtener configuraci√≥n del pre-procesador desde el perfil
        if pre_processor_type == 'common_block':
            return CommonBlockPreprocessor(config=preprocessor_config)
        # Agregar otros tipos seg√∫n sea necesario
        else:
            raise ValueError(f"Tipo de pre-procesador no soportado: {pre_processor_type}")

if __name__ == "__main__":
    # Configuraci√≥n b√°sica de logging
    logging.basicConfig(level=logging.INFO)
    
    # Ejemplo de uso
    manager = ProfileManager()
    print("Perfiles disponibles:")
    for profile in manager.list_profiles():
        print(f"- {profile['name']}: {profile['description']}")