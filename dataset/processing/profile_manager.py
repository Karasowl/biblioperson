import os
import yaml
import logging
from typing import Dict, List, Any, Optional, Type, Tuple
from pathlib import Path
import dataclasses
from datetime import datetime, timezone
import uuid
import importlib
import re

from langdetect import detect, LangDetectException
from dataset.scripts.data_models import ProcessedContentItem, BatchContext

# RECARGA FORZADA DE M√ìDULOS MODIFICADOS - V7.0
import dataset.processing.segmenters.heading_segmenter
importlib.reload(dataset.processing.segmenters.heading_segmenter)

import dataset.processing.loaders.pdf_loader
importlib.reload(dataset.processing.loaders.pdf_loader)

import dataset.processing.pre_processors.common_block_preprocessor
importlib.reload(dataset.processing.pre_processors.common_block_preprocessor)

from .segmenters.base import BaseSegmenter
from .segmenters.verse_segmenter import VerseSegmenter
from .segmenters.heading_segmenter import HeadingSegmenter
from .loaders import BaseLoader, MarkdownLoader, NDJSONLoader, JSONLoader, DocxLoader, txtLoader, PDFLoader, ExcelLoader, CSVLoader
from .pre_processors import CommonBlockPreprocessor

# A medida que se implementen, importar otros componentes:
# from .loaders.base import BaseLoader
# from .exporters.base import BaseExporter
# etc.

class ProfileManager:
    """
    Gestor del sistema de perfiles de procesamiento.
    
    Esta clase coordina todo el pipeline de procesamiento:
    - Carga de perfiles YAML
    - Selecci√≥n del loader adecuado
    - Inicializaci√≥n de segmentadores con sus configuraciones
    - Ejecuci√≥n de post-procesadores
    - Exportaci√≥n de resultados
    """
    
    def __init__(self, profiles_dir: str = None):
        """
        Inicializa el gestor de perfiles.
        
        Args:
            profiles_dir: Directorio donde se encuentran los perfiles YAML
        """
        self.logger = logging.getLogger(__name__)
        
        # Directorio de perfiles por defecto
        if profiles_dir is None:
            module_dir = os.path.dirname(os.path.abspath(__file__))
            profiles_dir = os.path.join(os.path.dirname(module_dir), 'config', 'profiles')
        
        self.profiles_dir = profiles_dir
        self.profiles = {}
        self._segmenter_registry = {}
        self._loader_registry = {}
        
        # Registrar componentes disponibles
        self.register_default_components()
        
        # Cargar perfiles desde directorio
        self.load_profiles()
    
    def reload_custom_segmenters(self):
        """Recarga los segmentadores personalizados. √ötil despu√©s de generar nuevos segmentadores."""
        self.logger.info("Recargando segmentadores personalizados...")
        self._load_custom_segmenters()
    
    def register_default_components(self):
        """Registra los componentes por defecto del sistema."""
        # Registrar segmentadores
        self.register_segmenter('verse', VerseSegmenter)
        self.register_segmenter('heading', HeadingSegmenter)
        
        # Registrar MarkdownVerseSegmenter manualmente
        try:
            from .segmenters.markdown_verse_segmenter import MarkdownVerseSegmenter
            self.register_segmenter('markdown_verse', MarkdownVerseSegmenter)
            self.logger.info("‚úÖ MarkdownVerseSegmenter registrado manualmente")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è No se pudo registrar MarkdownVerseSegmenter: {e}")
        
        # Cargar segmentadores personalizados din√°micamente
        self._load_custom_segmenters()
        
        # Registrar loaders
        self.register_loader('.md', MarkdownLoader)
        self.register_loader('.markdown', MarkdownLoader)
        self.register_loader('.ndjson', NDJSONLoader)
        self.register_loader('.docx', DocxLoader)
        self.register_loader('.txt', txtLoader)
        self.register_loader('.pdf', PDFLoader)
        self.register_loader('.xls', ExcelLoader)
        self.register_loader('.xlsx', ExcelLoader)
        self.register_loader('.xlsm', ExcelLoader)
        self.register_loader('.csv', CSVLoader)  # Usando CSVLoader espec√≠fico para CSV
        self.register_loader('.tsv', CSVLoader)  # Tambi√©n para archivos TSV (valores separados por tabulaciones)
        self.register_loader('.json', JSONLoader)
        
        # Registrar MarkdownPDFLoader manualmente
        try:
            from .loaders.markdown_pdf_loader import MarkdownPDFLoader
            self.register_loader('.pdf_markdown', MarkdownPDFLoader)  # Extensi√≥n especial
            self.logger.info("‚úÖ MarkdownPDFLoader registrado manualmente")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è No se pudo registrar MarkdownPDFLoader: {e}")
    
    def register_segmenter(self, name: str, segmenter_class: Type[BaseSegmenter]):
        """
        Registra un segmentador en el sistema.
        
        Args:
            name: Nombre para referencia en perfiles
            segmenter_class: Clase del segmentador
        """
        self._segmenter_registry[name] = segmenter_class
        self.logger.debug(f"Registrado segmentador '{name}'")
    
    def register_loader(self, extension: str, loader_class: Type[BaseLoader]):
        """
        Registra un loader para una extensi√≥n de archivo.
        
        Args:
            extension: Extensi√≥n del archivo (con punto)
            loader_class: Clase del loader
        """
        self._loader_registry[extension.lower()] = loader_class
        self.logger.debug(f"Registrado loader para {extension}")
    
    def _load_custom_segmenters(self):
        """Carga din√°micamente segmentadores personalizados desde el directorio de segmentadores."""
        import importlib.util
        import inspect
        
        # Directorio donde se guardan los segmentadores personalizados
        segmenters_dir = os.path.join(os.path.dirname(__file__), 'segmenters')
        
        if not os.path.exists(segmenters_dir):
            return
        
        # Buscar archivos Python que terminen en '_segmenter.py'
        for filename in os.listdir(segmenters_dir):
            if filename.endswith('_segmenter.py') and filename not in ['base.py', 'verse_segmenter.py', 'heading_segmenter.py']:
                module_path = os.path.join(segmenters_dir, filename)
                module_name = filename[:-3]  # Remover .py
                
                try:
                    # Cargar el m√≥dulo din√°micamente
                    spec = importlib.util.spec_from_file_location(module_name, module_path)
                    if spec and spec.loader:
                        module = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(module)
                        
                        # Buscar clases que hereden de BaseSegmenter
                        for name, obj in inspect.getmembers(module, inspect.isclass):
                            if (obj != BaseSegmenter and 
                                issubclass(obj, BaseSegmenter) and 
                                hasattr(obj, '__module__') and 
                                obj.__module__ == module_name):
                                
                                # Registrar el segmentador usando el nombre del archivo sin '_segmenter'
                                segmenter_name = module_name.replace('_segmenter', '')
                                self.register_segmenter(segmenter_name, obj)
                                self.logger.info(f"Cargado segmentador personalizado: {segmenter_name} -> {name}")
                                
                except Exception as e:
                    self.logger.warning(f"No se pudo cargar segmentador personalizado {filename}: {str(e)}")
    
    def get_loader_for_file(self, file_path: str, profile_name: str = None) -> Optional[tuple]:
        """
        Obtiene el loader apropiado para un archivo y determina el tipo de contenido.
        
        Args:
            file_path: Ruta al archivo
            profile_name: Nombre del perfil (opcional)
            
        Returns:
            Tuple con (clase del loader, tipo de contenido) o None si no hay loader registrado
        """
        extension = Path(file_path).suffix.lower()
        
        # SPECIAL CASE: Si es perfil verso y archivo PDF, usar MarkdownPDFLoader
        if profile_name == 'verso' and extension == '.pdf':
            try:
                from .loaders.markdown_pdf_loader import MarkdownPDFLoader
                self.logger.info("üéØ Usando MarkdownPDFLoader para perfil verso")
                loader_class = MarkdownPDFLoader
            except ImportError:
                self.logger.warning("‚ö†Ô∏è MarkdownPDFLoader no disponible, usando PDFLoader tradicional")
                loader_class = self._loader_registry.get(extension)
        else:
            loader_class = self._loader_registry.get(extension)
        
        if not loader_class:
            self.logger.error(f"No hay loader registrado para extensi√≥n: {extension}")
            return None
        
        # Determinar el tipo de contenido basado en el perfil si est√° disponible
        content_type = 'escritos'  # Valor por defecto
        
        if profile_name and profile_name in self.profiles:
            profile = self.profiles[profile_name]
            
            # Si el perfil especifica un tipo de contenido por defecto, usarlo
            if 'default_content_type' in profile:
                content_type = profile['default_content_type']
            
            # Si el perfil tiene mapeo de extensiones a tipos, usarlo
            elif 'extension_types' in profile and extension in profile['extension_types']:
                content_type = profile['extension_types'][extension]
            
            # O inferir basado en el nombre del perfil
            elif 'poem' in profile_name or 'lyric' in profile_name:
                content_type = 'poemas' if 'poem' in profile_name else 'canciones'
        
        # Tambi√©n podemos intentar inferir del nombre del archivo
        filename = Path(file_path).stem.lower()
        if not profile_name or content_type == 'escritos':
            if any(word in filename for word in ['poema', 'poem', 'verso']):
                content_type = 'poemas'
            elif any(word in filename for word in ['cancion', 'song', 'lyric']):
                content_type = 'canciones'
        
        self.logger.debug(f"Loader para {extension}: {loader_class.__name__}, tipo: {content_type}")
        return (loader_class, content_type)
        
    def load_profiles(self):
        """Carga todos los perfiles YAML del directorio configurado y subcarpetas."""
        if not os.path.exists(self.profiles_dir):
            self.logger.warning(f"Directorio de perfiles no encontrado: {self.profiles_dir}")
            return

        # Cargar perfiles del directorio ra√≠z (compatibilidad)
        self._load_profiles_from_directory(self.profiles_dir)
        
        # Cargar perfiles de subcarpetas (core, special)
        for subdir in ['core', 'special']:
            subdir_path = os.path.join(self.profiles_dir, subdir)
            if os.path.exists(subdir_path):
                self._load_profiles_from_directory(subdir_path, category=subdir)
    
    def _load_profiles_from_directory(self, directory: str, category: str = None):
        """Carga perfiles de un directorio espec√≠fico."""
        for filename in os.listdir(directory):
            if filename.endswith(('.yaml', '.yml')):
                profile_path = os.path.join(directory, filename)
                try:
                    with open(profile_path, 'r', encoding='utf-8') as f:
                        profile = yaml.safe_load(f)
                    
                    if 'name' in profile:
                        # Agregar categor√≠a al perfil para organizaci√≥n
                        if category:
                            profile['_category'] = category
                        self.profiles[profile['name']] = profile
                        category_info = f" ({category})" if category else ""
                        self.logger.info(f"Cargado perfil: {profile['name']}{category_info}")
                    else:
                        self.logger.warning(f"Perfil sin nombre en {filename}")
                except Exception as e:
                    self.logger.error(f"Error al cargar perfil {filename}: {str(e)}")
    
    def get_profile(self, profile_name: str) -> Optional[Dict[str, Any]]:
        """
        Obtiene un perfil por su nombre.
        
        Args:
            profile_name: Nombre del perfil
            
        Returns:
            Diccionario con la configuraci√≥n del perfil o None si no existe
        """
        return self.profiles.get(profile_name)
    
    def list_profiles(self) -> List[Dict[str, Any]]:
        """
        Lista todos los perfiles disponibles.
        
        Returns:
            Lista con informaci√≥n b√°sica de cada perfil
        """
        return [
            {
                'name': profile['name'],
                'description': profile.get('description', ''),
                'segmenter': profile.get('segmenter', 'unknown'),
                'file_types': profile.get('file_types', [])
            }
            for profile in self.profiles.values()
        ]
    
    def create_segmenter(self, profile_name: str) -> Optional[BaseSegmenter]:
        """
        Crea una instancia de segmentador seg√∫n el perfil.
        
        Args:
            profile_name: Nombre del perfil a usar
            
        Returns:
            Instancia del segmentador configurada o None si hay error
        """
        profile = self.get_profile(profile_name)
        if not profile:
            self.logger.error(f"Perfil no encontrado: {profile_name}")
            return None
        
        segmenter_type = profile.get('segmenter')
        if not segmenter_type:
            self.logger.error(f"Tipo de segmentador no especificado en perfil: {profile_name}")
            return None
        
        if segmenter_type not in self._segmenter_registry:
            self.logger.error(f"Segmentador '{segmenter_type}' no registrado")
            return None
        
        # Configuraci√≥n para el segmentador
        config = {}
        
        # Usar configuraci√≥n espec√≠fica del segmentador si existe
        if 'segmenter_config' in profile:
            config.update(profile['segmenter_config'])
        
        # Mantener compatibilidad con configuraci√≥n legacy
        # Copiar thresholds del perfil (si no est√°n en segmenter_config)
        if 'thresholds' in profile and 'thresholds' not in config:
            config['thresholds'] = profile['thresholds']
        
        # Copiar patrones espec√≠ficos (si no est√°n en segmenter_config)
        for key in ['title_patterns', 'paragraph_patterns', 'section_patterns']:
            if key in profile and key not in config:
                config[key] = profile[key]
        
        # Crear instancia del segmentador
        try:
            segmenter_class = self._segmenter_registry[segmenter_type]
            return segmenter_class(config)
        except Exception as e:
            self.logger.error(f"Error al crear segmentador '{segmenter_type}': {str(e)}")
            return None
    
    def process_file(self, 
                    file_path: str, 
                    profile_name: str, 
                    output_file: Optional[str] = None,
                    encoding: str = 'utf-8',
                    force_content_type: Optional[str] = None,
                    confidence_threshold: float = 0.5,
                    job_config_dict: Optional[Dict[str, Any]] = None,
                    language_override: Optional[str] = None,
                    author_override: Optional[str] = None,
                    output_format: str = "ndjson",
                    folder_structure_info: Optional[Dict[str, Any]] = None) -> List[Any]:
        """
        Procesa un archivo completo usando un perfil.
        
        Args:
            file_path: Ruta al archivo a procesar
            profile_name: Nombre del perfil a usar
            output_file: Archivo para guardar resultados (opcional)
            encoding: Codificaci√≥n para abrir el archivo (por defecto utf-8)
            force_content_type: Forzar un tipo espec√≠fico de contenido (ignora detecci√≥n autom√°tica)
            confidence_threshold: Umbral de confianza para detecci√≥n de poemas (0.0-1.0)
            job_config_dict: Diccionario con la configuraci√≥n del job actual (opcional)
            language_override: C√≥digo de idioma para override (opcional)
            author_override: Nombre del autor para override (opcional)
            output_format: Formato de salida ("ndjson" o "json")
            folder_structure_info: Informaci√≥n sobre la estructura de carpetas del archivo (opcional)
            
        Returns:
            Tuple con: (Lista de unidades procesadas, Estad√≠sticas del segmentador, Metadatos del documento)
        """
        if not os.path.exists(file_path):
            self.logger.error(f"Archivo no encontrado: {file_path}")
            # Devolver la estructura de tupla esperada por process_file.py
            return [], {}, {'error': f"Archivo no encontrado: {file_path}"}
        
        # 1. Obtener loader apropiado y tipo de contenido
        loader_result = self.get_loader_for_file(file_path, profile_name)
        if not loader_result:
            return [], {}, {'error': f"No se pudo obtener el loader para: {file_path}"}
            
        loader_class, content_type = loader_result
        
        # Si el usuario forz√≥ un tipo espec√≠fico, usarlo
        if force_content_type:
            self.logger.info(f"Forzando tipo de contenido: {force_content_type}")
            content_type = force_content_type
            
        # 2. Crear loader e intentar cargar el archivo
        raw_blocks: List[Dict[str, Any]] = []
        raw_document_metadata: Dict[str, Any] = {}
        try:
            self.logger.info(f"Usando loader: {loader_class.__name__}")
            
            # Si es JSONLoader y el perfil tiene configuraci√≥n JSON, usarla
            loader_kwargs = {'encoding': encoding}
            if loader_class.__name__ == 'JSONLoader':
                profile = self.get_profile(profile_name)
                if profile and 'json_config' in profile:
                    json_config = profile['json_config']
                    self.logger.info(f"Aplicando configuraci√≥n JSON del perfil: {profile_name}")
                    loader_kwargs.update(json_config)
            
            loader = loader_class(file_path, **loader_kwargs)
            loaded_data = loader.load()
            
            # Asegurar que las claves existan, incluso si est√°n vac√≠as
            raw_blocks = loaded_data.get('blocks', [])
            raw_document_metadata = loaded_data.get('document_metadata', {})
            raw_document_metadata.setdefault('source_file_path', str(Path(file_path).absolute()))
            raw_document_metadata.setdefault('file_format', Path(file_path).suffix.lower())

            # === AGREGAR INFORMACI√ìN DE ESTRUCTURA DE CARPETAS ===
            if folder_structure_info:
                # Agregar informaci√≥n de estructura de carpetas a los metadatos del documento
                raw_document_metadata.update({
                    "folder_structure": folder_structure_info
                })
                self.logger.debug(f"Informaci√≥n de estructura de carpetas agregada a metadatos: {folder_structure_info}")

            # Si el loader report√≥ un error, lo propagamos antes del pre-procesamiento
            if raw_document_metadata.get('error'):
                self.logger.error(f"Error reportado por loader {loader_class.__name__} para {file_path}: {raw_document_metadata['error']}")
                return [], {}, raw_document_metadata

            if not raw_blocks and not raw_document_metadata.get('warning'): # Si no hay bloques y no es una advertencia de archivo vac√≠o
                self.logger.info(f"Loader {loader_class.__name__} no devolvi√≥ bloques para {file_path} y no hay advertencia de archivo vac√≠o.")

        except Exception as e:
            self.logger.error(f"Excepci√≥n al instanciar o usar loader {loader_class.__name__} para archivo {file_path}: {str(e)}", exc_info=True)
            error_metadata = {
                'source_file_path': str(Path(file_path).absolute()),
                'file_format': Path(file_path).suffix.lower(),
                'error': f"Excepci√≥n en ProfileManager durante la carga con loader: {str(e)}"
            }
            return [], {}, error_metadata
        
        # 2.5 Aplicar Pre-procesador Com√∫n
        # Obtener configuraci√≥n del preprocesador desde el perfil YAML
        profile = self.get_profile(profile_name)
        
        # ===== IDENTIFICADOR √öNICO ProfileManager V2.0 =====
        self.logger.warning("üí•üí•üí• PROFILE MANAGER V2.0 - SOLUCI√ìN ESPACIOS M√öLTIPLES ACTIVA üí•üí•üí•")
        print("üí•üí•üí• PROFILE MANAGER V2.0 - SOLUCI√ìN ESPACIOS M√öLTIPLES ACTIVA üí•üí•üí•")
        
        preprocessor_config = profile.get('pre_processor_config') if profile else None
        self.logger.warning(f"üí• CONFIG PARA CREAR PREPROCESSOR: {preprocessor_config}")
        print(f"üí• CONFIG PARA CREAR PREPROCESSOR: {preprocessor_config}")
        
        # FORZAR RECARGA DE COMMONBLOCKPREPROCESSOR PARA EVITAR CACHE
        import importlib
        import dataset.processing.pre_processors.common_block_preprocessor
        importlib.reload(dataset.processing.pre_processors.common_block_preprocessor)
        from dataset.processing.pre_processors.common_block_preprocessor import CommonBlockPreprocessor
        
        common_preprocessor = CommonBlockPreprocessor(config=preprocessor_config)
        try:
            # Debug: verificar que el hash est√© presente antes del preprocessor
            hash_before_preprocessor = raw_document_metadata.get("hash_documento_original")
            self.logger.debug(f"hash_documento_original antes del preprocessor: '{hash_before_preprocessor}' (tipo: {type(hash_before_preprocessor)})")
            self.logger.info(f"Aplicando CommonBlockPreprocessor a {len(raw_blocks)} bloques.")
            processed_blocks, processed_document_metadata = common_preprocessor.process(raw_blocks, raw_document_metadata)
            self.logger.debug(f"CommonBlockPreprocessor finalizado. Bloques resultantes: {len(processed_blocks)}.")
            # Debug: verificar que el hash se preserve despu√©s del preprocessor
            hash_after_preprocessor = processed_document_metadata.get("hash_documento_original")
            self.logger.debug(f"hash_documento_original despu√©s del preprocessor: '{hash_after_preprocessor}' (tipo: {type(hash_after_preprocessor)})")
        except Exception as e:
            self.logger.error(f"Excepci√≥n durante CommonBlockPreprocessor para archivo {file_path}: {str(e)}", exc_info=True)
            # Si el preprocesador falla, usamos los datos crudos del loader pero a√±adimos un error
            existing_error = raw_document_metadata.get('error', '') or ''
            raw_document_metadata['error'] = (existing_error + 
                                             f"; Excepci√≥n en CommonBlockPreprocessor: {str(e)}").strip('; ')
            # Devolver datos crudos del loader con el error del preprocesador a√±adido
            return raw_blocks, {}, raw_document_metadata 

        # Si el preprocesador marc√≥ un error, lo propagamos.
        # Los errores del loader ya se manejaron, as√≠ que esto ser√≠a un error del preprocesador.
        if processed_document_metadata.get('error'):
            self.logger.error(f"Error reportado por CommonBlockPreprocessor para {file_path}: {processed_document_metadata['error']}")
            return [], {}, processed_document_metadata
            
        # 3. Crear segmentador seg√∫n perfil
        profile = self.get_profile(profile_name) # Recargar perfil por si se modific√≥
        segmenter = self.create_segmenter(profile_name)
        if not segmenter:
            # Si no se pudo crear el segmentador, devolver los bloques pre-procesados con un error.
            existing_error = processed_document_metadata.get('error', '') or ''
            processed_document_metadata['error'] = (existing_error + 
                                                 f"; No se pudo crear el segmentador '{profile.get('segmenter') if profile else 'desconocido'}' para el perfil '{profile_name}'").strip('; ')
            return processed_blocks, {}, processed_document_metadata
        
        # Configurar umbral de confianza si el segmentador lo soporta
        if hasattr(segmenter, 'set_confidence_threshold'):
            segmenter.set_confidence_threshold(confidence_threshold)
            self.logger.debug(f"Umbral de confianza establecido a: {confidence_threshold}")
        
        # 4. Segmentar contenido (usando los bloques pre-procesados)
        self.logger.info(f"Segmentando archivo: {file_path} con {len(processed_blocks)} bloques pre-procesados.")
        segments = segmenter.segment(blocks=processed_blocks)
        segmenter_stats = segmenter.get_stats() if hasattr(segmenter, 'get_stats') else {}
        
        # 4.1. Detectar idioma del documento (o usar override)
        detected_lang = None
        if language_override:
            # Validar c√≥digo de idioma antes de usarlo
            if len(language_override.strip()) < 2 or len(language_override.strip()) > 5:
                self.logger.warning(f"C√≥digo de idioma inv√°lido: '{language_override}' (debe tener 2-5 caracteres). Usando detecci√≥n autom√°tica.")
                # No asignar detected_lang, continuar con detecci√≥n autom√°tica
            elif any(char.isdigit() or not char.isalnum() for char in language_override.strip() if char != '-'):
                self.logger.warning(f"C√≥digo de idioma contiene caracteres inv√°lidos: '{language_override}'. Usando detecci√≥n autom√°tica.")
                # No asignar detected_lang, continuar con detecci√≥n autom√°tica  
            else:
                detected_lang = language_override.strip().lower()
                self.logger.info(f"Usando idioma forzado para {file_path}: {detected_lang}")
        elif processed_blocks:
            try:
                # Concatenar texto de los primeros bloques para obtener una muestra representativa
                sample_texts = []
                total_chars = 0
                max_chars = 1000  # L√≠mite de caracteres para la muestra
                max_blocks = 5    # M√°ximo n√∫mero de bloques a usar
                
                for i, block in enumerate(processed_blocks[:max_blocks]):
                    if total_chars >= max_chars:
                        break
                    
                    block_text = ""
                    if isinstance(block, dict):
                        # Extraer texto del bloque seg√∫n su estructura
                        block_text = block.get('text', '') or block.get('content', '') or str(block.get('cleaned_text', ''))
                    else:
                        block_text = str(block)
                    
                    if block_text.strip():
                        remaining_chars = max_chars - total_chars
                        if len(block_text) > remaining_chars:
                            block_text = block_text[:remaining_chars]
                        sample_texts.append(block_text.strip())
                        total_chars += len(block_text)
                
                sample_text = " ".join(sample_texts).strip()
                
                # Intentar detectar idioma si hay suficiente texto
                if len(sample_text) >= 20:  # M√≠nimo de caracteres para detecci√≥n confiable
                    detected_lang = detect(sample_text)
                    self.logger.info(f"Idioma detectado autom√°ticamente para {file_path}: {detected_lang}")
                else:
                    self.logger.debug(f"Texto insuficiente para detecci√≥n de idioma en {file_path} (solo {len(sample_text)} caracteres)")
                    detected_lang = "und"
                    
            except LangDetectException as e:
                self.logger.debug(f"No se pudo detectar idioma para {file_path}: {str(e)}")
                detected_lang = "und"
            except Exception as e:
                self.logger.warning(f"Error inesperado durante detecci√≥n de idioma para {file_path}: {str(e)}")
                detected_lang = "und"
        else:
            self.logger.debug(f"No hay bloques procesados para detectar idioma en {file_path}")
            detected_lang = "und"
        
        # 4.5. Transformar segmentos (diccionarios) en instancias de ProcessedContentItem
        processed_content_items: List[ProcessedContentItem] = []
        if segments:
            # Extraer informaci√≥n general del job_config_dict si existe
            # y de processed_document_metadata (que ya contiene info del loader y preprocessor)
            
            # Crear un objeto BatchContext (si job_config_dict est√° disponible)
            batch_context_obj = None
            if job_config_dict:
                batch_context_obj = BatchContext(
                    author_name=job_config_dict.get("author_name", processed_document_metadata.get("autor_documento", "Desconocido")),
                    language_code=job_config_dict.get("language_code", processed_document_metadata.get("idioma_documento", "und")),
                    origin_type_name=job_config_dict.get("origin_type_name", "Desconocido"),
                    acquisition_date=job_config_dict.get("acquisition_date"),
                    force_null_publication_date=job_config_dict.get("force_null_publication_date", False)
                )

            # Informaci√≥n com√∫n a todos los segmentos de este documento
            id_doc_fuente = processed_document_metadata.get("hash_documento_original") or str(processed_document_metadata.get("ruta_archivo_original", uuid.uuid4()))
            titulo_doc = processed_document_metadata.get("titulo_documento", Path(file_path).stem)
            
            # Aplicar override de autor si est√° presente
            if author_override:
                # Validar nombre de autor antes de usarlo
                author_clean = author_override.strip()
                if len(author_clean) == 0:
                    self.logger.warning(f"Nombre de autor vac√≠o. Usando detecci√≥n autom√°tica.")
                    # No cambiar autor_doc, mantener detecci√≥n autom√°tica
                elif len(author_clean) > 200:
                    self.logger.warning(f"Nombre de autor muy largo ({len(author_clean)} caracteres). Truncando a 200 caracteres.")
                    autor_doc = author_clean[:200].strip()
                    self.logger.info(f"Usando autor forzado (truncado) para {file_path}: '{autor_doc}'")
                elif any(char in author_clean for char in ['<', '>', '|', ':', '*', '?', '"', '\\', '/', '\n', '\r', '\t']):
                    self.logger.warning(f"Nombre de autor contiene caracteres problem√°ticos: '{author_clean}'. Limpiando caracteres especiales.")
                    # Limpiar caracteres problem√°ticos
                    import re
                    autor_doc = re.sub(r'[<>|:*?"\\\/\n\r\t]', '', author_clean).strip()
                    if len(autor_doc) > 0:
                        self.logger.info(f"Usando autor forzado (limpiado) para {file_path}: '{autor_doc}'")
                    else:
                        self.logger.warning(f"Nombre de autor qued√≥ vac√≠o despu√©s de limpiar. Usando detecci√≥n autom√°tica.")
                        # No cambiar autor_doc, mantener detecci√≥n autom√°tica
                else:
                    autor_doc = author_clean
                    self.logger.info(f"Usando autor forzado para {file_path}: '{autor_doc}'")
            else:
                autor_doc = batch_context_obj.author_name if batch_context_obj else processed_document_metadata.get("autor_documento")
                if autor_doc:
                    self.logger.info(f"Usando autor detectado para {file_path}: '{autor_doc}'")
                else:
                    self.logger.debug(f"No se detect√≥ autor para {file_path}")
            
            fecha_pub_doc = processed_document_metadata.get("fecha_publicacion_documento") # Ya deber√≠a estar parseada por el loader o preprocessor
            
            ruta_original_doc = str(processed_document_metadata.get("ruta_archivo_original", processed_document_metadata.get("source_file_path", file_path)))
            
            # Metadatos adicionales de la fuente (del loader/preprocessor)
            meta_adicionales_fuente = processed_document_metadata.get("metadatos_adicionales_fuente", {})
            if batch_context_obj: # A√±adir info del job a los metadatos adicionales
                meta_adicionales_fuente["job_id"] = job_config_dict.get("job_id", "N/A")
                meta_adicionales_fuente["job_origin_type_name"] = batch_context_obj.origin_type_name
                if batch_context_obj.acquisition_date:
                    meta_adicionales_fuente["job_acquisition_date"] = batch_context_obj.acquisition_date

            current_timestamp_iso = datetime.now(timezone.utc).isoformat()

            # Log de debug para verificar el idioma detectado antes del bucle
            if detected_lang:
                self.logger.debug(f"Idioma detectado disponible para asignaci√≥n: '{detected_lang}'")
            else:
                self.logger.debug(f"No se detect√≥ idioma v√°lido, detected_lang = {detected_lang}")
            
            # Log de debug para verificar las claves disponibles en processed_document_metadata
            self.logger.debug(f"Claves disponibles en processed_document_metadata: {list(processed_document_metadata.keys())}")
            self.logger.debug(f"Valor de hash_documento_original en metadata: '{processed_document_metadata.get('hash_documento_original')}'")
            self.logger.debug(f"Valor de ruta_archivo_original en metadata: '{processed_document_metadata.get('ruta_archivo_original')}'")
            self.logger.debug(f"Valor de source_file_path en metadata: '{processed_document_metadata.get('source_file_path')}'")
            self.logger.debug(f"Valor de idioma_documento en metadata: '{processed_document_metadata.get('idioma_documento')}'")
            
            # Log de debug para verificar variables locales cr√≠ticas
            self.logger.debug(f"Variable detected_lang: '{detected_lang}' (tipo: {type(detected_lang)})")
            self.logger.debug(f"Variable batch_context_obj: {batch_context_obj}")
            if batch_context_obj:
                self.logger.debug(f"batch_context_obj.language_code: '{batch_context_obj.language_code}'")

            for i, segment_dict in enumerate(segments):
                # segment_dict es el diccionario devuelto por el segmentador (ej. {'type': 'poem', 'title': '...', 'verses': [...]})
                # Necesitamos mapear esto a ProcessedContentItem

                texto_segmento_final = ""
                tipo_segmento_final = segment_dict.get("type", "desconocido")
                jerarquia_final = segment_dict.get("jerarquia_contextual", {}) # Los segmentadores deber√≠an empezar a popular esto

                if tipo_segmento_final == "poem" and "numbered_verses" in segment_dict:
                    # Reconstruir el texto del poema desde los versos numerados si es necesario
                    # o usar un campo 'full_text' si el segmentador ya lo provee.
                    # VerseSegmenter.get_full_text_from_numbered_verses() podr√≠a ser √∫til aqu√≠.
                    # Por ahora, asumimos que el segmentador provee 'text' o se puede tomar 'title' + 'verses'
                    # Esto necesita refinamiento seg√∫n la salida EXACTA de cada segmentador.
                    # Ejemplo simple:
                    temp_text_parts = []
                    if segment_dict.get("title"):
                        temp_text_parts.append(segment_dict["title"])
                    if "numbered_verses" in segment_dict and isinstance(segment_dict["numbered_verses"], list):
                         # Asumimos que VerseSegmenter fue modificado para a√±adir 'text' a cada verso en numbered_verses
                        temp_text_parts.extend([v_dict.get("text", "") for v_dict in segment_dict["numbered_verses"] if v_dict.get("text","").strip()])
                    elif "verses" in segment_dict and isinstance(segment_dict["verses"], list): # Fallback si no hay numbered_verses
                        temp_text_parts.extend(segment_dict["verses"])

                    texto_segmento_final = "\n".join(filter(None,temp_text_parts)).strip()
                    # La jerarqu√≠a podr√≠a ser el t√≠tulo del poema o la estrofa
                    if segment_dict.get("title"):
                        jerarquia_final.setdefault("poema_titulo", segment_dict.get("title"))
                    # Aqu√≠ podr√≠as a√±adir info de estrofa/verso si el segmentador lo desglosa a ese nivel para cada CHUNK.
                    # Actualmente, el segmentador devuelve el poema ENTERO como un chunk.

                elif tipo_segmento_final == "section" and "content" in segment_dict:
                    # Para HeadingSegmenter, 'content' puede ser una lista de p√°rrafos o ya un string.
                    # Y 'title' es el t√≠tulo de la secci√≥n.
                    content_data = segment_dict.get("content")
                    if isinstance(content_data, list):
                        texto_segmento_final = "\n\n".join(p.get("text") if isinstance(p, dict) else str(p) for p in content_data if (p.get("text") if isinstance(p,dict) else str(p)).strip())
                    elif isinstance(content_data, str):
                        texto_segmento_final = content_data
                    else: # Fallback si 'content' no est√° o es inesperado
                        texto_segmento_final = segment_dict.get("text", segment_dict.get("title", ""))

                    jerarquia_final.setdefault(f"nivel_{segment_dict.get('level', 'desconocido')}", segment_dict.get("title", "Sin T√≠tulo"))
                    # Aqu√≠ se podr√≠a construir una jerarqu√≠a m√°s rica si el segmentador devuelve info de parent_id, etc.

                elif "text" in segment_dict: # Para p√°rrafos u otros tipos simples
                    texto_segmento_final = str(segment_dict["text"])
                
                if not texto_segmento_final.strip(): # Omitir segmentos vac√≠os
                    self.logger.debug(f"Omitiendo segmento vac√≠o o sin texto procesable: {segment_dict}")
                    continue

                # L√≥gica de precedencia para idioma del documento (corregida)
                final_idioma_doc = "und"  # Valor por defecto

                # Log detallado de la l√≥gica de precedencia (solo para el primer segmento)
                if i == 0:
                    self.logger.debug(f"=== INICIO L√ìGICA DE PRECEDENCIA IDIOMA ===")
                    self.logger.debug(f"batch_context_obj existe: {batch_context_obj is not None}")
                    if batch_context_obj:
                        self.logger.debug(f"batch_context_obj.language_code: '{batch_context_obj.language_code}'")
                        self.logger.debug(f"batch_context_obj.language_code v√°lido: {batch_context_obj.language_code and batch_context_obj.language_code.lower() != 'und'}")
                    self.logger.debug(f"detected_lang: '{detected_lang}'")
                    self.logger.debug(f"detected_lang v√°lido: {detected_lang and str(detected_lang).lower() != 'und'}")
                    self.logger.debug(f"idioma_documento en metadata: '{processed_document_metadata.get('idioma_documento')}'")

                # 1. Intentar con batch_context si existe y tiene un idioma v√°lido
                if batch_context_obj and batch_context_obj.language_code and batch_context_obj.language_code.lower() != "und":
                    final_idioma_doc = batch_context_obj.language_code
                    if i == 0:  # Solo log en el primer segmento para evitar spam
                        self.logger.debug(f"‚úì Usando idioma del job config: {final_idioma_doc}")
                # 2. Si no, intentar con el idioma detectado (variable definida ANTES del bucle) si es v√°lido
                elif detected_lang and str(detected_lang).lower() != "und":
                    final_idioma_doc = str(detected_lang)
                    if i == 0:  # Solo log en el primer segmento para evitar spam
                        self.logger.debug(f"‚úì Usando idioma detectado autom√°ticamente: {final_idioma_doc}")
                # 3. Si no, intentar con el idioma de los metadatos del documento (del loader) si es v√°lido
                elif processed_document_metadata.get("idioma_documento") and str(processed_document_metadata.get("idioma_documento")).lower() != "und":
                    final_idioma_doc = str(processed_document_metadata.get("idioma_documento"))
                    if i == 0:  # Solo log en el primer segmento para evitar spam
                        self.logger.debug(f"‚úì Usando idioma de metadatos del loader: {final_idioma_doc}")
                # Si todo lo anterior falla, se queda con "und"
                else:
                    if i == 0:  # Solo log en el primer segmento para evitar spam
                        self.logger.debug(f"‚úó Usando idioma por defecto 'und' para todos los segmentos")
                        self.logger.debug(f"=== FIN L√ìGICA DE PRECEDENCIA IDIOMA ===")

                # Log de verificaci√≥n para el primer segmento
                if i == 0:
                    self.logger.debug(f"final_idioma_doc determinado para todos los segmentos: '{final_idioma_doc}'")
                    # Debug adicional para rastrear el problema
                    hash_value = processed_document_metadata.get("hash_documento_original")
                    self.logger.debug(f"hash_documento_original en processed_document_metadata: '{hash_value}' (tipo: {type(hash_value)})")
                    self.logger.debug(f"detected_lang variable: '{detected_lang}' (tipo: {type(detected_lang)})")
                    self.logger.debug(f"final_idioma_doc variable: '{final_idioma_doc}' (tipo: {type(final_idioma_doc)})")

                # Log adicional justo antes de crear el ProcessedContentItem
                if i == 0:
                    hash_for_item = processed_document_metadata.get("hash_documento_original")
                    self.logger.debug(f"Valores para ProcessedContentItem - hash_documento_original: '{hash_for_item}', idioma_documento: '{final_idioma_doc}'")

                # Asignaciones expl√≠citas para mayor claridad
                hash_documento_para_item = processed_document_metadata.get("hash_documento_original")
                idioma_documento_para_item = final_idioma_doc
                
                # Log adicional para verificar las asignaciones expl√≠citas
                if i == 0:
                    self.logger.debug(f"=== VERIFICACI√ìN ASIGNACIONES EXPL√çCITAS ===")
                    self.logger.debug(f"hash_documento_para_item: '{hash_documento_para_item}' (tipo: {type(hash_documento_para_item)})")
                    self.logger.debug(f"idioma_documento_para_item: '{idioma_documento_para_item}' (tipo: {type(idioma_documento_para_item)})")
                    self.logger.debug(f"¬øhash_documento_para_item es None?: {hash_documento_para_item is None}")
                    self.logger.debug(f"¬øhash_documento_para_item es cadena vac√≠a?: {hash_documento_para_item == ''}")
                    self.logger.debug(f"¬øidioma_documento_para_item es None?: {idioma_documento_para_item is None}")
                    self.logger.debug(f"¬øidioma_documento_para_item es 'und'?: {idioma_documento_para_item == 'und'}")

                item = ProcessedContentItem(
                    id_segmento=str(uuid.uuid4()),
                    id_documento_fuente=id_doc_fuente,
                    ruta_archivo_original=ruta_original_doc,
                    hash_documento_original=hash_documento_para_item,
                    titulo_documento=titulo_doc,
                    autor_documento=autor_doc,
                    fecha_publicacion_documento=fecha_pub_doc,
                    editorial_documento=processed_document_metadata.get("editorial_documento"), # Tomar del doc si existe
                    isbn_documento=processed_document_metadata.get("isbn_documento"), # Tomar del doc si existe
                    idioma_documento=idioma_documento_para_item,
                    metadatos_adicionales_fuente=meta_adicionales_fuente,
                    texto_segmento=texto_segmento_final,
                    tipo_segmento=tipo_segmento_final,
                    orden_segmento_documento=segment_dict.get("order_in_document", i), # Usar orden del segmentador si existe
                    jerarquia_contextual=jerarquia_final,
                    longitud_caracteres_segmento=len(texto_segmento_final),
                    embedding_vectorial=None, # Se generar√° despu√©s
                    timestamp_procesamiento=current_timestamp_iso,
                    version_pipeline_etl="1.0.0-refactor-ui", # Actualizar seg√∫n sea necesario
                    nombre_segmentador_usado=profile.get('segmenter', 'desconocido') if profile else 'desconocido',
                    notas_procesamiento_segmento=segment_dict.get("processing_notes") # Si el segmentador a√±ade notas
                )
                
                # Log adicional despu√©s de crear el ProcessedContentItem
                if i == 0:
                    self.logger.debug(f"ProcessedContentItem creado - hash_documento_original: '{item.hash_documento_original}', idioma_documento: '{item.idioma_documento}'")
                    self.logger.debug(f"Verificaci√≥n de variables expl√≠citas - hash_documento_para_item: '{hash_documento_para_item}', idioma_documento_para_item: '{idioma_documento_para_item}'")
                
                processed_content_items.append(item)
        else: # No hay segmentos del segmentador
            # Si no hay segmentos pero el loader/preprocessor gener√≥ metadatos (ej. archivo vac√≠o o error manejado),
            # podr√≠amos crear un ProcessedContentItem "placeholder" o simplemente devolver la lista vac√≠a.
            # Por ahora, si no hay segmentos del segmentador, processed_content_items quedar√° vac√≠a.
            pass
        
        # 5. TODO: Aplicar post-procesador si est√° configurado
        
        # 6. Exportar si se especific√≥ ruta de salida
        # Usar processed_document_metadata para la parte de metadatos del documento
        # y processed_content_items para los segmentos.
        if output_file: # ‚úÖ CORREGIDO: output_file es la ruta del archivo de salida
            # El primer if es para si manager.process_file devolvi√≥ segmentos (ahora processed_content_items)
            if processed_content_items:
                self._export_results(processed_content_items, output_file, processed_document_metadata, output_format)
            # El segundo elif es para cuando no hubo segmentos (lista vac√≠a) pero S√ç hay un output_path
            # y queremos exportar los metadatos del documento (que pueden contener un error o advertencia).
            elif not processed_content_items: 
                self.logger.info(f"No se generaron ProcessedContentItems para {file_path}, pero se exportar√°n metadatos a {output_file}")
                self._export_results([], output_file, processed_document_metadata, output_format)

        # Devolver la tupla completa como espera process_file.py, usando la nueva lista de dataclasses
        return processed_content_items, segmenter_stats, processed_document_metadata
    
    def _detect_extreme_corruption(self, text: str, corruption_threshold: float = 0.7) -> Tuple[bool, str]:
        """
        üîß NUEVA FUNCIONALIDAD - Detecta corrupci√≥n extrema en texto.
        
        Args:
            text: Texto a analizar
            corruption_threshold: Umbral de corrupci√≥n (0.7 = 70% de caracteres corruptos)
            
        Returns:
            Tuple[bool, str]: (is_corrupted, reason)
        """
        if not text or len(text.strip()) < 10:
            return False, ""
        
        text = text.strip()
        total_chars = len(text)
        
        # Contar caracteres problem√°ticos
        control_chars = sum(1 for c in text if ord(c) < 32 and c not in ['\n', '\r', '\t'])
        replacement_chars = text.count('ÔøΩ')  # Caracteres de reemplazo Unicode
        null_chars = text.count('\x00')
        
        # Contar secuencias repetitivas de caracteres extra√±os (solo contar si son significativas)
        weird_pattern_matches = re.findall(r'[^\w\s\.,;:!?¬°¬ø\-\(\)\[\]"\'√°√©√≠√≥√∫√±√º√Å√â√ç√ì√ö√ë√ú]+', text)
        weird_patterns = sum(len(match) for match in weird_pattern_matches if len(match) >= 3)
        
        # Calcular porcentaje de corrupci√≥n
        corrupted_chars = control_chars + replacement_chars + null_chars + weird_patterns
        corruption_ratio = corrupted_chars / total_chars if total_chars > 0 else 0
        
        # Detectar patrones espec√≠ficos de corrupci√≥n extrema
        extreme_patterns = [
            r'ÔøΩ{10,}',  # 10 o m√°s caracteres de reemplazo consecutivos
            r'[\x00-\x08\x0B\x0C\x0E-\x1F]{5,}',  # 5 o m√°s caracteres de control consecutivos
            r'^[\sÔøΩ\x00-\x1F]*$',  # Solo espacios, caracteres de reemplazo y control
        ]
        
        has_extreme_pattern = any(re.search(pattern, text) for pattern in extreme_patterns)
        
        # Determinar si est√° extremadamente corrupto (MUCHO M√ÅS RESTRICTIVO)
        is_extremely_corrupted = (
            replacement_chars >= 50 or  # Al menos 50 caracteres de reemplazo
            has_extreme_pattern or
            (replacement_chars >= 20 and replacement_chars > (total_chars * 0.5))  # 20+ caracteres Y m√°s del 50%
        )
        
        if is_extremely_corrupted:
            reason = f"Corrupci√≥n extrema detectada: {corruption_ratio:.1%} caracteres corruptos"
            if has_extreme_pattern:
                reason += " (patrones extremos detectados)"
            return True, reason
        
        return False, ""

    def _export_results(self, segments: List[Any], output_file: str, document_metadata: Optional[Dict[str, Any]] = None, output_format: str = "ndjson"):
        """
        üîß MEJORADO - Exporta resultados con detecci√≥n de corrupci√≥n extrema.
        CORREGIDO: Ahora maneja archivos correctamente, no directorios.
        ACTUALIZADO: Maneja objetos ProcessedContentItem en lugar de diccionarios.
        """
        import json
        
        try:
            output_path = Path(output_file)
            # ‚úÖ CORREGIDO: Solo crear el directorio padre, no el archivo como directorio
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            if not document_metadata:
                document_metadata = {}
            
            timestamp = datetime.now(timezone.utc).isoformat()
            
            results = []
            corrupted_segments_count = 0
            
            for i, segment in enumerate(segments):
                # ‚úÖ CORREGIDO: Detectar si es ProcessedContentItem o diccionario
                if hasattr(segment, 'texto_segmento'):
                    # Es un ProcessedContentItem
                    segment_text = segment.texto_segmento
                    segment_type = segment.tipo_segmento
                    segment_id = segment.id_segmento
                    document_id = segment.id_documento_fuente
                    segment_order = segment.orden_segmento_documento
                    processing_notes = segment.notas_procesamiento_segmento
                    segment_metadata = segment.metadatos_adicionales_fuente or {}
                    segment_hierarchy = segment.jerarquia_contextual or {}
                    
                    # Usar datos del objeto ProcessedContentItem
                    segment_data = {
                        "id_segmento": segment_id,
                        "id_documento_fuente": document_id,
                        "idioma_documento": segment.idioma_documento,
                        "texto_segmento": segment_text,
                        "tipo_segmento": segment_type,
                        "orden_segmento_documento": segment_order,
                        "longitud_caracteres_segmento": segment.longitud_caracteres_segmento,
                        "timestamp_procesamiento": segment.timestamp_procesamiento,
                        "ruta_archivo_original": segment.ruta_archivo_original,
                        "hash_documento_original": segment.hash_documento_original,
                        "titulo_documento": segment.titulo_documento,
                        "autor_documento": segment.autor_documento,
                        "fecha_publicacion_documento": segment.fecha_publicacion_documento,
                        "editorial_documento": segment.editorial_documento,
                        "isbn_documento": segment.isbn_documento,
                        "metadatos_adicionales_fuente": segment_metadata,
                        "jerarquia_contextual": segment_hierarchy,
                        "embedding_vectorial": segment.embedding_vectorial,
                        "version_pipeline_etl": segment.version_pipeline_etl,
                        "nombre_segmentador_usado": segment.nombre_segmentador_usado,
                        "notas_procesamiento_segmento": processing_notes
                    }
                else:
                    # Es un diccionario (compatibilidad hacia atr√°s)
                    segment_text = segment.get('text', '')
                    segment_type = segment.get('type', 'unknown')
                    segment_id = str(uuid.uuid4())
                    document_id = str(uuid.uuid4())
                    segment_order = i + 1
                    processing_notes = segment.get('notes', None)
                    segment_metadata = segment.get('metadata', {})
                    segment_hierarchy = segment.get('hierarchy', {})
                    
                    segment_data = {
                        "id_segmento": segment_id,
                        "id_documento_fuente": document_id,
                        "idioma_documento": document_metadata.get('language', 'es'),
                        "texto_segmento": segment_text,
                        "tipo_segmento": segment_type,
                        "orden_segmento_documento": segment_order,
                        "longitud_caracteres_segmento": len(segment_text),
                        "timestamp_procesamiento": timestamp,
                        "ruta_archivo_original": document_metadata.get('file_path', ''),
                        "hash_documento_original": document_metadata.get('hash', None),
                        "titulo_documento": document_metadata.get('title', ''),
                        "autor_documento": document_metadata.get('author', None),
                        "fecha_publicacion_documento": document_metadata.get('publication_date', None),
                        "editorial_documento": document_metadata.get('publisher', None),
                        "isbn_documento": document_metadata.get('isbn', None),
                        "metadatos_adicionales_fuente": segment_metadata,
                        "jerarquia_contextual": segment_hierarchy,
                        "embedding_vectorial": None,
                        "version_pipeline_etl": "1.0.0-refactor-ui",
                        "nombre_segmentador_usado": document_metadata.get('segmenter_name', 'unknown'),
                        "notas_procesamiento_segmento": processing_notes
                    }
                
                # üîß NUEVA FUNCIONALIDAD - Detectar y manejar corrupci√≥n extrema
                is_corrupted, corruption_reason = self._detect_extreme_corruption(segment_text)
                
                if is_corrupted:
                    corrupted_segments_count += 1
                    # Reemplazar texto corrupto con mensaje descriptivo
                    segment_data["texto_segmento"] = f"[TEXTO CORRUPTO EN ARCHIVO DE ORIGEN]\n\nSegmento #{i+1} del documento contiene texto extremadamente corrupto que no se puede procesar correctamente.\n\nRaz√≥n: {corruption_reason}\n\nSe recomienda revisar el archivo PDF original o intentar con OCR avanzado."
                    segment_data["longitud_caracteres_segmento"] = len(segment_data["texto_segmento"])
                    
                    # Log del problema
                    self.logger.warning(f"üö® Corrupci√≥n extrema en segmento #{i+1}: {corruption_reason}")
                    
                    # Agregar informaci√≥n espec√≠fica de corrupci√≥n
                    segment_data["metadatos_adicionales_fuente"]["corruption_detected"] = True
                    segment_data["metadatos_adicionales_fuente"]["corruption_reason"] = corruption_reason
                    segment_data["metadatos_adicionales_fuente"]["original_length"] = len(segment_text)
                    segment_data["notas_procesamiento_segmento"] = corruption_reason
                
                results.append(segment_data)
            
            # Log resumen de corrupci√≥n
            if corrupted_segments_count > 0:
                self.logger.warning(f"üö® RESUMEN DE CORRUPCI√ìN: {corrupted_segments_count}/{len(segments)} segmentos ten√≠an corrupci√≥n extrema y fueron reemplazados")
            
            # Exportar resultados
            if output_format.lower() == "json":
                # Formato JSON: crear un objeto con metadatos y array de segmentos
                output_data = {
                    "document_metadata": document_metadata,
                    "segments": results
                }
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(output_data, f, ensure_ascii=False, indent=2)
            else:
                # Formato NDJSON (por defecto): una l√≠nea por segmento
                with open(output_path, 'w', encoding='utf-8') as f:
                    for result in results:
                        f.write(json.dumps(result, ensure_ascii=False) + '\n')
            
            self.logger.info(f"Resultados exportados en formato {output_format.upper()}: {output_path}")
            if corrupted_segments_count > 0:
                self.logger.info(f"üìä Estad√≠sticas de corrupci√≥n: {corrupted_segments_count} segmentos reemplazados por texto corrupto")
                
        except Exception as e:
            self.logger.error(f"Error al exportar resultados: {str(e)}")

    def get_profile_for_file(self, file_path: str) -> Optional[str]:
        """
        Sugiere el perfil m√°s adecuado para un archivo basado en su nombre y extensi√≥n.
        
        Args:
            file_path: Ruta al archivo
            
        Returns:
            Nombre del perfil sugerido o None si no hay sugerencia
        """
        file_path = Path(file_path)
        extension = file_path.suffix.lower()
        filename = file_path.stem.lower()
        
        # Palabras clave que sugieren tipos de contenido
        poem_keywords = ['poema', 'poemas', 'poes√≠a', 'poes√≠as', 'versos', 'verso', 'estrofa', 'poeta', 
                         'poem', 'poetry', 'lyric', 'lyrics', 'canci√≥n', 'canciones']
        book_keywords = ['libro', 'capitulo', 'cap√≠tulos', 'book', 'chapter', 'section', 'manual', 
                        'gu√≠a', 'documento', 'texto', 'escrito']
        
        # Comprobar primero en el nombre del archivo
        for keyword in poem_keywords:
            if keyword in filename:
                self.logger.debug(f"Perfil poem_or_lyrics detectado por palabra clave en el nombre: {keyword}")
                return 'poem_or_lyrics'
                
        for keyword in book_keywords:
            if keyword in filename:
                self.logger.debug(f"Perfil book_structure detectado por palabra clave en el nombre: {keyword}")
                return 'book_structure'
                
        # Verificar si alg√∫n perfil tiene configuraci√≥n espec√≠fica para esta extensi√≥n
        for profile_name, profile in self.profiles.items():
            if 'file_types' in profile and extension in profile['file_types']:
                # Si ambos perfiles soportan el tipo, poem_or_lyrics tiene preferencia para documentos personales
                if extension in ['.txt', '.md', '.docx']:
                    if 'poem_or_lyrics' in self.profiles:
                        self.logger.debug(f"Perfil poem_or_lyrics sugerido para extensi√≥n personal: {extension}")
                        return 'poem_or_lyrics'
                
                self.logger.debug(f"Perfil {profile_name} sugerido por tipo de archivo: {extension}")
                return profile_name
        
        # Si todo lo dem√°s falla, poem_or_lyrics es una buena opci√≥n predeterminada para archivos personales
        if extension in ['.txt', '.md', '.docx']:
            if 'poem_or_lyrics' in self.profiles:
                self.logger.debug(f"Perfil poem_or_lyrics sugerido por defecto para archivos personales")
                return 'poem_or_lyrics'
        
        return None

    def create_pre_processor(self, pre_processor_type: str, profile: Optional[Dict] = None) -> 'BasePreProcessor':
        """
        Crea una instancia del pre-procesador especificado.
        
        Args:
            pre_processor_type: Tipo de pre-procesador a crear
            profile: Perfil opcional para configuraci√≥n espec√≠fica
            
        Returns:
            Instancia del pre-procesador
        """
        # LOGGING DETALLADO PARA DEBUG
        print(f"üîßüîßüîß CREANDO PRE-PROCESADOR: {pre_processor_type} üîßüîßüîß")
        print(f"üîßüîßüîß PERFIL RECIBIDO: {profile.get('name') if profile else 'None'} üîßüîßüîß")
        
        preprocessor_config = profile.get('pre_processor_config') if profile else None
        print(f"üîßüîßüîß CONFIG DEL PRE-PROCESADOR: {preprocessor_config} üîßüîßüîß")
        
        # Obtener configuraci√≥n del pre-procesador desde el perfil
        if pre_processor_type == 'common_block':
            return CommonBlockPreprocessor(config=preprocessor_config)
        # Agregar otros tipos seg√∫n sea necesario
        else:
            raise ValueError(f"Tipo de pre-procesador no soportado: {pre_processor_type}")

if __name__ == "__main__":
    # Configuraci√≥n b√°sica de logging
    logging.basicConfig(level=logging.INFO)
    
    # Ejemplo de uso
    manager = ProfileManager()
    print("Perfiles disponibles:")
    for profile in manager.list_profiles():
        print(f"- {profile['name']}: {profile['description']}")