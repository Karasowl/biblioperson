from datetime import datetime, timezone, timedelta
import re
import hashlib
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime, timezone, timedelta
# import PyPDF2 # Eliminado PyPDF2
import fitz  # Importaci√≥n de PyMuPDF
import logging

from .base_loader import BaseLoader

def _calculate_sha256(file_path: Path) -> str:
    """Calcula el hash SHA256 de un archivo."""
    hash_sha256 = hashlib.sha256()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_sha256.update(chunk)
    return hash_sha256.hexdigest()

logger = logging.getLogger(__name__)

class PDFLoader(BaseLoader):
    """Loader para archivos PDF (.pdf) usando PyMuPDF (fitz) para mayor rendimiento."""
    
    def __init__(self, file_path: str | Path, tipo: str = 'escritos', encoding: str = 'utf-8'):
        """
        Inicializa el loader para archivos PDF.
        
        Args:
            file_path: Ruta al archivo PDF
            tipo: Tipo de contenido (no se usa directamente aqu√≠ pero podr√≠a ser √∫til para perfiles)
            encoding: Codificaci√≥n (no se usa directamente en PDFs pero se mantiene por consistencia)
        """
        super().__init__(file_path)
        self.tipo = tipo.lower()
        # self.encoding = encoding # Encoding no es tan relevante para fitz de esta manera
        logger.debug(f"PDFLoader (fitz) inicializado para: {self.file_path} con tipo: {self.tipo}")

    def _reconstruct_paragraphs(self, lines: List[str]) -> str:
        """
        üö® VERSI√ìN 6.0 - RECONSTRUCCI√ìN ANTI-SALTO DE P√ÅGINA üö®
        
        Reconstruye p√°rrafos inteligentemente a partir de l√≠neas individuales.
        
        Heur√≠sticas aplicadas:
        1. Las l√≠neas que terminan sin puntuaci√≥n se unen con la siguiente
        2. Las l√≠neas muy cortas se unen con la anterior/siguiente
        3. Se detectan saltos de p√°rrafo reales vs saltos de l√≠nea por formato
        
        Args:
            lines: Lista de l√≠neas de texto del bloque
            
        Returns:
            str: Texto reconstruido con p√°rrafos separados por doble salto de l√≠nea
        """
        print("üö®üö®üö® PDLOADER V6.0 - RECONSTRUCCI√ìN ANTI-SALTO üö®üö®üö®")
        logger.warning("üö®üö®üö® PDLOADER V6.0 - RECONSTRUCCI√ìN ANTI-SALTO üö®üö®üö®")
        if not lines:
            return ""
        
        if len(lines) == 1:
            single_line_result = lines[0]
            print(f"üîç PDLOADER: L√≠nea √∫nica detectada: '{single_line_result[:100]}...'")
            return single_line_result
            
        paragraphs = []
        current_paragraph = []
        
        print(f"üîç PDLOADER: Procesando {len(lines)} l√≠neas para reconstrucci√≥n")
        logger.warning(f"üîç PDLOADER: Procesando {len(lines)} l√≠neas para reconstrucci√≥n")
        
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
                
            # Heur√≠stica 1: Si la l√≠nea anterior no termina con puntuaci√≥n fuerte
            # y la l√≠nea actual no empieza con may√∫scula o numeraci√≥n, unir
            if current_paragraph:
                previous_line = current_paragraph[-1]
                is_break = self._is_paragraph_break(previous_line, line)
                
                # DIAGN√ìSTICO ESPEC√çFICO para caso "atractivo"
                if "atractivo" in previous_line.lower() or "atractivo" in line.lower():
                    print(f"üö® DIAGN√ìSTICO CR√çTICO ATRACTIVO:")
                    print(f"   L√≠nea anterior: '{previous_line}'")
                    print(f"   L√≠nea actual: '{line}'")
                    print(f"   ¬øEs salto de p√°rrafo?: {is_break}")
                    logger.warning(f"üö® ATRACTIVO - Anterior: '{previous_line}' | Actual: '{line}' | Break: {is_break}")
                
                if not is_break:
                    current_paragraph.append(line)
                else:
                    # Nuevo p√°rrafo
                    if current_paragraph:
                        paragraphs.append(' '.join(current_paragraph))
                    current_paragraph = [line]
            else:
                current_paragraph = [line]
        
        # Agregar el √∫ltimo p√°rrafo
        if current_paragraph:
            paragraphs.append(' '.join(current_paragraph))
        
        # Unir p√°rrafos con doble salto de l√≠nea
        return '\n\n'.join(paragraphs)
    
    def _is_paragraph_break(self, previous_line: str, current_line: str) -> bool:
        """
        Determina si debe haber un salto de p√°rrafo entre dos l√≠neas.
        
        HEUR√çSTICAS ANTI-SALTO DE P√ÅGINA:
        1. Si la l√≠nea anterior termina sin puntuaci√≥n fuerte ‚Üí UNIR
        2. Si la l√≠nea actual empieza con min√∫scula ‚Üí UNIR (continuaci√≥n)
        3. Si la l√≠nea actual contin√∫a una frase obvio ‚Üí UNIR
        4. ESPEC√çFICO: Detectar casos como "atractivo" + "de esta idea"
        
        Args:
            previous_line: L√≠nea anterior
            current_line: L√≠nea actual
            
        Returns:
            bool: True si debe haber salto de p√°rrafo, False si unir
        """
        import re
        
        prev_stripped = previous_line.strip()
        curr_stripped = current_line.strip()
        
        # ===== DETECCI√ìN ESPEC√çFICA DE ORACIONES DIVIDIDAS =====
        prev_words = prev_stripped.split()
        curr_words = curr_stripped.split()
        
        if prev_words and curr_words:
            last_word = prev_words[-1].lower().rstrip('.,!?:;')
            first_word = curr_words[0].lower()
            
            # Casos espec√≠ficos de divisi√≥n artificial
            artificial_splits = [
                # "atractivo" + "de esta idea"
                (last_word == "atractivo" and first_word in ["de", "del"]),
                # Otros patrones comunes
                (last_word in ["muy", "m√°s", "tan", "poco", "tanto"] and 
                 first_word not in ["pero", "sin", "embargo", "aunque"]),
                # Preposiciones que requieren continuaci√≥n
                (last_word in ["para", "por", "en", "con", "sin", "de", "del", "al", "hacia"] and
                 not curr_stripped[0].isupper()),
                # Adjetivos que claramente contin√∫an
                (last_word in ["hermoso", "interesante", "importante", "necesario", "posible"] and
                 first_word in ["de", "para", "que", "en"]),
            ]
            
            if any(artificial_splits):
                print(f"üîó DETECTADA DIVISI√ìN ARTIFICIAL: '{prev_stripped}' + '{curr_stripped}'")
                logger.warning(f"üîó DIVISI√ìN ARTIFICIAL: '{prev_stripped}' + '{curr_stripped}'")
                return False
        
        # REGLA 1: Si la l√≠nea anterior NO termina con puntuaci√≥n fuerte ‚Üí UNIR
        if not re.search(r'[.!?][\s"]*$', prev_stripped):
            return False
        
        # REGLA 2: Si la l√≠nea actual empieza con min√∫scula ‚Üí UNIR (continuaci√≥n)
        if re.match(r'^[a-z√°√©√≠√≥√∫√±√º]', curr_stripped):
            return False
            
        # REGLA 3: Patrones de continuaci√≥n obvia ‚Üí UNIR
        # Ej: "atractivo" + "de esta idea"
        continuation_patterns = [
            r'^(de|del|la|el|en|con|por|para|que|y|o|pero|sin|sobre|bajo)\s+',
            r'^(esta|este|esa|ese|aquella|aquel)\s+',
            r'^(muy|m√°s|menos|tan|tanto)\s+',
            r'^(todos|todas|algunos|muchos|pocos)\s+'
        ]
        
        for pattern in continuation_patterns:
            if re.match(pattern, curr_stripped, re.IGNORECASE):
                return False
        
        # REGLA 4: Si la l√≠nea anterior termina con puntuaci√≥n Y la actual empieza con may√∫scula
        if re.search(r'[.!?][\s"]*$', prev_stripped):
            if re.match(r'^[A-Z√Å√â√ç√ì√ö√ë√ú\d]', curr_stripped):
                return True
        
        # REGLA 5: Si la l√≠nea actual empieza con gui√≥n o numeraci√≥n ‚Üí nuevo p√°rrafo
        if re.match(r'^[-‚Ä¢]\s+|^\d+[\.\)]\s+', curr_stripped):
            return True
            
        # REGLA 6: Si ambas l√≠neas son muy cortas ‚Üí probablemente mismo p√°rrafo
        if len(prev_stripped) < 40 and len(curr_stripped) < 40:
            return False
            
        # POR DEFECTO: Si llegamos aqu√≠, es muy probable que sea continuaci√≥n
        # (la mayor√≠a de saltos de p√°gina NO son cambios de p√°rrafo)
        return False

    @staticmethod
    def _parse_pdf_datetime_str(date_str: Optional[str]) -> Tuple[Optional[str], Optional[str]]:
        """Parsea una cadena de fecha/hora de metadatos PDF (ej. 'D:YYYYMMDDHHMMSS[Z|+-HH\'MM\']').

        Returns:
            Tuple[Optional[str], Optional[str]]: (fecha YYYY-MM-DD, fecha ISO completa) o (None, None).
        """
        if not date_str or not isinstance(date_str, str) or not date_str.startswith('D:'):
            return None, None
        
        raw_dt_part = date_str[2:] # Remover 'D:'
        
        # Extraer la parte principal de la fecha/hora (YYYYMMDDHHMMSS)
        dt_core = raw_dt_part[:14]
        
        try:
            parsed_dt = datetime.strptime(dt_core, '%Y%m%d%H%M%S')
            
            # Manejar la informaci√≥n de la zona horaria si existe
            tz_part = raw_dt_part[14:]
            if tz_part:
                if tz_part == 'Z':
                    parsed_dt = parsed_dt.replace(tzinfo=timezone.utc)
                elif tz_part.startswith(('+', '-')) and len(tz_part) >= 3:
                    try:
                        offset_hours = int(tz_part[1:3])
                        offset_minutes = int(tz_part[4:6]) if len(tz_part) >= 6 and tz_part[3] == "'" else 0
                        delta = timezone(timedelta(hours=offset_hours, minutes=offset_minutes) * (1 if tz_part[0] == '+' else -1))
                        parsed_dt = parsed_dt.replace(tzinfo=delta)
                    except ValueError:
                        # Si la zona horaria es inv√°lida, mantener como naive pero loguear
                        logger.debug(f"No se pudo parsear la zona horaria de PDF: {tz_part} para fecha {date_str}")
                        # Devolver la fecha parseada sin tzinfo expl√≠cito, pero s√≠ como ISO
                        return parsed_dt.strftime('%Y-%m-%d'), parsed_dt.isoformat() 
            
            return parsed_dt.strftime('%Y-%m-%d'), parsed_dt.isoformat()
        
        except ValueError as e:
            logger.warning(f"No se pudo parsear la parte central de la fecha PDF '{dt_core}' de '{date_str}': {e}")
            return None, date_str # Devolver la cadena original como fecha ISO si falla el parseo

    def _extract_pdf_metadata(self, doc) -> Dict[str, Any]:
        """
        Extrae metadatos del documento PDF.
        
        Args:
            doc: Documento PyMuPDF abierto
            
        Returns:
            Dict[str, Any]: Metadatos estructurados del PDF
        """
        try:
            pdf_meta = doc.metadata
            
            # Parsear fechas
            creation_date_str = pdf_meta.get('creationDate') if pdf_meta else None
            mod_date_str = pdf_meta.get('modDate') if pdf_meta else None
            
            parsed_creation_date_simple, parsed_creation_date_iso = self._parse_pdf_datetime_str(creation_date_str)
            parsed_mod_date_simple, parsed_mod_date_iso = self._parse_pdf_datetime_str(mod_date_str)
            
            return {
                'titulo_documento': pdf_meta.get('title') if pdf_meta and pdf_meta.get('title') else self.file_path.stem,
                'autor_documento': pdf_meta.get('author') if pdf_meta and pdf_meta.get('author') else None,
                'fecha_publicacion_documento': parsed_creation_date_simple or parsed_mod_date_simple,
                'ruta_archivo_original': str(self.file_path.resolve()),
                'file_format': 'pdf',
                'hash_documento_original': _calculate_sha256(self.file_path),
                'idioma_documento': 'und',
                'metadatos_adicionales_fuente': {
                    'loader_used': 'PDFLoader',
                    'loader_config': {'tipo': self.tipo},
                    'original_fuente': 'pdf_file',
                    'original_contexto': 'document_processing',
                    'blocks_are_fitz_native': True,
                    'pdf_subject': pdf_meta.get('subject') if pdf_meta else None,
                    'pdf_keywords': pdf_meta.get('keywords') if pdf_meta else None,
                    'pdf_creator': pdf_meta.get('creator') if pdf_meta else None,
                    'pdf_producer': pdf_meta.get('producer') if pdf_meta else None,
                    'pdf_creation_date_iso': parsed_creation_date_iso,
                    'pdf_modified_date_iso': parsed_mod_date_iso,
                    'pdf_page_count': doc.page_count
                }
            }
        except Exception as e:
            logger.warning(f"Error extrayendo metadatos PDF: {e}")
            return {
                'titulo_documento': self.file_path.stem,
                'autor_documento': None,
                'fecha_publicacion_documento': None,
                'ruta_archivo_original': str(self.file_path.resolve()),
                'file_format': 'pdf',
                'hash_documento_original': _calculate_sha256(self.file_path),
                'idioma_documento': 'und',
                'metadatos_adicionales_fuente': {
                    'loader_used': 'PDFLoader',
                    'loader_config': {'tipo': self.tipo},
                    'original_fuente': 'pdf_file',
                    'original_contexto': 'document_processing',
                    'blocks_are_fitz_native': True,
                    'pdf_page_count': doc.page_count if doc else 0
                }
            }

    def load(self) -> Dict[str, Any]:
        """
        Carga y procesa el PDF completo, aplicando fusi√≥n inteligente entre p√°ginas.
        """
        if not self.file_path.exists():
            raise FileNotFoundError(f"El archivo PDF no existe: {self.file_path}")
        
        logger.info(f"Cargando PDF: {self.file_path}")
        
        doc = fitz.open(str(self.file_path))
        
        # Extraer metadatos del PDF
        metadata = self._extract_pdf_metadata(doc)
        
        # üö® NUEVA L√ìGICA: Procesar todas las p√°ginas y detectar divisiones entre p√°ginas
        all_pages_blocks = []
        
        for page_num in range(len(doc)):
            page = doc[page_num]
            
            # Extraer bloques de esta p√°gina usando el m√©todo dict para m√°ximo detalle
            try:
                text_dict = page.get_text("dict")
                page_blocks = []
                
                for block in text_dict.get("blocks", []):
                    if "lines" in block:  # Solo bloques de texto
                        # Reconstruir texto del bloque Y extraer informaci√≥n visual
                        lines = []
                        font_sizes = []
                        font_weights = []
                        font_names = []
                        
                        for line in block["lines"]:
                            line_text = ""
                            line_fonts = []
                            
                            for span in line["spans"]:
                                line_text += span["text"]
                                # Extraer informaci√≥n de formato de cada span
                                line_fonts.append({
                                    'size': span.get('size', 0),
                                    'flags': span.get('flags', 0),  # bit flags para bold, italic, etc.
                                    'font': span.get('font', ''),
                                    'color': span.get('color', 0)
                                })
                            
                            if line_text.strip():
                                lines.append(line_text.strip())
                                font_sizes.extend([f['size'] for f in line_fonts])
                                font_weights.extend([f['flags'] for f in line_fonts])
                                font_names.extend([f['font'] for f in line_fonts])
                        
                        if lines:
                            # Aplicar reconstrucci√≥n de p√°rrafos
                            reconstructed_text = self._reconstruct_paragraphs(lines)
                            if reconstructed_text.strip():
                                # Calcular caracter√≠sticas visuales promedio
                                avg_font_size = sum(font_sizes) / len(font_sizes) if font_sizes else 12
                                is_bold = any(flags & 2**4 for flags in font_weights)  # Bold flag
                                is_italic = any(flags & 2**1 for flags in font_weights)  # Italic flag
                                dominant_font = max(set(font_names), key=font_names.count) if font_names else ''
                                
                                # Calcular alineaci√≥n bas√°ndose en posici√≥n del bloque
                                bbox = block.get('bbox', [0, 0, 0, 0])
                                page_width = page.rect.width
                                left_margin = bbox[0]
                                right_margin = page_width - bbox[2]
                                width = bbox[2] - bbox[0]
                                
                                # Heur√≠stica simple para alineaci√≥n
                                if abs(left_margin - right_margin) < 20:  # Centrado
                                    alignment = 'center'
                                elif left_margin < 50:  # Izquierda
                                    alignment = 'left'
                                else:  # Indentado
                                    alignment = 'indented'
                                
                                page_blocks.append({
                                    'text': reconstructed_text,
                                    'bbox': bbox,
                                    'page': page_num + 1,
                                    'block_type': 'text',
                                    # üÜï METADATOS VISUALES PARA DETECCI√ìN DE T√çTULOS
                                    'visual_metadata': {
                                        'avg_font_size': avg_font_size,
                                        'is_bold': is_bold,
                                        'is_italic': is_italic,
                                        'dominant_font': dominant_font,
                                        'alignment': alignment,
                                        'text_length': len(reconstructed_text),
                                        'line_count': len(lines),
                                        'bbox_width': width,
                                        'page_position': left_margin / page_width if page_width > 0 else 0
                                    }
                                })
                
                all_pages_blocks.extend(page_blocks)
                logger.debug(f"P√°gina {page_num + 1}: {len(page_blocks)} bloques extra√≠dos")
                
            except Exception as e:
                logger.warning(f"Error procesando p√°gina {page_num + 1}: {e}")
                # Fallback al m√©todo blocks si falla dict
                blocks_raw = page.get_text("blocks")
                for block in blocks_raw:
                    if len(block) >= 5 and block[4].strip():
                        all_pages_blocks.append({
                            'text': block[4].strip(),
                            'bbox': [block[0], block[1], block[2], block[3]],
                            'page': page_num + 1,
                            'block_type': 'text'
                        })
        
        doc.close()
        
        # üö® APLICAR FUSI√ìN ENTRE P√ÅGINAS üö®
        final_blocks = self._merge_cross_page_sentences(all_pages_blocks)
        
        # Convertir al formato esperado por el pipeline
        formatted_blocks = []
        for i, block in enumerate(final_blocks):
            formatted_blocks.append({
                'type': 'text_block',
                'text': block['text'],
                'order_in_document': i,
                'source_page_number': block['page'],
                'source_block_number': i,
                'coordinates': {
                    'x0': block['bbox'][0], 'y0': block['bbox'][1], 
                    'x1': block['bbox'][2], 'y1': block['bbox'][3]
                },
                'merged_from_pages': block.get('merged_from_pages', [block['page']])
            })
        
        logger.info(f"PDF procesado: {len(all_pages_blocks)} bloques iniciales ‚Üí {len(formatted_blocks)} bloques finales")
        
        return {
            'blocks': formatted_blocks,
            'document_metadata': metadata
        }
    
    def _merge_cross_page_sentences(self, blocks: List[Dict]) -> List[Dict]:
        """
        üö® FUSI√ìN ANTI-SALTO DE P√ÅGINA üö®
        
        Detecta y fusiona oraciones divididas artificialmente entre p√°ginas.
        
        Args:
            blocks: Lista de bloques con p√°gina, texto y bbox
            
        Returns:
            List[Dict]: Bloques con fusiones aplicadas
        """
        print("üö®üö®üö® APLICANDO FUSI√ìN ENTRE P√ÅGINAS üö®üö®üö®")
        logger.warning("üö®üö®üö® APLICANDO FUSI√ìN ENTRE P√ÅGINAS üö®üö®üö®")
        
        if len(blocks) < 2:
            return blocks
            
        merged_blocks = []
        i = 0
        
        while i < len(blocks):
            current_block = blocks[i].copy()
            
            # Verificar si hay un siguiente bloque
            if i + 1 < len(blocks):
                next_block = blocks[i + 1]
                
                # ¬øEst√°n en p√°ginas consecutivas?
                if (next_block['page'] == current_block['page'] + 1):
                    
                    current_text = current_block['text'].strip()
                    next_text = next_block['text'].strip()
                    
                    # DIAGN√ìSTICO ESPEC√çFICO
                    if ("atractivo" in current_text.lower() and 
                        "de esta idea" in next_text.lower()):
                        print(f"üéØ DIVISI√ìN ENTRE P√ÅGINAS DETECTADA:")
                        print(f"   P√°gina {current_block['page']}: '{current_text[-50:]}'")
                        print(f"   P√°gina {next_block['page']}: '{next_text[:50]}'")
                        logger.warning(f"üéØ DIVISI√ìN CR√çTICA: '{current_text[-50:]}' | '{next_text[:50]}'")
                    
                    # Aplicar heur√≠sticas de fusi√≥n
                    if self._should_merge_cross_page(current_text, next_text):
                        print(f"üîó FUSIONANDO P√ÅGINAS {current_block['page']} y {next_block['page']}")
                        logger.warning(f"üîó FUSI√ìN APLICADA: p√°ginas {current_block['page']}-{next_block['page']}")
                        
                        # Fusionar textos
                        merged_text = current_text + " " + next_text
                        current_block['text'] = merged_text
                        current_block['merged_from_pages'] = [current_block['page'], next_block['page']]
                        
                        # Saltar el siguiente bloque (ya fue fusionado)
                        i += 2
                        merged_blocks.append(current_block)
                        continue
            
            # No se fusion√≥, agregar bloque normal
            merged_blocks.append(current_block)
            i += 1
        
        print(f"üìä FUSI√ìN COMPLETADA: {len(blocks)} ‚Üí {len(merged_blocks)} bloques")
        logger.warning(f"üìä FUSI√ìN COMPLETADA: {len(blocks)} ‚Üí {len(merged_blocks)} bloques")
        
        return merged_blocks
    
    def _should_merge_cross_page(self, page1_text: str, page2_text: str) -> bool:
        """
        Determina si dos textos de p√°ginas consecutivas deben fusionarse.
        
        PATRONES DE FUSI√ìN ESPEC√çFICOS:
        1. P√°gina anterior termina sin puntuaci√≥n + p√°gina siguiente empieza con min√∫scula
        2. Patrones espec√≠ficos como "atractivo" + "de esta idea"
        3. Preposiciones y art√≠culos al inicio de p√°gina siguiente
        
        Args:
            page1_text: Texto del final de la p√°gina anterior
            page2_text: Texto del inicio de la p√°gina siguiente
            
        Returns:
            bool: True si deben fusionarse
        """
        import re
        
        # Obtener las √∫ltimas palabras de la p√°gina anterior
        page1_words = page1_text.strip().split()
        page2_words = page2_text.strip().split()
        
        if not page1_words or not page2_words:
            return False
            
        last_word = page1_words[-1].lower().rstrip('.,!?:;')
        first_word = page2_words[0].lower()
        
        # üéØ PATRONES ESPEC√çFICOS DE DIVISI√ìN ARTIFICIAL
        specific_patterns = [
            # El caso exacto que estamos resolviendo
            (last_word == "atractivo" and first_word == "de"),
            # Otros patrones comunes de divisi√≥n por p√°gina
            (last_word in ["muy", "m√°s", "tan", "poco", "tanto"] and first_word not in ["pero", "sin"]),
            (last_word in ["para", "por", "en", "con", "sin", "del", "al"] and not page2_text[0].isupper()),
            (last_word in ["que", "como", "cuando", "donde", "quien"] and not page2_text[0].isupper()),
            # Art√≠culos y preposiciones al inicio de p√°gina siguiente
            (first_word in ["de", "del", "la", "el", "los", "las", "en", "con", "por", "para", "que"]),
        ]
        
        if any(specific_patterns):
            return True
        
        # REGLA GENERAL: Si p√°gina anterior no termina con puntuaci√≥n Y p√°gina siguiente empieza con min√∫scula
        if (not re.search(r'[.!?][\s"]*$', page1_text.strip()) and 
            page2_text[0].islower()):
            return True
            
        return False