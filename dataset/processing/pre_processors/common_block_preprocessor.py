from typing import List, Dict, Tuple, Optional, Union, Any
from pathlib import Path
import re
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

class CommonBlockPreprocessor:
    """Pre-procesador com√∫n para bloques de texto y metadatos.

    Este pre-procesador realiza operaciones agn√≥sticas al formato de archivo
    despu√©s de que los datos han sido cargados por un Loader y antes de
    ser pasados a un Segmenter.

    Operaciones configurables:
    - Extracci√≥n de fecha desde el nombre del archivo.
    - Limpieza de texto (normalizaci√≥n de saltos de l√≠nea, eliminaci√≥n de NULs).
    - Filtrado agresivo de fragmentos insignificantes (especialmente para PDFs).
    """

    DEFAULT_CONFIG = {
        'remove_nul_bytes': True,
        'normalize_line_endings': True,
        'extract_date_from_filename': True,
        'split_blocks_into_paragraphs': True,
        'min_chars_for_paragraph_split': 100,  # Umbral principal para dividir por \\n\\n
        'max_vertical_gap_for_merge_pt': 10,
        'min_block_area_for_split': 1000,
        'try_single_newline_split_if_block_longer_than': 500, 
        'min_chars_for_single_newline_paragraph': 75,
        # Nuevo filtrado agresivo para PDFs
        'filter_insignificant_blocks': True,
        'min_block_chars_to_keep': 15,  # Bloques menores a esto se descartan
        'max_single_word_length': 50,  # Palabras solas mayores a esto se descartan
        'discard_only_numbers': True,  # Descartar bloques que solo contienen n√∫meros
        'discard_common_pdf_artifacts': True,  # Descartar artefactos comunes de PDF
        'aggressive_merge_for_pdfs': True,  # Fusi√≥n m√°s agresiva para PDFs
        'max_vertical_gap_aggressive_pt': 20,  # Gap m√°s grande para fusi√≥n agresiva
        # NUEVA FUNCIONALIDAD: Limpieza autom√°tica de corrupci√≥n Unicode
        'clean_unicode_corruption': True,  # Limpiar caracteres Unicode corruptos autom√°ticamente
        # NUEVA FUNCIONALIDAD: Detecci√≥n de elementos estructurales repetitivos
        'filter_structural_elements': True,  # Filtrar headers/footers repetitivos
        'structural_frequency_threshold': 0.9,  # 90% de p√°ginas = elemento estructural
        'min_pages_for_structural_detection': 5,  # M√≠nimo de p√°ginas para activar detecci√≥n
        # Nueva configuraci√≥n para longitud m√≠nima de p√°rrafo y divisi√≥n inteligente por may√∫scula
        'min_paragraph_length': 10,  # Longitud m√≠nima para aceptar un p√°rrafo
        'split_on_newline_capital': True,  # Dividir cuando \n y la siguiente l√≠nea inicia con may√∫scula
        # NUEVA: Configuraci√≥n espec√≠fica para documentos largos
        'preserve_long_documents': False,  # Preservar contenido completo en documentos largos
        'max_consecutive_empty_blocks': 5,  # M√°ximo de bloques vac√≠os consecutivos antes de parar
    }

    def __init__(self, config: Optional[Dict] = None):
        """
        Inicializa el CommonBlockPreprocessor.

        Args:
            config: Configuraci√≥n opcional para el pre-procesador.
                    Sobrescribe los valores de DEFAULT_CONFIG.
        """
        # PRINT VISIBLE PARA CONFIRMAR QUE SE EJECUTA NUESTRO C√ìDIGO
        print("üö®üö®üö® COMMONBLOCKPREPROCESSOR MODIFICADO SE EST√Å EJECUTANDO üö®üö®üö®")
        print(f"üö®üö®üö® CONFIG RECIBIDA: {config} üö®üö®üö®")
        
        self.config = {**CommonBlockPreprocessor.DEFAULT_CONFIG, **(config if config else {})}
        
        # LOGGING DETALLADO PARA DEBUG - ver qu√© configuraci√≥n recibe realmente
        logger.warning(f"üîß CommonBlockPreprocessor inicializado")
        logger.warning(f"   üì• Config recibida: {config}")
        logger.warning(f"   ‚öôÔ∏è  Config final: {self.config}")
        logger.warning(f"   üîó Fusi√≥n agresiva: {self.config.get('aggressive_merge_for_pdfs', 'NO DEFINIDA')}")
        logger.warning(f"   üõ°Ô∏è  Filtrado activo: {self.config.get('filter_insignificant_blocks', 'NO DEFINIDA')}")
        logger.warning(f"   üìè Gap m√°ximo: {self.config.get('max_vertical_gap_aggressive_pt', 'NO DEFINIDA')}")
        
        logger.debug(f"CommonBlockPreprocessor inicializado con config: {self.config}")

    def _is_insignificant_block(self, text: str) -> bool:
        """
        Determina si un bloque de texto es insignificante y debe descartarse.
        
        Args:
            text: Texto del bloque ya limpiado
            
        Returns:
            True si el bloque debe descartarse, False en caso contrario
        """
        if not self.config.get('filter_insignificant_blocks', True):
            return False
            
        # Filtro por longitud m√≠nima
        min_chars = self.config.get('min_block_chars_to_keep', 15)
        if len(text) < min_chars:
            return True
            
        # Filtro de solo n√∫meros (n√∫meros de p√°gina, etc.)
        if self.config.get('discard_only_numbers', True):
            if text.isdigit() or re.match(r'^\d+[.\-]*\d*$', text):
                return True
                
        # Filtro de palabras solas muy largas (probablemente errores de OCR)
        words = text.split()
        if len(words) == 1:
            max_single_word = self.config.get('max_single_word_length', 50)
            if len(words[0]) > max_single_word:
                return True
                
        # Filtro de artefactos comunes de PDF
        if self.config.get('discard_common_pdf_artifacts', True):
            # Preposiciones y art√≠culos sueltos
            if text.lower() in ['del', 'de', 'la', 'el', 'los', 'las', 'un', 'una', 'y', 'o', 'a', 'en', 'con', 'por', 'para', 'que']:
                return True
                
            # Letras solas o combinaciones muy cortas
            if len(text) <= 3 and not text.isdigit():
                return True
                
            # Patrones t√≠picos de headers/footers
            if re.match(r'^[IVXivx]+$', text):  # Numeraci√≥n romana sola
                return True
                
            if re.match(r'^[a-zA-Z]$', text):  # Letras solas
                return True
                
            # Fechas aisladas que probablemente son headers/footers
            if re.match(r'^\d{1,2}/\d{1,2}/\d{2,4}$', text):
                return True
                
        return False

    def _extract_date_from_filename(self, filename: str) -> Optional[str]:
        # Implementaci√≥n placeholder, ya que la l√≥gica principal se movi√≥
        # pero la opci√≥n de config existe.
        if self.config.get('extract_date_from_filename', True):
            # Ejemplo: Intenta encontrar YYYY-MM-DD o YYYYMMDD
            match = re.search(r'(\d{4}-\d{2}-\d{2}|\d{8})', filename)
            if match:
                date_str = match.group(1)
                try:
                    # Validar y normalizar si es necesario
                    if '-' in date_str:
                        datetime.strptime(date_str, '%Y-%m-%d')
                    else:
                        datetime.strptime(date_str, '%Y%m%d')
                        date_str = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
                    return date_str
                except ValueError:
                    logger.warning(f"Formato de fecha inv√°lido '{date_str}' encontrado en el nombre de archivo '{filename}'.")
        return None

    def _clean_block_text(self, text: str) -> str:
        """Limpia el texto de un bloque de forma conservadora seg√∫n la configuraci√≥n."""
        if text is None:
            return None
        
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        if self.config.get('remove_nul_bytes', True):
            text = text.replace('\x00', '')
        
        # NUEVA FUNCIONALIDAD: Limpieza de corrupci√≥n Unicode
        if self.config.get('clean_unicode_corruption', True):
            text = self._clean_unicode_corruption(text)
        
        return text.strip() # Strip al final para quitar espacios/saltos al inicio/fin

    def _clean_unicode_corruption(self, text: str) -> str:
        """
        Limpia texto corrupto con caracteres Unicode de escape y caracteres de control.
        
        Esta funci√≥n elimina autom√°ticamente texto corrupto com√∫n en PDFs:
        - Secuencias backslash-u seguidas de 4 caracteres hex
        - Caracteres de control ASCII (0x00-0x1F excepto salto de l√≠nea, retorno de carro, tabulador)
        - Caracteres de control Unicode extendidos (0x7F-0x9F)
        - Detecta y remueve texto predominantemente corrupto
        """
        if not text:
            return text
        
        # 1. Remover secuencias \u seguidas de 4 caracteres hex
        text = re.sub(r'\\u[0-9a-fA-F]{4}', ' ', text)
        
        # 2. LIMPIEZA AGRESIVA: Remover TODOS los caracteres de control problem√°ticos
        # Caracteres de control ASCII (0x00-0x1F) excepto \n=0x0A, \r=0x0D, \t=0x09
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F]', ' ', text)
        
        # Caracteres de control extendidos (0x7F-0x9F) - TODOS
        text = re.sub(r'[\x7F-\x9F]', ' ', text)
        
        # 3. NUEVA: Limpieza espec√≠fica de caracteres problem√°ticos para JSON
        # Remover caracteres que pueden causar "Invalid control character" en JSON
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F\uFEFF\u200B-\u200F\u2028\u2029]', ' ', text)
        
        # 4. Remover secuencias de escape Unicode mal formadas
        text = re.sub(r'\\[uU][0-9a-fA-F]{0,3}(?![0-9a-fA-F])', ' ', text)
        
        # 5. NUEVA FUNCIONALIDAD: Detectar texto predominantemente corrupto
        total_chars = len(text.replace(' ', '').replace('\n', ''))
        if total_chars > 0:
            # Contar caracteres reconocibles vs corruptos
            letras_validas = len(re.findall(r'[a-zA-Z√°√©√≠√≥√∫√º√±√Å√â√ç√ì√ö√ú√ë]', text))
            palabras_validas = len(re.findall(r'\b[a-zA-Z√°√©√≠√≥√∫√º√±√Å√â√ç√ì√ö√ú√ë]{3,}\b', text))
            
            # Si menos del 30% son letras v√°lidas y hay menos de 2 palabras, es corrupto
            ratio_letras = letras_validas / total_chars
            if ratio_letras < 0.3 and palabras_validas < 2 and total_chars > 50:
                # Marcar como corrupto para filtrado posterior
                return "texto corrupto en archivo de origen"  # Placeholder descriptivo
        
        # 6. Limpiar patrones espec√≠ficos de corrupci√≥n en texto parcialmente corrupto
        # Remover secuencias de s√≠mbolos matem√°ticos sueltos
        text = re.sub(r'\s[+\-*/&%#$@!]{1,2}\s', ' ', text)
        
        # Remover letras sueltas entre espacios (probable corrupci√≥n)
        text = re.sub(r'\s[a-zA-Z]\s', ' ', text)
        
        # Remover n√∫meros sueltos de 1-2 d√≠gitos entre espacios
        text = re.sub(r'\s\d{1,2}\s', ' ', text)
        
        # 7. Normalizar espacios m√∫ltiples
        text = re.sub(r'\s+', ' ', text)
        
        # 8. Limpiar l√≠neas vac√≠as excesivas (mantener m√°ximo 2 saltos consecutivos)
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        
        # 9. VALIDACI√ìN FINAL: Asegurar que no queden caracteres problem√°ticos
        # Filtrar cualquier car√°cter que no sea imprimible o v√°lido para JSON
        cleaned_chars = []
        for char in text:
            # Permitir solo caracteres seguros para JSON
            if (ord(char) >= 32 and ord(char) != 127) or char in '\n\r\t':
                cleaned_chars.append(char)
            else:
                cleaned_chars.append(' ')  # Reemplazar con espacio
        
        text = ''.join(cleaned_chars)
        
        return text.strip()

    def _split_text_into_paragraphs(self, text: str, base_order: float, original_coordinates: Optional[Dict] = None) -> List[Tuple[str, float, Optional[Dict]]]:
        """
        üö® VERSI√ìN 7.0 - ALGORITMO SIMPLE PRIMERO üö®
        
        Prioriza el algoritmo simple (doble salto de l√≠nea) como hac√≠a antes
        cuando extra√≠amos 60 poemas de Mario Benedetti.
        
        Solo usa fallback si obtiene muy pocos segmentos (‚â§ 3).
        
        Args:
            text: Texto completo a dividir
            base_order: Orden base para los p√°rrafos
            original_coordinates: Coordenadas del bloque original
            
        Returns:
            Lista de tuplas (texto_p√°rrafo, orden, coordenadas)
        """
        print("üö®üö®üö® COMMONBLOCK V7.1 - FUSI√ìN INTELIGENTE OCR üö®üö®üö®")
        logger.warning("üö®üö®üö® COMMONBLOCK V7.1 - FUSI√ìN INTELIGENTE OCR üö®üö®üö®")
        
        # DEBUG: Ver qu√© texto estamos recibiendo
        print(f"üìä DEBUG _split_text_into_paragraphs:")
        print(f"   - Longitud del texto: {len(text)}")
        has_newlines = '\\n' in text
        print(f"   - Contiene saltos de l√≠nea: {has_newlines}")
        newline_count = text.count('\\n')
        print(f"   - N√∫mero de saltos de l√≠nea: {newline_count}")
        print(f"   - Primeros 200 chars: {repr(text[:200])}")
        logger.warning(f"üìä DEBUG _split_text_into_paragraphs:")
        logger.warning(f"   - Longitud: {len(text)}, Saltos: {newline_count}")
        logger.warning(f"   - Inicio: {repr(text[:200])}")
        
        if not text or len(text.strip()) < 10:
            return []
        
        # ======= ALGORITMO PRINCIPAL: DOBLE SALTO DE L√çNEA =======
        # Este es el que funcionaba antes para extraer los 60 poemas
        
        paragraphs = []
        
        # Dividir por doble salto de l√≠nea (como antes)
        raw_paragraphs = re.split(r'\n\s*\n', text)
        
        # ‚ú® MEJORADO: Aplicar divisi√≥n por punto + salto + may√∫scula en TODOS los p√°rrafos
        # Esto detecta los finales de p√°rrafo t√≠picos en PDFs
        enhanced_paragraphs = []
        for paragraph in raw_paragraphs:
            if paragraph.strip():
                # Dividir por patr√≥n: punto final + salto de l√≠nea + may√∫scula inicial
                sentence_splits = re.split(r'(?<=[.!?])\s*\n\s*(?=[A-Z√Å√â√ç√ì√ö√ú√ë])', paragraph)
                enhanced_paragraphs.extend(sentence_splits)
        
        if len(enhanced_paragraphs) > len(raw_paragraphs):
            print(f"üìÑ DIVISI√ìN MEJORADA: {len(raw_paragraphs)} ‚Üí {len(enhanced_paragraphs)} p√°rrafos")
            logger.warning(f"üìÑ DIVISI√ìN MEJORADA: {len(raw_paragraphs)} ‚Üí {len(enhanced_paragraphs)} p√°rrafos")
            raw_paragraphs = enhanced_paragraphs
        
        # ‚ú® ADICIONAL: dividir cuando hay salto de l√≠nea seguido de may√∫scula (si est√° configurado)
        if self.config.get('split_on_newline_capital', False) and len(raw_paragraphs) < 10:
            # Solo aplicar si tenemos pocos p√°rrafos (evitar sobre-divisi√≥n)
            refined_paragraphs = []
            for par in raw_paragraphs:
                # Dividir par√°grafo por "\nLetraMay√∫scula" que indica nuevo p√°rrafo indentado en PDF
                sub_parts = re.split(r'\n(?=[A-Z√Å√â√ç√ì√ö√ú√ë])', par)
                refined_paragraphs.extend(sub_parts)
            
            if len(refined_paragraphs) > len(raw_paragraphs):
                print(f"üìÑ DIVISI√ìN ADICIONAL: {len(raw_paragraphs)} ‚Üí {len(refined_paragraphs)} p√°rrafos")
                raw_paragraphs = refined_paragraphs
        
        for i, paragraph in enumerate(raw_paragraphs):
            paragraph = paragraph.strip()
            if not paragraph:
                continue
                
            # Limpiar el p√°rrafo
            paragraph = re.sub(r'\s+', ' ', paragraph)  # Normalizar espacios
            
            min_len = self.config.get('min_paragraph_length', 20)
            if len(paragraph) >= min_len:
                # Fusi√≥n de l√≠neas internas para PROSA/OCR ‚Äì evita textos truncados
                paragraph_merged = self._merge_lines_in_paragraph(paragraph)

                order = base_order + (i * 0.001)
                paragraphs.append((paragraph_merged, order, original_coordinates))
        
        print(f"üéØ ALGORITMO SIMPLE: {len(paragraphs)} p√°rrafos extra√≠dos")
        logger.warning(f"üéØ ALGORITMO SIMPLE: {len(paragraphs)} p√°rrafos extra√≠dos")
        
        # ======= FALLBACK: SOLO SI HAY MUY POCOS SEGMENTOS =======
        # Solo aplicar si obtenemos ‚â§ 3 segmentos (como sugiri√≥ el usuario)
        
        if len(paragraphs) <= 3:
            print("‚ö†Ô∏è MUY POCOS SEGMENTOS: Aplicando algoritmo fallback")
            logger.warning("‚ö†Ô∏è MUY POCOS SEGMENTOS: Aplicando algoritmo fallback")
            
            # Fallback 1: Intentar con espacios m√∫ltiples (como sugerencia del usuario)
            fallback_paragraphs = []
            
            # Buscar patrones de 2 o m√°s espacios consecutivos
            space_split = re.split(r'  +', text)  # 2 o m√°s espacios
            
            for i, segment in enumerate(space_split):
                segment = segment.strip()
                min_len = self.config.get('min_paragraph_length', 20)
                if segment and len(segment) >= min_len:
                    order = base_order + (i * 0.001)
                    fallback_paragraphs.append((segment, order, original_coordinates))
            
            if len(fallback_paragraphs) > len(paragraphs):
                print(f"‚úÖ FALLBACK ESPACIOS: {len(fallback_paragraphs)} p√°rrafos (mejor que {len(paragraphs)})")
                logger.warning(f"‚úÖ FALLBACK ESPACIOS: {len(fallback_paragraphs)} p√°rrafos")
                return fallback_paragraphs
            
            # Fallback 2: Detectar patrones sin saltos de l√≠nea (punto + espacio + may√∫scula)
            if len(paragraphs) <= 1 and '\n' not in text:
                print("üîß APLICANDO DIVISI√ìN SIN SALTOS DE L√çNEA")
                logger.warning("üîß APLICANDO DIVISI√ìN SIN SALTOS DE L√çNEA")
                
                # Primero, separar n√∫meros romanos o t√≠tulos muy cortos al inicio
                title_pattern = r'^(\*\*)?([IVXLCDM]+|\d+|Cap√≠tulo\s+[IVXLCDM]+|Cap√≠tulo\s+\d+|Cap\.\s*\d+)(\*\*)?\s+'
                title_match = re.match(title_pattern, text, re.IGNORECASE)
                
                smart_paragraphs = []
                remaining_text = text
                
                if title_match:
                    # Extraer el t√≠tulo como p√°rrafo separado
                    title_text = title_match.group(0).strip()
                    smart_paragraphs.append((title_text, base_order, original_coordinates))
                    remaining_text = text[len(title_match.group(0)):]
                    print(f"üìñ T√çTULO DETECTADO: '{title_text}'")
                    logger.warning(f"üìñ T√çTULO DETECTADO: '{title_text}'")
                
                # Dividir el resto por punto + espacio + may√∫scula
                sentence_pattern = r'(?<=[.!?])\s+(?=[A-Z√Å√â√ç√ì√ö√ú√ë])'
                sentences = re.split(sentence_pattern, remaining_text)
                
                # Agrupar oraciones en p√°rrafos l√≥gicos
                current_paragraph = ""
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                    
                    # Criterios para iniciar nuevo p√°rrafo
                    should_start_new = (
                        not current_paragraph or  # Primer p√°rrafo
                        len(current_paragraph) > 300 or  # P√°rrafo ya largo
                        sentence.startswith('‚Äî') or  # Di√°logo
                        re.match(r'^(Cuando|Despu√©s|Entonces|As√≠|De esta manera|Por eso|En ese momento)', sentence)
                    )
                    
                    if should_start_new and current_paragraph:
                        order = base_order + (len(smart_paragraphs) * 0.001)
                        smart_paragraphs.append((current_paragraph.strip(), order, original_coordinates))
                        current_paragraph = sentence
                    else:
                        current_paragraph += (' ' if current_paragraph else '') + sentence
                
                # Agregar √∫ltimo p√°rrafo
                if current_paragraph:
                    order = base_order + (len(smart_paragraphs) * 0.001)
                    smart_paragraphs.append((current_paragraph.strip(), order, original_coordinates))
                
                if len(smart_paragraphs) > len(paragraphs):
                    print(f"‚úÖ DIVISI√ìN SIN SALTOS: {len(smart_paragraphs)} p√°rrafos")
                    logger.warning(f"‚úÖ DIVISI√ìN SIN SALTOS: {len(smart_paragraphs)} p√°rrafos")
                    return smart_paragraphs
            
            # Fallback 3: Fusi√≥n inteligente de l√≠neas (para OCR)
            elif len(paragraphs) <= 1:
                print("üîß APLICANDO FUSI√ìN INTELIGENTE DE L√çNEAS (OCR)")
                logger.warning("üîß APLICANDO FUSI√ìN INTELIGENTE DE L√çNEAS (OCR)")
                
                lines = text.split('\n')
                smart_paragraphs = self._smart_merge_lines(lines, base_order, original_coordinates)
                
                if len(smart_paragraphs) > len(paragraphs):
                    print(f"‚úÖ FUSI√ìN INTELIGENTE: {len(smart_paragraphs)} p√°rrafos")
                    logger.warning(f"‚úÖ FUSI√ìN INTELIGENTE: {len(smart_paragraphs)} p√°rrafos")
                    return smart_paragraphs
        
        # Heur√≠stica adicional eliminada ‚Äî se reemplaza por detecci√≥n de fragmentaci√≥n OCR integrada a nivel de bloque.

        print(f"üéØ RESULTADO FINAL: {len(paragraphs)} p√°rrafos tras heur√≠sticas")
        logger.warning(f"üéØ RESULTADO FINAL: {len(paragraphs)} p√°rrafos tras heur√≠sticas")
        
        return paragraphs

    def _merge_lines_in_paragraph(self, par_text: str) -> str:
        """Fusiona l√≠neas internas de un mismo p√°rrafo cuando parecen fragmentadas.

        Heur√≠stica: si el p√°rrafo contiene numerosos saltos de l√≠nea internos (\n) y
        la longitud media por l√≠nea es baja (<90-100 caracteres), intentamos fusionar
        usando la l√≥gica de _should_merge_lines para reconstruir oraciones completas.
        """

        if '\n' not in par_text:
            return par_text.strip()

        lines_local = [l.rstrip() for l in par_text.split('\n') if l.strip()]

        # Calcular longitud media para decidir si vale la pena fusionar
        avg_line_len = sum(len(l) for l in lines_local) / max(len(lines_local), 1)

        # Umbral conservador ‚Äì solo fusionar si son l√≠neas cortas en promedio
        if avg_line_len > 100:
            return par_text.strip()

        merged = lines_local[0]
        for ln in lines_local[1:]:
            if self._should_merge_lines(merged, ln):
                sep = self._get_merge_separator(merged, ln)
                merged = f"{merged}{sep}{ln.lstrip()}"
            else:
                merged = f"{merged}\n{ln}"  # Mantener salto si claramente inicia nuevo p√°rrafo

        return merged.strip()

    def _smart_merge_lines(self, lines: List[str], base_order: float, original_coordinates: Optional[Dict] = None) -> List[Tuple[str, float, Optional[Dict]]]:
        """
        Fusiona l√≠neas inteligentemente para formar p√°rrafos coherentes.
        Especialmente √∫til para texto extra√≠do por OCR.
        """
        if not lines:
            return []
        
        merged_paragraphs = []
        current_paragraph = ""
        paragraph_count = 0
        
        for line in lines:
            line = line.strip()
            if not line:
                # L√≠nea vac√≠a - finalizar p√°rrafo actual si existe
                min_len_cfg = self.config.get('min_paragraph_length', 30)
                if current_paragraph and len(current_paragraph) >= min_len_cfg:
                    order = base_order + (paragraph_count * 0.001)
                    merged_paragraphs.append((current_paragraph.strip(), order, original_coordinates))
                    paragraph_count += 1
                current_paragraph = ""
                continue
            
            if not current_paragraph:
                # Iniciar nuevo p√°rrafo
                current_paragraph = line
            else:
                # Decidir si fusionar con el p√°rrafo actual
                if self._should_merge_lines(current_paragraph, line):
                    # Fusionar con espacio
                    current_paragraph += " " + line
                else:
                    # Finalizar p√°rrafo actual y empezar uno nuevo
                    min_len_cfg = self.config.get('min_paragraph_length', 30)
                    if current_paragraph and len(current_paragraph) >= min_len_cfg:
                        order = base_order + (paragraph_count * 0.001)
                        merged_paragraphs.append((current_paragraph.strip(), order, original_coordinates))
                        paragraph_count += 1
                    current_paragraph = line
        
        # Agregar el √∫ltimo p√°rrafo si existe
        min_len_cfg = self.config.get('min_paragraph_length', 30)
        if current_paragraph and len(current_paragraph) >= min_len_cfg:
            order = base_order + (paragraph_count * 0.001)
            merged_paragraphs.append((current_paragraph.strip(), order, original_coordinates))
        
        return merged_paragraphs
    
    def _should_merge_lines(self, current_paragraph: str, new_line: str) -> bool:
        """
        Determina si una nueva l√≠nea debe fusionarse con el p√°rrafo actual.
        """
        current_stripped = current_paragraph.strip()
        new_stripped = new_line.strip()
        
        if not current_stripped or not new_stripped:
            return False
        
        # No fusionar si el p√°rrafo actual es muy largo (probablemente ya completo)
        if len(current_stripped) > 500:
            return False
        
        # No fusionar si la nueva l√≠nea parece ser un t√≠tulo o encabezado
        if self._looks_like_title(new_stripped):
            return False
        
        # Fusionar si el p√°rrafo actual no termina con puntuaci√≥n fuerte
        ends_without_punctuation = not current_stripped.endswith(('.', '!', '?', ':', ';'))
        
        # Fusionar si la nueva l√≠nea empieza con min√∫scula (continuaci√≥n)
        starts_lowercase = new_stripped[0].islower()
        
        # Fusionar si el p√°rrafo actual termina con palabra que requiere continuaci√≥n
        current_words = current_stripped.split()
        if current_words:
            last_word = current_words[-1].lower()
            continuation_words = ['de', 'del', 'en', 'con', 'por', 'para', 'que', 'y', 'o', 'pero', 'sin', 'a', 'al']
            if last_word in continuation_words:
                return True
        
        return ends_without_punctuation and starts_lowercase
    
    def _looks_like_title(self, text: str) -> bool:
        """
        Detecta si un texto parece ser un t√≠tulo o encabezado.
        """
        text_stripped = text.strip()
        
        # T√≠tulos suelen ser cortos
        if len(text_stripped) > 100:
            return False
        
        # T√≠tulos suelen empezar con may√∫scula
        if not text_stripped[0].isupper():
            return False
        
        # T√≠tulos suelen tener todas las palabras importantes en may√∫scula
        words = text_stripped.split()
        if len(words) <= 5:  # Solo para textos cortos
            capitalized_words = sum(1 for word in words if word[0].isupper())
            if capitalized_words / len(words) > 0.6:  # M√°s del 60% en may√∫scula
                return True
        
        return False

    def _merge_contiguous_fitz_blocks(self, blocks: List[Dict]) -> List[Dict]:
        """
        Fusiona bloques contiguos aplicando heur√≠sticas de separaci√≥n vertical
        y detecci√≥n de p√°rrafos divididos artificialmente.
        """
        if not blocks:
            return blocks

        merged_blocks = []
        current_block = None

        for i, block in enumerate(blocks):
            if current_block is None:
                current_block = block.copy()
                continue

            # Fusi√≥n especial para oraciones divididas por saltos de p√°gina
            if self._should_merge_split_sentences(current_block, block):
                logger.info(f"üîó FUSIONANDO ORACI√ìN DIVIDIDA: '{current_block.get('text', '')[-30:]}' + '{block.get('text', '')[:30]}'")
                sep = self._get_merge_separator(current_block['text'], block['text'])
                current_block['text'] = current_block['text'].rstrip() + sep + block['text'].lstrip()
                current_block['order'] = min(current_block.get('order', 0), block.get('order', 0))
                continue

            # L√≥gica de fusi√≥n normal existente
            curr_text = current_block.get('text', '').strip()
            next_text = block.get('text', '').strip()
            
            if not curr_text or not next_text:
                merged_blocks.append(current_block)
                current_block = block.copy()
                continue

            curr_page = current_block.get('page', 0)
            next_page = block.get('page', 0)
            
            vertical_gap = abs(block.get('bbox', [0, 0, 0, 0])[1] - current_block.get('bbox', [0, 0, 0, 0])[3])
            max_gap = self.config.get('max_vertical_gap_for_merge', 20)
            
            if self._should_merge_blocks(curr_text, next_text, curr_page, next_page, vertical_gap, max_gap):
                sep = self._get_merge_separator(curr_text, next_text)
                current_block['text'] = curr_text.rstrip() + sep + next_text.lstrip()
                current_block['order'] = min(current_block.get('order', 0), block.get('order', 0))
            else:
                merged_blocks.append(current_block)
                current_block = block.copy()

        if current_block:
            merged_blocks.append(current_block)

        return merged_blocks

    def _should_merge_split_sentences(self, block1: Dict, block2: Dict) -> bool:
        """
        Detecta si dos bloques representan una oraci√≥n dividida artificialmente.
        
        Casos t√≠picos:
        - Bloque 1 termina sin puntuaci√≥n final
        - Bloque 2 empieza con min√∫scula o continuaci√≥n l√≥gica
        - Son de p√°ginas consecutivas o la misma p√°gina
        """
        text1 = block1.get('text', '').strip()
        text2 = block2.get('text', '').strip()
        
        if not text1 or not text2:
            return False
        
        # NUEVA VALIDACI√ìN: No fusionar si el segundo bloque es un t√≠tulo corto
        if self._looks_like_short_title(text2):
            logger.info(f"üö´ NO FUSIONAR ORACIONES: segundo bloque es t√≠tulo: '{text2[:50]}'")
            return False
        
        page1 = block1.get('page', 0)
        page2 = block2.get('page', 0)
        
        # Solo fusionar si est√°n en la misma p√°gina o p√°ginas consecutivas
        if abs(page2 - page1) > 1:
            return False
        
        # Detectar patrones de divisi√≥n artificial
        last_word = text1.split()[-1] if text1.split() else ""
        first_word = text2.split()[0] if text2.split() else ""
        
        # Caso 1: El primer bloque termina sin puntuaci√≥n fuerte
        ends_without_punctuation = not text1.endswith(('.', '!', '?', ':', ';'))
        
        # Caso 2: El segundo bloque empieza con min√∫scula (continuaci√≥n)
        starts_lowercase = text2[0].islower() if text2 else False
        
        # Caso 3: Patrones espec√≠ficos de divisi√≥n
        specific_patterns = [
            # "atractivo" + "de esta idea"
            (last_word.lower() in ['atractivo', 'hermoso', 'interesante', 'importante'] and 
             first_word.lower() in ['de', 'del', 'que', 'para']),
            # Palabras que claramente contin√∫an
            (last_word.lower() in ['muy', 'm√°s', 'tan', 'poco'] and 
             first_word.lower() not in ['pero', 'sin', 'embargo', 'aunque']),
            # Preposiciones al final que requieren continuaci√≥n
            (last_word.lower() in ['para', 'por', 'en', 'con', 'sin', 'de', 'del', 'al'] and 
             not starts_lowercase == False)  # No empezar con may√∫scula
        ]
        
        should_merge = (
            ends_without_punctuation and 
            (starts_lowercase or any(specific_patterns))
        )
        
        if should_merge:
            logger.info(f"üîç DETECTADA DIVISI√ìN ARTIFICIAL:")
            logger.info(f"   Bloque 1: '{text1[-50:]}'")
            logger.info(f"   Bloque 2: '{text2[:50]}'")
            logger.info(f"   Sin puntuaci√≥n: {ends_without_punctuation}, Min√∫scula: {starts_lowercase}")
        
        return should_merge

    def _should_merge_blocks(self, curr_text: str, next_text: str, curr_page: int, 
                           next_page: int, vertical_gap: float, max_gap: float) -> bool:
        """
        Determina si dos bloques contiguos deben fusionarse.
        
        NUEVA L√ìGICA: M√°s conservador con t√≠tulos y n√∫meros romanos
        """
        curr_stripped = curr_text.strip()
        next_stripped = next_text.strip()
        
        # NO fusionar si el siguiente bloque parece ser un t√≠tulo corto
        if self._looks_like_short_title(next_stripped):
            logger.info(f"üö´ NO FUSIONAR: siguiente bloque parece t√≠tulo: '{next_stripped[:50]}'")
            return False
        
        # NO fusionar si son de p√°ginas diferentes Y el siguiente es un t√≠tulo potencial
        if curr_page != next_page and len(next_stripped) < 100:
            if next_stripped[0].isupper() or re.match(r'^[IVXLCDM]+\s*$', next_stripped):
                logger.info(f"üö´ NO FUSIONAR: cambio de p√°gina con posible t√≠tulo: '{next_stripped[:50]}'")
                return False
        
        # L√≥gica existente de fusi√≥n
        # Fusi√≥n m√°s agresiva para PDFs con OCR (problemas de l√≠neas cortadas)
        if self.config.get('aggressive_merge_for_pdfs', True):
            # Criterios para NO fusionar
            if self._looks_like_title(next_text):
                return False
            
            # No fusionar si hay un cambio grande de p√°gina
            if abs(next_page - curr_page) > 1:
                return False
            
            # NUEVA L√ìGICA: No fusionar si el gap vertical es grande Y el siguiente texto empieza con may√∫scula
            # (indica nuevo p√°rrafo/secci√≥n)
            if vertical_gap > 15 and next_stripped and next_stripped[0].isupper():
                return False
            
            # Fusionar si:
            # 1. Est√°n en la misma p√°gina o p√°ginas consecutivas
            # 2. El gap vertical es razonable
            # 3. El texto actual no termina con puntuaci√≥n fuerte
            same_or_consecutive_page = abs(next_page - curr_page) <= 1
            reasonable_gap = vertical_gap <= self.config.get('max_vertical_gap_aggressive_pt', 20)
            needs_continuation = not curr_text.rstrip().endswith(('.', '!', '?', ':', ';'))
            
            return same_or_consecutive_page and reasonable_gap and needs_continuation
        
        # L√≥gica original m√°s conservadora
        return (curr_page == next_page and 
                vertical_gap <= max_gap and 
                not self._looks_like_title(next_text))
    
    def _looks_like_short_title(self, text: str) -> bool:
        """
        Detecta t√≠tulos muy cortos como n√∫meros romanos o cap√≠tulos
        """
        text_stripped = text.strip()
        
        # N√∫meros romanos solos
        if re.match(r'^[IVXLCDM]+\s*$', text_stripped):
            return True
        
        # Texto muy corto (menos de 20 chars) todo en may√∫sculas
        if len(text_stripped) < 20 and text_stripped.isupper():
            return True
        
        # Patrones de cap√≠tulo/parte muy cortos
        short_title_patterns = [
            r'^(Cap√≠tulo|CAP√çTULO|Cap\.?|CAP\.?)\s*[IVXLCDM0-9]+\s*$',
            r'^(Parte|PARTE)\s*[IVXLCDM0-9]+\s*$',
            r'^\d+\s*$',  # Solo n√∫meros
            r'^[A-Z]\s*$',  # Una sola letra may√∫scula
        ]
        
        for pattern in short_title_patterns:
            if re.match(pattern, text_stripped, re.IGNORECASE):
                return True
        
        return False

    def _is_title_or_header_text(self, text: str) -> bool:
        """
        Detectar t√≠tulos o encabezados usando estructura markdown y patrones sem√°nticos.
        Versi√≥n adaptada del MarkdownSegmenter para el CommonBlockPreprocessor.
        """
        text_stripped = text.strip()
        
        # 1. ENCABEZADOS MARKDOWN (###, ####, etc.)
        if re.match(r'^#{1,6}\s+', text_stripped):
            return True
        
        # 2. TEXTO EN NEGRITA QUE PARECE T√çTULO/SECCI√ìN
        # Detectar **TEXTO** que parece encabezado de secci√≥n
        bold_pattern = r'^\*\*([^*]+)\*\*$'
        bold_match = re.match(bold_pattern, text_stripped)
        if bold_match:
            bold_content = bold_match.group(1).strip()
            # Si est√° en may√∫sculas o parece t√≠tulo de secci√≥n
            if (bold_content.isupper() or 
                len(bold_content) < 100 or
                any(indicator in bold_content.lower() for indicator in 
                    ['radio', 'cap√≠tulo', 'parte', 'secci√≥n', 'pr√≥logo', 'ep√≠logo', 'mi√©rcoles', 'lunes', 'martes', 'jueves', 'viernes', 's√°bado', 'domingo'])):
                return True
        
        # 3. TEXTO TODO EN MAY√öSCULAS (probable t√≠tulo)
        if text_stripped.isupper() and len(text_stripped) > 5:
            return True
        
        # 4. PATRONES DE T√çTULOS TRADICIONALES
        if len(text_stripped) < 100 and not text_stripped.endswith('.'):
            title_indicators = ['cap√≠tulo', 'parte', 'secci√≥n', 'pr√≥logo', 'ep√≠logo']
            if any(indicator in text_stripped.lower() for indicator in title_indicators):
                return True
        
        # 5. PATRONES DE FECHA/LUGAR QUE INDICAN NUEVA SECCI√ìN
        # Ej: "RADIO EXTERIOR DE ESPA√ëA. MI√âRCOLES 13 MARZO"
        date_place_patterns = [
            r'[A-Z\s]+\.\s+(LUNES|MARTES|MI√âRCOLES|JUEVES|VIERNES|S√ÅBADO|DOMINGO)',
            r'[A-Z\s]+\s+\d{1,2}\s+(ENERO|FEBRERO|MARZO|ABRIL|MAYO|JUNIO|JULIO|AGOSTO|SEPTIEMBRE|OCTUBRE|NOVIEMBRE|DICIEMBRE)',
            r'^[A-Z\s]{10,}\.$'  # Texto largo en may√∫sculas terminado en punto
        ]
        
        for pattern in date_place_patterns:
            if re.search(pattern, text_stripped, re.IGNORECASE):
                return True
        
        return False

    def _get_merge_separator(self, prev_text: str, curr_text: str) -> str:
        """
        Determina el separador apropiado para fusionar dos bloques de texto.
        
        Args:
            prev_text: Texto del bloque anterior
            curr_text: Texto del bloque actual
            
        Returns:
            Separador apropiado (' ', '', etc.)
        """
        prev_stripped = prev_text.strip()
        curr_stripped = curr_text.strip()
        
        # Sin separador si el anterior termina con gui√≥n (palabra cortada)
        if prev_stripped.endswith('-') and curr_stripped and curr_stripped[0].islower():
            return ''
            
        # Sin separador si el actual empieza con puntuaci√≥n
        if curr_stripped.startswith((',', '.', ';', ':', '!', '?', ')', ']', '¬ª', '"')):
            return ''
            
        # Espacio en todos los dem√°s casos
        return ' '

    def process(self, blocks: List[Dict], document_metadata: Dict[str, Any]) -> List[Dict]:
        """
        üö® VERSI√ìN 7.1 - FUSI√ìN INTELIGENTE OCR üö®
        
        Procesa bloques con fusi√≥n inteligente para OCR que genera l√≠neas fragmentadas.
        Detecta autom√°ticamente si el texto viene de OCR y aplica fusi√≥n apropiada.
        
        Args:
            blocks: Lista de bloques de texto con metadatos.
            document_metadata: Metadatos del documento.
            
        Returns:
            Lista de bloques procesados.
        """
        print("üö®üö®üö® COMMONBLOCK V7.1 - FUSI√ìN INTELIGENTE OCR üö®üö®üö®")
        logger.warning("üö®üö®üö® COMMONBLOCK V7.1 - FUSI√ìN INTELIGENTE OCR üö®üö®üö®")
        
        # DEBUG: Ver qu√© bloques estamos recibiendo
        print(f"üìä DEBUG process - Bloques recibidos: {len(blocks)}")
        logger.warning(f"üìä DEBUG process - Bloques recibidos: {len(blocks)}")
        for i, block in enumerate(blocks[:3]):  # Primeros 3 bloques
            text = block.get('text', '')
            print(f"   Bloque {i}: {len(text)} chars, inicio: {repr(text[:100])}")
            logger.warning(f"   Bloque {i}: {len(text)} chars")
        
        if not blocks:
            logger.info("No hay bloques para procesar.")
            return []
        
        logger.info(f"Iniciando procesamiento de {len(blocks)} bloques.")
        
        # üÜï PASO 1: Detectar y filtrar elementos estructurales
        structural_elements = self._detect_structural_elements(blocks)
        if structural_elements:
            blocks = self._filter_structural_elements(blocks, structural_elements)
            logger.info(f"‚úÖ Filtrado estructural aplicado: {len(structural_elements)} elementos eliminados")
        
        # üÜï PASO 2: Detectar si es texto OCR y aplicar fusi√≥n inteligente
        ocr_detected = self._detect_ocr_fragmentation(blocks, document_metadata.get('doc_profile_used', ''))
        if ocr_detected:
            logger.warning("üîç TEXTO OCR DETECTADO - APLICANDO FUSI√ìN INTELIGENTE")
            blocks = self._merge_contiguous_fitz_blocks(blocks)
            logger.warning(f"‚úÖ FUSI√ìN OCR APLICADA: {len(blocks)} bloques despu√©s de fusi√≥n")
        
        processed_blocks = []
        
        for i, block in enumerate(blocks):
            # Extraer texto del bloque
            text = block.get('text', '').strip()
            if not text:
                continue
            
            # Limpieza b√°sica
            text = self._clean_block_text(text)
            if not text:
                continue
            
            # Filtrar bloques insignificantes
            if self._is_insignificant_block(text):
                continue
            
            # ‚úÖ RESPETA CONFIGURACI√ìN: split_blocks_into_paragraphs
            if self.config.get('split_blocks_into_paragraphs', True):
                # Para texto OCR FUSIONADO, dividir en p√°rrafos bas√°ndose en puntuaci√≥n
                if ocr_detected and len(text) > 1000:  # Texto fusionado grande
                    logger.info(f"üìÑ Dividiendo texto fusionado OCR en p√°rrafos: {len(text)} caracteres")
                    paragraph_blocks = self._split_fused_text_into_paragraphs(
                        text, 
                        i, 
                        block.get('metadata', {})
                    )
                    # Convertir la estructura devuelta por _split_fused_text_into_paragraphs
                    for para_block in paragraph_blocks:
                        new_block = {
                            'text': para_block['text'],
                            'metadata': {
                                'order': para_block['metadata']['order'],
                                'source_block': i,
                                'type': para_block['metadata'].get('type', 'paragraph'),
                                **block.get('metadata', {})
                            }
                        }
                        processed_blocks.append(new_block)
                elif ocr_detected:
                    # Para fragmentos OCR peque√±os, usar fusi√≥n inteligente de l√≠neas
                    paragraphs = self._smart_merge_lines(
                        text.split('\n'), 
                        i, 
                        block.get('metadata', {})
                    )
                    # Agregar cada p√°rrafo como un bloque separado
                    for paragraph_text, order, coords in paragraphs:
                        new_block = {
                            'text': paragraph_text,
                            'metadata': {
                                'order': order,
                                'source_block': i,
                                'type': block.get('metadata', {}).get('type', 'paragraph'),
                                **block.get('metadata', {})
                            }
                        }
                        processed_blocks.append(new_block)
                else:
                    # Dividir el bloque en p√°rrafos usando el algoritmo simple
                    paragraphs = self._split_text_into_paragraphs(
                        text, 
                        i, 
                        block.get('metadata', {})
                    )
                    # Agregar cada p√°rrafo como un bloque separado
                    for paragraph_text, order, coords in paragraphs:
                        new_block = {
                            'text': paragraph_text,
                            'metadata': {
                                'order': order,
                                'source_block': i,
                                'type': block.get('metadata', {}).get('type', 'paragraph'),
                                **block.get('metadata', {})
                            }
                        }
                        processed_blocks.append(new_block)
            else:
                # üéµ MODO VERSO: Mantener bloque original sin dividir
                print(f"üéµ MODO VERSO: Manteniendo bloque sin dividir: {repr(text[:50])}")
                logger.warning(f"üéµ MODO VERSO: Manteniendo bloque sin dividir: {repr(text[:50])}")
                
                new_block = {
                    'text': text,
                    'metadata': {
                        'order': i,
                        'source_block': i,
                        'type': block.get('metadata', {}).get('type', 'paragraph'),
                        **block.get('metadata', {})
                    }
                }
                processed_blocks.append(new_block)
        
        print(f"‚úÖ PROCESO COMPLETADO: {len(blocks)} ‚Üí {len(processed_blocks)} bloques")
        logger.warning(f"‚úÖ PROCESO COMPLETADO: {len(blocks)} ‚Üí {len(processed_blocks)} bloques")
        
        return processed_blocks, document_metadata

    def _detect_ocr_fragmentation(self, blocks: List[Dict], doc_profile: str = "") -> bool:
        """
        Detecta si los bloques provienen de OCR y est√°n excesivamente fragmentados.
        
        Criterios para detectar OCR fragmentado:
        - Muchos bloques muy cortos (< 100 caracteres)
        - Alta proporci√≥n de bloques de una sola l√≠nea
        - Patrones t√≠picos de OCR l√≠nea por l√≠nea
        
        Returns:
            True si se detecta fragmentaci√≥n OCR, False en caso contrario
        """
        # Evitar analizar documentos de verso: la poes√≠a leg√≠timamente produce l√≠neas cortas
        if doc_profile and doc_profile.lower() == 'verso':
            return False

        if len(blocks) < 50:  # Muy pocos bloques para ser OCR fragmentado
            return False
        
        short_blocks = 0
        single_line_blocks = 0
        total_text_blocks = 0
        no_punct_blocks = 0
        
        for block in blocks:
            text = block.get('text', '').strip()
            if not text:
                continue
                
            total_text_blocks += 1
            
            # Contar bloques cortos
            if len(text) < 100:
                short_blocks += 1
            
            # Contar bloques de una sola l√≠nea
            if '\n' not in text:
                single_line_blocks += 1
            
            # Contar bloques que NO terminan en puntuaci√≥n fuerte
            if not text.endswith(('.', '!', '?', ':', ';')):
                no_punct_blocks += 1
        
        if total_text_blocks == 0:
            return False
        
        # Calcular proporciones
        short_ratio = short_blocks / total_text_blocks
        single_line_ratio = single_line_blocks / total_text_blocks
        no_punct_ratio = no_punct_blocks / total_text_blocks
        
        # M√©trica adicional: longitud media por bloque
        total_chars = sum(len(b.get('text','')) for b in blocks if b.get('text'))
        avg_len = (total_chars / total_text_blocks) if total_text_blocks else 0

        # Detectar OCR si se cumple UNO de los siguientes escenarios:
        # 1) Criterios estrictos (65/75/70) -> OCR fragmentado evidente
        # 2) Documento muy fragmentado (>500 bloques) Y la longitud media es baja (<120) Y al menos 50 % de bloques son cortos
        ocr_detected = (
            (short_ratio > 0.65 and single_line_ratio > 0.75 and no_punct_ratio > 0.7) or
            (total_text_blocks > 500 and avg_len < 120 and short_ratio > 0.5)
        )

        if ocr_detected:
            logger.warning("üîç OCR FRAGMENTADO DETECTADO:")
            logger.warning(f"   üìä Bloques totales: {total_text_blocks}")
            logger.warning(f"   üìè Bloques cortos (<100 chars): {short_blocks} ({short_ratio:.1%})")
            logger.warning(f"   üìù Bloques una l√≠nea: {single_line_blocks} ({single_line_ratio:.1%})")
            logger.warning(f"   ‚ùå Bloques sin puntuaci√≥n final: {no_punct_blocks} ({no_punct_ratio:.1%})")
            logger.warning(f"   ‚û°Ô∏è  Longitud media por bloque: {avg_len:.1f} chars")
        
        return ocr_detected

    def _normalize_text_for_structural_detection(self, text: str) -> str:
        """
        üîß MEJORADO: Normaliza texto para detecci√≥n de elementos estructurales.
        
        Maneja variaciones corruptas/formateadas del mismo texto usando normalizaci√≥n agresiva:
        - "*Antolo* *g* *√≠a* *Rub√©n Dar√≠o*" ‚Üí "antologia ruben dario"
        - "A n t o l o g √≠ a   R u b √© n   D a r √≠ o" ‚Üí "antologia ruben dario"
        - "ANTOLOG√çA RUB√âN DAR√çO" ‚Üí "antologia ruben dario"
        
        Args:
            text: Texto original (posiblemente corrupto)
            
        Returns:
            Texto normalizado para comparaci√≥n
        """
        if not text:
            return ""
        
        import unicodedata
        
        # 1. Remover asteriscos y caracteres de formato especiales
        normalized = re.sub(r'[*_`~\[\]{}()]+', '', text)
        
        # 2. Remover caracteres de puntuaci√≥n, mantener solo letras, n√∫meros y espacios
        normalized = re.sub(r'[^\w\s\-√°√©√≠√≥√∫√º√±√Å√â√ç√ì√ö√ú√ë]', ' ', normalized)
        
        # 3. Normalizar acentos Unicode (NFD = descomponer, luego filtrar solo ASCII)
        normalized = unicodedata.normalize('NFD', normalized)
        normalized = ''.join(c for c in normalized if unicodedata.category(c) != 'Mn')
        
        # 4. Todo a min√∫sculas
        normalized = normalized.lower()
        
        # 5. AGRESIVO: Unir fragmentos cortos que probablemente son palabras divididas
        # "antolo g ia" ‚Üí "antologia"
        words = normalized.split()
        merged_words = []
        i = 0
        
        while i < len(words):
            current_word = words[i]
            
            # Si la palabra actual es muy corta (‚â§ 3 chars), intentar unir con siguientes
            if len(current_word) <= 3 and i + 1 < len(words):
                # Unir hasta encontrar una palabra m√°s larga o llegar al final
                merged = current_word
                j = i + 1
                
                while j < len(words) and len(words[j]) <= 3:
                    merged += words[j]
                    j += 1
                
                # Si la siguiente palabra tambi√©n existe y es parte del t√≠tulo, agregarla
                if j < len(words):
                    # Palabras como "ruben", "dario" son parte del nombre
                    next_word = words[j]
                    if next_word in ['ruben', 'dario', 'rub√©n', 'dar√≠o'] or len(merged) + len(next_word) <= 15:
                        merged += next_word
                        j += 1
                
                merged_words.append(merged)
                i = j
            else:
                merged_words.append(current_word)
                i += 1
        
        # 6. Unir palabras y normalizar espacios finales
        normalized = ' '.join(merged_words)
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        
        return normalized

    def _detect_structural_elements(self, blocks: List[Dict]) -> List[str]:
        """
        üîß MEJORADO: Detecta elementos estructurales repetitivos con normalizaci√≥n robusta.
        
        Ahora maneja variaciones corruptas del mismo texto:
        - "*Antolo* *g* *√≠a* *Rub√©n Dar√≠o*" se detecta como "Antolog√≠a Rub√©n Dar√≠o"
        """
        if not self.config.get('filter_structural_elements', True):
            return []
            
        # Recopilar informaci√≥n de p√°ginas y textos CON NORMALIZACI√ìN
        text_to_pages = {}  # {texto_normalizado: {'pages': set(), 'original_texts': set()}}
        all_pages = set()
        
        for block in blocks:
            original_text = block.get('text', '').strip()
            if not original_text or len(original_text) < 3:
                continue
                
            # NORMALIZAR texto para detecci√≥n
            normalized_text = self._normalize_text_for_structural_detection(original_text)
            if not normalized_text or len(normalized_text) < 3:
                continue
                
            # Obtener n√∫mero de p√°gina del metadata
            page_num = None
            metadata = block.get('metadata', {})
            
            # Intentar m√∫ltiples formas de obtener el n√∫mero de p√°gina
            if 'page_number' in metadata:
                page_num = metadata['page_number']
            elif 'source_page_number' in metadata:
                page_num = metadata['source_page_number']
            elif 'page' in metadata:
                page_num = metadata['page']
            
            if page_num is not None:
                all_pages.add(page_num)
                
                if normalized_text not in text_to_pages:
                    text_to_pages[normalized_text] = {
                        'pages': set(),
                        'original_texts': set()
                    }
                
                text_to_pages[normalized_text]['pages'].add(page_num)
                text_to_pages[normalized_text]['original_texts'].add(original_text)
        
        total_pages = len(all_pages)
        min_pages = self.config.get('min_pages_for_structural_detection', 5)
        
        # Solo analizar si hay suficientes p√°ginas
        if total_pages < min_pages:
            logger.debug(f"üìÑ Solo {total_pages} p√°ginas, omitiendo detecci√≥n estructural (m√≠nimo: {min_pages})")
            return []
        
        # Detectar elementos estructurales
        structural_elements = []
        threshold = self.config.get('structural_frequency_threshold', 0.9)
        
        logger.info(f"üîç Analizando elementos estructurales en {total_pages} p√°ginas (umbral: {threshold*100}%)")
        
        # M√âTODO 1: Detecci√≥n por normalizaci√≥n (como antes)
        for normalized_text, data in text_to_pages.items():
            pages = data['pages']
            original_texts = data['original_texts']
            frequency = len(pages) / total_pages
            
            # Si aparece en m√°s del umbral de p√°ginas, es estructural
            if frequency >= threshold:
                # Agregar TODAS las variaciones originales del texto
                for original_text in original_texts:
                    structural_elements.append(original_text)
                
                # Log con ejemplo de variaciones detectadas
                examples = list(original_texts)[:3]  # Mostrar m√°ximo 3 ejemplos
                logger.warning(f"üö´ Elemento estructural detectado ({frequency*100:.1f}%): '{normalized_text}'")
                logger.warning(f"   üìù Variaciones encontradas: {examples}")
        
        # M√âTODO 2: Detecci√≥n por patrones espec√≠ficos conocidos (NUEVO)
        # Buscar patrones como "*Antolo*" que claramente son corrupci√≥n de "Antolog√≠a"
        pattern_pages = {}  # {pattern: set(pages)}
        
        for block in blocks:
            original_text = block.get('text', '').strip()
            if not original_text or len(original_text) < 5:
                continue
                
            page_num = None
            metadata = block.get('metadata', {})
            if 'page_number' in metadata:
                page_num = metadata['page_number']
            elif 'source_page_number' in metadata:
                page_num = metadata['source_page_number']
            elif 'page' in metadata:
                page_num = metadata['page']
                
            if page_num is not None:
                # Buscar patrones espec√≠ficos de corrupci√≥n de "Antolog√≠a Rub√©n Dar√≠o"
                patterns_to_check = [
                    r'\*Antolo\*.*\*[g√≠]\*.*\*[√≠i]a\*.*Rub[e√©]n.*Dar[√≠i]o',  # *Antolo* *g* *√≠a* *Rub√©n Dar√≠o*
                    r'Antolo.*Rub[e√©]n.*Dar[√≠i]o',  # Variaciones simples de "Antolog√≠a Rub√©n Dar√≠o"
                    r'\*.*Antolo.*\*.*Rub[e√©]n.*Dar[√≠i]o',  # Cualquier cosa con asteriscos, Antolo, Rub√©n, Dar√≠o
                ]
                
                for pattern in patterns_to_check:
                    if re.search(pattern, original_text, re.IGNORECASE):
                        pattern_key = "antologia_ruben_dario_pattern"
                        if pattern_key not in pattern_pages:
                            pattern_pages[pattern_key] = set()
                        pattern_pages[pattern_key].add(page_num)
                        logger.debug(f"üéØ Patr√≥n detectado en p√°gina {page_num}: '{original_text[:50]}...'")
                        break
        
        # Verificar si los patrones aparecen en suficientes p√°ginas
        for pattern_key, pages in pattern_pages.items():
            frequency = len(pages) / total_pages
            if frequency >= threshold:
                logger.warning(f"üö´ Patr√≥n estructural detectado ({frequency*100:.1f}%): {pattern_key}")
                
                # Agregar todos los bloques que coincidan con este patr√≥n
                for block in blocks:
                    original_text = block.get('text', '').strip()
                    if original_text and len(original_text) >= 5:
                        # Aplicar los mismos patrones de b√∫squeda
                        patterns_to_check = [
                            r'\*Antolo\*.*\*[g√≠]\*.*\*[√≠i]a\*.*Rub[e√©]n.*Dar[√≠i]o',
                            r'Antolo.*Rub[e√©]n.*Dar[√≠i]o',
                            r'\*.*Antolo.*\*.*Rub[e√©]n.*Dar[√≠i]o',
                        ]
                        
                        for pattern in patterns_to_check:
                            if re.search(pattern, original_text, re.IGNORECASE):
                                structural_elements.append(original_text)
                                logger.debug(f"üéØ Agregado por patr√≥n: '{original_text[:30]}...'")
                                break
        
        logger.info(f"üîç Detecci√≥n completada: {len(structural_elements)} elementos estructurales encontrados")
        return structural_elements

    def _filter_structural_elements(self, blocks: List[Dict], structural_elements: List[str]) -> List[Dict]:
        """
        üîß MEJORADO: Filtra elementos estructurales de los bloques Y limpia texto que los contenga.
        
        Args:
            blocks: Lista de bloques originales
            structural_elements: Lista de textos estructurales a filtrar
            
        Returns:
            Lista de bloques filtrados sin elementos estructurales
        """
        if not structural_elements:
            return blocks
            
        filtered_blocks = []
        filtered_count = 0
        cleaned_count = 0
        
        for block in blocks:
            text = block.get('text', '').strip()
            
            # Verificar si el texto ES EXACTAMENTE un elemento estructural
            is_structural = text in structural_elements
            
            if is_structural:
                filtered_count += 1
                logger.debug(f"üö´ Filtrando bloque estructural completo: '{text[:30]}{'...' if len(text) > 30 else ''}'")
                continue
            
            # NUEVA FUNCIONALIDAD: Limpiar elementos estructurales DENTRO del texto
            cleaned_text = text
            text_was_cleaned = False
            
            for structural_element in structural_elements:
                if structural_element in cleaned_text:
                    # Remover el elemento estructural del texto
                    cleaned_text = cleaned_text.replace(structural_element, '').strip()
                    text_was_cleaned = True
                    logger.debug(f"üßπ Limpiando elemento estructural '{structural_element[:20]}...' del bloque")
            
            # Limpiar saltos de l√≠nea y espacios excesivos despu√©s de la limpieza
            if text_was_cleaned:
                cleaned_text = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned_text)  # Max 2 saltos consecutivos
                cleaned_text = re.sub(r'^\s+|\s+$', '', cleaned_text, flags=re.MULTILINE)  # Limpiar espacios al inicio/fin de l√≠neas
                cleaned_text = cleaned_text.strip()
                cleaned_count += 1
            
            # Solo agregar si queda contenido despu√©s de la limpieza
            if cleaned_text:
                # Crear bloque con texto limpio
                cleaned_block = block.copy()
                cleaned_block['text'] = cleaned_text
                filtered_blocks.append(cleaned_block)
            else:
                # El bloque qued√≥ vac√≠o despu√©s de limpiar elementos estructurales
                filtered_count += 1
                logger.debug(f"üóëÔ∏è Bloque vac√≠o despu√©s de limpieza: eliminado")
        
        logger.info(f"üîÑ Filtrado estructural: {len(blocks)} ‚Üí {len(filtered_blocks)} bloques")
        logger.info(f"   üìä {filtered_count} bloques eliminados, {cleaned_count} bloques limpiados")
        return filtered_blocks

    def _split_fused_text_into_paragraphs(self, text: str, base_order: float, original_metadata: Dict) -> List[Dict]:
        """
        Divide texto fusionado en p√°rrafos usando √öNICAMENTE la estructura sem√°ntica del documento.
        NO hace divisiones arbitrarias por longitud.
        """
        if not text.strip():
            return []
        
        logger = logging.getLogger(__name__)
        logger.info(f"üîÑ DIVIDIENDO TEXTO FUSIONADO: {len(text)} caracteres usando estructura sem√°ntica")
        
        # M√©todo 1: Divisi√≥n por dobles saltos de l√≠nea (estructura natural del documento)
        natural_paragraphs = re.split(r'\n\s*\n\s*', text)
        if len(natural_paragraphs) > 1:
            logger.info(f"üìÑ Encontrados {len(natural_paragraphs)} p√°rrafos naturales (dobles saltos)")
            paragraphs = []
            for i, para_text in enumerate(natural_paragraphs):
                para_text = para_text.strip()
                if para_text:  # Cualquier p√°rrafo con contenido es v√°lido
                    paragraphs.append({
                        'text': para_text,
                        'metadata': {
                            'order': base_order + (i * 0.01),
                            'source_block': original_metadata.get('source_block', 0),
                            'type': 'paragraph',
                            'post_ocr_split': True,
                            'split_method': 'semantic_natural_paragraphs',
                            **original_metadata
                        }
                    })
            if paragraphs:
                logger.info(f"‚úÖ Divisi√≥n sem√°ntica por p√°rrafos naturales: {len(paragraphs)} p√°rrafos")
                return paragraphs
        
        # M√©todo 2: Divisi√≥n por headers markdown y patrones sem√°nticos espec√≠ficos
        semantic_patterns = [
            r'(?<=[.!?])\s*\n\s*(?=\*\*[A-Z√Å√â√ç√ì√ö√ú√ë][^*]*\*\*)',  # **HEADERS EN MAY√öSCULAS**
            r'(?<=[.!?])\s*\n\s*(?=#{1,6}\s+)',  # Headers markdown (# ## ###)
            r'(?<=[.!?])\s*\n\s*(?=RADIO\s+EXTERIOR)',  # "RADIO EXTERIOR"
            r'(?<=[.!?])\s*\n\s*(?=RADIO\s+REBELDE)',  # "RADIO REBELDE"
            r'(?<=[.!?])\s*\n\s*(?=MI√âRCOLES|LUNES|MARTES|JUEVES|VIERNES|S√ÅBADO|DOMINGO)',  # D√≠as de la semana
            r'(?<=[.!?])\s*\n\s*(?=[A-Z√Å√â√ç√ì√ö√ú√ë][A-Z√Å√â√ç√ì√ö√ú√ë\s]{15,})',  # TEXTO EN MAY√öSCULAS LARGO (t√≠tulos)
            r'(?<=[.!?])\s*\n\s*(?=\d+\.)',  # Punto + salto + n√∫mero (listas)
            r'(?<=[.!?])\s*\n\s*(?=[-‚Ä¢*]\s)',  # Punto + salto + vi√±eta
        ]
        
        for pattern in semantic_patterns:
            splits = re.split(pattern, text)
            if len(splits) > 1:  # Al menos 2 segmentos
                logger.info(f"üìÑ Patr√≥n sem√°ntico encontr√≥ {len(splits)} segmentos")
                paragraphs = []
                
                for i, segment in enumerate(splits):
                    segment = segment.strip()
                    if segment:  # Cualquier segmento con contenido es v√°lido
                        paragraphs.append({
                            'text': segment,
                            'metadata': {
                                'order': base_order + (i * 0.01),
                                'source_block': original_metadata.get('source_block', 0),
                                'type': 'paragraph',
                                'post_ocr_split': True,
                                'split_method': 'semantic_pattern',
                                **original_metadata
                            }
                        })
                
                if len(paragraphs) > 1:
                    logger.info(f"‚úÖ Divisi√≥n sem√°ntica por patr√≥n: {len(paragraphs)} p√°rrafos")
                    return paragraphs
        
        # M√©todo 3: Divisi√≥n por estructura de p√°rrafos (puntuaci√≥n + salto + may√∫scula)
        paragraph_pattern = r'(?<=[.!?])\s*\n\s*(?=[A-Z√Å√â√ç√ì√ö√ú√ë])'
        sentences = re.split(paragraph_pattern, text)
        if len(sentences) > 1:
            logger.info(f"üìÑ Divisi√≥n por estructura de p√°rrafos: {len(sentences)} segmentos")
            paragraphs = []
            
            for i, sentence in enumerate(sentences):
                sentence = sentence.strip()
                if sentence:  # Cualquier p√°rrafo con contenido es v√°lido
                    paragraphs.append({
                        'text': sentence,
                        'metadata': {
                            'order': base_order + (i * 0.01),
                            'source_block': original_metadata.get('source_block', 0),
                            'type': 'paragraph',
                            'post_ocr_split': True,
                            'split_method': 'semantic_paragraph_structure',
                            **original_metadata
                        }
                    })
            
            if len(paragraphs) > 1:
                logger.info(f"‚úÖ Divisi√≥n sem√°ntica por estructura: {len(paragraphs)} p√°rrafos")
                return paragraphs
        
        # Si no se puede dividir sem√°nticamente, mantener como p√°rrafo √∫nico
        logger.info("üìÑ Manteniendo texto como p√°rrafo √∫nico (no se encontr√≥ estructura sem√°ntica)")
        return [{
            'text': text.strip(),
            'metadata': {
                'order': base_order,
                'source_block': original_metadata.get('source_block', 0),
                'type': 'paragraph',
                'post_ocr_split': False,
                'split_method': 'semantic_single_paragraph',
                **original_metadata
            }
        }]

if __name__ == '__main__':
    # Ejemplo de uso b√°sico
    logging.basicConfig(level=logging.DEBUG)
    
    print("--- Ejemplo 1: Configuraci√≥n por defecto ---")
    preprocessor_default = CommonBlockPreprocessor()
    sample_metadata_1 = {
        'source_file_path': '/path/to/my_document_2023-10-25.txt',
        'author': 'Test Author'
    }
    sample_blocks_1 = [
        {'text': 'Bloque 1.\r\nSalto Windows. \x00 NUL char.\n\nEste es otro p√°rrafo en el mismo bloque.', 'order_in_document': 0, 'source_page_number': 1},
        {'text': 'Bloque 2\rSalto Mac.', 'order_in_document': 1, 'source_page_number': 2}
    ]
    processed_blocks_1, processed_metadata_1 = preprocessor_default.process(sample_blocks_1, sample_metadata_1)
    print("Metadata 1:", processed_metadata_1)
    print("Blocks 1:")
    for b in processed_blocks_1: print(b)

    print("\n--- Ejemplo 2: Sin extracci√≥n de fecha del nombre de archivo y sin divisi√≥n de p√°rrafos ---")
    config_no_split = {
        'extract_filename_date': False, 
        'normalize_line_endings': True, 
        'remove_nul_chars': True,
        'split_blocks_into_paragraphs': False
    }
    preprocessor_no_split = CommonBlockPreprocessor(config=config_no_split)
    sample_metadata_2 = {
        'source_file_path': '/path/to/another_doc_2024_01_15.md',
        'detected_date': '2024-01-01' # Fecha preexistente de propiedades del doc
    }
    sample_blocks_2 = [
        {'text': 'P√°rrafo √∫nico.\nAunque tenga saltos.\n\nY una l√≠nea vac√≠a en medio.', 'order_in_document': 'A'}
    ]
    processed_blocks_2, processed_metadata_2 = preprocessor_no_split.process(sample_blocks_2, sample_metadata_2)
    print("Metadata 2:", processed_metadata_2)
    print("Blocks 2:")
    for b in processed_blocks_2: print(b)

    print("\n--- Ejemplo 3: Texto corto, no se divide ---")
    preprocessor_short_text = CommonBlockPreprocessor(config={'min_chars_for_paragraph_split': 100})
    sample_blocks_short = [
        {'text': 'Texto corto.\nNo se divide.', 'order_in_document': 0}
    ]
    processed_blocks_short, processed_metadata_short = preprocessor_short_text.process(sample_blocks_short, {})
    print("Blocks Short:")
    for b in processed_blocks_short: print(b)

    print("\n--- Ejemplo 4: Bloque sin texto ---")
    sample_blocks_no_text = [
        {'order_in_document': 0, 'source_page_number': 1, 'type': 'image_placeholder'}
    ]
    processed_blocks_no_text, processed_metadata_no_text = preprocessor_default.process(sample_blocks_no_text, {})
    print("Blocks No Text:")
    for b in processed_blocks_no_text: print(b) 