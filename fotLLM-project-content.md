## Table of Contents
- [test prosa](#test-prosa)
- [package lock](#package-lock)
- [test poema](#test-poema)
- [requirements](#requirements)
- [README](#readme)
- [package](#package)
- [example prd](#example-prd)
- [prd](#prd)
- [config](#config)
- [nodemon](#nodemon)
- [INDICE](#indice)
- [JERARQUIA CONTEXTUAL ESPECIFICACION](#jerarquia-contextual-especificacion)
- [GUIA PROCESAMIENTO DATOS](#guia-procesamiento-datos)
- [GUIA COMANDOS PROCESAMIENTO](#guia-comandos-procesamiento)
- [INICIO RAPIDO](#inicio-rapido)
- [ALGORITMOS PROPUESTOS](#algoritmos-propuestos)
- [PIPELINE NDJSON](#pipeline-ndjson)
- [IMPLEMENTACION PERFILES](#implementacion-perfiles)
- [GUIA LOADERS](#guia-loaders)
- [NDJSON ESPECIFICACION](#ndjson-especificacion)
- [ESTRATEGIA PROCESAMIENTO](#estrategia-procesamiento)
- [BIBLIOPERSON ARQUITECTURA](#biblioperson-arquitectura)
- [CONFIGURACION PROCESAMIENTO](#configuracion-procesamiento)
- [GUIA MEILISEARCH](#guia-meilisearch)
- [GUIA GESTION DATOS](#guia-gestion-datos)
- [tailwind.config](#tailwind.config)
- [index](#index)
- [eslint.config](#eslint.config)
- [postcss.config](#postcss.config)
- [vite.config](#vite.config)
- [tsconfig.app](#tsconfig.app)
- [tsconfig.node](#tsconfig.node)
- [tsconfig](#tsconfig)
- [mcp](#mcp)
- [data models](#data-models)
- [probar loaders](#probar-loaders)
- [processors](#processors)
- [process file](#process-file)
- [utils](#utils)
- [app depuracion](#app-depuracion)
- [test heading segmenter](#test-heading-segmenter)
- [converters](#converters)
- [chunking strategies](#chunking-strategies)
- [cli](#cli)
- [  init  ](#--init--)
- [profile manager](#profile-manager)
- [content profiles.example](#content-profiles.example)
- [content profiles](#content-profiles)
- [jobs config](#jobs-config)
- [ejemplo](#ejemplo)
- [heading segmenter test](#heading-segmenter-test)
- [heading segmenter flat](#heading-segmenter-flat)
- [heading segmenter](#heading-segmenter)
- [base](#base)
- [verse segmenter](#verse-segmenter)
- [txt loader](#txt-loader)
- [pdf loader](#pdf-loader)
- [excel loader](#excel-loader)
- [base loader](#base-loader)
- [docx loader](#docx-loader)
- [md loader](#md-loader)
- [csv loader](#csv-loader)
- [markdown loader](#markdown-loader)
- [ndjson loader](#ndjson-loader)
- [common block preprocessor](#common-block-preprocessor)
- [content profiles.schema](#content-profiles.schema)
- [jobs config.schema](#jobs-config.schema)
- [book structure](#book-structure)
- [chapter heading](#chapter-heading)
- [perfil docx heading](#perfil-docx-heading)
- [poem or lyrics](#poem-or-lyrics)
- [levantar meilisearch](#levantar-meilisearch)
- [probar embeddings](#probar-embeddings)
- [api conexion](#api-conexion)
- [inicializar semantica](#inicializar-semantica)
- [indexar meilisearch](#indexar-meilisearch)
- [limpiar datos duplicados](#limpiar-datos-duplicados)
- [importar datos](#importar-datos)
- [procesar semantica](#procesar-semantica)
- [embedding service](#embedding-service)
- [schema](#schema)
- [limpiar base datos](#limpiar-base-datos)
- [mostrar esquema db](#mostrar-esquema-db)
- [generador asistido](#generador-asistido)
- [importar completo](#importar-completo)
- [preparar importacion](#preparar-importacion)
- [vite](#vite)
- [vite env.d](#vite-env.d)
- [main](#main)
- [App](#app)
- [ResultCard](#resultcard)
- [SearchBar](#searchbar)
- [Pagination](#pagination)
- [Skeleton](#skeleton)
- [Navbar](#navbar)
- [api](#api)
- [Home](#home)
- [Explore](#explore)
- [Search](#search)
- [SemanticSearch](#semanticsearch)
- [Generate](#generate)
- [react](#react)
- [self improve](#self-improve)
- [roo rules](#roo-rules)
- [dev workflow](#dev-workflow)
- [taskmaster](#taskmaster)

---

# test prosa

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** biblioperson

# test prosa

El camino hacia la sabidura La bsqueda del conocimiento es un camino largo y sinuoso que todos debemos recorrer. A veces nos encontramos con obstculos que parecen insuperables, pero la perseverancia es la clave para seguir adelante. Cuando era joven, pensaba que la sabidura vendra simplemente con la edad. Pero ahora entiendo que no es el paso del tiempo lo que nos hace sabios, sino las experiencias que acumulamos y las lecciones que aprendemos de ellas. Hay personas que viven muchos aos pero aprenden poco. Otras, en cambio, extraen profundas enseanzas de cada momento vivido, convirtiendo incluso los fracasos en valiosas oportunidades de crecimiento. La verdadera sabidura no consiste en acumular datos o informacin, sino en comprender la vida en su esencia. Es la capacidad de discernir lo importante de lo trivial, de mantener la calma en medio de la tormenta, y de actuar con compasin hacia los dems. Los libros pueden ensearnos mucho, pero la experiencia directa es insustituible. Debemos aprender a observar, a escuchar, a sentir. Solo as podremos integrar el conocimiento terico con la realidad prctica. El sabio no es quien tiene todas las respuestas, sino quien ha aprendido a hacer las preguntas correctas. La curiosidad es el motor que impulsa nuestra bsqueda, mantenindonos siempre abiertos a nuevas perspectivas y horizontes. En ltima instancia, la sabidura nos ensea que somos parte de algo ms grande que nosotros mismos. Nos ayuda a encontrar nuestro lugar en el mundo y a vivirlo con plenitud y gratitud.
---

# package lock

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** biblioperson

# package lock

{ "name": "biblioperson", "version": "1.0.0", "lockfileVersion": 3, "requires": true, "packages": { "": { "name": "biblioperson", "version": "1.0.0", "dependencies": { "react-icons": "^5.5.0" }, "devDependencies": { "concurrently": "^8.2.2" } }, "node_modules/@babel/runtime": { "version": "7.27.1", "resolved": "https://registry.npmjs.org/@babel/runtime/-/runtime-7.27.1.tgz", "integrity": "sha512-1x3D2xEk2fRo3PAhwQwu5UubzgiVWSXTBfWpVd2Mx2AzRqJuDJCsgaDVZ7HB5iGzDW1Hl1sWN2mFyKjmR9uAog==", "dev": true, "license": "MIT", "engines": { "node": ">=6.9.0" } }, "node_modules/ansi-regex": { "version": "5.0.1", "resolved": "https://registry.npmjs.org/ansi-regex/-/ansi-regex-5.0.1.tgz", "integrity": "sha512-quJQXlTSUGL2LH9SUXo8VwsY4soanhgo6LNSm84E1LBcE8s3O0wpdiRzyR9z/ZZJMlMWv37qOOb9pdJlMUEKFQ==", "dev": true, "license": "MIT", "engines": { "node": ">=8" } }, "node_modules/ansi-styles": { "version": "4.3.0", "resolved": "https://registry.npmjs.org/ansi-styles/-/ansi-styles-4.3.0.tgz", "integrity": "sha512-zbB9rCJAT1rbjiVDb2hqKFHNYLxgtk8NURxZ3IZwD3F6NtxbXZQCnnSi1Lkx+IDohdPlFp222wVALIheZJQSEg==", "dev": true, "license": "MIT", "dependencies": { "color-convert": "^2.0.1" }, "engines": { "node": ">=8" }, "funding": { "url": "https://github.com/chalk/ansi-styles?sponsor=1" } }, "node_modules/chalk": { "version": "4.1.2", "resolved": "https://registry.npmjs.org/chalk/-/chalk-4.1.2.tgz", "integrity": "sha512-oKnbhFyRIXpUuez8iBMmyEa4nbj4IOQyuhc/wy9kY7/WVPcwIO9VA668Pu8RkO7+0G76SLROeyw9CpQ061i4mA==", "dev": true, "license": "MIT", "dependencies": { "ansi-styles": "^4.1.0", "supports-color": "^7.1.0" }, "engines": { "node": ">=10" }, "funding": { "url": "https://github.com/chalk/chalk?sponsor=1" } }, "node_modules/chalk/node_modules/supports-color": { "version": "7.2.0", "resolved": "https://registry.npmjs.org/supports-color/-/supports-color-7.2.0.tgz", "integrity": "sha512-qpCAvRl9stuOHveKsn7HncJRvv501qIacKzQlO/+Lwxc9+0q2wLyv4Dfvt80/DPn2pqOBsJdDiogXGR9+OvwRw==", "dev": true, "license": "MIT", "dependencies": { "has-flag": "^4.0.0" }, "engines": { "node": ">=8" } }, "node_modules/cliui": { "version": "8.0.1", "resolved": "https://registry.npmjs.org/cliui/-/cliui-8.0.1.tgz", "integrity": "sha512-BSeNnyus75C4//NQ9gQt1/csTXyo/8Sb+afLAkzAptFuMsod9HFokGNudZpi/oQV73hnVK+sR+5PVRMd+Dr7YQ==", "dev": true, "license": "ISC", "dependencies": { "string-width": "^4.2.0", "strip-ansi": "^6.0.1", "wrap-ansi": "^7.0.0" }, "engines": { "node": ">=12" } }, "node_modules/color-convert": { "version": "2.0.1", "resolved": "https://registry.npmjs.org/color-convert/-/color-convert-2.0.1.tgz", "integrity": "sha512-RRECPsj7iu/xb5oKYcsFHSppFNnsj/52OVTRKb4zP5onXwVF3zVmmToNcOfGC+CRDpfK/U584fMg38ZHCaElKQ==", "dev": true, "license": "MIT", "dependencies": { "color-name": "~1.1.4" }, "engines": { "node": ">=7.0.0" } }, "node_modules/color-name": { "version": "1.1.4", "resolved": "https://registry.npmjs.org/color-name/-/color-name-1.1.4.tgz", "integrity": "sha512-dOy+3AuW3a2wNbZHIuMZpTcgjGuLU/uBL/ubcZF9OXbDo8ff4O8yVp5Bf0efS8uEoYo5q4Fx7dY9OgQGXgAsQA==", "dev": true, "license": "MIT" }, "node_modules/concurrently": { "version": "8.2.2", "resolved": "https://registry.npmjs.org/concurrently/-/concurrently-8.2.2.tgz", "integrity": "sha512-1dP4gpXFhei8IOtlXRE/T/4H88ElHgTiUzh71YUmtjTEHMSRS2Z/fgOxHSxxusGHogsRfxNq1vyAwxSC+EVyDg==", "dev": true, "license": "MIT", "dependencies": { "chalk": "^4.1.2", "date-fns": "^2.30.0", "lodash": "^4.17.21", "rxjs": "^7.8.1", "shell-quote": "^1.8.1", "spawn-command": "0.0.2", "supports-color": "^8.1.1", "tree-kill": "^1.2.2", "yargs": "^17.7.2" }, "bin": { "conc": "dist/bin/concurrently.js", "concurrently": "dist/bin/concurrently.js" }, "engines": { "node": "^14.13.0 || >=16.0.0" }, "funding": { "url": "https://github.com/open-cli-tools/concurrently?sponsor=1" } }, "node_modules/date-fns": { "version": "2.30.0", "resolved": "https://registry.npmjs.org/date-fns/-/date-fns-2.30.0.tgz", "integrity": "sha512-fnULvOpxnC5/Vg3NCiWelDsLiUc9bRwAPs/+LfTLNvetFCtCTN+yQz15C/fs4AwX1R9K5GLtLfn8QW+dWisaAw==", "dev": true, "license": "MIT", "dependencies": { "@babel/runtime": "^7.21.0" }, "engines": { "node": ">=0.11" }, "funding": { "type": "opencollective", "url": "https://opencollective.com/date-fns" } }, "node_modules/emoji-regex": { "version": "8.0.0", "resolved": "https://registry.npmjs.org/emoji-regex/-/emoji-regex-8.0.0.tgz", "integrity": "sha512-MSjYzcWNOA0ewAHpz0MxpYFvwg6yjy1NG3xteoqz644VCo/RPgnr1/GGt+ic3iJTzQ8Eu3TdM14SawnVUmGE6A==", "dev": true, "license": "MIT" }, "node_modules/escalade": { "version": "3.2.0", "resolved": "https://registry.npmjs.org/escalade/-/escalade-3.2.0.tgz", "integrity": "sha512-WUj2qlxaQtO4g6Pq5c29GTcWGDyd8itL8zTlipgECz3JesAiiOKotd8JU6otB3PACgG6xkJUyVhboMS+bje/jA==", "dev": true, "license": "MIT", "engines": { "node": ">=6" } }, "node_modules/get-caller-file": { "version": "2.0.5", "resolved": "https://registry.npmjs.org/get-caller-file/-/get-caller-file-2.0.5.tgz", "integrity": "sha512-DyFP3BM/3YHTQOCUL/w0OZHR0lpKeGrxotcHWcqNEdnltqFwXVfhEBQ94eIo34AfQpo0rGki4cyIiftY06h2Fg==", "dev": true, "license": "ISC", "engines": { "node": "6.* || 8.* || >= 10.*" } }, "node_modules/has-flag": { "version": "4.0.0", "resolved": "https://registry.npmjs.org/has-flag/-/has-flag-4.0.0.tgz", "integrity": "sha512-EykJT/Q1KjTWctppgIAgfSO0tKVuZUjhgMr17kqTumMl6Afv3EISleU7qZUzoXDFTAHTDC4NOoG/ZxU3EvlMPQ==", "dev": true, "license": "MIT", "engines": { "node": ">=8" } }, "node_modules/is-fullwidth-code-point": { "version": "3.0.0", "resolved": "https://registry.npmjs.org/is-fullwidth-code-point/-/is-fullwidth-code-point-3.0.0.tgz", "integrity": "sha512-zymm5+u+sCsSWyD9qNaejV3DFvhCKclKdizYaJUuHA83RLjb7nSuGnddCHGv0hk+KY7BMAlsWeK4Ueg6EV6XQg==", "dev": true, "license": "MIT", "engines": { "node": ">=8" } }, "node_modules/lodash": { "version": "4.17.21", "resolved": "https://registry.npmjs.org/lodash/-/lodash-4.17.21.tgz", "integrity": "sha512-v2kDEe57lecTulaDIuNTPy3Ry4gLGJ6Z1O3vE1krgXZNrsQ+LFTGHVxVjcXPs17LhbZVGedAJv8XZ1tvj5FvSg==", "dev": true, "license": "MIT" }, "node_modules/react": { "version": "19.1.0", "resolved": "https://registry.npmjs.org/react/-/react-19.1.0.tgz", "integrity": "sha512-FS+XFBNvn3GTAWq26joslQgWNoFu08F4kl0J4CgdNKADkdSGXQyTCnKteIAJy96Br6YbpEU1LSzV5dYtjMkMDg==", "license": "MIT", "peer": true, "engines": { "node": ">=0.10.0" } }, "node_modules/react-icons": { "version": "5.5.0", "resolved": "https://registry.npmjs.org/react-icons/-/react-icons-5.5.0.tgz", "integrity": "sha512-MEFcXdkP3dLo8uumGI5xN3lDFNsRtrjbOEKDLD7yv76v4wpnEq2Lt2qeHaQOr34I/wPN3s3+N08WkQ+CW37Xiw==", "license": "MIT", "peerDependencies": { "react": "*" } }, "node_modules/require-directory": { "version": "2.1.1", "resolved": "https://registry.npmjs.org/require-directory/-/require-directory-2.1.1.tgz", "integrity": "sha512-fGxEI7+wsG9xrvdjsrlmL22OMTTiHRwAMroiEeMgq8gzoLC/PQr7RsRDSTLUg/bZAZtF+TVIkHc6/4RIKrui+Q==", "dev": true, "license": "MIT", "engines": { "node": ">=0.10.0" } }, "node_modules/rxjs": { "version": "7.8.2", "resolved": "https://registry.npmjs.org/rxjs/-/rxjs-7.8.2.tgz", "integrity": "sha512-dhKf903U/PQZY6boNNtAGdWbG85WAbjT/1xYoZIC7FAY0yWapOBQVsVrDl58W86//e1VpMNBtRV4MaXfdMySFA==", "dev": true, "license": "Apache-2.0", "dependencies": { "tslib": "^2.1.0" } }, "node_modules/shell-quote": { "version": "1.8.2", "resolved": "https://registry.npmjs.org/shell-quote/-/shell-quote-1.8.2.tgz", "integrity": "sha512-AzqKpGKjrj7EM6rKVQEPpB288oCfnrEIuyoT9cyF4nmGa7V8Zk6f7RRqYisX8X9m+Q7bd632aZW4ky7EhbQztA==", "dev": true, "license": "MIT", "engines": { "node": ">= 0.4" }, "funding": { "url": "https://github.com/sponsors/ljharb" } }, "node_modules/spawn-command": { "version": "0.0.2", "resolved": "https://registry.npmjs.org/spawn-command/-/spawn-command-0.0.2.tgz", "integrity": "sha512-zC8zGoGkmc8J9ndvml8Xksr1Amk9qBujgbF0JAIWO7kXr43w0h/0GJNM/Vustixu+YE8N/MTrQ7N31FvHUACxQ==", "dev": true }, "node_modules/string-width": { "version": "4.2.3", "resolved": "https://registry.npmjs.org/string-width/-/string-width-4.2.3.tgz", "integrity": "sha512-wKyQRQpjJ0sIp62ErSZdGsjMJWsap5oRNihHhu6G7JVO/9jIB6UyevL+tXuOqrng8j/cxKTWyWUwvSTriiZz/g==", "dev": true, "license": "MIT", "dependencies": { "emoji-regex": "^8.0.0", "is-fullwidth-code-point": "^3.0.0", "strip-ansi": "^6.0.1" }, "engines": { "node": ">=8" } }, "node_modules/strip-ansi": { "version": "6.0.1", "resolved": "https://registry.npmjs.org/strip-ansi/-/strip-ansi-6.0.1.tgz", "integrity": "sha512-Y38VPSHcqkFrCpFnQ9vuSXmquuv5oXOKpGeT6aGrr3o3Gc9AlVa6JBfUSOCnbxGGZF+/0ooI7KrPuUSztUdU5A==", "dev": true, "license": "MIT", "dependencies": { "ansi-regex": "^5.0.1" }, "engines": { "node": ">=8" } }, "node_modules/supports-color": { "version": "8.1.1", "resolved": "https://registry.npmjs.org/supports-color/-/supports-color-8.1.1.tgz", "integrity": "sha512-MpUEN2OodtUzxvKQl72cUF7RQ5EiHsGvSsVG0ia9c5RbWGL2CI4C7EpPS8UTBIplnlzZiNuV56w+FuNxy3ty2Q==", "dev": true, "license": "MIT", "dependencies": { "has-flag": "^4.0.0" }, "engines": { "node": ">=10" }, "funding": { "url": "https://github.com/chalk/supports-color?sponsor=1" } }, "node_modules/tree-kill": { "version": "1.2.2", "resolved": "https://registry.npmjs.org/tree-kill/-/tree-kill-1.2.2.tgz", "integrity": "sha512-L0Orpi8qGpRG//Nd+H90vFB+3iHnue1zSSGmNOOCh1GLJ7rUKVwV2HvijphGQS2UmhUZewS9VgvxYIdgr+fG1A==", "dev": true, "license": "MIT", "bin": { "tree-kill": "cli.js" } }, "node_modules/tslib": { "version": "2.8.1", "resolved": "https://registry.npmjs.org/tslib/-/tslib-2.8.1.tgz", "integrity": "sha512-oJFu94HQb+KVduSUQL7wnpmqnfmLsOA/nAh6b6EH0wCEoK0/mPeXU6c3wKDV83MkOuHPRHtSXKKU99IBazS/2w==", "dev": true, "license": "0BSD" }, "node_modules/wrap-ansi": { "version": "7.0.0", "resolved": "https://registry.npmjs.org/wrap-ansi/-/wrap-ansi-7.0.0.tgz", "integrity": "sha512-YVGIj2kamLSTxw6NsZjoBxfSwsn0ycdesmc4p+Q21c5zPuZ1pl+NfxVdxPtdHvmNVOQ6XSYG4AUtyt/Fi7D16Q==", "dev": true, "license": "MIT", "dependencies": { "ansi-styles": "^4.0.0", "string-width": "^4.1.0", "strip-ansi": "^6.0.0" }, "engines": { "node": ">=10" }, "funding": { "url": "https://github.com/chalk/wrap-ansi?sponsor=1" } }, "node_modules/y18n": { "version": "5.0.8", "resolved": "https://registry.npmjs.org/y18n/-/y18n-5.0.8.tgz", "integrity": "sha512-0pfFzegeDWJHJIAmTLRP2DwHjdF5s7jo9tuztdQxAhINCdvS+3nGINqPd00AphqJR/0LhANUS6/+7SCb98YOfA==", "dev": true, "license": "ISC", "engines": { "node": ">=10" } }, "node_modules/yargs": { "version": "17.7.2", "resolved": "https://registry.npmjs.org/yargs/-/yargs-17.7.2.tgz", "integrity": "sha512-7dSzzRQ++CKnNI/krKnYRV7JKKPUXMEh61soaHKg9mrWEhzFWhFnxPxGl+69cD1Ou63C13NUPCnmIcrvqCuM6w==", "dev": true, "license": "MIT", "dependencies": { "cliui": "^8.0.1", "escalade": "^3.1.1", "get-caller-file": "^2.0.5", "require-directory": "^2.1.1", "string-width": "^4.2.3", "y18n": "^5.0.5", "yargs-parser": "^21.1.1" }, "engines": { "node": ">=12" } }, "node_modules/yargs-parser": { "version": "21.1.1", "resolved": "https://registry.npmjs.org/yargs-parser/-/yargs-parser-21.1.1.tgz", "integrity": "sha512-tVpsJW7DdjecAiFpbIB1e3qxIQsE6NoPc5/eTdrbbIC4h0LVsWhnoa3g+m2HclBIujHzsxZ4VJVA+GUuc2/LBw==", "dev": true, "license": "ISC", "engines": { "node": ">=12" } } } }
---

# test poema

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** biblioperson

# test poema

La Luna y el Sol La luna se baa en plata el sol resplandece en oro, llenan el cielo de gloria y de infinito tesoro. Cuando la noche desciende la luna toma su trono, vigilante de los sueos y de misterios custodio. Al amanecer el sol disipa sombras vibrante, despierta la vida nueva con su calor abrazante. Y as en danza milenaria los dos astros se suceden, guardianes del universo donde los tiempos florecen.
---

# requirements

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** biblioperson

# requirements

# Este archivo solo contiene dependencias globales o de desarrollo para el proyecto Biblioperson. # Para dependencias especficas, usa: # - backend/requirements.txt para el backend Flask # - dataset/requirements.txt para procesamiento de datos y dataset # Ejemplo de dependencias globales (puedes agregar aqu herramientas de linting, testing, etc.) # pip install -r backend/requirements.txt # pip install -r dataset/requirements.txt # Dependencias para Biblioperson - Data Processing ijson==3.2.1 python-dateutil==2.8.2 python-docx==1.0.1 pandas==2.1.0 openpyxl==3.1.2 PyPDF2==3.0.1 textract==1.6.5 # Dependencias para procesamiento semntico sentence-transformers==2.2.2 numpy==1.24.3 spacy==3.5.3 tqdm==4.66.1 # Dependencias para APIs de LLM flask flask-cors python-dotenv scikit-learn google-generativeai openai requests
---

# README

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** biblioperson

# README

# Biblioteca de Conocimiento Personal Sistema para gestionar y analizar contenido personal de diferentes plataformas y fuentes. ## ndice - [Biblioteca de Conocimiento Personal](#biblioteca-de-conocimiento-personal) - [ndice](#ndice) - [Caractersticas](#caractersticas) - [Requisitos](#requisitos) - [Instalacin](#instalacin) - [Estructura del Proyecto](#estructura-del-proyecto) - [Importar nuevos datos](#importar-nuevos-datos) - [Procesar y poblar la base de datos](#procesar-y-poblar-la-base-de-datos) - [Generar embeddings semnticos](#generar-embeddings-semnticos) - [Ejecutar la aplicacin](#ejecutar-la-aplicacin) - [Bsqueda Full-Text con Meilisearch](#bsqueda-full-text-con-meilisearch) - [Instalacin y uso local](#instalacin-y-uso-local) - [Nota sobre Meilisearch y el backend](#nota-sobre-meilisearch-y-el-backend) - [Notas](#notas) - [Indexacin incremental en Meilisearch](#indexacin-incremental-en-meilisearch) - [Verificacin y depuracin de la indexacin en Meilisearch](#verificacin-y-depuracin-de-la-indexacin-en-meilisearch) - [Importante: Borrar el ndice de Meilisearch](#importante-borrar-el-ndice-de-meilisearch) - [Preguntas frecuentes](#preguntas-frecuentes) - [Licencia](#licencia) - [Regenerar todos los embeddings semnticos](#regenerar-todos-los-embeddings-semnticos) - [Levantar la base de datos, backend y frontend](#levantar-la-base-de-datos-backend-y-frontend) - [1. Levantar la base de datos (SQLite)](#1-levantar-la-base-de-datos-sqlite) - [2. Levantar el backend (API Flask)](#2-levantar-el-backend-api-flask) - [3. Levantar el frontend (React)](#3-levantar-el-frontend-react) - [4. Consultas bsicas en la base de datos (SQLite)](#4-consultas-bsicas-en-la-base-de-datos-sqlite) ## Caractersticas - Importacin de contenido desde mltiples fuentes (Facebook, Twitter, Telegram, documentos) - Exploracin de contenido por temas, fechas y plataformas - Generacin de material para nuevo contenido - API REST para acceso a los datos - Interfaz web para exploracin y anlisis ## Requisitos - Python 3.8+ - Node.js 16+ - SQLite3 - Navegador web moderno ## Instalacin 1. Clonar el repositorio: ```bash git clone [URL_DEL_REPOSITORIO] cd biblioperson ``` 2. Crear y activar entorno virtual de Python: ```bash python -m venv venv # En Windows: .\venv\Scripts\activate # En Linux/Mac: source venv/bin/activate ``` 3. Instalar dependencias de Python segn el rea de trabajo: - **Para el backend Flask:** ```bash pip install -r backend/requirements.txt ``` - **Para procesamiento de datos/dataset:** ```bash pip install -r dataset/requirements.txt ``` 4. Instalar dependencias de Node.js para el frontend: ```bash cd frontend npm install cd .. ``` ## Estructura del Proyecto - `backend/`: Backend y scripts de procesamiento - `backend/data/import/`: Carpeta donde colocar archivos NDJSON o TXT para importar - `backend/scripts/`: Scripts de Python para la API y procesamiento - `frontend/`: Frontend (React) - `shared/`: Recursos compartidos - `dataset/`: Procesamiento y normalizacin de datasets ## Importar nuevos datos 1. **Coloca tus archivos NDJSON en la carpeta:** - `backend/data/import/` - O bien, ten a mano la ruta de tu archivo NDJSON unificado. 2. **Ubcate en la carpeta de scripts:** ```bash cd backend/scripts ``` 3. **Ejecuta el script de importacin completa:** - Si ya tienes archivos NDJSON en la carpeta de importacin: ```bash python importar_completo.py --solo-importar ``` - Si quieres importar un archivo NDJSON especfico y copiarlo automticamente: ```bash python importar_completo.py /ruta/a/tu/archivo.ndjson ``` - Si quieres reiniciar la base de datos antes de importar (esto borra todo lo anterior!): ```bash python importar_completo.py /ruta/a/tu/archivo.ndjson --reiniciar-db ``` - El script se encargar de copiar el archivo (si lo especificas), importar los datos y generar los ndices necesarios. 4. **Verifica que la importacin fue exitosa revisando los mensajes en consola.** ## Procesar y poblar la base de datos - Si necesitas limpiar o inicializar la base de datos, ejecuta: ```bash python inicializar_db.py ``` - Para actualizar o aadir nuevos registros, simplemente repite el paso de importacin. ## Generar embeddings semnticos - Una vez que los nuevos contenidos estn en la base de datos, genera los embeddings: ```bash python procesar_semantica.py ``` - Este script solo procesar los contenidos nuevos que no tengan embedding. ## Ejecutar la aplicacin - Desde la raz del proyecto: ```bash npm run dev ``` - Esto levantar tanto el backend como el frontend. ## Bsqueda Full-Text con Meilisearch Este proyecto utiliza [Meilisearch](https://www.meilisearch.com/) como motor de bsqueda de texto completo para ofrecer bsquedas rpidas y modernas. ### Instalacin y uso local 1. **Descarga Meilisearch:** - Ve a [https://github.com/meilisearch/meilisearch/releases](https://github.com/meilisearch/meilisearch/releases) - Descarga el archivo `meilisearch-windows-amd64.exe.zip` (o el que corresponda a tu sistema operativo). - Descomprime el archivo. 2. **Ejecuta Meilisearch:** - Abre una terminal en la carpeta donde est el ejecutable. - Ejecuta: ``` .\meilisearch.exe ``` - El servidor estar disponible en [http://127.0.0.1:7700](http://127.0.0.1:7700) 3. **Master Key:** - Al iniciar, Meilisearch genera una master key (token de administracin). Ejemplo: ``` --master-key zg0_by09jiVS_kcLdoVgvO9J7fefp1uGvyF-YNonMRg ``` - Guarda este valor, lo necesitars para configurar el backend y proteger el acceso en produccin. 4. **Documentacin oficial:** [https://www.meilisearch.com/docs](https://www.meilisearch.com/docs) ### Nota sobre Meilisearch y el backend - Meilisearch se inicia automticamente cuando levantas el backend (por ejemplo, ejecutando `api_conexion.py`). - El ejecutable de Meilisearch debe estar en `backend/meilisearch/meilisearch-windows-amd64.exe`. - Si deseas iniciar Meilisearch manualmente, puedes ejecutar: ```bash python backend/scripts/levantar_meilisearch.py ``` - Si cambias de carpeta o servidor, recuerda copiar tambin el archivo/carpeta `data.ms` para conservar la indexacin, o vuelve a indexar desde la base de datos. ### Notas - Si actualizas Meilisearch, repite el proceso de descarga y reemplaza el ejecutable. - Si cambias de PC o servidor, repite estos pasos. - Para produccin, configura la master key y revisa la documentacin de seguridad. ## Indexacin incremental en Meilisearch Cuando aadas nuevos contenidos a la base de datos, puedes indexar solo los nuevos documentos en Meilisearch (sin volver a subir todo) ejecutando: ```bash python backend/scripts/indexar_meilisearch.py --indexar-nuevos ``` Esto detecta automticamente los registros que an no estn en Meilisearch y los aade al ndice. Es til para mantener la bsqueda actualizada despus de cada importacin o edicin masiva. ## Verificacin y depuracin de la indexacin en Meilisearch Si notas que el nmero de documentos en Meilisearch es menor que en tu base de datos: 1. **Verifica cuntos documentos hay en la base de datos y en Meilisearch:** Puedes usar el siguiente script para comparar y listar los IDs faltantes: ```python import sqlite3 import meilisearch DB_PATH = '../../backend/data/biblioteca.db' MEILI_URL = 'http://127.0.0.1:7700' MEILI_INDEX = 'contenidos' MEILI_KEY = None conn = sqlite3.connect(DB_PATH) cursor = conn.cursor() cursor.execute("SELECT id FROM contenidos") db_ids = set(row[0] for row in cursor.fetchall()) client = meilisearch.Client(MEILI_URL, MEILI_KEY) index = client.index(MEILI_INDEX) meili_ids = set() offset = 0 limit = 10000 while True: docs = index.get_documents({'fields': ['id'], 'limit': limit, 'offset': offset}) if not docs: break for doc in docs: meili_ids.add(doc['id']) if len(docs) < limit: break offset += limit print(f'IDs en base de datos: {len(db_ids)}') print(f'IDs en Meilisearch: {len(meili_ids)}') faltan = db_ids - meili_ids print(f'IDs que faltan en Meilisearch: {len(faltan)}') if faltan: print(f'Ejemplo de IDs faltantes: {list(faltan)[:10]}') ``` 2. **Causas comunes de diferencias:** - Documentos demasiado grandes (ms de 2 MB) no se indexan. - IDs duplicados en la base de datos. - Errores de red o timeouts durante la indexacin. 3. **Solucin:** - Revisa los IDs faltantes y verifica si hay problemas con esos registros. - Puedes reintentar indexar solo los faltantes, o limpiar/ajustar los datos problemticos y volver a indexar. 4. **Reindexar todo si es necesario:** Si hay muchos problemas, puedes borrar el ndice en Meilisearch y volver a indexar desde cero. Recuerda detener Meilisearch antes de borrar la carpeta `data.ms`. Esto te ayudar a mantener la integridad entre tu base de datos y el ndice de bsqueda. ## Importante: Borrar el ndice de Meilisearch Si necesitas borrar el ndice (por ejemplo, para reindexar desde cero): 1. **Detn Meilisearch completamente** - Si lo levantaste con el script o desde el backend, detn el backend o cierra la terminal donde se ejecut Meilisearch. - O bien, busca el proceso `meilisearch-windows-amd64.exe` en el Administrador de tareas de Windows y finalzalo. 2. **Borra la carpeta de datos** - Borra la carpeta `backend/meilisearch/data.ms/` (o el archivo/carpeta de datos que corresponda). 3. **Vuelve a iniciar Meilisearch** - Se crear una nueva carpeta de datos vaca y podrs reindexar desde cero. > **Nota:** No puedes borrar los ndices mientras Meilisearch est corriendo. Siempre detn el proceso primero. ## Preguntas frecuentes - **Puedo importar nuevos datos sin perder los anteriores?** S, los scripts estn diseados para aadir solo los nuevos registros y generar embeddings solo para ellos. - **Dnde deben estar los archivos de importacin?** En `backend/data/import/`. - **Cmo s si los embeddings estn actualizados?** El script `procesar_semantica.py` solo procesa los contenidos que no tienen embedding. - Para `/api/busqueda` usa `contenido_texto` - Para `/api/busqueda/semantica` usa `contenido_texto` ## Licencia [ESPECIFICAR LICENCIA] ## Regenerar todos los embeddings semnticos Si necesitas **regenerar los embeddings para todos los contenidos** (por ejemplo, tras cambiar de modelo o para limpiar embeddings antiguos), sigue estos pasos: 1. **Vaca la tabla de embeddings en la base de datos:** Puedes hacerlo desde SQLite con el siguiente comando: ```sql DELETE FROM contenido_embeddings; ``` O, si prefieres borrar y recrear la tabla: ```sql DROP TABLE IF EXISTS contenido_embeddings; -- Luego ejecuta el script de inicializacin semntica para recrearla ``` 2. **Ejecuta el script de procesamiento semntico:** ```bash python procesar_semantica.py ``` Esto generar embeddings para **todos** los contenidos, ya que la tabla estar vaca. > **Nota:** Si tienes muchos contenidos, este proceso puede tardar varios minutos u horas segn la cantidad y el modelo usado. ## Levantar la base de datos, backend y frontend ### 1. Levantar la base de datos (SQLite) No necesitas un servidor especial, solo asegrate de que el archivo `backend/data/biblioteca.db` existe. Si necesitas crearla desde cero: ```bash cd backend/scripts python inicializar_db.py ``` Esto crear la estructura bsica de la base de datos si no existe. ### 2. Levantar el backend (API Flask) Desde la raz del proyecto o desde `backend/scripts`: ```bash python api_conexion.py ``` Esto iniciar el backend en `http://localhost:5000`. ### 3. Levantar el frontend (React) Desde la carpeta `frontend`: ```bash cd frontend npm install # Solo la primera vez npm run dev ``` Esto abrir la aplicacin en `http://localhost:5173` (o el puerto que indique la consola). ### 4. Consultas bsicas en la base de datos (SQLite) Puedes usar la terminal de SQLite para hacer consultas directas: ```bash sqlite3 backend/data/biblioteca.db ``` Ejemplos de consultas tiles: - Contar todos los contenidos: ```sql SELECT COUNT(*) FROM contenidos; ``` - Listar autores nicos: ```sql SELECT DISTINCT autor FROM contenidos WHERE autor IS NOT NULL AND autor != ''; ``` - Contar contenidos por autor: ```sql SELECT autor, COUNT(*) FROM contenidos GROUP BY autor; ``` - Ver los primeros 5 contenidos: ```sql SELECT id, contenido_texto FROM contenidos LIMIT 5; ```
---

# package

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** biblioperson

# package

{ "name": "biblioperson", "version": "1.0.0", "description": "Biblioteca Personal", "scripts": { "dev:frontend": "cd frontend && npm run dev", "dev:backend": "cd backend && python -m flask --app scripts/api_conexion.py run --debug", "dev": "npx concurrently \"npm run dev:frontend\" \"npm run dev:backend\"" }, "devDependencies": { "concurrently": "^8.2.2" }, "dependencies": { "react-icons": "^5.5.0" } }
---

# example prd

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# example prd

<context> # Overview [Provide a high-level overview of your product here. Explain what problem it solves, who it's for, and why it's valuable.] # Core Features [List and describe the main features of your product. For each feature, include: - What it does - Why it's important - How it works at a high level] # User Experience [Describe the user journey and experience. Include: - User personas - Key user flows - UI/UX considerations] </context> <PRD> # Technical Architecture [Outline the technical implementation details: - System components - Data models - APIs and integrations - Infrastructure requirements] # Development Roadmap [Break down the development process into phases: - MVP requirements - Future enhancements - Do not think about timelines whatsoever -- all that matters is scope and detailing exactly what needs to be build in each phase so it can later be cut up into tasks] # Logical Dependency Chain [Define the logical order of development: - Which features need to be built first (foundation) - Getting as quickly as possible to something usable/visible front end that works - Properly pacing and scoping each feature so it is atomic but can also be built upon and improved as development approaches] # Risks and Mitigations [Identify potential risks and how they'll be addressed: - Technical challenges - Figuring out the MVP that we can build upon - Resource constraints] # Appendix [Include any additional information: - Research findings - Technical specifications] </PRD>
---

# prd

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# prd

# Overview Biblioperson is a system designed to manage, analyze, and provide advanced search capabilities over personal knowledge from various platforms and document formats. It addresses the problem of fragmented information by centralizing content and enabling users to gain deeper insights, discover connections, and generate new content. # Core Features - **Import Content from Multiple Sources:** - What it does: Imports content from diverse sources, including DOCX, PDF, TXT, MD, EXCEL, CSV, XML, HTML, and JSON files, as well as data from social media. - Why it's important: Consolidates information, eliminating the need to search across multiple platforms and formats. - How it works: ETL scripts in the `dataset/` directory process and convert files into a standardized NDJSON format, preserving key metadata. The ingestion process is designed to be modular and extensible, using loaders, segmenters, post-processors, and exporters. - **Modular Document Processing:** - What it does: Employs a modular pipeline for processing documents, allowing for flexible configuration and adaptation to different content types. - Why it's important: Improves the accuracy of content segmentation and enhances the system's ability to handle various document structures. - How it works: The pipeline consists of loaders (extract content), segmenters (define semantic units), post-processors (filter and enrich), and exporters (serialize data). Perfiles (profiles) defined in YAML files control the behavior of this pipeline. - **Semantic Search and Analysis:** - What it does: Enables users to search and analyze content using both full-text and semantic search capabilities. - Why it's important: Semantic search allows users to find information based on meaning and context, not just keywords. - How it works: The backend uses Meilisearch for efficient search and `sentence-transformers` for generating embeddings to support semantic queries. The frontend provides tools for exploring content by topics, dates, authors, and sources. - **Content Generation Assistance:** - What it does: Assists users in generating new content based on the analysis of existing documents. - Why it's important: Facilitates the creation of relevant and engaging content by leveraging the user's knowledge base. - How it works: (Future Enhancement) The system will analyze existing content to suggest topics, styles, and relevant information for new content. # User Experience - **User Personas:** - Knowledge Workers: Researchers, analysts, and writers who need to manage and analyze large volumes of text. - Content Creators: Social media managers, Youtubers, influencers, bloggers, and marketers who want to generate new content efficiently from existing knowledge. - Personal Knowledge Managers: Individuals who want to organize and explore their personal notes, documents, and digital information. - **Key User Flows:** - Import and Organize: Users import content, which the system processes and stores in a structured format. - Search and Discover: Users search for information using keywords or semantic queries and explore the results. - Analyze and Generate: Users analyze content to gain insights and use the system to assist in generating new content. - **UI/UX Considerations:** - Intuitive Interface: User-friendly design for easy navigation and interaction. - Clear Search Results: Presentation of search results in a clear and organized manner. - Advanced Search Options: Filters and sorting options to refine search queries. - Content Visualization: Tools for visualizing content relationships and trends. </PRD> # Technical Architecture - **System Components:** - Frontend: React SPA for the user interface. - Backend: Flask API for handling requests and business logic. - Database: SQLite for storing content, metadata, and embeddings. - Search Engine: Meilisearch for full-text and semantic search. - ETL Pipeline: Python scripts for data ingestion and processing. - Dataset: The collection of scripts, data structures, and processes that constitute the ETL Pipeline. It encompasses data loading, conversion, segmentation, normalization, and preparation for database ingestion. - **Data Models:** - `content`: Stores the processed text content and associated metadata. - `content_embeddings`: Stores vector embeddings of the text. - **APIs and Integrations:** - REST API: Flask exposes endpoints for the frontend to interact with the backend. - Meilisearch API: Backend communicates with Meilisearch for search functionality. - **Infrastructure Requirements:** - Python 3.8+ - Node.js 16+ - SQLite - Meilisearch - Web Browser # Development Roadmap - **MVP Requirements:** - Basic content import from local files (TXT, MD, JSON, CSV, EXCEL, DOCX, PDF, XML, HTML, NDJSON). - Full-text search functionality. - Basic UI for displaying search results. - SQLite database setup and integration. - Flask backend with essential API endpoints. - Initial React frontend with search and display. - Modular ETL pipeline with basic loaders and segmenters. - **Future Enhancements:** - Support for more file formats (OCR PDF). - Advanced semantic search capabilities. - User authentication and authorization. - Content exploration by diagram and analysis tools. - Content generation assistance features. - Integration with online sources. - Improved UI/UX. # Logical Dependency Chain - **Foundation:** - Set up the SQLite database and backend API. - Implement basic content import and processing. - Create the initial React frontend. - **Core Functionality:** - Integrate frontend and backend for search. - Implement full-text search with Meilisearch. - Develop the modular ETL pipeline. - **Advanced Features:** - Implement semantic search. - Add support for more file formats. - Develop content analysis and generation tools. - Enhance UI/UX. # Risks and Mitigations - **Technical Challenges:** - Accurate and efficient content processing. - Optimizing search performance. - Integrating semantic search effectively. - *Mitigation:* Thorough testing, use of robust libraries, and iterative development. - **MVP Definition:** - Balancing features and development time. - *Mitigation:* Prioritize essential features and focus on a functional core. - **Resource Constraints:** - Limited development resources. - *Mitigation:* Agile development and efficient task management. # Appendix - **Research Findings:** - Need for a centralized platform for personal knowledge. - Importance of semantic search. - Value of content generation assistance. - **Technical Specifications:** - (Details on specific libraries, algorithms, etc.)
---

# config

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** dataset

# config

{ "last_input_folder": "E:\\dev-projects\\biblioperson\\dataset\\fuentes", "last_output_folder": "E:\\dev-projects\\biblioperson\\dataset\\salidas", "last_unify_input_folder": "" }
---

# requirements

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** dataset

# requirements

ijson==3.2.1 python-dateutil==2.8.2 python-docx==1.0.1 pandas==2.1.0 openpyxl==3.1.2 PyPDF2==3.0.1 textract==1.6.5 numpy==1.24.3 spacy==3.5.3 tqdm==4.66.1 markdownify jsonschema PyMuPDF
---

# requirements

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** backend

# requirements

annotated-types==0.7.0 argcomplete==1.10.3 beautifulsoup4==4.8.2 blinker==1.9.0 blis==1.3.0 camel-converter==4.0.1 catalogue==2.0.10 certifi==2025.4.26 cffi==1.17.1 chardet==3.0.4 charset-normalizer==3.4.1 click==8.1.8 cloudpathlib==0.21.0 colorama==0.4.6 compressed-rtf==1.0.7 confection==0.1.5 cryptography==44.0.2 cymem==2.0.11 docx2txt==0.9 ebcdic==1.1.1 et_xmlfile==2.0.0 extract-msg==0.28.7 filelock==3.18.0 Flask==3.1.0 Flask-Cors==5.0.0 fsspec==2025.3.2 huggingface-hub==0.30.2 idna==3.10 ijson==3.3.0 IMAPClient==2.1.0 itsdangerous==2.2.0 Jinja2==3.1.6 joblib==1.5.0 langcodes==3.5.0 language_data==1.3.0 lxml==5.4.0 marisa-trie==1.2.1 markdown-it-py==3.0.0 MarkupSafe==3.0.2 mdurl==0.1.2 meilisearch==0.34.1 mpmath==1.3.0 murmurhash==1.0.12 networkx==3.4.2 numpy==2.2.5 olefile==0.47 openpyxl==3.1.5 packaging==25.0 pandas==2.2.3 pdfminer.six==20250416 pillow==11.2.1 preshed==3.0.9 psutil==7.0.0 pyasn1==0.6.1 pycparser==2.22 pycryptodome==3.22.0 pydantic==2.11.4 pydantic_core==2.33.2 Pygments==2.19.1 PyPDF2==3.0.1 PySimpleGUI==5.0.10 python-dateutil==2.9.0.post0 python-docx==1.1.2 python-dotenv==1.0.0 python-pptx==0.6.23 pytz==2025.2 PyYAML==6.0.2 regex==2024.11.6 requests==2.32.3 rich==14.0.0 rsa==4.9.1 safetensors==0.5.3 scikit-learn==1.6.1 scipy==1.15.2 sentence-transformers==4.1.0 shellingham==1.5.4 six==1.12.0 smart-open==7.1.0 sortedcontainers==2.4.0 soupsieve==2.7 spacy==3.8.5 spacy-legacy==3.0.12 spacy-loggers==1.0.5 SpeechRecognition==3.8.1 srsly==2.5.1 sympy==1.14.0 textract==1.6.5 thinc==8.3.6 threadpoolctl==3.6.0 tokenizers==0.21.1 torch==2.7.0 tqdm==4.67.1 transformers==4.51.3 typer==0.15.3 typing-inspection==0.4.0 typing_extensions==4.13.2 tzdata==2025.2 tzlocal==5.3.1 urllib3==2.4.0 wasabi==1.1.3 weasel==0.4.1 Werkzeug==3.1.3 wrapt==1.17.2 xlrd==1.2.0 XlsxWriter==3.2.3
---

# nodemon

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** backend

# nodemon

{ "watch": ["scripts/"], "ext": "py", "ignore": ["*.pyc", "__pycache__"], "exec": "python ./scripts/api_conexion.py", "cwd": "." }
---

# README

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** backend

# README

# Indexacin en Meilisearch Este README documenta el proceso de indexacin de documentos en Meilisearch. ## Iniciar el servidor Meilisearch Para iniciar el servidor Meilisearch con configuracin optimizada: ```bash # Desde la carpeta de Meilisearch cd E:\dev-projects\biblioperson\backend\meilisearch .\meilisearch-windows-amd64.exe --max-indexing-threads 6 ``` ## Indexar documentos en Meilisearch Para indexar documentos desde la base de datos SQLite a Meilisearch: ```bash # Desde la carpeta de scripts del backend cd E:\dev-projects\biblioperson\backend\scripts python indexar_meilisearch.py --hilos 12 ``` ## Opciones de configuracin para la indexacin El script acepta varios parmetros para optimizar el rendimiento: ```bash python indexar_meilisearch.py [opciones] Opciones: --tabla NOMBRE Nombre de la tabla que contiene los documentos (por defecto: contenidos) --db-path RUTA Ruta a la base de datos SQLite (por defecto: E:\dev-projects\biblioperson\backend\data\biblioteca.db) --hilos NUM Nmero de hilos a utilizar (por defecto: 8) --batch-size NUM Tamao del lote de la base de datos (por defecto: 5000) --docs-per-request NUM Documentos por solicitud a Meilisearch (por defecto: 1000) --timeout NUM Timeout para solicitudes HTTP en segundos (por defecto: 60) --listar Listar todas las tablas disponibles en la base de datos ``` ## Configuracin recomendada para mejor rendimiento Para obtener el mejor rendimiento en un sistema de gama alta: ```bash # Para CPU i7/i9 con 32GB RAM: python indexar_meilisearch.py --hilos 12 --batch-size 5000 --docs-per-request 1000 --timeout 120 ``` ## Rendimiento esperado Con la configuracin recomendada, se pueden alcanzar velocidades de indexacin de: - Aproximadamente 450-500 documentos por segundo - Completar la indexacin de 234,528 documentos en ~8-10 minutos ## Configuracin del ndice El script configura automticamente el ndice "documentos" en Meilisearch con: - Clave primaria: "id" - Atributos buscables: contenido_texto, autor, contexto, url_original - Atributos filtrables: id, autor, fecha_creacion, fecha_importacion, fuente_id, plataforma_id, idioma - Atributos ordenables: id, fecha_creacion, fecha_importacion - Incluye los embeddings para cada documento ## Solucin de problemas - Si ves errores relacionados con "0 successful tasks and X failed tasks", revisa la configuracin del ndice - Para reiniciar la indexacin desde cero, elimina el ndice desde la interfaz web o API de Meilisearch
---

# INDICE

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** docs

# INDICE

# ndice de Documentacin - Biblioperson ##  Navegacin por Objetivo ###  **Quiero empezar rpidamente** 1. [**Inicio Rpido**](INICIO_RAPIDO.md) - Configuracin en 5 pasos 2. [**README Principal**](README.md) - Visin general del proyecto ###  **Quiero procesar mis documentos** 1. [**Gua de Procesamiento de Datos**](GUIA_PROCESAMIENTO_DATOS.md) - Proceso completo de conversin de documentos 2. [**Gua de Loaders**](GUIA_LOADERS.md) - Carga de diferentes formatos de archivo 3. [**Pipeline NDJSON**](PIPELINE_NDJSON.md) - Gua completa paso a paso 4. [**Especificacin NDJSON**](NDJSON_ESPECIFICACION.md) - Formato tcnico detallado 5. [**Jerarqua Contextual**](JERARQUIA_CONTEXTUAL_ESPECIFICACION.md) - Estructura de documentos ###  Quiero configurar el sistema - [Arquitectura del Sistema](BIBLIOPERSON_ARQUITECTURA.md) - Diseo tcnico completo - [Configuracin del Procesamiento](CONFIGURACION_PROCESAMIENTO.md) - Cmo funciona `app_depuracion.py` - [Configuracin de Meilisearch](configuracion_meilisearch.md) - Motor de bsqueda - [Gestin de Datos](gestion_datos.md) - Base de datos e importacin ###  **Quiero entender los algoritmos** 1. [**Estrategia de Procesamiento**](ESTRATEGIA_PROCESAMIENTO.md) - Mtodos actuales 2. [**Algoritmos Propuestos**](ALGORITMOS_PROPUESTOS.md) - Mejoras futuras 3. [**Algoritmos Anteriores**](ALGORITMOS_ANTERIORES/) - Historial de desarrollo ###  **Quiero personalizar la aplicacin** 1. [**Implementacin de Perfiles**](IMPLEMENTACION_PERFILES.md) - Configuracin avanzada --- ##  Documentacin por Categora ###  **Introduccin y Setup** | Documento | Descripcin | Audiencia | |-----------|-------------|----------| | [README.md](README.md) | Visin general y navegacin principal | Todos | | [INICIO_RAPIDO.md](INICIO_RAPIDO.md) | Configuracin paso a paso | Nuevos usuarios | | [BIBLIOPERSON_ARQUITECTURA.md](BIBLIOPERSON_ARQUITECTURA.md) | Diseo tcnico del sistema | Desarrolladores | ###  **Procesamiento de Documentos** | Documento | Descripcin | Audiencia | |-----------|-------------|----------| | [GUIA_PROCESAMIENTO_DATOS.md](GUIA_PROCESAMIENTO_DATOS.md) | Proceso completo de conversin | Usuarios activos | | [GUIA_LOADERS.md](GUIA_LOADERS.md) | Carga de diferentes formatos | Usuarios activos | | [PIPELINE_NDJSON.md](PIPELINE_NDJSON.md) | Gua prctica completa | Usuarios activos | | [NDJSON_ESPECIFICACION.md](NDJSON_ESPECIFICACION.md) | Especificacin tcnica del formato | Desarrolladores | | [JERARQUIA_CONTEXTUAL_ESPECIFICACION.md](JERARQUIA_CONTEXTUAL_ESPECIFICACION.md) | Estructura de documentos | Desarrolladores | ###  Configuracin y Administracin - [Configuracin del Procesamiento](CONFIGURACION_PROCESAMIENTO.md) - [Configuracin de Meilisearch](configuracion_meilisearch.md) - [Gestin de Datos](gestion_datos.md) - [Configuracin Avanzada](configuracion_avanzada.md) ###  **Algoritmos y Mtodos** | Documento | Descripcin | Audiencia | |-----------|-------------|----------| | [ESTRATEGIA_PROCESAMIENTO.md](ESTRATEGIA_PROCESAMIENTO.md) | Mtodos de procesamiento actuales | Desarrolladores | | [ALGORITMOS_PROPUESTOS.md](ALGORITMOS_PROPUESTOS.md) | Mejoras y optimizaciones futuras | Investigadores | | [ALGORITMOS_ANTERIORES/](ALGORITMOS_ANTERIORES/) | Historial de desarrollo | Desarrolladores | --- ##  Flujos de Trabajo Comunes ###  **Usuario Nuevo** ``` 1. README.md (visin general) 2. INICIO_RAPIDO.md (configuracin) 3. GUIA_PROCESAMIENTO_DATOS.md (primer documento) 4. GUIA_GESTION_DATOS.md (importacin) ``` ###  **Procesamiento de Documentos** ``` 1. GUIA_PROCESAMIENTO_DATOS.md (proceso completo de conversin) 2. GUIA_LOADERS.md (formatos de archivo especficos) 3. PIPELINE_NDJSON.md (gua prctica) 4. NDJSON_ESPECIFICACION.md (si necesitas detalles tcnicos) 5. JERARQUIA_CONTEXTUAL_ESPECIFICACION.md (para documentos complejos) ``` ###  Quiero configurar el sistema 1. [Arquitectura del Sistema](BIBLIOPERSON_ARQUITECTURA.md) - Entender la estructura 2. [Configuracin del Procesamiento](CONFIGURACION_PROCESAMIENTO.md) - Ajustar `app_depuracion.py` 3. [Configuracin de Meilisearch](configuracion_meilisearch.md) - Motor de bsqueda 4. [Configuracin Avanzada](configuracion_avanzada.md) - Ajustes finos ###  **Desarrollo e Investigacin** ``` 1. BIBLIOPERSON_ARQUITECTURA.md (arquitectura) 2. GUIA_LOADERS.md (crear nuevos loaders) 3. ESTRATEGIA_PROCESAMIENTO.md (mtodos actuales) 4. ALGORITMOS_PROPUESTOS.md (mejoras futuras) 5. ALGORITMOS_ANTERIORES/ (contexto histrico) ``` --- ##  Convenciones de Documentacin ###  **Iconos y Smbolos** -  **Inicio rpido** - Para empezar inmediatamente -  **Procesamiento** - Relacionado con documentos y NDJSON -  **Configuracin** - Setup y administracin -  **Avanzado** - Desarrollo e investigacin -  **Rpido** - Comandos y acciones directas -  **Referencia** - Especificaciones tcnicas ###  **Estructura de Documentos** - **Resumen ejecutivo** al inicio - **Ejemplos prcticos** antes que teora - **Solucin de problemas** al final - **Referencias cruzadas** entre documentos ###  **Enlaces y Navegacin** - Enlaces relativos entre documentos - Breadcrumbs en documentos largos - "Prximos pasos" al final de cada gua --- ##  Ayuda Rpida ###  **No encuentras lo que buscas?** **Para tareas especficas:** - **Instalar**  [INICIO_RAPIDO.md](INICIO_RAPIDO.md) - **Procesar documentos**  [PIPELINE_NDJSON.md](PIPELINE_NDJSON.md) - **Configurar bsqueda**  [GUIA_MEILISEARCH.md](GUIA_MEILISEARCH.md) - **Problemas tcnicos**  Seccin "Solucin de problemas" en cada gua **Para entender conceptos:** - **Qu es Biblioperson?**  [README.md](README.md) - **Cmo funciona?**  [BIBLIOPERSON_ARQUITECTURA.md](BIBLIOPERSON_ARQUITECTURA.md) - **Qu es NDJSON?**  [NDJSON_ESPECIFICACION.md](NDJSON_ESPECIFICACION.md) ###  **Actualizaciones de Documentacin** Esta documentacin se actualiza regularmente. Si encuentras informacin desactualizada o errores: 1. Verifica que ests usando la versin ms reciente 2. Consulta el historial de cambios en Git 3. Reporta problemas o sugiere mejoras --- # ndice de Documentacin - [Introduccin](INTRODUCCION.md) - [Instalacin y Requisitos](INSTALACION.md) - [Estructura del Proyecto](ESTRUCTURA.md) - [Configuracin y Administracin](CONFIGURACION_PROCESAMIENTO.md) - [Gua de Comandos para Procesamiento de Documentos](GUIA_COMANDOS_PROCESAMIENTO.md) - [Gua de Carga de Documentos (Loaders)](LOADERS.md) - [Desarrollo y Extensin](DESARROLLO.md) - [Preguntas Frecuentes](FAQ.md) - [Glosario](GLOSARIO.md) ## Quiero configurar el sistema - [Configuracin de procesamiento y perfiles](CONFIGURACION_PROCESAMIENTO.md) - [Gua de comandos para procesamiento de documentos](GUIA_COMANDOS_PROCESAMIENTO.md) ## Gua de Carga de Documentos (Loaders) - [Gua de comandos para procesamiento de documentos](GUIA_COMANDOS_PROCESAMIENTO.md) ## Configuracin y Administracin - [Configuracin de procesamiento y perfiles](CONFIGURACION_PROCESAMIENTO.md) - [Gua de comandos para procesamiento de documentos](GUIA_COMANDOS_PROCESAMIENTO.md) ## ltima actualizacin 24 de mayo del 2025 *Versin de documentacin: 2.0*
---

# JERARQUIA CONTEXTUAL ESPECIFICACION

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** docs

# JERARQUIA CONTEXTUAL ESPECIFICACION

# Especificacin de `jerarquia_contextual` para Biblioperson Este documento detalla la estructura y el uso del campo `jerarquia_contextual` dentro de los registros NDJSON enriquecidos del proyecto Biblioperson. Esta especificacin corresponde a la Tarea #26 y es complementaria a `NDJSON_ESPECIFICACION.md` (Tarea #23). ## Propsito El campo `jerarquia_contextual` tiene como objetivo describir la posicin de un segmento de texto dentro de la estructura lgica y semntica del documento fuente. Esto es crucial para: * Reconstruir la navegacin tipo e-book. * Permitir bsquedas contextuales (ej. "encontrar 'X' en el captulo 3"). * Entender las relaciones entre segmentos de texto. ## Estructura General `jerarquia_contextual` es un objeto JSON. Las claves de este objeto representan el tipo de nivel jerrquico (ej. "capitulo", "seccion") y los valores representan el designador especfico de ese nivel (ej. un nmero de captulo, el ttulo de una seccin). La estructura es flexible para acomodar diferentes tipos de documentos, pero se deben seguir convenciones para mantener la consistencia. ## Especificacin Detallada 1. **Formato del Objeto JSON**: * Debe ser un objeto JSON plano (no anidado profundamente dentro de s mismo, aunque los valores pueden ser strings o nmeros). * Las claves deben ser strings descriptivos del nivel jerrquico. * Los valores pueden ser strings o nmeros. Si un nivel tiene tanto un nmero como un ttulo (ej. "Captulo 1: El Comienzo"), se pueden usar claves separadas o un formato consistente en el valor (ej. `"capitulo": "1"`, `"titulo_capitulo": "El Comienzo"`). 2. **Vocabulario Recomendado para Claves Jerrquicas**: Esta lista es una base y puede extenderse segn las necesidades de los tipos de documentos procesados. Se prioriza el uso de trminos en espaol donde sea lgico para el proyecto. * `volumen`: Nmero o designador del volumen. * `parte`: Nmero o designador de la parte. * `libro`: Designador del libro (para obras compuestas por mltiples "libros" internos). * `capitulo`: Nmero o designador del captulo. * `titulo_capitulo`: Ttulo textual del captulo, si existe y es distinto del nmero. * `seccion`: Nmero o designador de la seccin. * `titulo_seccion`: Ttulo textual de la seccin. * `subseccion`: Nmero o designador de la subseccin. * `titulo_subseccion`: Ttulo textual de la subseccin. * `apartado`: Designador de un apartado. * `articulo_num`: Nmero de artculo (ej. en leyes, estatutos). * `inciso_letra`: Letra de inciso (ej. en leyes). * `parrafo_num`: Numeracin explcita de un prrafo si el documento la provee (distinto del `orden_segmento_documento` que es global). * `item_lista_nivel_1`, `item_lista_nivel_2`, ...: Para identificar la profundidad en listas anidadas. * `poema_titulo`: Ttulo de un poema dentro de una coleccin. * `estrofa_num`: Nmero de estrofa. * `verso_num`: Nmero de verso dentro de una estrofa o poema. * `acto_num`: Nmero de acto en obras de teatro. * `escena_num`: Nmero de escena en obras de teatro. * `dialogo_personaje`: Nombre del personaje en un dilogo. * `pagina_original_num`: Nmero de pgina del documento fsico o PDF original, si es relevante para la contextualizacin. * `nombre_hoja_excel`: Nombre de la hoja en un archivo de spreadsheet. * `fila_tabla_num`: Nmero de fila en una tabla. * `columna_tabla_num_o_nombre`: Nmero o nombre de la columna en una tabla. * `figura_num`: Nmero de una figura o ilustracin. * `tabla_num`: Nmero de una tabla. 3. **Combinacin de Niveles**: Se deben incluir todos los niveles jerrquicos relevantes para un segmento. Ejemplo: `{"capitulo": "IV", "titulo_capitulo": "Avances en la Investigacin", "seccion": "A", "titulo_seccion": "Resultados Preliminares", "parrafo_num": "12"}` 4. **Ejemplos para Diversos Tipos de Documentos**: * **Novela Estndar**: Un prrafo en el captulo 5: `{"capitulo": "5", "parrafo_num": "23"}` Si tiene partes: `{"parte": "1", "capitulo": "5", "parrafo_num": "23"}` * **Texto Acadmico**: Un prrafo en la subseccin 2.3.1: `{"capitulo": "2", "titulo_capitulo": "Metodologa", "seccion": "2.3", "titulo_seccion": "Anlisis de Datos", "subseccion": "2.3.1", "titulo_subseccion": "Modelos Estadsticos", "parrafo_num": "3"}` * **Poemario (Poema "Oda al Tiempo")**: Segundo verso de la tercera estrofa: `{"poema_titulo": "Oda al Tiempo", "estrofa_num": "3", "verso_num": "2"}` * **Obra de Teatro**: Lnea de dilogo de un personaje: `{"acto_num": "1", "escena_num": "2", "dialogo_personaje": "Hamlet"}` * **Documento Legal (Ley X, Artculo Y, Inciso Z)**: `{"titulo_norma": "Ley X de Contrataciones Pblicas", "articulo_num": "Y", "inciso_letra": "Z"}` 5. **Directrices para Extraccin/Inferencia**: * **HTML/Markdown**: Utilizar etiquetas de encabezado (`<h1>`-`<h6>`), listas (`<ol>`, `<ul>`, `<li>`), etc. * **DOCX**: Mapear estilos de prrafo (ej. "Heading 1", "Heading 2", "ListParagraph") a niveles jerrquicos. * **PDF**: Utilizar marcadores (bookmarks) si estn presentes. Para PDFs sin estructura, la inferencia ser ms compleja y puede requerir anlisis de layout o heursticas basadas en tamao de fuente y espaciado (esto excede la especificacin bsica y entra en la implementacin del segmentador). * Los segmentadores deben ser configurables para mapear las caractersticas del formato fuente a las claves de `jerarquia_contextual`. 6. **Relacin con `tipo_segmento`**: El campo `tipo_segmento` identifica la naturaleza intrnseca del bloque de texto (ej. si ES un ttulo, un prrafo, un item de lista). El campo `jerarquia_contextual` identifica DNDE EST ese bloque dentro de la estructura mayor. *Ejemplo*: Un `texto_segmento` puede ser de `tipo_segmento: "parrafo"`. Su `jerarquia_contextual` podra ser `{"capitulo": "1", "seccion": "Introduccin"}`. Otro `texto_segmento` podra ser de `tipo_segmento: "encabezado_h2"`. Su `jerarquia_contextual` podra ser `{"capitulo": "1", "seccion": "Introduccin"}` (indicando que este es el ttulo de esa seccin). 7. **Documentacin Formal**: Esta especificacin se mantiene en este documento, `docs/JERARQUIA_CONTEXTUAL_ESPECIFICACION.md`. ## Consideraciones Adicionales * La profundidad y granularidad de la jerarqua pueden variar segn el documento. * Es preferible tener una jerarqua parcial pero correcta, a una detallada pero con errores. * Los loaders y segmentadores son responsables de poblar este campo de la manera ms precisa posible.
---

# GUIA PROCESAMIENTO DATOS

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** docs

# GUIA PROCESAMIENTO DATOS

# Gua de Procesamiento de Datos - Biblioperson ## Objetivo Esta gua detalla el proceso completo para convertir mltiples fuentes textuales (mensajes, escritos, poemas, canciones, etc.) en una base de datos depurada y estandarizada que alimente el backend de Biblioperson como base de un "gemelo digital" consultable por IA. ##  Arquitectura del Procesamiento ``` Archivos Originales  Conversin  Segmentacin  Enriquecimiento  NDJSON  Base de Datos (Mltiples (Loaders) (Reglas) (Metadatos) (Estndar) (SQLite + formatos) Meilisearch) ``` ##  Estructura de Directorios ### Estructura Recomendada ``` dataset/  raw_data/ # Archivos originales organizados por autor   autor1/    libro1.pdf    articulo1.docx    poemas/    poema1.txt    poema2.md   autor2/   ensayo1.txt   redes_sociales/   telegram_export.ndjson  processed_data/ # Archivos NDJSON procesados   autor1_libro1.ndjson   autor1_articulo1.ndjson   autor1_poemas.ndjson   autor2_ensayo1.ndjson  output/ # Resultados finales y unificados   unified/   dataset_final.ndjson  scripts/ # Scripts de procesamiento  app_depuracion.py ``` ### Organizacin por Autor **Principio fundamental**: Cada autor tiene su propio directorio en `raw_data/` ``` raw_data/  garcia_marquez/ # Nombre del autor como directorio   cien_anos_soledad.pdf # Libros principales   cronica_muerte.docx # Otros trabajos   entrevistas/ # Subdirectorios por tipo   entrevista_1982.txt  borges/   ficciones.pdf   laberintos.txt   poemas/   fervor.md   luna_enfrente.txt  usuario_personal/ # Para contenido personal  escritos/   ensayo1.md   reflexiones.txt  poemas/   mis_poemas.txt  redes_sociales/  telegram_export.ndjson  twitter_backup.json ``` ##  Formato Estndar NDJSON ### Estructura de Campos Cada lnea del archivo NDJSON es un objeto JSON con estos campos: ```json { "id_unico": "autor_documento_001", "texto_segmento": "Contenido textual del segmento", "autor_documento": "Nombre del Autor", "titulo_documento": "Ttulo del Documento", "orden_segmento_documento": 1, "tipo_segmento": "parrafo", "jerarquia_contextual": {"capitulo": "1", "seccion": "A"}, "idioma_documento": "es", "fecha_publicacion_documento": "2023-12-01", "fuente_original": "escritos", "contexto_archivo": "raw_data/autor/libro.pdf", "hash_documento_original": "abc123def456" } ``` ### Campos Explicados | Campo | Descripcin | Ejemplo | Obligatorio | |-------|-------------|---------|-------------| | `id_unico` | Identificador nico del segmento | `"garcia_marquez_cien_anos_001"` |  | | `texto_segmento` | Contenido textual | `"Muchos aos despus..."` |  | | `autor_documento` | Nombre del autor | `"Gabriel Garca Mrquez"` |  | | `titulo_documento` | Ttulo del documento | `"Cien aos de soledad"` |  | | `orden_segmento_documento` | Posicin en el documento | `1, 2, 3...` |  | | `tipo_segmento` | Tipo de contenido | `"parrafo", "titulo", "lista"` |  | | `jerarquia_contextual` | Estructura del documento | `{"capitulo": "1"}` |  | | `idioma_documento` | Cdigo ISO del idioma | `"es", "en", "fr"` |  | | `fecha_publicacion_documento` | Fecha de publicacin | `"1967-05-30"` |  | | `fuente_original` | Tipo de fuente | `"escritos", "poemas", "redes_sociales"` |  | | `contexto_archivo` | Ruta del archivo original | `"raw_data/autor/archivo.pdf"` |  | | `hash_documento_original` | Hash del documento | `"abc123def456"` |  | ### Ejemplos por Tipo de Contenido #### Libro/Ensayo ```json {"id_unico":"garcia_marquez_cien_anos_001","texto_segmento":"Muchos aos despus, frente al pelotn de fusilamiento, el coronel Aureliano Buenda haba de recordar aquella tarde remota en que su padre lo llev a conocer el hielo.","autor_documento":"Gabriel Garca Mrquez","titulo_documento":"Cien aos de soledad","orden_segmento_documento":1,"tipo_segmento":"parrafo","jerarquia_contextual":{"capitulo":"1"},"idioma_documento":"es","fecha_publicacion_documento":"1967-05-30","fuente_original":"escritos","contexto_archivo":"raw_data/garcia_marquez/cien_anos_soledad.pdf","hash_documento_original":"abc123def456"} ``` #### Poema ```json {"id_unico":"borges_fervor_001","texto_segmento":"Las calles de Buenos Aires\nya son mi entraa.\nNo las vidas calles,\nincmodas de turba y ajetreo,\nsino las calles desganadas del barrio,\ncasi invisibles de habituales,\nenternecidas de penumbra y de ocaso","autor_documento":"Jorge Luis Borges","titulo_documento":"Fervor de Buenos Aires","orden_segmento_documento":1,"tipo_segmento":"poema_completo","jerarquia_contextual":{"seccion":"Calles"},"idioma_documento":"es","fecha_publicacion_documento":"1923","fuente_original":"poemas","contexto_archivo":"raw_data/borges/fervor_buenos_aires.txt","hash_documento_original":"def456ghi789"} ``` #### Mensaje de Red Social ```json {"id_unico":"usuario_telegram_001","texto_segmento":"Interesante reflexin sobre la naturaleza del tiempo en la fsica cuntica. Ser que el tiempo es realmente una ilusin como sugiere Einstein?","autor_documento":"Usuario Personal","titulo_documento":"Conversaciones Telegram","orden_segmento_documento":1,"tipo_segmento":"mensaje","jerarquia_contextual":{"chat":"Debates Filosofa","fecha_mensaje":"2023-11-15"},"idioma_documento":"es","fecha_publicacion_documento":"2023-11-15","fuente_original":"redes_sociales","contexto_archivo":"raw_data/usuario_personal/telegram_export.ndjson","hash_documento_original":"ghi789jkl012"} ``` ##  Reglas de Segmentacin ### 1. Poemas y Canciones **Estrategia**: Archivo completo como una sola entrada **Caractersticas**: - **Un segmento por archivo**: Todo el poema/cancin en un solo registro - **Preservacin de estructura**: Mantiene saltos de lnea y estrofas - **Tipo de segmento**: `"poema_completo"` o `"cancion_completa"` - **Jerarqua contextual**: Puede incluir seccin, libro, lbum **Ejemplo de procesamiento**: ``` Archivo: raw_data/poeta/mi_poema.txt  Un solo registro NDJSON con todo el contenido ``` ### 2. Mensajes y Redes Sociales **Estrategia**: Cada mensaje es una entrada independiente **Caractersticas**: - **Un segmento por mensaje**: Cada mensaje/comentario/post individual - **Preservacin de metadatos**: Fecha, hora, contexto de conversacin - **Tipo de segmento**: `"mensaje"`, `"comentario"`, `"post"` - **Jerarqua contextual**: Chat, hilo, conversacin **Ejemplo de procesamiento**: ``` Archivo NDJSON con 100 mensajes  100 registros NDJSON individuales ``` ### 3. Escritos y Libros **Estrategia**: Segmentacin por prrafos (doble salto de lnea `\n\n`) **Caractersticas**: - **Un segmento por prrafo**: Divisin natural del texto - **Deteccin de ttulos**: Prrafos cortos y aislados como ttulos - **Tipo de segmento**: `"parrafo"`, `"titulo"`, `"subtitulo"` - **Jerarqua contextual**: Captulo, seccin, subseccin **Ejemplo de procesamiento**: ``` Archivo con 50 prrafos  50 registros NDJSON (prrafos + ttulos) ``` ### 4. Documentos Estructurados (PDF, DOCX) **Estrategia**: Segmentacin basada en estructura del documento **Caractersticas**: - **Respeta jerarqua**: Ttulos, subttulos, prrafos - **Preserva formato**: Listas, tablas, citas - **Tipo de segmento**: `"titulo"`, `"parrafo"`, `"lista"`, `"tabla"`, `"cita"` - **Jerarqua contextual**: Estructura completa del documento ### 5. Datos Tabulares (CSV, Excel) **Estrategia**: Conversin a formato narrativo **Caractersticas**: - **Filas como registros**: Cada fila se convierte en texto descriptivo - **Encabezados como contexto**: Nombres de columnas proporcionan estructura - **Tipo de segmento**: `"registro_datos"`, `"encabezado_tabla"` - **Jerarqua contextual**: Hoja, tabla, seccin ##  Proceso de Depuracin y Estandarizacin ### Fase 1: Conversin de Formatos **Objetivo**: Convertir todos los archivos a formatos procesables **Acciones**: 1. **Detectar formato** del archivo de entrada 2. **Seleccionar loader** apropiado automticamente 3. **Convertir contenido** a estructura comn 4. **Validar extraccin** de contenido **Herramientas**: - **Loaders especficos**: Para cada formato de archivo - **ProfileManager**: Seleccin automtica de loader - **Validacin**: Verificacin de contenido extrado ```python # Ejemplo de conversin automtica from dataset.processing.profile_manager import ProfileManager manager = ProfileManager() resultados = manager.process_file("documento.pdf", profile_name="book_structure") ``` ### Fase 2: Segmentacin Inteligente **Objetivo**: Dividir contenido segn reglas especficas por tipo **Acciones**: 1. **Identificar tipo** de contenido (libro, poema, mensaje) 2. **Aplicar reglas** de segmentacin correspondientes 3. **Generar segmentos** con metadatos apropiados 4. **Validar coherencia** de segmentacin **Algoritmos**: - **Deteccin de prrafos**: Por doble salto de lnea - **Identificacin de ttulos**: Por longitud y posicin - **Preservacin de estructura**: Para poemas y cdigo - **Extraccin de listas**: Para contenido enumerado ### Fase 3: Enriquecimiento de Metadatos **Objetivo**: Aadir informacin contextual y estructural **Acciones**: 1. **Extraer fecha** del archivo o contenido 2. **Inferir autor** del directorio padre 3. **Detectar idioma** del contenido 4. **Generar jerarqua** contextual 5. **Calcular hash** del documento original **Fuentes de metadatos**: - **Nombre de archivo**: Fechas, ttulos, versiones - **Propiedades de archivo**: Fecha de creacin/modificacin - **Estructura de directorios**: Autor, categora, tipo - **Contenido del documento**: Ttulos, fechas internas - **Metadatos embebidos**: PDF, DOCX properties ### Fase 4: Eliminacin de Duplicados **Objetivo**: Detectar y eliminar contenido duplicado **Estrategias**: 1. **Hash exacto**: Para duplicados idnticos 2. **Similaridad textual**: Para duplicados aproximados 3. **Anlisis semntico**: Para contenido similar 4. **Validacin manual**: Para casos ambiguos **Algoritmos**: ```python # Deteccin de duplicados por hash import hashlib def detect_exact_duplicates(segments): seen_hashes = set() duplicates = [] for segment in segments: text_hash = hashlib.md5(segment['texto_segmento'].encode()).hexdigest() if text_hash in seen_hashes: duplicates.append(segment) else: seen_hashes.add(text_hash) return duplicates # Deteccin por similaridad from difflib import SequenceMatcher def detect_similar_duplicates(segments, threshold=0.9): duplicates = [] for i, seg1 in enumerate(segments): for j, seg2 in enumerate(segments[i+1:], i+1): similarity = SequenceMatcher(None, seg1['texto_segmento'], seg2['texto_segmento']).ratio() if similarity > threshold: duplicates.append((seg1, seg2, similarity)) return duplicates ``` ### Fase 5: Estandarizacin de Campos **Objetivo**: Asegurar consistencia en formato y contenido **Validaciones**: 1. **Campos obligatorios**: Verificar presencia de todos los campos requeridos 2. **Formato de fechas**: Normalizar a ISO 8601 3. **Cdigos de idioma**: Validar cdigos ISO 639-1 4. **Tipos de segmento**: Verificar vocabulario controlado 5. **Jerarqua contextual**: Validar estructura JSON **Normalizaciones**: ```python # Normalizacin de fechas from datetime import datetime import re def normalize_date(date_string): """Normaliza diferentes formatos de fecha a ISO 8601.""" if not date_string: return None # Patrones comunes patterns = [ r'(\d{4})-(\d{2})-(\d{2})', # YYYY-MM-DD r'(\d{4})-(\d{2})', # YYYY-MM r'(\d{4})', # YYYY r'(\d{2})/(\d{2})/(\d{4})', # MM/DD/YYYY r'(\d{2})-(\d{2})-(\d{4})', # DD-MM-YYYY ] for pattern in patterns: match = re.match(pattern, date_string.strip()) if match: # Procesar segn el patrn return format_date(match.groups()) return None # Normalizacin de tipos de segmento VOCABULARIO_TIPOS = { 'parrafo', 'titulo', 'subtitulo', 'lista', 'cita', 'nota', 'tabla', 'codigo', 'formula', 'referencia', 'poema_completo', 'cancion_completa', 'mensaje', 'comentario', 'post' } def validate_segment_type(tipo): """Valida que el tipo de segmento est en el vocabulario controlado.""" return tipo.lower() in VOCABULARIO_TIPOS ``` ### Fase 6: Unificacin y Exportacin **Objetivo**: Combinar todos los archivos procesados en un dataset final **Acciones**: 1. **Recopilar archivos** NDJSON procesados 2. **Asignar IDs nicos** globales 3. **Validar integridad** del dataset completo 4. **Generar estadsticas** de procesamiento 5. **Exportar dataset final** unificado **Estructura final**: ``` output/  unified/   dataset_final.ndjson # Dataset completo   estadisticas.json # Mtricas del procesamiento   errores.log # Log de errores encontrados   metadatos.json # Informacin del dataset  por_autor/  garcia_marquez.ndjson # Dataset por autor  borges.ndjson  usuario_personal.ndjson ``` ##  Divisin de Responsabilidades ###  Responsabilidades del Usuario 1. **Organizacin inicial**: - Colocar archivos en estructura de directorios correcta - Nombrar directorios con nombres de autores - Organizar archivos por tipo cuando sea necesario 2. **Configuracin de procesamiento**: - Seleccionar perfil de procesamiento apropiado - Indicar tipo de contenido cuando no sea obvio - Configurar parmetros especficos si es necesario 3. **Revisin y validacin**: - Revisar sugerencias de duplicados antes de eliminar - Corregir fechas o metadatos cuando el script no pueda inferirlos - Validar resultados de segmentacin en casos complejos 4. **Conversin de formatos**: - Convertir archivos muy especficos a formatos estndar - Preparar archivos corruptos o con problemas de encoding ###  Responsabilidades del Script 1. **Procesamiento automtico**: - Detectar formato de archivo automticamente - Seleccionar loader y estrategia de segmentacin apropiados - Extraer contenido y metadatos disponibles 2. **Segmentacin inteligente**: - Aplicar reglas de segmentacin segn tipo de contenido - Detectar ttulos, prrafos, listas automticamente - Preservar estructura y formato cuando sea relevante 3. **Enriquecimiento de datos**: - Inferir autor del directorio padre - Extraer fechas de nombres de archivo y metadatos - Generar jerarqua contextual automticamente - Calcular hashes y IDs nicos 4. **Control de calidad**: - Detectar duplicados exactos y similares - Validar formato y consistencia de datos - Generar reportes de errores y estadsticas - Sugerir correcciones cuando sea posible ##  Flujo de Trabajo Completo ### Preparacin ```bash # 1. Organizar archivos mkdir -p raw_data/mi_autor cp mis_documentos/* raw_data/mi_autor/ # 2. Verificar estructura ls -la raw_data/mi_autor/ ``` ### Procesamiento ```bash # 3. Ejecutar procesamiento cd dataset python scripts/app_depuracion.py --autor="mi_autor" --profile="book_structure" # 4. Verificar resultados ls -la processed_data/ head -n 5 processed_data/mi_autor_*.ndjson ``` ### Validacin ```bash # 5. Validar formato NDJSON python -c " import json with open('processed_data/mi_autor_libro.ndjson') as f: for i, line in enumerate(f, 1): try: json.loads(line) except json.JSONDecodeError as e: print(f'Error en lnea {i}: {e}') " # 6. Estadsticas bsicas wc -l processed_data/*.ndjson grep -c '"tipo_segmento":"titulo"' processed_data/*.ndjson ``` ### Unificacin ```bash # 7. Unificar dataset python scripts/unificar_dataset.py --input="processed_data/" --output="output/unified/" # 8. Verificar dataset final wc -l output/unified/dataset_final.ndjson cat output/unified/estadisticas.json ``` ### Importacin ```bash # 9. Importar a base de datos cd ../backend/scripts python importar_completo.py --source="../../dataset/output/unified/dataset_final.ndjson" # 10. Verificar importacin python -c " import sqlite3 conn = sqlite3.connect('../data/biblioteca.db') print('Segmentos importados:', conn.execute('SELECT COUNT(*) FROM segmentos').fetchone()[0]) print('Autores nicos:', conn.execute('SELECT COUNT(DISTINCT autor_documento) FROM segmentos').fetchone()[0]) " ``` ##  Consideraciones Tcnicas ### Rendimiento - **Procesamiento por streaming**: Para archivos grandes - **Paralelizacin**: Procesamiento simultneo de mltiples archivos - **Cach inteligente**: Evitar reprocesar archivos sin cambios - **Optimizacin de memoria**: Liberacin de recursos durante procesamiento ### Escalabilidad - **Procesamiento incremental**: Aadir nuevos documentos sin reprocesar todo - **Versionado de datasets**: Mantener historial de cambios - **Distribucin de carga**: Para bibliotecas muy grandes - **Monitoreo de recursos**: Control de uso de CPU y memoria ### Robustez - **Manejo de errores**: Continuar procesamiento aunque algunos archivos fallen - **Validacin exhaustiva**: Verificar integridad en cada paso - **Recuperacin de errores**: Reanudar procesamiento desde puntos de control - **Logging detallado**: Trazabilidad completa del proceso ##  Prximos Pasos Despus de completar el procesamiento de datos: 1. **Importacin**: [Gestin de Datos](GUIA_GESTION_DATOS.md) - Importar NDJSON a base de datos 2. **Configuracin**: [Meilisearch](GUIA_MEILISEARCH.md) - Configurar motor de bsqueda 3. **Uso**: [Inicio Rpido](INICIO_RAPIDO.md) - Usar la aplicacin completa 4. **Extensin**: [Loaders](GUIA_LOADERS.md) - Aadir soporte para nuevos formatos ##  Solucin de Problemas Comunes ### Error: "Archivo no procesable" **Sntomas**: El script no puede leer ciertos archivos **Causas comunes**: - Archivo corrupto o formato no estndar - Encoding incorrecto - Permisos de archivo insuficientes **Soluciones**: ```bash # Verificar encoding file -i mi_archivo.txt # Convertir encoding si es necesario iconv -f ISO-8859-1 -t UTF-8 mi_archivo.txt > mi_archivo_utf8.txt # Verificar permisos ls -la mi_archivo.txt chmod 644 mi_archivo.txt ``` ### Error: "Segmentacin incorrecta" **Sntomas**: Los prrafos se dividen mal o los ttulos no se detectan **Causas comunes**: - Formato de texto inconsistente - Reglas de segmentacin no apropiadas para el tipo de documento **Soluciones**: ```python # Usar perfil especfico manager.process_file("archivo.txt", profile_name="poetry_structure") # Configurar parmetros manualmente manager.configure_profile("custom", { "segmentation_strategy": "sentence_based", "min_segment_length": 30 }) ``` ### Error: "Metadatos faltantes" **Sntomas**: Fechas o ttulos no se extraen correctamente **Causas comunes**: - Nombres de archivo sin informacin de fecha - Metadatos no estndar en documentos **Soluciones**: ```python # Proporcionar metadatos manualmente resultados = manager.process_file( "archivo.pdf", override_metadata={ "fecha_publicacion": "2023-01-01", "titulo_documento": "Mi Ttulo" } ) ``` Esta gua proporciona todo lo necesario para entender y ejecutar el proceso completo de conversin de documentos a NDJSON en Biblioperson. Para casos especficos o problemas tcnicos, consulta las guas especializadas de loaders y especificaciones tcnicas.
---

# GUIA COMANDOS PROCESAMIENTO

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** docs

# GUIA COMANDOS PROCESAMIENTO

# Gua de Comandos para Procesamiento de Documentos Esta gua describe cmo utilizar los scripts de procesamiento de documentos en el proyecto Biblioperson, incluyendo cmo convertir documentos o carpetas completas a NDJSON. ## app_depuracion.py **Ruta:** `dataset/scripts/app_depuracion.py` ### Cmo funciona? El script `app_depuracion.py` NO utiliza argumentos de lnea de comandos. Todo el procesamiento se controla mediante archivos de configuracin JSON: - `jobs_config.json`: Define los trabajos a ejecutar (qu carpetas procesar, qu perfil usar, etc.). - `content_profiles.json`: Define los perfiles de procesamiento (algoritmos, estrategias de chunking, etc.). ### Cmo convertir documentos o carpetas completas a NDJSON? 1. **Configura tu trabajo en `jobs_config.json`:** - Especifica el `job_id`, el nombre del autor, el tipo de origen, el nombre de la carpeta de entrada (`source_directory_name`), el perfil de procesamiento (`content_profile_name`), y otros parmetros opcionales. - Ejemplo: ```json [ { "job_id": "ejemplo1", "author_name": "Autor Demo", "origin_type_name": "libro", "source_directory_name": "carpeta_a_procesar", "content_profile_name": "perfil_pdf", "enabled": true } ] ``` 2. **Configura el perfil en `content_profiles.json`:** - Define el perfil de procesamiento que ser referenciado en el job. 3. **Coloca los archivos a procesar en la carpeta indicada:** - Por ejemplo, si `source_directory_name` es `carpeta_a_procesar`, coloca los archivos en `dataset/raw_data/carpeta_a_procesar/`. 4. **Ejecuta el script:** - Desde la raz del proyecto, ejecuta: ```bash python dataset/scripts/app_depuracion.py ``` 5. **Salida:** - Los archivos NDJSON generados se guardarn en `dataset/output/{author_name}/{origin_type_name}/{job_id}/`. ### Resumen de parmetros relevantes - **`jobs_config.json`** - `job_id`: Identificador nico del trabajo - `author_name`: Nombre del autor - `origin_type_name`: Tipo de origen (ej. libro, artculo) - `source_directory_name`: Carpeta de entrada a procesar - `content_profile_name`: Perfil de procesamiento a usar - `enabled`: Si el trabajo est habilitado - **`content_profiles.json`** - Define el algoritmo, chunking, y procesamiento especfico para cada tipo de documento --- ## Otros scripts ### importar_datos.py **Ruta:** `backend/scripts/importar_datos.py` Este script S utiliza argumentos de lnea de comandos para importar NDJSON a la base de datos. Consulta la seccin correspondiente para detalles de uso. --- ## Resumen - Para convertir documentos o carpetas completas a NDJSON, configura los archivos JSON y ejecuta `app_depuracion.py` sin argumentos. - Para importar NDJSON a la base de datos, utiliza `importar_datos.py` con los argumentos adecuados. # Banderas y Argumentos CLI de los Scripts Principales A continuacin se documentan todas las banderas (argumentos de lnea de comandos) disponibles en los scripts principales de procesamiento e importacin de Biblioperson. Utiliza estos argumentos para personalizar el comportamiento de cada script segn tus necesidades. --- ## process_file.py (`dataset/scripts/process_file.py`) | Bandera / Posicional | Tipo | Descripcin | Valor por defecto | Ejemplo de uso | |-----------------------------|----------|-----------------------------------------------------------------------------|-------------------|------------------------------------------------| | `input_path` (posicional) | str | Ruta al archivo o directorio a procesar | Obligatorio | `python process_file.py carpeta/` | | `--list-profiles` | bool | Muestra los perfiles de procesamiento disponibles | False | `--list-profiles` | | `--profile`, `-p` | str | Nombre del perfil a utilizar (si se omite, se detecta automticamente) | None | `--profile=poesia` | | `--output`, `-o` | str | Ruta de salida para el archivo NDJSON generado | None | `--output=salida.ndjson` | | `--force-type` | str | Forzar tipo de documento (poemas, escritos, canciones, capitulos) | None | `--force-type=poemas` | | `--confidence-threshold` | float | Umbral de confianza para segmentacin automtica | 0.5 | `--confidence-threshold=0.7` | | `--verbose`, `-v` | bool | Muestra informacin de depuracin detallada | False | `--verbose` | | `--profiles-dir` | str | Ruta a un directorio de perfiles personalizado | None | `--profiles-dir=perfiles/` | | `--encoding` | str | Codificacin de caracteres del archivo de entrada | utf-8 | `--encoding=latin1` | --- ## importar_datos.py (`backend/scripts/importar_datos.py`) | Bandera | Tipo | Descripcin | Valor por defecto | Ejemplo de uso | |------------------------|--------|----------------------------------------------------------|---------------------------|--------------------------------------| | `--base-dir` | str | Directorio base del proyecto | (ruta base por defecto) | `--base-dir=../` | | `--contenido-dir` | str | Directorio con archivos a importar | None | `--contenido-dir=import/` | | `--db-path` | str | Ruta a la base de datos SQLite | (ruta por defecto) | `--db-path=mi_biblioteca.db` | | `--ndjson-file` | str | Ruta al archivo NDJSON unificado para importar | None | `--ndjson-file=unificado.ndjson` | --- ## importar_completo.py (`backend/scripts/importar_completo.py`) | Bandera / Posicional | Tipo | Descripcin | Valor por defecto | Ejemplo de uso | |-----------------------|--------|----------------------------------------------------------|-------------------|-----------------------------------------------| | `archivo_ndjson` | str | Ruta al archivo NDJSON a importar (opcional) | None | `python importar_completo.py datos.ndjson` | | `--reiniciar-db` | bool | Reinicia la base de datos antes de importar (borra todo) | False | `--reiniciar-db` | | `--solo-importar` | bool | Solo importa datos, sin reiniciar la base de datos | False | `--solo-importar` | --- ## indexar_meilisearch.py (`backend/scripts/indexar_meilisearch.py`) | Bandera | Tipo | Descripcin | Valor por defecto | Ejemplo de uso | |------------------------|--------|----------------------------------------------------------|---------------------------|--------------------------------------| | `--tabla` | str | Nombre de la tabla que contiene los documentos | None | `--tabla=documentos` | | `--listar` | bool | Listar todas las tablas disponibles | False | `--listar` | | `--db-path` | str | Ruta a la base de datos SQLite | (ruta por defecto) | `--db-path=mi_biblioteca.db` | | `--hilos` | int | Nmero de hilos a utilizar | (valor por defecto) | `--hilos=4` | | `--batch-size` | int | Tamao del lote de la base de datos | (valor por defecto) | `--batch-size=1000` | | `--docs-per-request` | int | Documentos por solicitud a Meilisearch | (valor por defecto) | `--docs-per-request=500` | | `--timeout` | int | Timeout para solicitudes HTTP en segundos | (valor por defecto) | `--timeout=60` | --- ## limpiar_datos_duplicados.py (`backend/scripts/limpiar_datos_duplicados.py`) | Bandera | Tipo | Descripcin | Valor por defecto | Ejemplo de uso | |-------------|--------|---------------------------------------------|-------------------|---------------------------------------| | `--db-path` | str | Ruta a la base de datos SQLite | Obligatorio | `--db-path=mi_biblioteca.db` | --- ## preparar_importacion.py (`backend/scripts/preparar_importacion.py`) | Bandera / Posicional | Tipo | Descripcin | Valor por defecto | Ejemplo de uso | |-----------------------|--------|----------------------------------------------------------|-------------------|-----------------------------------------------| | `archivo_ndjson` | str | Ruta al archivo NDJSON a preparar | Obligatorio | `python preparar_importacion.py datos.ndjson` | --- ## generador_asistido.py (`backend/scripts/generador_asistido.py`) | Bandera / Posicional | Tipo | Descripcin | Valor por defecto | Ejemplo de uso | |-----------------------|--------|----------------------------------------------------------|-------------------|-----------------------------------------------| | `tema` | str | Tema sobre el que generar contenido | Obligatorio | `python generador_asistido.py "El amor"` | | `--tipo`, `-t` | str | Tipo de contenido a generar (post, articulo, etc.) | post | `--tipo=articulo` | | `--estilo`, `-e` | str | Estilo de escritura (formal, conversacional, etc.) | None | `--estilo=formal` | | `--llm`, `-l` | str | Modelo LLM a utilizar (gemini, openai) | gemini | `--llm=openai` | | `--resultados`, `-r` | int | Nmero de resultados a generar | (valor por defecto)| `--resultados=3` | | `--solo-prompt`, `-p` | bool | Solo mostrar el prompt generado, sin ejecutar | False | `--solo-prompt` | --- ## cli.py (`dataset/scripts/cli.py`) | Bandera | Tipo | Descripcin | Valor por defecto | Ejemplo de uso | |-----------------|--------|---------------------------------------------|-------------------|---------------------------------------| | `--validate` | bool | Validar configuraciones existentes | False | `--validate` | --- > **Nota:** Para ver todas las banderas disponibles y su descripcin, ejecuta cualquier script con la opcin `--help`.
---

# INICIO RAPIDO

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** docs

# INICIO RAPIDO

# Inicio Rpido - Biblioperson ## Qu es Biblioperson? Biblioperson convierte tu biblioteca personal en una base de conocimiento consultable. Imagina poder preguntarle a todos tus libros y documentos: "Qu piensa este autor sobre X tema?" o "Dnde he ledo algo sobre Y concepto?" ## Qu puedes hacer? - **Buscar ideas especficas**: Encuentra qu dice un autor particular sobre cualquier tema - **Descubrir conexiones**: Encuentra relaciones entre diferentes textos y autores - **Generar contenido**: Crea material original basado en las ideas de tu biblioteca - **Navegar como ebooks**: Reconstruye y navega tus documentos originales - **Bsqueda semntica**: Encuentra conceptos similares aunque usen palabras diferentes ## Flujo de Trabajo Simple ``` Tus Documentos  Procesamiento  Base de Datos  Bsqueda Inteligente (PDF, DOCX, (NDJSON) (SQLite + (Web + API) TXT, MD) Meilisearch) ``` ## Instalacin Rpida (5 pasos) ### 1. Preparar el entorno ```bash # Clonar y entrar al proyecto git clone [URL_DEL_REPOSITORIO] cd biblioperson # Crear entorno virtual python -m venv venv .\venv\Scripts\activate # Windows ``` ### 2. Instalar dependencias ```bash # Backend pip install -r backend/requirements.txt # Dataset (procesamiento) pip install -r dataset/requirements.txt # Frontend cd frontend npm install cd .. ``` ### 3. Procesar tu primer documento ```bash # Coloca un documento de prueba en dataset/raw_data/autor_prueba/ # Luego procsalo: cd dataset python scripts/app_depuracion.py ``` ### 4. Importar a la base de datos ```bash cd ../backend/scripts python importar_completo.py ``` ### 5. Levantar la aplicacin ```bash # Terminal 1: Backend cd backend/scripts python api_conexion.py # Terminal 2: Frontend cd frontend npm run dev ``` Abre http://localhost:5173 y comienza a buscar! ## Ejemplo Prctico ### Documento de entrada: `mi_libro.pdf`  "Captulo 1: La filosofa griega..." ### Despus del procesamiento: - **Segmento 1**: `{"texto_segmento": "La filosofa griega...", "tipo_segmento": "parrafo", "autor_documento": "Platn", "jerarquia_contextual": {"capitulo": "1"}}` - **Bsqueda**: "filosofa antigua"  Encuentra este segmento aunque no diga "antigua" - **Navegacin**: Reconstruye el libro completo desde la base de datos ## Prximos Pasos - **Documentacin tcnica**: Ver `BIBLIOPERSON_ARQUITECTURA.md` - **Especificaciones NDJSON**: Ver `NDJSON_ESPECIFICACION.md` - **Configuracin avanzada**: Ver `GUIA_MEILISEARCH.md` - **Gestin de datos**: Ver `GUIA_GESTION_DATOS.md` ## Solucin de Problemas Comunes ### Error: "No se encuentra meilisearch" ```bash # Descargar meilisearch desde: # https://github.com/meilisearch/meilisearch/releases # Colocar el ejecutable en backend/meilisearch/ ``` ### Error: "Base de datos no existe" ```bash cd backend/scripts python inicializar_db.py ``` ### Error: "Puerto ocupado" - Backend usa puerto 5000 - Frontend usa puerto 5173 - Meilisearch usa puerto 7700 Necesitas ayuda? Revisa la documentacin completa en la carpeta `docs/`.
---

# ALGORITMOS PROPUESTOS

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** docs

# ALGORITMOS PROPUESTOS

# Algoritmos de Segmentacin Propuestos > **Documento tcnico:** Descripcin detallada de los algoritmos de segmentacin propuestos para la refactorizacin 2023-2024. --- --- ## Bloque 1: Algoritmo de deteccin de poemas / canciones (Descripcin Detallada) Antes de detallar cada algoritmo, es til entender el flujo general y los principios transversales: ### 1  Preprocesamiento - Lectura del archivo fuente (DOCX, Markdown, TXT, JSON) - Limpieza bsica (ej. normalizacin de saltos de lnea) - Etiquetado lnea a lnea: Cada lnea se analiza para extraer atributos observables (ver Bloque 1: Tipo de texto, Longitud, Vaca, Saltos acumulados, Nivel de Indentacin) ### 2  Deteccin de Estructura Principal - **Prioridad:** Se intenta detectar la estructura dominante. Para archivos de texto, se busca primero la estructura de prosa (Secciones/Captulos) basada en encabezados. Si no se encuentra una estructura de prosa clara, o dentro de bloques de prosa, se aplica el detector de poesa. Para JSON, se aplica el selector/filtrador de entradas. - **Archivos Mixtos:** En documentos que mezclan prosa y poesa (ej. ensayo con citas poticas), el detector de prosa define las secciones principales. Luego, el detector de poesa se aplica dentro de los bloques de texto identificados como prrafos de prosa, usando umbrales potencialmente ms estrictos o la regla del "< 20% del tamao" para clasificar como "Cita Potica" vs. "Poema Completo". ### 3  Deteccin de Sub-estructuras - Dentro de las secciones de prosa  Prrafos - Dentro de los bloques de poesa  Estrofas y Versos - Dentro de las entradas JSON  (Opcional) Aplicacin de detectores de prosa/poesa al contenido textual ### 4  Manejo de Casos Especiales y Paratexto Se aplican heursticas para identificar y etiquetar elementos como: - ndices (TOC) - Listas - Tablas - Citas en bloque - Notas al pie - Encabezados/Pies de pgina (si la fuente lo permite) Estos elementos pueden ser excluidos de la segmentacin principal o etiquetados especficamente. ### 5  Numeracin y Salida Se asignan identificadores jerrquicos y estables a cada unidad segmentada (Seccin, Prrafo, Poema, Estrofa, Verso, Entrada JSON). ### 6  Configurabilidad Todos los elementos son configurables externamente: - Umbrales numricos (longitud, huecos, ratios) - Definiciones (qu constituye "estilo") - Flags de comportamiento (ej. usar indentacin para prrafos) ### 7  Manejo de Ambigedad En casos donde una lnea o bloque podra satisfacer reglas de diferentes detectores (ej. lnea corta con estilo que podra ser ttulo de poema o encabezado de prosa breve): - Se aplica un orden de prioridad (configurable, por defecto: Prosa > Poesa > Otros) - O se utiliza un sistema de puntuacin basado en la fuerza de las evidencias contextuales --- ## Bloque 1: Algoritmo de deteccin de poemas / canciones (Descripcin Detallada) (Expresado solo en lenguaje natural, incorporando todas las relaciones de reglas que mencionaste y afinando los matices de "ttulo", "estrofa" y "fin") ### Clasificacin preliminar lnea a lnea Al leer el archivo se etiqueta cada lnea con cuatro atributos observables: | Atributo | Posibles valores | Cmo se reconoce | | :----------------- | :--------------------- | :---------------------------------------------------------------------------------------- | | Tipo de texto | "plana" / "con estilo" | En DOCX  Heading, negrita + centrado, etc.; en Markdown  #,  al comienzo. | | Longitud | "corta" / "larga" | Se compara con un umbral configurable (p. ej.  120 caracteres = corta). | | Vaca | S / No | Cadena sin texto tras strip(). | | Saltos acumulados | 0, 1, 2, 3 | Nmero de lneas vacas consecutivas antes de la actual. | Con eso bastan todas las reglas posteriores. ### Deteccin de ttulo Un candidato a ttulo solo es vlido si la combinacin de factores cuadra con lo que sigue. | Candidato | Verificacin | Confirmacin | | :----------------------------- | :------------------------------------------------------------------------------------------------------------ | :------------------------------------------------------------------------------------------------------- | | **ndice / TOC heurstica 70 %** | Indices cortos (35 entradas) no llegan al 70 %. |  Comprobar tambin patrn de lneas acabadas en nmeros de pgina o puntos  12.<br> Si la seccin tiene  2 prrafos y  1 de esos patrones, marcar como ndice. | | **Hash de duplicados en JSON** | Cambios mnimos ("hola " vs "hola") generan hash distinto  duplicado pasa.<br>Conversin de emojis o html entities altera hash. |  Normalizar texto: lowercase, strip, colapsar espacios, sustituir entidades.<br> Usar fuzzy distancia (Levenshtein < ) para decidir duplicidad opcional. | | **NoLatino / RTL scripts** | Reglas de "caracter alfabtico" y longitud pueden fallar con chino, rabe, etc. |  Calcular longitud en code points, no bytes.<br> Para "ratio alfabtico", admitir categoras Unicode "Letter" & "Mark". | | **Versos con acordes** | Algunas tablaturas colocan acordes arriba de cada verso (dos lneas cortas seguidas). Podran parecer estrofa aparte. |  Si se alterna ACORDE  verso corto  ACORDE  verso corto, colapsar pares en un solo verso lgico; o bien descartar la lnea de acorde antes del conteo. | | **Poemas embebidos en prosa** | Citas poticas dentro de un ensayo (tres versos cortos con sangra). Pueden disparar el detector incorrecto. |  Antes de aceptar un bloque como "poema", comprobar que ese bloque representa < 20 % del tamao total de la seccin; si es menor, etiquetarlo como "cita potica" pero no crear objeto Poema. | ### Recomendaciones globales [Poesa] - Todos los umbrales (longitud, huecos, proporciones) deben ser overridable por coleccin/autores; documenta valores por defecto y cmo afinarlos. - Logging multinivel: INFO para hits seguros, WARN para heursticas dudosas (haik corto, hueco extralargo). Permite auditar falsos positivos sin detener el pipeline. - Juego de pruebas vivo: cada vez que aparezca un caso lmite nuevo, agrgalo como fixture; as el detector "aprende" sin tocar lgica. - Metadatos de confianza: guarda en cada objeto un campo confidence (01). Las consultas downstream pueden filtrar o pedir "solo alta confianza". --- ## Bloque 3: Descripcin consolidada  Cmo segmentar poesa, prosa y JSONs (solo lenguaje natural, sin cdigo; incluye reglas relacionales, jerarquas y numeracin) ### 1  Definiciones comunes | Trmino | Significado operacional | | :------ | :--------------------- | | Lnea corta | Entre 1 y  120 caracteres (umbral ajustable). | | Lnea larga | > umbral corto. | | Lnea con estilo | Marcada en la fuente (Heading DOCX, # Markdown, negrita centrada, etc.). | | Lnea vaca | Cadena en blanco despus de strip(). | | Bloque de hueco |  2 lneas vacas consecutivas (produce "espacio vertical" visible). | > Todos los umbrales (longitud, tamao de hueco, etc.) viven en un archivo de configuracin para afinar por coleccin o autor. ### 2  Algoritmo de poesa / canciones #### 2.1 Deteccin del ttulo de un poema Un ttulo es una lnea candidata que cumple al mismo tiempo: 1. Corta y con estilo **o** (si no hay estilo) corta, centrada/negrita y rodeada de hueco. 2. Le siguen 1 a 3 lneas vacas. 3. Despus aparece un bloque de  3 lneas cortas, planas, separadas entre ellas por  2 lneas vacas. Si las tres condiciones pasan, la lnea se confirma como **Ttulo** y el poema empieza en la primera lnea del bloque detectado. #### 2.2 Construccin de versos y estrofas - **Verso** = cada lnea corta plana. - Una **estrofa** contiene  2 versos contiguos; entre ellos puede haber 0, 1 o 2 lneas vacas. - **Estrofas vecinas** se separan con un bloque de exactamente 2 o 3 lneas vacas. - Si surge una lnea con estilo entre estrofas, se considera que inicia otro bloque textual. #### 2.3 Criterios de trmino del poema El poema finaliza cuando aparece cualquiera de estos eventos: 1. > 3 lneas vacas seguidas. 2. Tras un bloque de 23 vacas, la siguiente lnea es con estilo. 3. Tras un bloque de 23 vacas, la lnea siguiente es larga o es una sola lnea corta aislada (sin formar estrofa). #### 2.4 Jerarqua de objetos y numeracin ```scss Poema (id)  metadatos (ttulo, autor, fuente, detector)  Estrofa 1  versos[1n]   Verso 1.1   Verso 1.2  Estrofa 2  Verso 2.1   ``` Se generan ndices 1based: `poem_id`, `stanza_index`, `verse_index`. As una consulta como "poema 42 / verso 3" siempre es nica. #### 2.5 Reglas auxiliares - Acordes (`[C]`, `[Am]`) se ignoran al contar versos. - Dilogos (`S.`) no cuentan como verso si terminan en puntuacin final. - Ruido OCR se descarta si < 70 % letras alfabticas. ### 3  Algoritmo de prosa (libros, cartas, artculos) #### 3.1 Pasada global para detectar niveles de seccin Recorre el documento y marca cada lnea candidata a encabezado: 1. Lnea con estilo **o** corta y centrada/negrita. 2. Agrupa encabezados por profundidad de estilo (Heading 1, 2, 3). 3. Si no hay estilos, infiere niveles comparando distancia entre encabezados y la longitud media de los bloques que siguen. 4. El encabezado de nivel 0 (el ms alto) se toma como **Ttulo principal** del documento o del "libro" dentro del archivo. #### 3.2 Construccin jerrquica - Cada seccin arranca en un encabezado y termina justo antes del siguiente encabezado de igual o mayor nivel. - Dentro se agrupan **prrafos:** bloques de texto separados por  1 lnea vaca. - Si tras un encabezado solo hallas  N caracteres (texto breve), lo reclasificas como **epgrafe** y mantienes la seccin padre activa. #### 3.3 Reglas prcticas | Situacin | Decisin | | :---------------------------------------------------------- | :--------------------------------------------------------------------------------------------------- | | Un documento contiene varios "libros" (p. ej. obra completa) | Cada Heading 1 se convierte en "Libro"; sus Heading 2+ forman captulos/secciones internas. | | Cartas sin subttulos | Tienen un solo encabezado (ttulo) y todos los prrafos dependen de l. | | ndices o TOC | Si una seccin contiene > 70 % de lneas que empiezan con nmeros o puntos de relleno (), se etiqueta como "ndice" y se descarta de la divisin en prrafos. | #### 3.4 Numeracin resultante ```css Documento  Seccin 1 (1)   Subsec 1.1    Prrafo 1.1.a   Subsec 1.2  Seccin 2 (2)  Prrafo 2.a ``` La clave de cada prrafo es `section_index[.subsection]-paragraph_letter` para poder citarlo: "2.1c". ### 4  Algoritmo para JSON heterogneos 1. **Seleccin de campo de texto:** El usuario indica la clave donde vive el contenido (`body`, `message`, etc.). 2. **Filtrado por claves/valores:** Modo inclusivo (qu conservar) o exclusivo (qu descartar) segn pares `clave = valor` que el usuario defina. 3. **Eliminacin de duplicados (opcional):** Hash de texto normalizado; si se repite, se descarta la copia. 4. **Numeracin estable:** Se asigna `entry_index` al orden de aparicin *despus* de filtros. 5. El JSON completo recibe `title` (dado por el usuario o inferido del nombre de archivo). ### 5  Ventajas de este enfoque "relacional" - **Reduccin de falsos positivos:** cada decisin depende de conjuntos de condiciones (ej. ttulo = estilo + hueco + versos). - **Jerarquas claras:** - Poema  Estrofa  Verso - Documento  Seccin (niveles)  Prrafo - JSON  Entrada todos con claves nicas para referencias finas. - **Configurable sin tocar la lgica:** basta editar los umbrales en el archivo de parmetros. - **Extensible:** aadir reglas auxiliares (p.ej. tablas, epgrafes, citas latinas) sin romper el ncleo. --- ## Bloque 4: Revisin exhaustiva de todos los detectores (Prosa y JSON) (poesa ya cubierta  ahora prosa y JSON heterogneos) ### 1  Detector de prosa (libros, cartas, artculos) | Bloque del algoritmo | Riesgo de error / ambigedad | Estrategia de mitigacin (cmo "calzarlo") | | :------------------ | :-------------------------- | :------------------------------------------- | | **Pasada global para contar niveles de seccin** | a) Documentos sin estilos, solo MAYSCULAS y subrayados.<br>b) Presentaciones (prrafos muy cortos bajo cada ttulo) confunden "ttulo" con "seccin vaca". |  Complementar "Heading" con heurstica: lnea centrada o  70 % maysculas o termina en nmero romano/arbigo.<br> Exigir que cada candidato a "seccin" est seguido de  N caracteres de texto largo antes de aceptar otro encabezado del mismo nivel. | | **Inferir jerarqua cuando faltan estilos** | Mezcla de CAPTULO I y Captulo 1 puede crear dos niveles falsos (I vs 1). |  Normalizar nmeros (romanosarbigos) antes de agrupar.<br> Si un encabezado coincide (tras normalizar) con otro ya visto, asignarle el mismo nivel. | | **Ttulos/epgrafes demasiado largos (> 120 chars)** | Se clasifican como prrafos y rompen la jerarqua. |  Relax: lnea con hueco antes y despus +  300 char puede ser ttulo.<br> Medir densidad de palabras: encabezados suelen tener  12 palabras aunque sean largos. | | **Encabezados "colgados" (sin texto debajo)** | ndices, dedicatorias o pginas de derechos dejan un ttulo solitario  se piensa que el resto del libro es su hijo. |  Si tras el encabezado hay < N caracteres y luego otro encabezado del mismo nivel, reclasificarlo como paratexto (no generar seccin). | | **ndice de contenidos (TOC)** | ndices pequeos (< 6 lneas) no alcanzan la regla "70 % patrones". |  Patrones extra: lneas terminan en nmeros agrupados derecha ( 12) o empiezan con nmero+espacio.<br> Si bloque de texto aparece antes de la primera seccin real y contiene esos patrones  marcar "ndice". | | **Cartas o documentos con un solo encabezado** | Algoritmo puede intentar asignar niveles ficticios si detecta letras grandes en la despedida ("Atentamente,"). |  Si solo se detecta un encabezado en todo el archivo  etiquetar el resto como "cuerpo" sin jerarqua extra. | | **Citas largas en bloque (blockquote)** | Texto sangrado / cursiva larga se confunde con lista de prrafos de nueva seccin. |  Si bloque est delimitado por comillas largas o sangra > 4 espacios y seguido de " Autor", tratar como cita y no subdividir. | | **Tablas o listas numeradas** | Se pueden medir como "muchos prrafos  seccin larga" y desbalancear niveles. |  Detectar lneas que empiezan con , -, 1. etc.; si  60 % del bloque  marcar como lista/tabla y excluir del conteo de "texto largo". | | **Textos en scripts RTL (rabe, hebreo)** | Centrado o maysculas no aplican; longitudes en bytes engaosas. |  Basarse en categoras Unicode "Letter" para contar caracteres.<br> Identificar encabezados RTL por patrn "lnea sola + hueco alrededor". | | **Fusin de varios "libros" en un mismo archivo** | Primera Heading 1 podra considerarse ttulo global y perder los siguientes libros. |  Si tras un Heading 1 aparece otro Heading 1 despus de un bloque grande, tratar cada Heading 1 como "Libro" separado y generar un contenedor superior "Compilacin". | #### Recomendaciones extra para prosa - **Campos de confiabilidad:** guarda `section_level_confidence` para cada encabezado. - **Pruebas dirigidas:** incluye "libro sin estilos", "artculo con subsubsecciones", "ndice breve" y "carta" en la suite. - **Parmetros override:** `min_chars_section`, `max_words_header`, `list_marker_ratio`, etc., editables por coleccin. ### 2  Detector de JSONs heterogneos | Etapa del algoritmo | Riesgo de error / ambigedad | Estrategia de mitigacin | | :----------------- | :-------------------------- | :----------------------- | | **Seleccin de campo de texto** | Usuario olvida indicar la ruta profunda (data.body.text). |  Permitir dotnotation (`data.body.text`).<br> Mostrar previa de campos detectados si no se especifica y pedir confirmacin. | | **Modo inclusivo / exclusivo de filtros** | Complejidad: usuario mezcla ambos modos sin querer. |  UI/CLI debe forzar eleccin de uno de los modos por operacin.<br> Validar reglas antes de ejecutar y mostrar cuntas entradas quedaran. | | **Tipos de valor variables** | Campo a filtrar cambia de string a int segn la entrada  fallo de comparacin. |  Normalizar valores a string antes de comparar, salvo que usuario especifique tipo. | | **Eliminacin de duplicados** | a) Texto casi idntico con emoji vs sin emoji.<br>b) URLs con tracking params generan hashes distintos. |  Normalizador configurable: lowercase, trim, borrar puntuacin ligera, sustituir emojis por alias.<br> Para URLs: parsear y mantener solo esquema+dominio+path para el hash. | | **Orden de aparicin  timestamps** | Si el JSON no est ordenado cronolgicamente, `entry_index` no refleja tiempo real. |  Si existe campo fecha/hora reconocido (ISO 8601, epoch)  ordenar por l antes de indexar.<br> Guardar ambos: `order_index` (original) y `ts_index` (fecha). | | **Ttulo del JSON** | Archivos annimos (.json) sin clave evidente para ttulo. |  Fallbacks: 1) nombre de archivo sin extensin, 2) primer campo string de nivel 0 que exceda N caracteres, 3) "Untitled Dataset". | | **Subcolecciones anidadas** | Arreglos dentro de arreglos (e.g. respuesta con hilos). |  Aplanar jerrquicamente: usar clave compuesta `parent_idx-child_idx` para mantener trazabilidad y citas finas. | | **Unicode y encodings** | Entradas mezclan UTF8 y Latin1  hashes diferentes. |  Decode a Unicode NFC antes de cualquier operacin. | | **Claves ausentes en algunas entradas** | Falta el campo de texto  se genera registro vaco. |  Omitir la entrada y emitir WARN con count.<br> Si > X % de entradas carecen del campo, abortar y pedir correccin de configuracin. | | **Tamao gigantesco de un nico valor** | Algunos mensajes traen blobs de base64  hash caro, storage intil. |  Si longitud > MaxBytes  guardar solo resumen SHA256 como texto sustituido "[BLOB omitted (size)]". | | **Duplicados "quasiidnticos" por ruido OCR** | Distancia de edicin mnima pero semnticamente igual. |  Opcional: Levenshtein/Tokensort ratio  threshold para fusionar.<br> Registrar `mergehistory` para auditora. | #### Recomendaciones extra para JSON - **Reporte de ingestin:** al final, lista  entradas totales, filtradas, duplicadas, omitidas, tamao medio del texto. - **Configuracin versionada:** gurdala con checksum; si el user cambia filtros, se reprocesa solo lo afectado. - **Pruebas:** incluye dataset con claves faltantes, textos distintos solo por emoji, URLs con `utm_*`, blobs base64, timestamps desordenados. ### 3  Resguardo general - Todos los detectores exponen sus umbrales y banderas en config YAML/JSON nico; mantn comentarios con ejemplos (`max_words_header: 12` # e.g. "Captulo Primero"`). - Flags de entorno para forzar "modo estricto" (rechaza dudas) o "modo laxo" (incluye WARN). - Campo `detector_version` en cada objeto persistido  facilita migraciones. - Suite de regresin viva: cada edgecase nuevo de produccin se convierte en fixture; antes de hacer release, todo debe pasar en CI.
---

# PIPELINE NDJSON

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** docs

# PIPELINE NDJSON

# Pipeline NDJSON - Gua Prctica ## Visin General del Pipeline El pipeline NDJSON es el corazn del procesamiento de documentos en Biblioperson. Convierte documentos de cualquier formato en registros estructurados listos para bsqueda semntica. ``` Documento Original  Extraccin  Segmentacin  Enriquecimiento  NDJSON  Base de Datos ``` ## Estructura del Directorio de Trabajo ``` dataset/  raw_data/   autor1/    libro1.pdf    articulo1.docx   autor2/   ensayo1.txt  processed_data/   autor1_libro1.ndjson   autor1_articulo1.ndjson   autor2_ensayo1.ndjson  scripts/  app_depuracion.py ``` ## Paso a Paso: De Documento a NDJSON ### 1. Preparar el Documento **Estructura requerida:** ``` raw_data/[nombre_autor]/[documento].[extensin] ``` **Formatos soportados:** - PDF (`.pdf`) - Word (`.docx`) - Texto plano (`.txt`) - Markdown (`.md`) **Ejemplo:** ``` raw_data/  garcia_marquez/  cien_anos_soledad.pdf ``` ### 2. Ejecutar el Procesamiento ```bash cd dataset python scripts/app_depuracion.py ``` **Lo que sucede internamente:** 1. **Extraccin**: Convierte PDF/DOCX a texto plano 2. **Segmentacin**: Divide en prrafos, ttulos, listas, etc. 3. **Enriquecimiento**: Aade metadatos y jerarqua contextual 4. **Generacin**: Crea archivo NDJSON en `processed_data/` ### 3. Verificar el Resultado **Archivo generado:** ``` processed_data/garcia_marquez_cien_anos_soledad.ndjson ``` **Contenido de ejemplo:** ```json {"id_unico":"garcia_marquez_cien_anos_soledad_001","texto_segmento":"Muchos aos despus, frente al pelotn de fusilamiento, el coronel Aureliano Buenda haba de recordar aquella tarde remota en que su padre lo llev a conocer el hielo.","autor_documento":"Gabriel Garca Mrquez","titulo_documento":"Cien aos de soledad","orden_segmento_documento":1,"tipo_segmento":"parrafo","jerarquia_contextual":{"capitulo":"1"},"idioma_documento":"es","fecha_publicacion_documento":"1967-05-30","hash_documento_original":"abc123def456"} {"id_unico":"garcia_marquez_cien_anos_soledad_002","texto_segmento":"El mundo era tan reciente, que muchas cosas carecan de nombre, y para mencionarlas haba que sealarlas con el dedo.","autor_documento":"Gabriel Garca Mrquez","titulo_documento":"Cien aos de soledad","orden_segmento_documento":2,"tipo_segmento":"parrafo","jerarquia_contextual":{"capitulo":"1"},"idioma_documento":"es","fecha_publicacion_documento":"1967-05-30","hash_documento_original":"abc123def456"} ``` ## Campos del NDJSON Explicados ### Campos Obligatorios | Campo | Descripcin | Ejemplo | |-------|-------------|----------| | `id_unico` | Identificador nico del segmento | `"autor_libro_001"` | | `texto_segmento` | Contenido textual del segmento | `"Este es el prrafo..."` | | `autor_documento` | Nombre del autor | `"Gabriel Garca Mrquez"` | | `orden_segmento_documento` | Posicin en el documento original | `1, 2, 3...` | | `tipo_segmento` | Tipo de contenido | `"parrafo", "titulo", "lista"` | ### Campos de Metadatos | Campo | Descripcin | Ejemplo | |-------|-------------|----------| | `titulo_documento` | Ttulo del documento | `"Cien aos de soledad"` | | `idioma_documento` | Cdigo ISO del idioma | `"es", "en", "fr"` | | `fecha_publicacion_documento` | Fecha de publicacin | `"1967-05-30"` | | `jerarquia_contextual` | Estructura del documento | `{"capitulo": "1", "seccion": "A"}` | ## Tipos de Segmento ### Vocabulario Controlado - **`parrafo`**: Prrafo de texto normal - **`titulo`**: Ttulos y encabezados - **`subtitulo`**: Subttulos y subencabezados - **`lista`**: Elementos de listas - **`cita`**: Citas textuales - **`nota`**: Notas al pie o marginales - **`tabla`**: Contenido de tablas - **`codigo`**: Bloques de cdigo - **`formula`**: Frmulas matemticas - **`referencia`**: Referencias bibliogrficas ### Ejemplos por Tipo ```json // Ttulo {"tipo_segmento":"titulo","texto_segmento":"Captulo 1: Los Fundamentos","jerarquia_contextual":{"capitulo":"1"}} // Prrafo {"tipo_segmento":"parrafo","texto_segmento":"El concepto de realidad mgica..."} // Lista {"tipo_segmento":"lista","texto_segmento":" Primer elemento de la lista"} // Cita {"tipo_segmento":"cita","texto_segmento":"\"La realidad supera a la ficcin\" - Garca Mrquez"} ``` ## Jerarqua Contextual ### Estructura Flexible La `jerarquia_contextual` se adapta al tipo de documento: **Libro acadmico:** ```json {"jerarquia_contextual": { "parte": "I", "capitulo": "3", "seccion": "3.2", "subseccion": "3.2.1" }} ``` **Artculo cientfico:** ```json {"jerarquia_contextual": { "seccion": "Metodologa", "subseccion": "Anlisis de datos" }} ``` **Novela:** ```json {"jerarquia_contextual": { "capitulo": "5" }} ``` ## Solucin de Problemas ### Error: "No se puede procesar el archivo" **Causa comn**: Archivo corrupto o formato no soportado **Solucin:** ```bash # Verificar que el archivo se puede abrir # Para PDF: python -c "import PyPDF2; print('PDF OK')" # Para DOCX: python -c "import docx; print('DOCX OK')" ``` ### Error: "Autor no detectado" **Causa**: El nombre del directorio no sigue la convencin **Solucin:** ```bash # Estructura correcta: raw_data/nombre_autor/documento.pdf # NO: raw_data/documentos/autor/documento.pdf ``` ### Error: "NDJSON malformado" **Verificacin:** ```bash # Validar NDJSON python -c " import json with open('processed_data/archivo.ndjson') as f: for i, line in enumerate(f): try: json.loads(line) except: print(f'Error en lnea {i+1}') " ``` ### Error: "Texto muy largo" **Causa**: Segmentos que exceden lmites de tokens **Solucin**: Ajustar parmetros de segmentacin en el script de procesamiento ## Optimizacin del Pipeline ### Procesamiento por Lotes ```bash # Procesar mltiples autores for autor in raw_data/*/; do echo "Procesando $autor" python scripts/app_depuracion.py --autor="$(basename "$autor")" done ``` ### Verificacin de Calidad ```bash # Contar segmentos por tipo grep -o '"tipo_segmento":"[^"]*"' processed_data/*.ndjson | sort | uniq -c # Verificar autores nicos grep -o '"autor_documento":"[^"]*"' processed_data/*.ndjson | sort | uniq # Estadsticas de longitud de texto grep -o '"texto_segmento":"[^"]*"' processed_data/*.ndjson | wc -c ``` ## Integracin con la Base de Datos Una vez generados los archivos NDJSON: ```bash # Importar a SQLite + Meilisearch cd ../backend/scripts python importar_completo.py # Verificar importacin python -c " import sqlite3 conn = sqlite3.connect('../data/biblioteca.db') print('Segmentos importados:', conn.execute('SELECT COUNT(*) FROM segmentos').fetchone()[0]) " ``` ## Mejores Prcticas 1. **Nombres de archivo**: Usa nombres descriptivos sin espacios 2. **Estructura de directorios**: Un directorio por autor 3. **Metadatos**: Incluye fechas y ttulos precisos cuando sea posible 4. **Verificacin**: Siempre valida el NDJSON antes de importar 5. **Backup**: Mantn copias de los archivos originales 6. **Versionado**: Usa nombres de archivo que incluyan versin si actualizas documentos ## Prximos Pasos Despus de generar NDJSON exitosamente: 1. **Importar**: `GUIA_GESTION_DATOS.md` 2. **Configurar bsqueda**: `GUIA_MEILISEARCH.md` 3. **Usar la aplicacin**: `INICIO_RAPIDO.md`
---

# IMPLEMENTACION PERFILES

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** docs

# IMPLEMENTACION PERFILES

# Implementacin del Sistema de Perfiles > Documento tcnico que describe la implementacin del sistema de perfiles para el procesamiento de documentos en Biblioperson. ## Introduccin Este documento detalla la implementacin tcnica del sistema de perfiles de procesamiento descrito en `docs/ESTRATEGIA_PROCESAMIENTO.md`. El sistema reemplaza los antiguos algoritmos monolticos por un pipeline modular y configurable mediante perfiles declarativos. ## Estructura de directorios ``` backend/ shared/ profiles/ # Mdulo principal __init__.py schema.py # Validacin de schema para perfiles profile_manager.py # Gestor centralizado loaders/ # Loaders para diferentes formatos __init__.py base.py docx_loader.py pdf_loader.py text_loader.py segmenters/ # Segmentadores por tipo de contenido __init__.py base.py verse_segmenter.py # Poemas/canciones heading_segmenter.py # Estructura de libros paragraph_segmenter.py message_segmenter.py post_processors/ # Filtrados y transformaciones __init__.py base.py text_normalizer.py exporters/ # Salida a diferentes formatos __init__.py base.py ndjson_exporter.py sqlite_exporter.py profiles/ # Perfiles YAML poem_or_lyrics.yaml book_structure.yaml generic_text.yaml json_messages.yaml ``` ## Diseo basado en mquina de estados Los segmentadores implementan mquinas de estado explcitas para modelar el procesamiento, lo que resulta en cdigo ms mantenible y menos propenso a errores lgicos. Por ejemplo, el `VerseSegmenter` utiliza los siguientes estados: ```python class VerseState(Enum): SEARCH_TITLE = 1 # Buscando ttulo de poema TITLE_FOUND = 2 # Ttulo encontrado, esperando versos iniciales COLLECTING_VERSE = 3 # Recolectando versos de un poema STANZA_GAP = 4 # En hueco entre estrofas END_POEM = 5 # Finalizando poema actual OUTSIDE_POEM = 6 # Fuera de poema ``` Cada estado tiene transiciones claras y predecibles basadas en las reglas definidas en `docs/ALGORITMOS_PROPUESTOS.md`, resultando en una implementacin que sigue esta estructura: ```python # Procesamiento basado en estados for bloque in bloques: if estado == VerseState.SEARCH_TITLE: # Lgica para buscar ttulo if [condiciones]: estado = VerseState.TITLE_FOUND elif estado == VerseState.TITLE_FOUND: # Lgica para verificar si lo que sigue al ttulo confirma que es un poema if [condiciones]: estado = VerseState.COLLECTING_VERSE # Otros estados... ``` ## Perfiles declarativos (YAML) Los perfiles se definen en archivos YAML, separando completamente la configuracin del cdigo. Ejemplo de perfil para poemas: ```yaml # backend/shared/profiles/profiles/poem_or_lyrics.yaml name: poem_or_lyrics description: "Detecta poemas y canciones en archivos de texto" segmenter: verse file_types: [".txt", ".md", ".docx", ".pdf"] thresholds: max_verse_length: 120 # Longitud mxima para considerar verso max_title_length: 80 # Longitud mxima para considerar ttulo max_space_ratio: 0.35 # Proporcin mxima de espacios/caracteres min_consecutive_verses: 3 # Mnimo de versos consecutivos para detectar poema min_stanza_verses: 2 # Mnimo de versos para formar estrofa max_consecutive_empty: 2 # Mximo de lneas vacas entre versos de misma estrofa title_patterns: # Patrones para detectar ttulos - "^# " - "^\\* " - "^> " - "^[A-Z ]{4,}:$" post_processor: text_normalizer post_processor_config: min_length: 30 # Longitud mnima para conservar unidad min_verses: 2 # Mnimo de versos para conservar poema metadata_map: # Mapeo de campos internos a nombres finales titulo: title versos: verses_count estrofas: stanzas_count exporter: ndjson ``` ## ProfileManager: Control centralizado El `ProfileManager` carga y gestiona los perfiles y componentes: ```python # Inicializacin profile_manager = ProfileManager("/ruta/a/perfiles") # Procesamiento de archivo units = profile_manager.process_file( file_path="mi_documento.docx", profile_name="poem_or_lyrics", output_path="resultados.ndjson" # Opcional ) ``` Internamente, el `ProfileManager` implementa el flujo completo: 1. Selecciona el loader adecuado segn la extensin del archivo 2. Carga el perfil y sus configuraciones 3. Instancia el segmenter apropiado con los thresholds del perfil 4. Aplica el post-processor especificado 5. Exporta los resultados si se proporciona output_path ## Implementacin del VerseSegmenter El `VerseSegmenter` es un ejemplo de implementacin basada en estados, que se gua por los algoritmos definidos en `docs/ALGORITMOS_PROPUESTOS.md`: ```python def segment(self, blocks: List[Dict[str, Any]]) -> List[Dict[str, Any]]: segments = [] state = VerseState.SEARCH_TITLE # Variables de contexto current_title = None current_verses = [] consecutive_empty = 0 for i, block in enumerate(blocks): # Determinar caractersticas del bloque actual text = block.get('text', '').strip() is_empty = not text if is_empty: consecutive_empty += 1 continue # Anlisis segn estado actual is_potential_title = self.has_title_format(block) is_verse = self.is_verse(text) # Mquina de estados para procesamiento if state == VerseState.SEARCH_TITLE: if is_potential_title: # Posible ttulo encontrado current_title = text state = VerseState.TITLE_FOUND elif is_verse: # Posible inicio de poema sin ttulo # ... # Otros estados y transiciones... ``` ### Caractersticas clave del VerseSegmenter: 1. **Deteccin de unidades**: Usa thresholds configurables para determinar qu es un verso, ttulo, etc. 2. **Preservacin de metadatos**: Genera informacin estructural como nmero de versos, estrofas, etc. 3. **Evaluacin de confianza**: Calcula un valor de confianza (0-1) para cada poema detectado. 4. **Look-ahead**: Implementa funciones de "mirada adelante" para confirmar patrones. ## Instrucciones para aadir nuevos componentes ### Creacin de un nuevo Loader Para soportar un nuevo formato, crea una subclase de `BaseLoader`: ```python # backend/shared/profiles/loaders/my_loader.py from typing import List, Dict, Any from .base import BaseLoader class MyCustomLoader(BaseLoader): @classmethod def supports_extension(cls, extension: str) -> bool: # Indica qu extensiones soporta este loader return extension.lower() in ['.custom', '.myformat'] def load(self, file_path: str) -> List[Dict[str, Any]]: # Implementar carga de archivo blocks = [] # ... Lgica para cargar el archivo y convertirlo en bloques ... return blocks ``` Registra el loader en el `ProfileManager`: ```python # En profile_manager.py o donde se use from .loaders.my_loader import MyCustomLoader profile_manager.register_loader('my_format', MyCustomLoader) ``` ### Creacin de un nuevo Segmenter Para implementar un nuevo algoritmo de segmentacin: ```python # backend/shared/profiles/segmenters/my_segmenter.py from enum import Enum from typing import List, Dict, Any from .base import BaseSegmenter class MyState(Enum): # Define tus estados aqu STATE_1 = 1 STATE_2 = 2 # ... class MySegmenter(BaseSegmenter): def __init__(self, config=None): super().__init__(config or {}) # Inicializar thresholds especficos def segment(self, blocks: List[Dict[str, Any]]) -> List[Dict[str, Any]]: # Implementar mquina de estados segments = [] state = MyState.STATE_1 # ... Lgica de segmentacin ... return segments ``` Registra el segmenter en el `ProfileManager`: ```python from .segmenters.my_segmenter import MySegmenter profile_manager.register_segmenter('my_algorithm', MySegmenter) ``` ### Creacin de un nuevo Perfil Crea un archivo YAML en `profiles/`: ```yaml # backend/shared/profiles/profiles/my_profile.yaml name: my_profile description: "Mi nuevo perfil de procesamiento" segmenter: my_algorithm file_types: [".custom", ".txt"] thresholds: # ... Parmetros para tu algoritmo ... post_processor: text_normalizer exporter: ndjson ``` ## Uso desde CLI El sistema incluye un script en `backend/scripts/process_file.py` para usarlo desde la lnea de comandos: ```bash python backend/scripts/process_file.py mi_archivo.docx --profile poem_or_lyrics --output resultados.ndjson ``` Para listar perfiles disponibles: ```bash python backend/scripts/process_file.py --list-profiles ``` ## Pruebas automatizadas Cada componente debe tener sus tests unitarios: ```python # tests/test_verse_segmenter.py def test_verse_recognition(): segmenter = VerseSegmenter({"thresholds": {"max_verse_length": 120}}) # Test casos bsicos assert segmenter.is_verse("Una lnea corta") assert not segmenter.is_verse("Una lnea muy larga " * 10) # Test deteccin de ttulos block = {"text": "# Ttulo", "is_heading": False} assert segmenter.has_title_format(block) ``` Para tests de integracin, usar fixtures de documentos reales. ## Conclusiones Esta implementacin ofrece notables ventajas: 1. **Mejor mantenibilidad**: Cdigo modular y mquinas de estado explcitas 2. **Flexibilidad**: Todos los umbrales configurables sin tocar cdigo 3. **Extensibilidad**: Fcil incorporacin de nuevos formatos y algoritmos 4. **Trazabilidad**: Mtricas de confianza y logs detallados 5. **Preservacin de formato**: Mantenimiento de metadatos crticos desde la fuente ## Referencias - [ALGORITMOS_PROPUESTOS.md](./ALGORITMOS_PROPUESTOS.md) - Algoritmos y reglas base - [ESTRATEGIA_PROCESAMIENTO.md](./ESTRATEGIA_PROCESAMIENTO.md) - Estrategia general - [ALGORITMOS_ANTERIORES](./ALGORITMOS_ANTERIORES) - Algoritmos histricos
---

# GUIA LOADERS

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** docs

# GUIA LOADERS

# Gua de Loaders - Biblioperson ## Introduccin Los loaders son componentes fundamentales del sistema Biblioperson que se encargan de leer archivos en diferentes formatos y convertirlos en una estructura de datos comn para su procesamiento. Esta gua detalla todos los loaders disponibles y cmo utilizarlos. ##  Arquitectura de Loaders ### Concepto Base Todos los loaders heredan de la clase `BaseLoader` y proporcionan un mtodo `load()` que devuelve un iterador de documentos. Esta arquitectura permite: - **Procesamiento uniforme** de diferentes formatos - **Extensibilidad** para nuevos tipos de archivo - **Consistencia** en la estructura de datos de salida - **Eficiencia** mediante procesamiento por streaming ### Estructura de Datos Comn Cada loader produce documentos con la siguiente estructura: ```python { 'text': "Contenido textual del segmento", 'is_heading': False, # True si es un ttulo/encabezado 'fuente': "nombre_autor", # Extrado del directorio padre 'contexto': "ruta/completa/al/archivo.ext", # Ruta completa del archivo 'fecha': "2023-12-01" # Fecha extrada o inferida } ``` ##  Loaders Disponibles ###  txtLoader **Propsito**: Procesa archivos de texto plano (`.txt`) **Caractersticas especiales**: - **Deteccin automtica de ttulo**: Si la primera lnea est separada del resto - **Extraccin de fecha**: Del nombre del archivo con formatos reconocibles - **Deteccin de tipo**: Distingue entre poemas y prosa usando heursticas - **Procesamiento lnea por lnea**: Mantiene estructura original **Ejemplo de uso**: ```python from dataset.processing.loaders import txtLoader loader = txtLoader("ruta/al/archivo.txt", tipo='escritos', encoding='utf-8') for documento in loader.load(): print(f"Texto: {documento['text'][:50]}...") print(f"Es ttulo: {documento['is_heading']}") ``` **Casos de uso ideales**: - Archivos de texto simple - Poemas y canciones - Notas y apuntes - Transcripciones ###  MarkdownLoader **Propsito**: Procesa archivos Markdown (`.md`, `.markdown`) **Caractersticas especiales**: - **Preserva estructura Markdown**: Mantiene formato y jerarqua - **Segmentacin inteligente**: Segn tipo de documento (escritos, poemas, canciones) - **Extraccin de metadatos**: Fecha del nombre o fecha de modificacin - **Deteccin de encabezados**: Identifica ttulos y subttulos automticamente **Ejemplo de uso**: ```python from dataset.processing.loaders import MarkdownLoader loader = MarkdownLoader("ruta/al/archivo.md", tipo='escritos', encoding='utf-8') for documento in loader.load(): if documento['is_heading']: print(f" Ttulo: {documento['text']}") else: print(f" Contenido: {documento['text'][:100]}...") ``` **Casos de uso ideales**: - Documentacin tcnica - Artculos y ensayos - Libros estructurados - Contenido web ###  NDJSONLoader **Propsito**: Procesa archivos NDJSON (JSON por lnea) **Caractersticas especiales**: - **Procesamiento lnea por lnea**: Cada lnea es un JSON independiente - **Compatibilidad flexible**: Diferentes estructuras de datos JSON - **Deteccin automtica de contenido**: Busca campos `texto`/`text`/`content` - **Preservacin de metadatos**: Mantiene campos adicionales del JSON original **Ejemplo de uso**: ```python from dataset.processing.loaders import NDJSONLoader loader = NDJSONLoader("ruta/al/archivo.ndjson", tipo='escritos', encoding='utf-8') for documento in loader.load(): print(f"Procesado: {documento['text'][:50]}...") ``` **Casos de uso ideales**: - Datos de redes sociales (Twitter, Telegram) - Logs estructurados - Datasets existentes - Mensajes y conversaciones ###  DocxLoader **Propsito**: Procesa documentos Microsoft Word (`.docx`) **Caractersticas especiales**: - **Extraccin de formato**: Preserva estructura de ttulos y secciones - **Procesamiento de estilos**: Detecta negrita, itlica, etc. - **Metadatos del documento**: Extrae fecha de propiedades del archivo - **Jerarqua de encabezados**: Identifica niveles de ttulos automticamente **Ejemplo de uso**: ```python from dataset.processing.loaders import DocxLoader loader = DocxLoader("ruta/al/archivo.docx", tipo='escritos', encoding='utf-8') for documento in loader.load(): nivel = "" if documento['is_heading'] else "" print(f"{nivel} {documento['text'][:80]}...") ``` **Casos de uso ideales**: - Documentos acadmicos - Informes y reportes - Libros y manuscritos - Documentacin corporativa ###  PDFLoader **Propsito**: Procesa documentos PDF (`.pdf`) **Caractersticas especiales**: - **Extraccin de texto**: Mantiene estructura bsica de prrafos - **Metadatos PDF**: Procesa ttulo, autor y propiedades del documento - **Referencia de pgina**: Mantiene informacin de ubicacin - **Manejo de formato**: Utiliza PyPDF2 para extraccin robusta **Ejemplo de uso**: ```python from dataset.processing.loaders import PDFLoader loader = PDFLoader("ruta/al/archivo.pdf", tipo='escritos', encoding='utf-8') for documento in loader.load(): print(f" Pgina: {documento.get('page', 'N/A')}") print(f" Texto: {documento['text'][:100]}...") ``` **Casos de uso ideales**: - Libros digitales - Artculos cientficos - Documentos oficiales - Manuales tcnicos ###  ExcelLoader **Propsito**: Procesa hojas de clculo Excel (`.xlsx`, `.xls`, `.xlsm`) **Caractersticas especiales**: - **Mltiples hojas**: Procesa todas las hojas del archivo - **Estructura tabular**: Convierte filas y columnas a texto legible - **Encabezados como ttulos**: Usa nombres de hojas y columnas - **Formato estructurado**: Convierte datos tabulares a narrativa **Ejemplo de uso**: ```python from dataset.processing.loaders import ExcelLoader loader = ExcelLoader("ruta/al/archivo.xlsx", tipo='escritos', encoding='utf-8') for documento in loader.load(): if documento['is_heading']: print(f" Hoja/Columna: {documento['text']}") else: print(f" Datos: {documento['text']}") ``` **Casos de uso ideales**: - Datos financieros - Inventarios y catlogos - Listas y registros - Anlisis y reportes ###  CSVLoader **Propsito**: Procesa archivos CSV y TSV (`.csv`, `.tsv`) **Caractersticas especiales**: - **Deteccin automtica**: Identifica delimitadores (coma, punto y coma, tabulacin) - **Encabezados inteligentes**: Procesa headers como ttulos - **Formato legible**: Convierte filas a formato "campo: valor" - **Codificacin flexible**: Manejo inteligente de diferentes encodings **Ejemplo de uso**: ```python from dataset.processing.loaders import CSVLoader loader = CSVLoader("ruta/al/archivo.csv", tipo='escritos', encoding='utf-8') for documento in loader.load(): print(f" Registro: {documento['text']}") ``` **Casos de uso ideales**: - Bases de datos exportadas - Logs de aplicaciones - Datos de investigacin - Registros histricos ##  ProfileManager ### Qu es el ProfileManager? El `ProfileManager` es el componente que facilita la seleccin automtica del loader adecuado basado en: - **Extensin del archivo** - **Perfil de procesamiento especfico** - **Configuracin de contenido** - **Umbrales de confianza** ### Uso Bsico ```python from dataset.processing.profile_manager import ProfileManager manager = ProfileManager() # Listar perfiles disponibles perfiles = manager.list_profiles() print("Perfiles disponibles:", perfiles) # Procesar archivo automticamente resultados = manager.process_file( "ruta/al/archivo.txt", profile_name="book_structure", encoding="utf-8", force_content_type="escritos", # Opcional: forzar tipo confidence_threshold=0.5 # Umbral para deteccin automtica ) for resultado in resultados: print(f"Procesado: {resultado['text'][:50]}...") ``` ### Configuracin Avanzada ```python # Configurar perfil personalizado manager.configure_profile("mi_perfil", { "segmentation_strategy": "paragraph", "min_segment_length": 50, "preserve_formatting": True, "extract_metadata": True }) # Procesar con perfil personalizado resultados = manager.process_file( "mi_documento.docx", profile_name="mi_perfil" ) ``` ##  Extensin del Sistema ### Crear un Nuevo Loader Para aadir soporte para un nuevo formato: #### 1. Crear la Clase Loader ```python from pathlib import Path from typing import Iterator, Dict, Any, Optional from .base_loader import BaseLoader class MiNuevoLoader(BaseLoader): """Loader para formato personalizado.""" def __init__(self, file_path: str | Path, tipo: str = 'escritos', encoding: str = 'utf-8'): super().__init__(file_path) self.tipo = tipo.lower() self.encoding = encoding def load(self) -> Iterator[Dict[str, Any]]: fuente, contexto = self.get_source_info() # Implementacin especfica para tu formato with open(self.file_path, 'r', encoding=self.encoding) as file: content = file.read() # Procesar contenido segn tu lgica segments = self._process_content(content) for i, segment in enumerate(segments): yield { 'text': segment['text'], 'is_heading': segment.get('is_heading', False), 'fuente': fuente, 'contexto': contexto, 'fecha': self._extract_date(), 'segment_order': i + 1 } def _process_content(self, content: str) -> List[Dict[str, Any]]: """Lgica especfica de procesamiento.""" # Implementar segn el formato pass def _extract_date(self) -> Optional[str]: """Extraer fecha del archivo o metadatos.""" # Implementar extraccin de fecha pass ``` #### 2. Registrar el Loader ```python # En __init__.py del mdulo loaders from .mi_nuevo_loader import MiNuevoLoader __all__ = [ 'txtLoader', 'MarkdownLoader', 'NDJSONLoader', 'DocxLoader', 'PDFLoader', 'ExcelLoader', 'CSVLoader', 'MiNuevoLoader' # Aadir aqu ] ``` #### 3. Configurar en ProfileManager ```python # En ProfileManager.register_default_components() self.register_loader('.mi_extension', MiNuevoLoader) ``` ### Mejores Prcticas para Nuevos Loaders 1. **Herencia consistente**: Siempre heredar de `BaseLoader` 2. **Manejo de errores**: Implementar try/catch para archivos corruptos 3. **Encoding flexible**: Soportar diferentes codificaciones 4. **Metadatos ricos**: Extraer toda la informacin disponible 5. **Segmentacin inteligente**: Adaptar segn el tipo de contenido 6. **Documentacin completa**: Incluir ejemplos y casos de uso ##  Solucin de Problemas ### Error: "Loader no encontrado" **Causa**: Extensin de archivo no registrada **Solucin**: ```python # Verificar loaders registrados manager = ProfileManager() print(manager.get_registered_loaders()) # Registrar manualmente si es necesario manager.register_loader('.mi_ext', MiLoader) ``` ### Error: "Encoding no soportado" **Causa**: Archivo con codificacin especial **Solucin**: ```python # Detectar encoding automticamente import chardet with open('archivo.txt', 'rb') as f: raw_data = f.read() encoding = chardet.detect(raw_data)['encoding'] print(f"Encoding detectado: {encoding}") # Usar encoding especfico loader = txtLoader('archivo.txt', encoding=encoding) ``` ### Error: "Archivo muy grande" **Causa**: Archivo excede memoria disponible **Solucin**: ```python # Procesar en chunks def process_large_file(file_path, chunk_size=1024*1024): # 1MB chunks with open(file_path, 'r', encoding='utf-8') as f: while True: chunk = f.read(chunk_size) if not chunk: break # Procesar chunk yield process_chunk(chunk) ``` ### Error: "Formato no reconocido" **Causa**: Archivo con formato interno inesperado **Solucin**: ```python # Validar formato antes de procesar def validate_file_format(file_path): try: # Intentar abrir con el loader esperado loader = get_loader_for_file(file_path) first_doc = next(loader.load()) return True except Exception as e: print(f"Formato no vlido: {e}") return False ``` ##  Recursos Adicionales ### Documentacin Relacionada - [**Pipeline NDJSON**](PIPELINE_NDJSON.md) - Flujo completo de procesamiento - [**Especificacin NDJSON**](NDJSON_ESPECIFICACION.md) - Formato de salida detallado - [**Gestin de Datos**](GUIA_GESTION_DATOS.md) - Importacin a base de datos ### Ejemplos Prcticos ```python # Procesamiento por lotes def process_directory(directory_path, file_pattern="*"): from pathlib import Path directory = Path(directory_path) manager = ProfileManager() for file_path in directory.glob(file_pattern): if file_path.is_file(): print(f"Procesando: {file_path}") try: results = manager.process_file(str(file_path)) for result in results: # Procesar resultado save_to_ndjson(result) except Exception as e: print(f"Error procesando {file_path}: {e}") # Uso process_directory("raw_data/mi_autor/", "*.txt") ``` ### Configuracin de Perfiles ```python # Perfil para libros acadmicos academic_profile = { "segmentation_strategy": "heading_based", "min_segment_length": 100, "preserve_citations": True, "extract_footnotes": True, "heading_levels": [1, 2, 3, 4] } # Perfil para poesa poetry_profile = { "segmentation_strategy": "stanza_based", "preserve_line_breaks": True, "detect_meter": True, "whole_file_as_segment": True } # Perfil para redes sociales social_profile = { "segmentation_strategy": "message_based", "extract_hashtags": True, "extract_mentions": True, "preserve_timestamps": True } ``` Esta gua proporciona todo lo necesario para entender, usar y extender el sistema de loaders de Biblioperson. Para casos especficos o problemas no cubiertos, consulta la documentacin tcnica adicional o el cdigo fuente de los loaders existentes.
---

# NDJSON ESPECIFICACION

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** docs

# NDJSON ESPECIFICACION

# Especificacin del Formato NDJSON Enriquecido para Biblioperson Este documento detalla la estructura y los campos requeridos para los archivos NDJSON generados por el pipeline ETL de Biblioperson. Esta especificacin corresponde a la Tarea #23 del proyecto. ## Estructura General Cada lnea en el archivo NDJSON representa un "segmento" de texto extrado de un documento fuente. Un segmento puede ser un prrafo, un ttulo, una nota al pie, etc. ## Campos Detallados La especificacin del NDJSON enriquecido debe incluir, como mnimo: ### 1. Identificadores Unvocos * **`id_segmento`**: (UUID string) UUID nico para cada segmento de texto. Generado durante el proceso ETL. * **`id_documento_fuente`**: (string) Identificador nico para el documento original del que proviene el segmento. Puede ser un hash del archivo original o un UUID asignado al documento durante su primera ingesta. ### 2. Metadatos del Documento Fuente Estos metadatos se replican en cada segmento para facilitar consultas y evitar joins constantes en la base de datos o al procesar el NDJSON. * **`ruta_archivo_original`**: (string) Ruta completa al archivo original en el sistema de archivos donde fue procesado. * **`hash_documento_original`**: (string) Hash (ej. SHA256) del archivo original. til para control de versiones y deteccin de duplicados. * **`titulo_documento`**: (string) Ttulo del documento, extrado de los metadatos del archivo o inferido. * **`autor_documento`**: (string) Autor(es) del documento. * **`fecha_publicacion_documento`**: (string) Fecha de publicacin original del documento (formato YYYY-MM-DD o YYYY). * **`editorial_documento`**: (string, Opcional) Editorial del documento. * **`isbn_documento`**: (string, Opcional) ISBN del documento. * **`idioma_documento`**: (string) Cdigo ISO 639-1 (ej. "es", "en") del idioma principal del documento. * **`metadatos_adicionales_fuente`**: (object, Opcional) Objeto JSON para cualquier otro metadato relevante extrado del archivo fuente (ej., propiedades de DOCX, metadatos de PDF, etc.). ### 3. Metadatos del Segmento * **`texto_segmento`**: (string) El contenido textual del segmento. * **`tipo_segmento`**: (string) Vocabulario controlado que define la naturaleza del segmento. Ver lista abajo. * **`orden_segmento_documento`**: (integer) Nmero secuencial del segmento dentro del documento fuente completo (global, 0-indexed o 1-indexed a definir). * **`jerarquia_contextual`**: (object) Objeto JSON que describe la posicin del segmento en la estructura jerrquica del documento. (Ver especificacin detallada en `JERARQUIA_CONTEXTUAL_ESPECIFICACION.md`, corresponde a Tarea #26). Ejemplo: `{"capitulo": "1", "seccion": "3", "parrafo_num": "5"}`. * **`longitud_caracteres_segmento`**: (integer) Nmero de caracteres en `texto_segmento`. * **`embedding_vectorial`**: (array de floats, Opcional) El vector de embedding del `texto_segmento`. Puede generarse en una etapa posterior al ETL inicial. ### 4. Metadatos del Proceso ETL * **`timestamp_procesamiento`**: (string) Fecha y hora en formato ISO 8601 (ej. `YYYY-MM-DDTHH:MM:SSZ`) de cuando el segmento fue procesado. * **`version_pipeline_etl`**: (string) Versin del pipeline ETL que gener el segmento (ej. "1.2.0"). * **`nombre_segmentador_usado`**: (string) Nombre del segmentador especfico que produjo este segmento (ej. "segmentador_prosa_general_v1"). ### Vocabulario Controlado para `tipo_segmento` (Inicial) Esta lista es inicial y puede expandirse: * `encabezado_h1` * `encabezado_h2` * `encabezado_h3` * `encabezado_h4` * `encabezado_h5` * `encabezado_h6` * `parrafo` * `item_lista_ordenada` * `item_lista_desordenada` * `cita_bloque` * `nota_al_pie` * `nota_al_final` * `celda_tabla_encabezado` * `celda_tabla_dato` * `titulo_tabla` * `descripcion_figura` * `epigrafe` * `verso_poema` * `encabezado_pagina` (header) * `pie_pagina` (footer) * `desconocido` (para contenido que no pudo ser clasificado) ## Ejemplo de un Registro NDJSON ```json { "id_segmento": "f47ac10b-58cc-4372-a567-0e02b2c3d479", "id_documento_fuente": "sha256-deadbeefcafe00112233445566778899aabbccddeeff", "ruta_archivo_original": "/mnt/docs_a_procesar/filosofia/Cosmologas de India.pdf", "hash_documento_original": "sha256-deadbeefcafe00112233445566778899aabbccddeeff", "titulo_documento": "Cosmologas de India: Vdica, Smkhya y Budista", "autor_documento": "Ernesto Aroldo Ceballos", "fecha_publicacion_documento": "2017-01-01", "editorial_documento": "Universidad Nacional de Crdoba", "isbn_documento": null, "idioma_documento": "es", "metadatos_adicionales_fuente": {"palabras_clave": ["filosofa", "india", "cosmologa"], "fuente_scan": "Archivo personal del autor"}, "texto_segmento": "El concepto de my es central en la Advaita Vednta, representando la ilusin csmica que vela la realidad ltima (Brahman).", "tipo_segmento": "parrafo", "orden_segmento_documento": 152, "jerarquia_contextual": {"capitulo": "3", "titulo_capitulo": "La No-Dualidad y la Ilusin", "subseccion": "2", "titulo_subseccion": "My y la Creacin Aparente", "parrafo_num": "4"}, "longitud_caracteres_segmento": 125, "embedding_vectorial": null, "timestamp_procesamiento": "2025-05-17T18:50:00Z", "version_pipeline_etl": "1.2.0", "nombre_segmentador_usado": "segmentador_prosa_general_v1" } ``` ## Consideraciones Adicionales * Todos los campos de texto deben estar en UTF-8. * Las fechas deben seguir el formato ISO 8601 siempre que sea posible. * La estructura de `jerarquia_contextual` se detallar en `JERARQUIA_CONTEXTUAL_ESPECIFICACION.md`.
---

# ESTRATEGIA PROCESAMIENTO

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** docs

# ESTRATEGIA PROCESAMIENTO

# Estrategia de Procesamiento: Refactorizacin 2023-2024 > **Documento tcnico:** Establece la estrategia de refactorizacin para el sistema de procesamiento de documentos de Biblioperson. ## 1. Estado actual y limitaciones detectadas El cdigo actual en `dataset/scripts/processors.py` contiene tres funciones principales que manejan la segmentacin de documentos: | Funcin | Tipo de texto | Heurstica de segmentacin | |---------|---------------|----------------------------| | `segmentar_poema_cancion` | Poemas/canciones | Detecta versos cortos consecutivos, ttulos en Markdown (`#...`) o lneas cortas como encabezados | | `segmentar_libro` | Libros/escritos largos | Busca encabezados Markdown (`#`, `##`), o patrones "lnea corta  prrafo largo" | | *(Sin nombre fijo)* | Mensajes/JSON de redes | No segmenta realmente: procesa cada mensaje JSON como unidad independiente | ### Problemas detectados 1. **Cdigo monoltico:** Lgica compleja con muchas condiciones anidadas, difcil de mantener y probar. 2. **Parmetros hard-coded:** Umbrales fijos (135 caracteres para versos, 80 para ttulos, 180 para prrafos) que no se adaptan a todos los casos. 3. **Prdida de estilos en conversin:** Al convertir documentos con Pandoc se pierden metadatos valiosos (encabezados Word, negritas, etc.). 4. **Dificultad para extender:** Aadir un nuevo algoritmo implica modificar un archivo monoltico. 5. **Sin aprovechamiento de formatos:** No se explotan las caractersticas estructurales de DOCX, PDF, etc. 6. **Pruebas limitadas:** Sin framework sistemtico de testing para validar algoritmos. ## 2. Nueva arquitectura propuesta Proponemos una arquitectura modular con cuatro componentes principales: ``` Documento  [LOADER]  bloques  [SEGMENTER]  unidades  [POST-PROCESSOR]  [EXPORTER]  NDJSON/DB ``` ### 2.1. Loader (extraccin de contenido) Responsable de **convertir cualquier formato a bloques de texto estructurados**, preservando metadatos crticos: - **Input:** Archivo (PDF, DOCX, TXT, MD, PPTX, JSON, etc.) - **Output:** Lista de bloques con metadatos (estilo, nivel de encabezado, pgina, etc.) - **Tecnologas:** - PDF  `pdfminer.six` o `PyMuPDF` - DOCX  `python-docx` (extrae estilos **antes** de convertir) - JSON  `ijson` para archivos grandes - Binarios no soportados  `apache-tika` o `unstructured` ```python # Ejemplo conceptual class DocxLoader(BaseLoader): def load(self, filepath): doc = Document(filepath) blocks = [] for paragraph in doc.paragraphs: blocks.append({ "text": paragraph.text, "style": paragraph.style.name, # Preserva "Heading 1", "Title", etc. "is_bold": any(run.bold for run in paragraph.runs), "is_heading": paragraph.style.name.startswith("Heading"), "heading_level": extract_heading_level(paragraph) }) return blocks ``` > **CRTICO:** Los loaders deben preservar informacin de estilo/formato para alimentar las heursticas de los segmenters. ### 2.2. Segmenter (definicin de unidad semntica) Define qu constituye una "unidad documental" (poema, prrafo, captulo, mensaje) segn reglas: - **Input:** Lista de bloques con metadatos - **Output:** Lista de unidades documentales discretas - **Interfaz comn:** ```python # dataset/segmenters/base.py class Segmenter: name: str # "poem", "paragraph", etc. config: dict # Parmetros cargados de config.json def segment(self, blocks: list[dict]) -> list[dict]: """Divide los bloques en unidades semnticas.""" ... ``` #### Implementaciones base (mnimo 4) | Segmentador | Uso | Heurstica principal | |-------------|-----|----------------------| | `VerseSegmenter` | Poemas/canciones | Lneas  120 chars o alta proporcin de espacios +  2 lneas similares seguidas | | `HeadingSegmenter` | Libros/escritos | Corta en encabezados (estilo Word "Heading 1-3" o markdown `#`) | | `ParagraphSegmenter` | Artculos, blogs | Doble salto (`\n\n`). Une prrafos pequeos hasta umbral configurable | | `MessageSegmenter` | JSON/feeds | Cada objeto del array es una unidad sin modificar | ### 2.3. Post-processor (filtrado y enriquecimiento) Aplicacin de filtros y transformaciones genricas a las unidades: - Filtrado por longitud mnima - Deteccin de idioma - Deduplicacin - Normalizacin Unicode - Clculo de estadsticas (tokens, frases, etc.) ### 2.4. Exporter (serializacin y almacenamiento) Volcado de las unidades procesadas a destinos configurable: - NDJSON (formato histrico) - SQLite - Meilisearch - MongoDB, etc. ## 3. Configuracin declarativa de perfiles Los perfiles se definirn en archivos YAML/JSON, no en cdigo: ```yaml # dataset/config/profiles/poem_or_lyrics.yaml name: poem_or_lyrics segmenter: VerseSegmenter file_types: [".txt", ".md", ".docx", ".pdf"] params: max_verse_length: 120 # Longitud mxima para considerar verso max_title_length: 80 # Longitud mxima para considerar ttulo max_space_ratio: 0.35 # Proporcin mxima de espacios/caracteres min_consecutive_verses: 2 # Mnimo de versos consecutivos para detectar poema stanza_separator: "\n\n" # Patrn que separa estrofas title_patterns: # Patrones para detectar ttulos - "^# " - "^\\* " - "^> " - "^[A-Z ]{4,}:$" post_processor: min_length: 30 # Longitud mnima para conservar unidad min_verses: 2 # Mnimo de versos para conservar poema metadata_map: titulo: detected_title versos: verses_count estrofas: stanzas_count ``` El sistema cargar dinmicamente el segmentador apropiado segn el perfil elegido: ```python # Mapa de perfiles a segmentadores en config.json { "profiles": { "poemas": "VerseSegmenter", "canciones": "VerseSegmenter", "libros": "HeadingSegmenter", "escritos": "ParagraphSegmenter", "tweets": "MessageSegmenter", "telegram": "MessageSegmenter" } } ``` ## 4. Mejoras especficas por algoritmo/segmentador ### 4.1. Mejoras para VerseSegmenter (poemas) ```python def is_verse(line: str, config) -> bool: """Determina si una lnea es un verso segn heursticas configurables.""" stripped = line.rstrip() return ( len(stripped) <= config["max_verse_length"] or spaces_ratio(stripped) >= config["max_space_ratio"] ) def segment(self, blocks: list[dict]) -> list[dict]: """Algoritmo mejorado para detectar poemas.""" segments = [] current_verses = [] for block in blocks: # Usa estilo si est disponible if block.get("is_heading") or block.get("style") == "Title": # Cerrar poema actual si existe if current_verses: segments.append({ "type": "poem", "verses": current_verses, "stanzas": count_stanzas(current_verses) }) current_verses = [] # Guardar ttulo como unidad separada segments.append({ "type": "title", "text": block["text"] }) continue # Verificar si es verso por longitud/espaciado if is_verse(block["text"], self.config): current_verses.append(block["text"]) # Si no es verso y hay acumulados, cerrar poema elif current_verses: segments.append({ "type": "poem", "verses": current_verses, "stanzas": count_stanzas(current_verses) }) current_verses = [] # El bloque actual inicia posible nuevo segmento if len(block["text"]) > 0: segments.append({ "type": "paragraph", "text": block["text"] }) # No olvidar ltimo poema si existe if current_verses: segments.append({ "type": "poem", "verses": current_verses, "stanzas": count_stanzas(current_verses) }) return segments ``` ### 4.2. Mejoras para HeadingSegmenter (libros) - Extraer estilos DOCX con `python-docx` antes de conversin - Usar informacin de jerarqua (nivel de encabezado) para mantener estructura - Algoritmo mejorado para deteccin de captulos en texto plano: ``` 1. Buscar lneas cortas (<80 chars) seguidas de 2+ prrafos largos 2. Verificar si la lnea corta tiene formato de ttulo o separador 3. Mantener jerarqua librocaptuloseccinprrafo ``` ### 4.3. Evaluacin automtica de segmentadores Propuesta para validar objetivamente las mejoras: 1. Crear dataset anotado manualmente (gold standard) 2. Mtricas: - F1-score: precisin/recall de puntos de corte - Error de segmentacin: distancia entre cortes reales y predichos 3. Script comparativo: ``` python -m segmenters.evaluate tests/gold_corpus.ndjson --profile=poems ``` ## 5. Plan de implementacin ### Fase 1: Preparacin (Semana 1) 1. Crear estructura de directorios: ``` dataset/ segmenters/ __init__.py base.py # Clase base Segmenter verse.py # VerseSegmenter heading.py # HeadingSegmenter paragraph.py # ParagraphSegmenter message.py # MessageSegmenter loaders/ __init__.py base.py docx.py pdf.py text.py json.py config/ profiles/ poem_or_lyrics.yaml book_structure.yaml article.yaml json_messages.yaml segmenters_config.json # Mapa perfilsegmentador tests/ fixtures/ # Ejemplos documentados test_verse.py test_heading.py ... ``` 2. Migrar lgica actual a `dataset/segmenters/legacy.py` (sin cambios) ### Fase 2: Base y prueba de concepto (Semana 2) 1. Implementar interfaces base: - `BaseLoader` - `BaseSegmenter` - `BasePostProcessor` - `BaseExporter` 2. Implementar primera versin de `VerseSegmenter` 3. Crear tests unitarios con casos sencillos 4. Validar equivalencia con funcin original `segmentar_poema_cancion` ### Fase 3: Implementacin incremental (Semanas 3-5) 1. Implementar dems segmentadores, uno a uno: - `HeadingSegmenter` (libros) - `ParagraphSegmenter` (artculos) - `MessageSegmenter` (JSON) 2. Aadir loaders por formato: - `DocxLoader` + preservacin de estilos - `PdfLoader` + deteccin de estructura - `JsonLoader` con streaming para archivos grandes 3. Integracin con la aplicacin principal ### Fase 4: Refinamiento y transicin (Semanas 6-8) 1. Completar suite de tests (cobertura >90%) 2. Script de migracin para configuraciones antiguas 3. Documentacin de usuario final (cmo crear perfiles personalizados) 4. Deprecacin de cdigo legacy ## 6. Consideraciones tcnicas importantes ### 6.1. Preservacin de estilos **CRTICO:** El sistema actual convierte todo a texto plano, perdiendo informacin valiosa de formato. Solucin: 1. Extraer metadatos de estilo **antes** de la conversin Pandoc 2. Para DOCX: usar `python-docx` y generar CSV auxiliar: ```csv line_number,text,style_name,is_bold,is_italic,is_heading,heading_level 1,"Mi ttulo",Heading1,True,False,True,1 2,"Primer prrafo",Normal,False,False,False,0 ``` 3. Para PDF: usar `PyMuPDF` para extraer tamao de fuente y estilo ### 6.2. Capacidad de extensin El sistema debe ser extensible sin modificar cdigo base: - Segmentadores registrados va entrypoints en `setup.cfg`: ```ini [options.entry_points] biblioperson.segmenters = verse = dataset.segmenters.verse:VerseSegmenter heading = dataset.segmenters.heading:HeadingSegmenter paragraph = dataset.segmenters.paragraph:ParagraphSegmenter message = dataset.segmenters.message:MessageSegmenter # Aadir nuevos segmentadores aqu ``` - Nuevos perfiles creados solo con YAML, sin tocar cdigo ### 6.3. Pruebas automticas Cada segmentador debe tener su suite de pruebas: ```python # tests/test_verse.py def test_basic_poem_detection(): segmenter = VerseSegmenter(config={"max_verse_length": 120, ...}) blocks = [ {"text": "TTULO DEL POEMA", "is_heading": True}, {"text": "Primera lnea del poema", "is_heading": False}, {"text": "Segunda lnea corta", "is_heading": False}, {"text": "", "is_heading": False}, {"text": "Nueva estrofa aqu", "is_heading": False}, ] result = segmenter.segment(blocks) assert len(result) == 2 # Ttulo + poema assert result[0]["type"] == "title" assert result[1]["type"] == "poem" assert len(result[1]["verses"]) == 3 assert result[1]["stanzas"] == 2 ``` ## 7. Beneficios esperados 1. **Mantenibilidad:** Cdigo modular, bien estructurado y testeado 2. **Flexibilidad:** Perfiles configurables sin cambiar cdigo 3. **Calidad:** Mejor deteccin de unidades semnticas 4. **Extensibilidad:** Fcil aadir nuevos formatos y algoritmos 5. **Rendimiento:** Procesamiento paralelo de documentos grandes 6. **Trazabilidad:** Mejor logging y diagnstico ## 8. Prximos pasos inmediatos 1. Crear repositorio de prueba con estructura propuesta 2. Implementar primer segmentador (`VerseSegmenter`) y tests 3. Validar con conjunto representativo de documentos reales 4. Iterar y refinar antes de implementacin completa --- > Documento generado: [Fecha]. Este documento debe ser revisado y actualizado segn avance la implementacin.
---

# BIBLIOPERSON ARQUITECTURA

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** docs

# BIBLIOPERSON ARQUITECTURA

# Arquitectura del Proyecto Biblioperson **ltima actualizacin:** (dd-mm-aaaa) ## Introduccin Este documento describe en detalle la arquitectura del proyecto "Biblioperson". Su propsito es servir como referencia tcnica centralizada para el equipo de desarrollo y para la asistencia de Inteligencia Artificial (IA) en tareas de desarrollo, depuracin y optimizacin. Proporciona una comprensin profunda de los componentes del sistema, sus interacciones, las tecnologas empleadas y los flujos de datos. --- ## 1. Visin General del Proyecto ### 1.1. Nombre del Proyecto Biblioperson ### 1.2. Objetivo Fundamental (Visin a Largo Plazo) Crear una aplicacin robusta que permita a los usuarios realizar bsquedas exhaustivas (tanto por palabras clave como semnticas) dentro de una vasta coleccin de textos indexados. Estos textos pueden provenir de diversos autores y formatos de archivo (Word, TXT, PDF, Markdown, etc.). La finalidad ltima es: * Poder interrogar la base de conocimiento para entender qu podra pensar un autor especfico sobre un tema determinado. * Analizar qu se puede inferir de la biblioteca completa de autores sobre un tema particular. * Facilitar la creacin de contenido original y atractivo para redes sociales, partiendo de las ideas fundamentales encontradas, adaptando el estilo para maximizar el *engagement* y el crecimiento de seguidores. ### 1.3. Objetivo de la Fase Actual de Desarrollo (Enfoque Inmediato) * Lograr que la aplicacin permita bsquedas efectivas por palabras y frases exactas en todo el corpus de contenido indexado. * Asegurar que el proceso de indexacin pueda manejar de manera inteligente diversos tipos de documentos alojados en una estructura de carpetas, identificando elementos estructurales como ttulos, poemas, canciones, o prrafos dentro de captulos de libros. * Implementar bsquedas rpidas y, crucialmente, bsquedas semnticas donde el significado y la intencin de la bsqueda prevalezcan sobre la coincidencia literal de palabras. --- ## 2. Pila Tecnolgica Principal * **Backend:** Python 3.8+, Flask (para la API REST). * **Frontend:** React, Vite, JavaScript/TypeScript, Tailwind CSS (para la Interfaz de Usuario SPA). * **Base de Datos:** SQLite (para almacenamiento de metadatos, contenido procesado y embeddings). * **Motor de Bsqueda e Indexacin:** Meilisearch (versin >= 1.3, con `vectorStore` habilitado para bsqueda vectorial). * **Generacin de Embeddings:** Modelos de `sentence-transformers` (ej. `paraphrase-multilingual-mpnet-base-v2`, generando vectores de 768 dimensiones). * **Procesamiento y Conversin de Documentos:** Pandoc, Python-Textract, y scripts personalizados en Python. * **Gestin de Dependencias Python:** `requirements.txt` (gestionado con `pip`). * **Gestin de Dependencias Node.js:** `package.json` (gestionado con `npm` o `yarn`). --- ## 3. Arquitectura Detallada del Sistema ### 3.1. Estructura de Carpetas Top-Level y Propsito * **`backend/`**: Contiene todo el cdigo del lado del servidor. * **`backend/api/`** (sugerido): Mdulos y rutas de la API Flask. * **`backend/scripts/`**: Scripts de apoyo para el backend, como importacin completa, procesamiento semntico, indexacin en Meilisearch, limpieza de base deatos. * **`backend/services/`** (sugerido): Lgica de negocio y servicios (ej. `embedding_service.py`). * **`backend/models/`** (sugerido): Si se usa un ORM o clases para representar los datos. * **`backend/data/`**: Almacena la base de datos SQLite (ej. `biblioteca.db`) y el esquema SQL (ej. `db_schema.sql`). * **`backend/meilisearch/`** (sugerido): Binarios o configuraciones de Meilisearch si se gestionan localmente. * `requirements.txt`: Dependencias especficas del backend. * `nodemon.json` (opcional): Configuracin si se usa `nodemon` para el desarrollo con Flask. * **`frontend/`**: Alberga la aplicacin web de tipo Single Page Application (SPA). * **`frontend/src/`**: Cdigo fuente principal de la aplicacin React. * **`frontend/src/components/`**: Componentes reutilizables de React. * **`frontend/src/pages/`**: Componentes que representan las vistas principales (ej. `SearchPage.tsx`, `SemanticSearchPage.tsx`). * **`frontend/src/services/`**: Lgica para interactuar con la API del backend (ej. `api.ts`). * **`frontend/src/hooks/`**: Custom hooks de React. * **`frontend/src/assets/`**: Imgenes, fuentes y otros activos estticos. * `App.tsx` (o `App.js`): Componente raz de la aplicacin. * `main.tsx` (o `index.js`): Punto de entrada de la aplicacin React. * `public/`: Activos estticos que se sirven directamente (ej. `index.html`, `favicon.ico`). * `package.json`: Dependencias y scripts de Node.js para el frontend. * `vite.config.js` (o `vite.config.ts`): Archivo de configuracin de Vite. * `tailwind.config.js`: Archivo de configuracin de Tailwind CSS. * **`dataset/`**: Destinada a los datos crudos (fuentes originales) y los scripts para el proceso de Ingesta, Transformacin y Carga (ETL). * **`dataset/fuentes/`**: Carpetas por autor o fuente conteniendo los documentos originales (DOCX, PDF, TXT, MD). * **`dataset/salidas/`**: Resultados del procesamiento de los scripts de `dataset/scripts/` (ej. archivos NDJSON, reportes de duplicados). * **`dataset/scripts/`**: Scripts Python para la conversin, limpieza, normalizacin y depuracin de documentos (ej. `app_depuracion.py`, `converters.py`, `processors.py`, `utils.py`). * `requirements.txt` (opcional): Dependencias especficas para los scripts de `dataset/`. * **`scripts/`** (opcional, a nivel raz): Directorio para scripts Python generales o utilidades que no encajan directamente en `backend` o `dataset` (ej. scripts de despliegue, tareas de mantenimiento globales). * **`docs/`**: Documentacin del proyecto. * `BIBLIOPERSON_ARQUITECTURA.md`: Este mismo archivo. * (Otros documentos: guas de instalacin, ejemplos de API, etc.). * `.gitignore`: Especifica los archivos y directorios ignorados por Git. * `README.md`: Descripcin general del proyecto, instrucciones de instalacin y uso bsico. ### 3.2. Patrones Arquitectnicos y Observaciones Clave * **Separacin Clara de Responsabilidades (SoC):** Existe una distincin marcada entre: * **Capa de Presentacin:** Gestionada por el frontend (React/Vite). * **Capa de Lgica de Negocio y Acceso a Datos (API):** Gestionada por el backend (Flask). * **Capa de Procesamiento e Ingesta de Datos (ETL):** Gestionada por scripts en `dataset/` y `backend/scripts/`. * **Mdulo de Ingestin/ETL Desacoplado:** Los procesos de ETL son en su mayora scripts batch que operan independientemente del servidor web principal. Esto permite ejecuciones manuales o programadas (ej. va `cron` o `celery` en el futuro) y facilita el mantenimiento y la escalabilidad de esta parte del sistema. * **API RESTful como Intermediario:** El frontend interacta con los datos y la lgica de negocio exclusivamente a travs de la API REST expuesta por el backend Flask. Esto promueve un bajo acoplamiento entre el frontend y el backend. * **Modelo Vista Controlador (MVC) Ligero (implcito en Backend):** * **Modelos:** Representados por las tablas de la base de datos SQLite y la lgica de acceso a datos (potencialmente a travs de un ORM ligero o funciones de acceso directo). * **Vistas:** Gestionadas principalmente por el frontend React, que consume los datos de la API. * **Controladores:** Implementados como las rutas y manejadores de peticiones en la aplicacin Flask. * **Single Page Application (SPA):** El frontend es una SPA, lo que implica que la interfaz de usuario se carga una sola vez y las actualizaciones de contenido se realizan dinmicamente mediante JavaScript, interactuando con la API. ### 3.3. Flujo de Datos y Componentes Principales del Sistema #### 3.3.1. Fuentes Externas de Datos * Documentos locales proporcionados por el usuario en diversos formatos: DOCX, PDF, TXT, MD. * Estos documentos se organizan tpicamente en `dataset/fuentes/<autor>/`. #### 3.3.2. Capa de Ingestin de Datos (ETL) * **Localizacin:** Principalmente scripts en `dataset/scripts/` (ej. `app_depuracion.py`, `converters.py`) y scripts de orquestacin o importacin en `backend/scripts/` (ej. `importar_completo.py`). * **Responsabilidades:** 1. **Deteccin y Lectura:** Identificar los archivos en las carpetas fuente. 2. **Conversin de Formato:** Utilizar herramientas como Pandoc y Textract para convertir DOCX, PDF, etc., a texto plano o Markdown. 3. **Normalizacin y Limpieza:** Eliminar caracteres no deseados, corregir codificacin, manejar saltos de lnea inconsistentes. 4. **Segmentacin de Contenido:** Dividir los documentos en unidades manejables (prrafos, secciones, o segn la estructura del documento original) para la indexacin y bsqueda. Identificar metadatos como ttulos, autor, fuente, fecha si es posible. 5. **Generacin de Formato Intermedio:** Usualmente se genera un archivo `NDJSON` (Newline Delimited JSON) con campos como `id`, `texto`, `fuente`, `autor`, `fecha_creacion`, `contexto`. 6. **Carga en Base de Datos (SQLite):** Los datos depurados y estructurados del NDJSON se importan a la tabla `contenidos` de la base de datos SQLite. 7. **Generacin de Embeddings:** Para cada segmento de texto relevante en `contenidos`, se generan embeddings vectoriales utilizando modelos de `sentence-transformers`. Estos embeddings se almacenan en la tabla `contenido_embeddings`, vinculada a `contenidos`. 8. **Indexacin en Meilisearch:** Los datos de `contenidos` junto con sus embeddings vectoriales (si Meilisearch los soporta directamente o se gestionan va IDs) se envan a Meilisearch para crear o actualizar los ndices de bsqueda full-text y vectorial. * **Scripts Clave (Ejemplos):** * `dataset/scripts/app_depuracion.py`: Orquesta la conversin y limpieza de documentos fuente a NDJSON. * `dataset/scripts/converters.py`: Contiene funciones especficas para convertir diferentes tipos de archivo. * `backend/scripts/importar_completo.py`: Gestiona la importacin del NDJSON final a SQLite. * `backend/scripts/procesar_semantica.py`: Genera los embeddings para el contenido en SQLite. * `backend/scripts/indexar_meilisearch.py`: Enva los datos de SQLite a Meilisearch para indexacin. #### 3.3.3. Almacenamiento (Storage) * **Tecnologa Principal:** Base de datos SQLite. * **Ubicacin del Archivo de BD:** `backend/data/biblioteca.db`. * **Esquema de la Base de Datos:** Definido en `backend/data/db_schema.sql` (o gestionado mediante migraciones si se usa un ORM). * **Tablas Principales (Ejemplos):** * `contenidos`: Almacena los segmentos de texto procesados, metadatos (autor, fuente, fecha, tipo de contenido), y un ID nico. * Campos: `id` (PK), `texto_original`, `texto_limpio`, `autor_id` (FK), `fuente_id` (FK), `fecha_creacion`, `metadatos_adicionales` (JSON). * `contenido_embeddings`: Almacena los embeddings vectoriales para los textos. * Campos: `id` (PK), `contenido_id` (FK a `contenidos.id`), `embedding_vector` (BLOB o TEXT), `modelo_embedding`. * `autores`: Informacin sobre los autores. * `fuentes`: Informacin sobre las fuentes de los documentos. * **Motor de Bsqueda:** Meilisearch (opera como un servicio separado, pero funcionalmente es parte de la capa de almacenamiento y recuperacin). * **ndices:** Un ndice principal (ej. "documentos") que contiene los textos y est configurado para bsqueda full-text y bsqueda vectorial (usando los embeddings). #### 3.3.4. Backend REST API * **Tecnologa:** Aplicacin Flask escrita en Python. * **Localizacin del Cdigo Principal:** `backend/` (posiblemente en un archivo como `backend/app.py` o mdulos dentro de `backend/api/`). * **Puerto de Escucha (Desarrollo Tpico):** `5000`. * **Responsabilidades:** 1. Exponer endpoints HTTP para que el frontend (u otros clientes) interacten con el sistema. 2. Manejar la lgica de autenticacin y autorizacin (si aplica en el futuro). 3. Procesar las solicitudes de bsqueda: * Para bsquedas full-text, construir y ejecutar consultas contra Meilisearch. * Para bsquedas semnticas, generar el embedding de la consulta del usuario y usarlo para buscar los vectores ms similares en Meilisearch (o directamente en SQLite/FAISS si se implementara as). 4. Recuperar y formatear los resultados de la base de datos SQLite y/o Meilisearch. 5. Potencialmente, ofrecer endpoints para la gestin de datos (CRUD de autores, fuentes, etc., si es necesario). 6. En el futuro, podra manejar la lgica para generar contenido nuevo basado en los resultados de bsqueda. * **Endpoints Principales (Ejemplos):** * `GET /api/busqueda?termino_busqueda=<query>[&autor=<autor_id>&...]`: Para bsqueda full-text. * `GET /api/busqueda/semantica?texto_consulta=<query>[&autor=<autor_id>&similitud_min=<float>&...]`: Para bsqueda semntica. * `GET /api/autores`: Para listar autores disponibles. * `GET /api/estadisticas`: Para obtener estadsticas del corpus. #### 3.3.5. Frontend SPA (Interfaz de Usuario) * **Tecnologa:** React con Vite (usando JavaScript o TypeScript). Estilos con Tailwind CSS. * **Localizacin del Cdigo Principal:** `frontend/src/`. * **Servidor de Desarrollo (Vite):** Tpicamente en el puerto `5173`. * **Responsabilidades:** 1. Presentar la interfaz de usuario al usuario final a travs de un navegador web. 2. Permitir al usuario ingresar consultas de bsqueda (textual y/o semntica) y seleccionar filtros (autor, fuente, etc.). 3. Realizar solicitudes HTTP (usando `Workspace` o `axios`) a los endpoints de la API del backend. 4. Recibir y mostrar los resultados de bsqueda de manera clara e interactiva. 5. Gestionar el estado de la aplicacin en el cliente (ej. estado de carga, errores, datos de usuario). 6. Navegacin dentro de la aplicacin sin recargas de pgina completa. #### 3.3.6. Usuarios Finales * Interactan con el sistema exclusivamente a travs del Frontend SPA, utilizando un navegador web estndar. ### 3.4. Diagrama de Flujo de Datos e Interacciones Principales +----------------------+ +----------------------+ +----------------------+ | Fuentes Externas | --> | Capa de Ingestin | --> | Almacenamiento | | (DOCX, PDF, TXT...) | | (ETL Scripts) | | (SQLite + Meilisearch| +----------------------+ +----------------------+ +----------^-----------+ | | (Lectura/Escritura) | +----------------------+ +----------------------+ +----------v-----------+ | Usuario Final | <-> | Frontend SPA | <-> | Backend REST API | | (Navegador Web) | | (React/Vite) | | (Flask) | +----------------------+ +----------------------+ +----------------------+ **Flujo de Ingesta:** 1. Usuario coloca documentos en `dataset/fuentes/`. 2. Se ejecutan scripts de ETL (`dataset/scripts/` y `backend/scripts/`). * Convierten, limpian, normalizan documentos. * Generan NDJSON. * Importan NDJSON a SQLite (`contenidos`). * Generan embeddings para `contenidos` y los guardan en `contenido_embeddings`. * Indexan datos de `contenidos` y embeddings en Meilisearch. **Flujo de Bsqueda:** 1. Usuario ingresa una consulta en el Frontend SPA. 2. Frontend enva la solicitud al Backend REST API. 3. Backend API: * Si es bsqueda semntica, genera embedding de la consulta. * Consulta a Meilisearch (y/o SQLite para metadatos adicionales). * Devuelve los resultados formateados (JSON) al Frontend. 4. Frontend muestra los resultados al Usuario. ### 3.5. Tecnologas Clave por Capa/Componente (Repaso) * **Ingestin/ETL:** Python, Pandas (para manipulacin de datos tabulares si es necesario), Textract, PyPandoc (o llamadas directas a Pandoc), `sentence-transformers`. * **Backend/API:** Python, Flask, Flask-RESTx (o similar para APIs estructuradas, opcional), Flask-CORS, `sqlite3` (mdulo Python), cliente `meilisearch` (Python). * **Frontend:** Node.js (entorno de desarrollo), Vite (empaquetador y servidor de desarrollo), React, TypeScript (preferido sobre JavaScript), Tailwind CSS, `axios` (o `Workspace API`). * **Almacenamiento:** SQLite3 (motor de base de datos), Meilisearch (motor de bsqueda). ### 3.6. Herramientas de Desarrollo * **Control de Versiones:** Git, GitHub/GitLab/Bitbucket. * **Entornos Virtuales Python:** `venv` o `conda`. * **Gestor de Paquetes Frontend:** `npm` o `yarn`. * **Servidor de Desarrollo Backend:** Servidor de desarrollo de Flask (para produccin, se usara Gunicorn/uWSGI + Nginx). `nodemon` para reinicio automtico durante el desarrollo. * **Servidor de Desarrollo Frontend:** Servidor de desarrollo de Vite (`npm run dev`). * **Linters y Formateadores:** * Python: Black, Flake8, Pylint, isort. * JavaScript/TypeScript: ESLint, Prettier. * **Testing:** * Python: `pytest`. * JavaScript/TypeScript: Jest, React Testing Library, Vitest. --- ## 4. Scripts Principales y Mdulos Relevantes (Referencia Rpida) *(Esta seccin se puede expandir con una lista ms detallada y descripciones a medida que el proyecto evoluciona)* * **`dataset/scripts/app_depuracion.py`**: Orquestador principal para la conversin de documentos fuente a un formato NDJSON limpio. * **`dataset/scripts/converters.py`**: Librera de funciones para convertir diferentes tipos de archivo (DOCX, PDF, etc.) a texto. * **`dataset/scripts/processors.py`**: Funciones para el procesamiento y limpieza de texto (ej. segmentacin, eliminacin de metadatos no deseados). * **`dataset/scripts/utils.py`**: Utilidades generales para los scripts de ETL (manejo de archivos, logging, etc.). * **`backend/scripts/importar_completo.py`**: Script para importar los datos del archivo NDJSON final a la base de datos SQLite. * **`backend/scripts/importar_datos.py`**: Lgica subyacente para la insercin de datos en SQLite, llamado por `importar_completo.py`. * **`backend/scripts/procesar_semantica.py`**: Script para generar embeddings para los textos almacenados en la base de datos. Llama a `embedding_service.py`. * **`backend/services/embedding_service.py`**: Servicio que encapsula la lgica de carga del modelo de embeddings y la generacin de vectores. * **`backend/scripts/indexar_meilisearch.py`**: Script para enviar datos desde SQLite a Meilisearch para su indexacin, incluyendo la configuracin para bsqueda vectorial. * **`backend/scripts/levantar_meilisearch.py`**: Script de utilidad para iniciar una instancia de Meilisearch. * **`backend/app.py` (o `backend/api/routes.py`)**: Archivo principal de la aplicacin Flask que define las rutas de la API. * **`frontend/src/services/api.ts`**: Mdulo en el frontend que encapsula la lgica para realizar llamadas a la API del backend. --- ## 5. Convenciones y Buenas Prcticas Especficas del Proyecto *(Esta seccin puede ser llenada o expandida ms adelante con decisiones especficas del proyecto)* * **Estilo de Cdigo:** Seguir las recomendaciones de los linters y formateadores configurados (Black, Flake8 para Python; ESLint, Prettier para TS/JS). * **Mensajes de Commit:** Utilizar Conventional Commits (`feat:`, `fix:`, `docs:`, `style:`, `refactor:`, `test:`, `chore:`). * **Manejo de Errores:** * API: Devolver cdigos de estado HTTP apropiados y mensajes de error JSON estructurados. * Scripts: Uso robusto de `try-except` y logging detallado. * **Logging:** Configurar logging centralizado y estructurado para todos los componentes backend y scripts. * **Variables de Entorno:** Utilizar archivos `.env` para configurar parmetros sensibles o especficos del entorno (URLs de bases de datos, claves API, puertos). No comitear archivos `.env` directamente, sino un `.env.example`. * **Documentacin de Cdigo:** Escribir docstrings para funciones y clases Python (estilo Google o NumPy). Comentarios JSDoc/TSDoc para funciones y componentes frontend. --- ## 6. Glosario de Trminos * **Embedding:** Representacin vectorial densa de un fragmento de texto en un espacio multidimensional, capturando su significado semntico. * **ETL:** Proceso de Extract, Transform, Load (Extraer, Transformar, Cargar) datos de una o ms fuentes a un destino. * **Full-text Search:** Bsqueda basada en la coincidencia de palabras clave exactas o variaciones morfolgicas dentro del contenido textual. * **Meilisearch:** Motor de bsqueda rpido y de cdigo abierto. * **NDJSON (Newline Delimited JSON):** Formato de datos donde cada lnea es un objeto JSON vlido. * **Pandoc:** Herramienta universal de conversin de documentos. * **Sentence Transformers:** Librera Python para generar embeddings de frases y prrafos. * **SPA (Single Page Application):** Aplicacin web que carga una nica pgina HTML y actualiza su contenido dinmicamente mediante JavaScript. * **SQLite:** Motor de base de datos relacional ligero basado en archivos. * **Textract (Python):** Librera Python para extraer texto de varios tipos de documentos. * **Vector Search (Bsqueda Semntica/Por Similitud):** Bsqueda basada en la similitud semntica entre el embedding de una consulta y los embeddings de los documentos almacenados, en lugar de coincidencias exactas de palabras clave. * **Vite:** Herramienta de frontend moderna que proporciona un servidor de desarrollo rpido y empaquetado optimizado para produccin.
---

# CONFIGURACION PROCESAMIENTO

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** docs

# CONFIGURACION PROCESAMIENTO

# Configuracin del Procesamiento de Documentos (`app_depuracion.py`) El script `app_depuracion.py` ubicado en `dataset/scripts/` es el motor principal para procesar documentos y convertirlos al formato NDJSON estndar de Biblioperson. Este script no utiliza argumentos de lnea de comandos tradicionales. En su lugar, se configura a travs de dos archivos JSON principales: `jobs_config.json` y `content_profiles.json`, ambos ubicados en `dataset/config/`. ## 1. `jobs_config.json` Este archivo define una lista de "trabajos" (jobs) de procesamiento. Cada objeto en el array representa un conjunto de documentos a procesar con una configuracin especfica. **Ubicacin**: `e:\dev-projects\biblioperson\dataset\config\jobs_config.json` **Esquema**: `e:\dev-projects\biblioperson\dataset\config\schema\jobs_config.schema.json` ### Parmetros por Job: | Parmetro | Tipo | Obligatorio | Descripcin | |-----------------------------|-------------------------------|-------------|-------------------------------------------------------------------------------------------------------------------------------------------| | `job_id` | `string` |  | Identificador nico para el trabajo (e.g., "autor_ensayos_01"). | | `author_name` | `string` |  | Nombre del autor para este trabajo. Se usar para organizar los archivos de salida. | | `language_code` | `string` |  | Cdigo de idioma principal del contenido (e.g., 'es', 'en'). | | `source_directory_name` | `string` |  | Nombre del subdirectorio dentro de `dataset/raw_data/` que contiene los archivos fuente para este trabajo (e.g., "mi_autor/libros_pdf"). | | `content_profile_name` | `string` |  | Nombre del perfil de contenido (definido en `content_profiles.json`) a utilizar para este trabajo. | | `origin_type_name` | `string` |  | Nombre descriptivo para el tipo de origen del contenido (e.g., "Telegram Export", "Libros Escaneados"). | | `acquisition_date` | `string` (YYYY-MM-DD) / `null`|  | Fecha en que se adquiri el material fuente. Opcional. | | `force_null_publication_date`| `boolean` |  | Si es `true`, fuerza que `publication_date` sea `null` en los metadatos, ignorando cualquier fecha extrada. Por defecto es `false`. | | `filter_rules` | `array` / `null` |  | Reglas de filtrado especficas para contenido de tipo `json_like` (ver `content_profiles.json`). Opcional. | | `job_specific_metadata` | `object` / `null` |  | Cualquier otro metadato especfico del trabajo que pueda ser til. Opcional. | **Ejemplo de `jobs_config.json`**: ```json [ { "job_id": "prueba_docx_headings_01", "author_name": "autor_prueba", "language_code": "es", "source_directory_name": "autor_prueba/documentos_generales", "content_profile_name": "perfil_docx_heading", "origin_type_name": "documentos_generales", "acquisition_date": null, "force_null_publication_date": false } ] ``` ## 2. `content_profiles.json` Este archivo define diferentes "perfiles de contenido". Cada perfil especifica cmo se deben procesar los archivos segn su formato y tipo de contenido. El `content_profile_name` en `jobs_config.json` hace referencia a una clave en este archivo. **Ubicacin**: `e:\dev-projects\biblioperson\dataset\config\content_profiles.json` **Esquema**: `e:\dev-projects\biblioperson\dataset\config\schema\content_profiles.schema.json` ### Estructura de un Perfil de Contenido: Cada clave de primer nivel en `content_profiles.json` es un nombre de perfil (e.g., `"perfil_docx_heading"`). El valor es un objeto con los siguientes parmetros: | Parmetro | Tipo | Obligatorio | Descripcin | |---------------------------|-------------------------------------------|-------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | `description` | `string` |  | Descripcin legible del perfil. | | `source_format_group` | `string` (enum) |  | Grupo general del formato fuente: `"document"` (DOCX, PDF, TXT), `"text_plain"`, `"json_like"` (JSON, NDJSON). | | `content_kind` | `string` (enum) |  | Tipo de contenido que maneja el perfil: `"prose"` (libros, artculos), `"messages"` (chats), `"reference_material"`, `"poetry"`, `"structured_data"`. | | `parser_config` | `object` / `null` |  | Configuracin para el parseo si `source_format_group` es `"json_like"`. Nulo para otros tipos. Contiene sub-parmetros como `json_item_prefix_ijson`, `text_property_paths`, etc. | | `converter_config` | `object` / `null` |  | Configuracin para la conversin de archivos (e.g., opciones de Pandoc para DOCX a Markdown, encoding de texto). Nulo si no aplica. | | `chunking_strategy_name` | `string` |  | Nombre de la clase de la estrategia de segmentacin (ChunkingStrategy) a utilizar (e.g., `"ParagraphChunkerStrategy"`). | | `chunking_config` | `object` |  | Configuracin especfica para la estrategia de segmentacin elegida (e.g., `{"min_chunk_size": 10}`). | | `post_chunk_processors` | `array` de `string` |  | Lista de nombres de funciones de post-procesamiento que se aplicarn despus de la segmentacin. | **Ejemplo de `content_profiles.json`**: ```json { "perfil_docx_heading": { "description": "Procesa archivos DOCX usando DocxLoader y HeadingSegmenter", "source_format_group": "document", "content_kind": "prose", "chunking_strategy_name": "ParagraphChunkerStrategy", "chunking_config": { "min_chunk_size": 10 } } } ``` ## Flujo de Configuracin: 1. El script `app_depuracion.py` lee `jobs_config.json`. 2. Para cada "job" habilitado: * Identifica el `source_directory_name` para encontrar los archivos de entrada. * Utiliza el `content_profile_name` para buscar la configuracin detallada en `content_profiles.json`. * Aplica las estrategias de parseo, conversin y segmentacin definidas en el perfil de contenido a los archivos del job. * Guarda los resultados en un subdirectorio dentro de `dataset/output/` estructurado por `author_name`, `origin_type_name` y `job_id`. Para modificar el comportamiento del procesamiento, se deben editar estos archivos JSON en lugar de pasar argumentos al script. Consulte los esquemas (`*.schema.json`) para obtener detalles completos sobre todos los campos y valores permitidos.
---

# README

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** docs

# README

# Biblioperson - Tu Biblioteca Personal Inteligente ## Qu es Biblioperson? Biblioperson transforma tu biblioteca personal en una base de conocimiento consultable con inteligencia artificial. Imagina poder preguntarle a todos tus libros y documentos: *"Qu piensa este autor sobre X tema?"* o *"Dnde he ledo algo sobre Y concepto?"* ##  Inicio Rpido **Primera vez aqu?**  Lee la [**Gua de Inicio Rpido**](INICIO_RAPIDO.md) **Quieres procesar documentos?**  Ve al [**Pipeline NDJSON**](PIPELINE_NDJSON.md) ##  Qu puedes hacer? ###  **Bsqueda Inteligente** - Encuentra ideas por significado, no solo palabras exactas - Busca qu dice un autor especfico sobre cualquier tema - Descubre conexiones entre diferentes textos y autores ###  **Anlisis Profundo** - Explora el pensamiento completo de cada autor en tu biblioteca - Genera inferencias cruzadas entre mltiples fuentes - Reconstruye y navega tus documentos originales ###  **Generacin de Contenido** - Crea material original basado en las ideas de tu biblioteca - Sintetiza informacin de mltiples fuentes - Mantn la trazabilidad hacia los textos originales ##  Arquitectura del Sistema ``` Tus Documentos  Procesamiento  Base de Datos  Aplicacin Web (PDF, DOCX, (NDJSON) (SQLite + (React + TXT, MD) Meilisearch) Flask API) ``` ##  Caractersticas Tcnicas - **Procesamiento inteligente**: Convierte PDFs, DOCX, TXT, Markdown, entre otros en datos estructurados - **Bsqueda semntica avanzada**: Powered by Meilisearch + sentence-transformers - **Interfaz web moderna**: React frontend con diseo intuitivo - **API REST completa**: Integracin programtica para aplicaciones externas - **Base de datos hbrida**: SQLite para estructura + Meilisearch para bsqueda - **Procesamiento por lotes**: Maneja bibliotecas grandes eficientemente ##  Documentacin ###  **Para Empezar** - [**Inicio Rpido**](INICIO_RAPIDO.md) - Configuracin en 5 pasos - [**Pipeline NDJSON**](PIPELINE_NDJSON.md) - Procesamiento de documentos paso a paso ###  **Documentacin Completa** - [** ndice General**](INDICE.md) - Navegacin completa por objetivos - [** Arquitectura**](BIBLIOPERSON_ARQUITECTURA.md) - Diseo tcnico del sistema - [** Configuracin**](GUIA_MEILISEARCH.md) - Bsqueda semntica avanzada - [** Especificaciones**](NDJSON_ESPECIFICACION.md) - Formato de datos tcnico > ** Tip**: No sabes por dnde empezar? Ve al [**ndice General**](INDICE.md) para navegacin por objetivos. ## ndice Detallado - [Biblioteca de Conocimiento Personal](#biblioteca-de-conocimiento-personal) - [ndice](#ndice) - [Caractersticas](#caractersticas) - [Requisitos](#requisitos) - [Instalacin](#instalacin) - [Estructura del Proyecto](#estructura-del-proyecto) - [Importar nuevos datos](#importar-nuevos-datos) - [Procesar y poblar la base de datos](#procesar-y-poblar-la-base-de-datos) - [Generar embeddings semnticos](#generar-embeddings-semnticos) - [Ejecutar la aplicacin](#ejecutar-la-aplicacin) - [Bsqueda Full-Text con Meilisearch](#bsqueda-full-text-con-meilisearch) - [Instalacin y uso local](#instalacin-y-uso-local) - [Nota sobre Meilisearch y el backend](#nota-sobre-meilisearch-y-el-backend) - [Notas](#notas) - [Indexacin incremental en Meilisearch](#indexacin-incremental-en-meilisearch) - [Verificacin y depuracin de la indexacin en Meilisearch](#verificacin-y-depuracin-de-la-indexacin-en-meilisearch) - [Importante: Borrar el ndice de Meilisearch](#importante-borrar-el-ndice-de-meilisearch) - [Preguntas frecuentes](#preguntas-frecuentes) - [Licencia](#licencia) - [Regenerar todos los embeddings semnticos](#regenerar-todos-los-embeddings-semnticos) - [Levantar la base de datos, backend y frontend](#levantar-la-base-de-datos-backend-y-frontend) - [1. Levantar la base de datos (SQLite)](#1-levantar-la-base-de-datos-sqlite) - [2. Levantar el backend (API Flask)](#2-levantar-el-backend-api-flask) - [3. Levantar el frontend (React)](#3-levantar-el-frontend-react) - [4. Consultas bsicas en la base de datos (SQLite)](#4-consultas-bsicas-en-la-base-de-datos-sqlite) ## Caractersticas - Importacin de contenido desde mltiples fuentes (Facebook, Twitter, Telegram, documentos) - Exploracin de contenido por temas, fechas, autores, idiomas y plataformas - Generacin de material para nuevo contenido - API REST para acceso a los datos - Interfaz web para exploracin y anlisis - Lectura de los documentos originales tipo ebook al reconostruirlos con la base de datos. ##  Instalacin Rpida ### Requisitos Mnimos - **Python 3.8+** y **Node.js 16+** - **4GB RAM** y espacio en disco para tu biblioteca - **Git** para clonar el repositorio ### Instalacin en 3 Comandos ```bash # 1. Clonar y preparar git clone [URL_DEL_REPOSITORIO] && cd biblioperson python -m venv venv && .\venv\Scripts\activate # Windows # 2. Instalar dependencias pip install -r backend/requirements.txt -r dataset/requirements.txt cd frontend && npm install && cd .. # 3. Configurar y ejecutar cd dataset && python scripts/app_depuracion.py # Procesar primer documento cd ../backend/scripts && python importar_completo.py # Importar a DB ``` **Necesitas ms detalles?**  Ver [**Gua de Inicio Rpido**](INICIO_RAPIDO.md) ## Estructura del Proyecto - `backend/`: Backend y scripts de procesamiento - `backend/data/import/`: Carpeta donde colocar archivos NDJSON o TXT para importar - `backend/scripts/`: Scripts de Python para la API y procesamiento - `frontend/`: Frontend (React) - `shared/`: Recursos compartidos - `dataset/`: Procesamiento y normalizacin de datasets ## Importar nuevos datos 1. **Coloca tus archivos NDJSON en la carpeta:** - `backend/data/import/` - O bien, ten a mano la ruta de tu archivo NDJSON unificado. 2. **Ubcate en la carpeta de scripts:** ```bash cd backend/scripts ``` 3. **Ejecuta el script de importacin completa:** - Si ya tienes archivos NDJSON en la carpeta de importacin: ```bash python importar_completo.py --solo-importar ``` - Si quieres importar un archivo NDJSON especfico y copiarlo automticamente: ```bash python importar_completo.py /ruta/a/tu/archivo.ndjson ``` - Si quieres reiniciar la base de datos antes de importar (esto borra todo lo anterior!): ```bash python importar_completo.py /ruta/a/tu/archivo.ndjson --reiniciar-db ``` - El script se encargar de copiar el archivo (si lo especificas), importar los datos y generar los ndices necesarios. 4. **Verifica que la importacin fue exitosa revisando los mensajes en consola.** ## Procesar y poblar la base de datos - Si necesitas limpiar o inicializar la base de datos, ejecuta: ```bash python inicializar_db.py ``` - Para actualizar o aadir nuevos registros, simplemente repite el paso de importacin. ## Generar embeddings semnticos - Una vez que los nuevos contenidos estn en la base de datos, genera los embeddings: ```bash python procesar_semantica.py ``` - Este script solo procesar los contenidos nuevos que no tengan embedding. ## Ejecutar la aplicacin - Desde la raz del proyecto: ```bash npm run dev ``` - Esto levantar tanto el backend como el frontend. ## Bsqueda Full-Text con Meilisearch Este proyecto utiliza [Meilisearch](https://www.meilisearch.com/) como motor de bsqueda de texto completo para ofrecer bsquedas rpidas y modernas. ### Instalacin y uso local 1. **Descarga Meilisearch:** - Ve a [https://github.com/meilisearch/meilisearch/releases](https://github.com/meilisearch/meilisearch/releases) - Descarga el archivo `meilisearch-windows-amd64.exe.zip` (o el que corresponda a tu sistema operativo). - Descomprime el archivo. 2. **Ejecuta Meilisearch:** - Abre una terminal en la carpeta donde est el ejecutable. - Ejecuta: ``` .\meilisearch.exe ``` - El servidor estar disponible en [http://127.0.0.1:7700](http://127.0.0.1:7700) 3. **Master Key:** - Al iniciar, Meilisearch genera una master key (token de administracin). Ejemplo: ``` --master-key zg0_by09jiVS_kcLdoVgvO9J7fefp1uGvyF-YNonMRg ``` - Guarda este valor, lo necesitars para configurar el backend y proteger el acceso en produccin. 4. **Documentacin oficial:** [https://www.meilisearch.com/docs](https://www.meilisearch.com/docs) ### Nota sobre Meilisearch y el backend - Meilisearch se inicia automticamente cuando levantas el backend (por ejemplo, ejecutando `api_conexion.py`). - El ejecutable de Meilisearch debe estar en `backend/meilisearch/meilisearch-windows-amd64.exe`. - Si deseas iniciar Meilisearch manualmente, puedes ejecutar: ```bash python backend/scripts/levantar_meilisearch.py ``` - Si cambias de carpeta o servidor, recuerda copiar tambin el archivo/carpeta `data.ms` para conservar la indexacin, o vuelve a indexar desde la base de datos. ### Notas - Si actualizas Meilisearch, repite el proceso de descarga y reemplaza el ejecutable. - Si cambias de PC o servidor, repite estos pasos. - Para produccin, configura la master key y revisa la documentacin de seguridad. ## Indexacin incremental en Meilisearch Cuando aadas nuevos contenidos a la base de datos, puedes indexar solo los nuevos documentos en Meilisearch (sin volver a subir todo) ejecutando: ```bash python backend/scripts/indexar_meilisearch.py --indexar-nuevos ``` Esto detecta automticamente los registros que an no estn en Meilisearch y los aade al ndice. Es til para mantener la bsqueda actualizada despus de cada importacin o edicin masiva. ## Verificacin y depuracin de la indexacin en Meilisearch Si notas que el nmero de documentos en Meilisearch es menor que en tu base de datos: 1. **Verifica cuntos documentos hay en la base de datos y en Meilisearch:** Puedes usar el siguiente script para comparar y listar los IDs faltantes: ```python import sqlite3 import meilisearch DB_PATH = '../../backend/data/biblioteca.db' MEILI_URL = 'http://127.0.0.1:7700' MEILI_INDEX = 'contenidos' MEILI_KEY = None conn = sqlite3.connect(DB_PATH) cursor = conn.cursor() cursor.execute("SELECT id FROM contenidos") db_ids = set(row[0] for row in cursor.fetchall()) client = meilisearch.Client(MEILI_URL, MEILI_KEY) index = client.index(MEILI_INDEX) meili_ids = set() offset = 0 limit = 10000 while True: docs = index.get_documents({'fields': ['id'], 'limit': limit, 'offset': offset}) if not docs: break for doc in docs: meili_ids.add(doc['id']) if len(docs) < limit: break offset += limit print(f'IDs en base de datos: {len(db_ids)}') print(f'IDs en Meilisearch: {len(meili_ids)}') faltan = db_ids - meili_ids print(f'IDs que faltan en Meilisearch: {len(faltan)}') if faltan: print(f'Ejemplo de IDs faltantes: {list(faltan)[:10]}') ``` 2. **Causas comunes de diferencias:** - Documentos demasiado grandes (ms de 2 MB) no se indexan. - IDs duplicados en la base de datos. - Errores de red o timeouts durante la indexacin. 3. **Solucin:** - Revisa los IDs faltantes y verifica si hay problemas con esos registros. - Puedes reintentar indexar solo los faltantes, o limpiar/ajustar los datos problemticos y volver a indexar. 4. **Reindexar todo si es necesario:** Si hay muchos problemas, puedes borrar el ndice en Meilisearch y volver a indexar desde cero. Recuerda detener Meilisearch antes de borrar la carpeta `data.ms`. Esto te ayudar a mantener la integridad entre tu base de datos y el ndice de bsqueda. ## Importante: Borrar el ndice de Meilisearch Si necesitas borrar el ndice (por ejemplo, para reindexar desde cero): 1. **Detn Meilisearch completamente** - Si lo levantaste con el script o desde el backend, detn el backend o cierra la terminal donde se ejecut Meilisearch. - O bien, busca el proceso `meilisearch-windows-amd64.exe` en el Administrador de tareas de Windows y finalzalo. 2. **Borra la carpeta de datos** - Borra la carpeta `backend/meilisearch/data.ms/` (o el archivo/carpeta de datos que corresponda). 3. **Vuelve a iniciar Meilisearch** - Se crear una nueva carpeta de datos vaca y podrs reindexar desde cero. > **Nota:** No puedes borrar los ndices mientras Meilisearch est corriendo. Siempre detn el proceso primero. ## Preguntas frecuentes - **Puedo importar nuevos datos sin perder los anteriores?** S, los scripts estn diseados para aadir solo los nuevos registros y generar embeddings solo para ellos. - **Dnde deben estar los archivos de importacin?** En `backend/data/import/`. - **Cmo s si los embeddings estn actualizados?** El script `procesar_semantica.py` solo procesa los contenidos que no tienen embedding. - Para `/api/busqueda` usa `contenido_texto` - Para `/api/busqueda/semantica` usa `contenido_texto` ## Licencia [ESPECIFICAR LICENCIA] ## Regenerar todos los embeddings semnticos Si necesitas **regenerar los embeddings para todos los contenidos** (por ejemplo, tras cambiar de modelo o para limpiar embeddings antiguos), sigue estos pasos: 1. **Vaca la tabla de embeddings en la base de datos:** Puedes hacerlo desde SQLite con el siguiente comando: ```sql DELETE FROM contenido_embeddings; ``` O, si prefieres borrar y recrear la tabla: ```sql DROP TABLE IF EXISTS contenido_embeddings; -- Luego ejecuta el script de inicializacin semntica para recrearla ``` 2. **Ejecuta el script de procesamiento semntico:** ```bash python procesar_semantica.py ``` Esto generar embeddings para **todos** los contenidos, ya que la tabla estar vaca. > **Nota:** Si tienes muchos contenidos, este proceso puede tardar varios minutos u horas segn la cantidad y el modelo usado. ## Levantar la base de datos, backend y frontend ### 1. Levantar la base de datos (SQLite) No necesitas un servidor especial, solo asegrate de que el archivo `backend/data/biblioteca.db` existe. Si necesitas crearla desde cero: ```bash cd backend/scripts python inicializar_db.py ``` Esto crear la estructura bsica de la base de datos si no existe. ### 2. Levantar el backend (API Flask) Desde la raz del proyecto o desde `backend/scripts`: ```bash python api_conexion.py ``` Esto iniciar el backend en `http://localhost:5000`. ### 3. Levantar el frontend (React) Desde la carpeta `frontend`: ```bash cd frontend npm install # Solo la primera vez npm run dev ``` Esto abrir la aplicacin en `http://localhost:5173` (o el puerto que indique la consola). ### 4. Consultas bsicas en la base de datos (SQLite) Puedes usar la terminal de SQLite para hacer consultas directas: ```bash sqlite3 backend/data/biblioteca.db ``` Ejemplos de consultas tiles: - Contar todos los contenidos: ```sql SELECT COUNT(*) FROM contenidos; ``` - Listar autores nicos: ```sql SELECT DISTINCT autor FROM contenidos WHERE autor IS NOT NULL AND autor != ''; ``` - Contar contenidos por autor: ```sql SELECT autor, COUNT(*) FROM contenidos GROUP BY autor; ``` - Ver los primeros 5 contenidos: ```sql SELECT id, contenido_texto FROM contenidos LIMIT 5; ```
---

# GUIA MEILISEARCH

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** docs

# GUIA MEILISEARCH

# Gua de Meilisearch para Biblioperson **ltima actualizacin:** 9-05-2025 ## ndice - [Gua de Meilisearch para Biblioperson](#gua-de-meilisearch-para-biblioperson) - [ndice](#ndice) - [Introduccin](#introduccin) - [1. Instalacin y Configuracin Inicial de Meilisearch](#1-instalacin-y-configuracin-inicial-de-meilisearch) - [1.1. Descarga de Meilisearch](#11-descarga-de-meilisearch) - [1.2. Primera Ejecucin y Master Key](#12-primera-ejecucin-y-master-key) - [1.3. Ubicacin del Ejecutable en el Proyecto](#13-ubicacin-del-ejecutable-en-el-proyecto) - [1.4. Documentacin Oficial](#14-documentacin-oficial) - [2. Integracin con el Backend de Biblioperson](#2-integracin-con-el-backend-de-biblioperson) - [2.1. Inicio Automtico de Meilisearch](#21-inicio-automtico-de-meilisearch) - [2.2. Inicio Manual de Meilisearch](#22-inicio-manual-de-meilisearch) - [2.3. Persistencia de Datos (data.ms)](#23-persistencia-de-datos-datams) - [3. Indexacin de Contenidos](#3-indexacin-de-contenidos) - [3.1. Indexacin Completa](#31-indexacin-completa) - [3.2. Indexacin Incremental](#32-indexacin-incremental) - [4. Verificacin y Depuracin del ndice](#4-verificacin-y-depuracin-del-ndice) - [4.1. Comparacin de Documentos](#41-comparacin-de-documentos) - [4.2. Causas Comunes de Discrepancias](#42-causas-comunes-de-discrepancias) - [4.3. Soluciones y Reindexacin](#43-soluciones-y-reindexacin) - [5. Mantenimiento del ndice](#5-mantenimiento-del-ndice) - [5.1. Borrar el ndice (Reindexacin desde Cero)](#51-borrar-el-ndice-reindexacin-desde-cero) - [6. Consideraciones Adicionales](#6-consideraciones-adicionales) - [6.1. Actualizacin de Meilisearch](#61-actualizacin-de-meilisearch) - [6.2. Migracin a Otro Servidor/PC](#62-migracin-a-otro-servidorpc) - [6.3. Seguridad en Produccin](#63-seguridad-en-produccin) --- ## Introduccin Este documento proporciona una gua detallada sobre la instalacin, configuracin, uso y mantenimiento de [Meilisearch](https://www.meilisearch.com/) dentro del proyecto Biblioperson. Meilisearch se utiliza como motor de bsqueda de texto completo (full-text search) y para capacidades de bsqueda vectorial, ofreciendo resultados rpidos y relevantes. --- ## 1. Instalacin y Configuracin Inicial de Meilisearch ### 1.1. Descarga de Meilisearch 1. Visita la pgina de lanzamientos de Meilisearch: [https://github.com/meilisearch/meilisearch/releases](https://github.com/meilisearch/meilisearch/releases). 2. Descarga la versin ms reciente adecuada para tu sistema operativo (ej. `meilisearch-windows-amd64.exe.zip` para Windows 64-bit). 3. Descomprime el archivo descargado. Obtendrs un archivo ejecutable (`meilisearch.exe` o `meilisearch`). ### 1.2. Primera Ejecucin y Master Key 1. Abre una terminal o lnea de comandos en la carpeta donde descomprimiste el ejecutable. 2. Ejecuta Meilisearch: - **Windows:** ```bash .\meilisearch.exe ``` - **Linux/macOS:** ```bash ./meilisearch ``` 3. Al iniciarse por primera vez (o sin `master-key`), Meilisearch generar una. Vers mensajes como: ```text INFO meilisearch_http::helpers: Server is running with a randomly generated master key. INFO meilisearch_http::helpers: Listening on 127.0.0.1:7700 ``` Para produccin, establece siempre una master-key: ```bash # Usando variable de entorno export MEILI_MASTER_KEY="TuClaveSuperSegura" # O al iniciar ./meilisearch --master-key="TuClaveSuperSegura" ``` El servidor quedar disponible en [http://127.0.0.1:7700](http://127.0.0.1:7700). ### 1.3. Ubicacin del Ejecutable en el Proyecto Coloca el binario de Meilisearch en: ```bash backend/meilisearch/meilisearch-windows-amd64.exe ``` Si lo mueves, actualiza los scripts del backend que lo invoquen. ### 1.4. Documentacin Oficial Para ms detalles, consulta la [Documentacin Oficial de Meilisearch](https://www.meilisearch.com/docs/). --- ## 2. Integracin con el Backend de Biblioperson ### 2.1. Inicio Automtico de Meilisearch El backend (por ejemplo, `backend/scripts/api_conexion.py`) arranca Meilisearch si no detecta una instancia en ejecucin, usando el ejecutable configurado. ### 2.2. Inicio Manual de Meilisearch Si prefieres iniciarlo t mismo: ```bash python backend/scripts/levantar_meilisearch.py ``` O inicia el binario tal como en la seccin 1.2. Asegrate de hacerlo antes de arrancar el backend. ### 2.3. Persistencia de Datos (data.ms) Meilisearch guarda ndices y configuracin en `data.ms` (creada en el directorio de ejecucin). > **Nota:** Mantn la misma carpeta `data.ms` si cambias la ubicacin o mtodo de arranque, para conservar tus ndices. --- ## 3. Indexacin de Contenidos ### 3.1. Indexacin Completa Para la importacin inicial o masiva de datos: ```bash python backend/scripts/indexar_meilisearch.py ``` Puedes usar opciones como `--hilos` o `--docs-per-request`. Revisa la ayuda: ```bash python backend/scripts/indexar_meilisearch.py --help ``` ### 3.2. Indexacin Incremental Para aadir solo nuevos registros: ```bash python backend/scripts/indexar_meilisearch.py --indexar-nuevos ``` --- ## 4. Verificacin y Depuracin del ndice ### 4.1. Comparacin de Documentos Usa el siguiente script para comparar IDs entre SQLite y Meilisearch: ```python # backend/scripts/utils/verificar_indice_meili.py import sqlite3 import meilisearch import os # --- Configuracin --- BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')) DB_PATH = os.path.join(BASE_DIR, 'backend', 'data', 'biblioteca.db') MEILI_URL = 'http://127.0.0.1:7700' MEILI_INDEX_NAME = 'contenidos' MEILI_API_KEY = None # --- Fin Configuracin --- def verificar_sincronizacion(): try: conn = sqlite3.connect(DB_PATH) cursor = conn.cursor() cursor.execute("SELECT id FROM contenidos") db_ids = set(str(row[0]) for row in cursor.fetchall()) conn.close() except sqlite3.Error as e: print(f"Error en SQLite: {e}") return try: client = meilisearch.Client(MEILI_URL, MEILI_API_KEY) index = client.index(MEILI_INDEX_NAME) meili_ids = set() offset, limit = 0, 1000 while True: docs = index.get_documents({'fields': ['id'], 'limit': limit, 'offset': offset}) if not docs: break meili_ids.update(str(doc['id']) for doc in docs) if len(docs) < limit: break offset += limit except Exception as e: print(f"Error en Meilisearch: {e}") return faltantes = db_ids - meili_ids extras = meili_ids - db_ids print(f"SQLite: {len(db_ids)} IDs, Meilisearch: {len(meili_ids)} IDs") if faltantes: print(f"IDs faltantes ({len(faltantes)}): {list(faltantes)[:20]}") if extras: print(f"IDs extraos ({len(extras)}): {list(extras)[:20]}") if not faltantes and not extras: print("Sincronizacin perfecta!") if __name__ == '__main__': verificar_sincronizacion() ``` Gurdalo como `verificar_indice_meili.py`, ajusta la configuracin y ejectalo: ```bash python backend/scripts/utils/verificar_indice_meili.py ``` ### 4.2. Causas Comunes de Discrepancias - **Tamao de documentos:** lmite ~100 KB JSON; payload global ~2 GB. - **Errores de indexacin:** timeouts, red o datos corruptos. - **IDs duplicados/invlidos:** asegrate de unicidad y formato. - **Procesos interrumpidos:** scripts detenidos prematuramente. ### 4.3. Soluciones y Reindexacin - Identificar y corregir datos problemticos. - Reintentar indexacin selectiva de documentos corregidos. - Reindexacin completa: borrar ndice y volver a indexar (ver 5.1). --- ## 5. Mantenimiento del ndice ### 5.1. Borrar el ndice (Reindexacin desde Cero) 1. Detn Meilisearch: cierra el proceso o servicio. 2. Elimina `data.ms`: ```bash rm -rf backend/meilisearch/data.ms ``` 3. Reinicia Meilisearch: crea una carpeta `data.ms` vaca. 4. Indexacin completa: ```bash python backend/scripts/indexar_meilisearch.py ``` --- ## 6. Consideraciones Adicionales ### 6.1. Actualizacin de Meilisearch 1. Descarga el nuevo ejecutable (ver 1.1). 2. Detn la instancia actual. 3. Sustituye el binario. 4. Inicia la nueva versin. 5. Haz copia de seguridad de `data.ms` antes de actualizaciones mayores. ### 6.2. Migracin a Otro Servidor/PC 1. Instala la misma versin o compatible. 2. Copia la carpeta `data.ms`. 3. Arranca Meilisearch en el nuevo entorno. ### 6.3. Seguridad en Produccin - **Master Key:** obligatoria (`MEILI_MASTER_KEY` o `--master-key`). - **API Keys:** crea claves con permisos restringidos. - **HTTPS:** expn bajo TLS. - **Firewall:** limita acceso slo a mquinas necesarias.
---

# GUIA GESTION DATOS

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** docs

# GUIA GESTION DATOS

# Gua de Gestin de Datos para Biblioperson **ltima actualizacin:** (dd-mm-aaaa) ## ndice - [Gua de Gestin de Datos para Biblioperson](#gua-de-gestin-de-datos-para-biblioperson) - [ndice](#ndice) - [Introduccin](#introduccin) - [1. Preparacin Inicial de la Base de Datos](#1-preparacin-inicial-de-la-base-de-datos) - [1.1. Creacin e Inicializacin de la Base de Datos (SQLite)](#11-creacin-e-inicializacin-de-la-base-de-datos-sqlite) - [2. Proceso de Importacin de Contenidos](#2-proceso-de-importacin-de-contenidos) - [2.1. Formato y Ubicacin de los Archivos de Entrada](#21-formato-y-ubicacin-de-los-archivos-de-entrada) - [2.2. Ejecucin del Script de Importacin (`importar_completo.py`)](#22-ejecucin-del-script-de-importacin-importar_completopy) - [2.2.1. Opciones del Script](#221-opciones-del-script) - [2.3. Verificacin de la Importacin](#23-verificacin-de-la-importacin) - [3. Generacin de Embeddings Semnticos](#3-generacin-de-embeddings-semnticos) - [3.1. Procesamiento de Nuevos Contenidos (`procesar_semantica.py`)](#31-procesamiento-de-nuevos-contenidos-procesar_semanticapy) - [3.2. Regeneracin Completa de Todos los Embeddings](#32-regeneracin-completa-de-todos-los-embeddings) - [3.2.1. Pasos para la Regeneracin](#321-pasos-para-la-regeneracin) - [4. Consultas y Verificacin Directa en la Base de Datos](#4-consultas-y-verificacin-directa-en-la-base-de-datos) - [4.1. Acceso a la Terminal de SQLite](#41-acceso-a-la-terminal-de-sqlite) - [4.2. Ejemplos de Consultas tiles](#42-ejemplos-de-consultas-tiles) - [5. Preguntas Frecuentes sobre Gestin de Datos](#5-preguntas-frecuentes-sobre-gestin-de-datos) --- ## Introduccin Esta gua detalla los procedimientos para la gestin de datos dentro del proyecto Biblioperson. Cubre desde la configuracin inicial de la base de datos, la importacin de contenido desde archivos fuente, hasta la generacin de embeddings semnticos necesarios para la bsqueda avanzada. --- ## 1. Preparacin Inicial de la Base de Datos ### 1.1. Creacin e Inicializacin de la Base de Datos (SQLite) Biblioperson utiliza SQLite como su base de datos, la cual se almacena en un archivo local. * **Ubicacin del archivo de base de datos:** `backend/data/biblioteca.db` Si la base de datos no existe o necesitas recrear su estructura (tablas, ndices, etc.) desde cero, puedes utilizar el script de inicializacin: 1. Navega a la carpeta de scripts del backend: ```bash cd backend/scripts ``` 2. Ejecuta el script de inicializacin: ```bash python inicializar_db.py ``` Este script crear el archivo `biblioteca.db` (si no existe) y definir el esquema necesario (tablas como `contenidos`, `contenido_embeddings`, `autores`, etc.). **Advertencia:** Si la base de datos ya existe y contiene datos, ejecutar este script podra borrar las tablas existentes y recrearlas vacas, dependiendo de su implementacin. Revisa el contenido del script `inicializar_db.py` para entender su comportamiento exacto con bases de datos preexistentes. --- ## 2. Proceso de Importacin de Contenidos Los contenidos se importan a Biblioperson a partir de archivos en formato NDJSON (Newline Delimited JSON). Estos archivos NDJSON son el resultado del procesamiento realizado por los scripts en la carpeta `dataset/scripts/` (como `app_depuracion.py`). ### 2.1. Formato y Ubicacin de los Archivos de Entrada * **Formato Esperado:** Archivos `.ndjson` donde cada lnea es un objeto JSON que representa un fragmento de contenido con sus metadatos (ej. `id`, `texto_original`, `autor`, `fuente`, `fecha_creacion`, etc.). * **Ubicacin Predeterminada para Importacin:** Los scripts de importacin pueden buscar archivos NDJSON en la siguiente carpeta: `backend/data/import/` Puedes colocar tus archivos `.ndjson` directamente aqu. ### 2.2. Ejecucin del Script de Importacin (`importar_completo.py`) El script principal para importar los datos desde los archivos NDJSON a la base de datos SQLite es `importar_completo.py`. 1. Navega a la carpeta de scripts del backend: ```bash cd backend/scripts ``` 2. Ejecuta el script `importar_completo.py` con las opciones adecuadas: #### 2.2.1. Opciones del Script * **Importar archivos desde la carpeta `backend/data/import/`:** Si ya has colocado tus archivos NDJSON en la carpeta de importacin predeterminada. ```bash python importar_completo.py --solo-importar ``` * **Importar un archivo NDJSON especfico (copindolo a la carpeta de importacin):** Si tu archivo NDJSON unificado est en otra ubicacin, este comando lo copiar primero a `backend/data/import/` y luego lo procesar. ```bash python importar_completo.py /ruta/completa/a/tu/archivo.ndjson ``` * **Reiniciar la base de datos antes de importar:** **Advertencia! Esta opcin borrar todos los datos existentes en las tablas relevantes** antes de realizar la nueva importacin. til si quieres empezar desde cero. ```bash python importar_completo.py /ruta/completa/a/tu/archivo.ndjson --reiniciar-db ``` Tambin puedes usar `--reiniciar-db` con `--solo-importar` si los archivos ya estn en la carpeta de importacin: ```bash python importar_completo.py --solo-importar --reiniciar-db ``` El script se encargar de leer los archivos NDJSON, procesar cada registro e insertarlo en las tablas correspondientes de la base de datos SQLite. Los scripts estn diseados para aadir solo nuevos registros si no se especifica `--reiniciar-db`, evitando duplicados si un contenido con el mismo ID ya existe. ### 2.3. Verificacin de la Importacin * Revisa los mensajes y logs generados por el script en la consola para confirmar que la importacin se complet sin errores. * Puedes realizar consultas directas a la base de datos (ver seccin 4) para verificar que los datos se han cargado correctamente. --- ## 3. Generacin de Embeddings Semnticos Una vez que los contenidos textuales estn en la base de datos SQLite, es necesario generar los embeddings vectoriales para habilitar la bsqueda semntica. ### 3.1. Procesamiento de Nuevos Contenidos (`procesar_semantica.py`) El script `procesar_semantica.py` se encarga de generar estos embeddings. 1. Asegrate de estar en la carpeta `backend/scripts/`. 2. Ejecuta el script: ```bash python procesar_semantica.py ``` * **Comportamiento:** Este script est diseado para ser eficiente. Por defecto, solo procesar aquellos contenidos en la tabla `contenidos` que an no tengan un embedding asociado en la tabla `contenido_embeddings`. Esto significa que puedes ejecutarlo despus de cada importacin para generar embeddings nicamente para los datos nuevos. ### 3.2. Regeneracin Completa de Todos los Embeddings Puede haber situaciones en las que necesites regenerar los embeddings para *todos* los contenidos almacenados, por ejemplo: * Si cambias el modelo de `sentence-transformers` utilizado para generar los embeddings. * Si detectas problemas o inconsistencias en los embeddings existentes. * Despus de una limpieza o modificacin significativa de los textos base. #### 3.2.1. Pasos para la Regeneracin 1. **Vaciar la Tabla de Embeddings:** La forma ms directa de forzar la regeneracin es eliminar todos los registros existentes en la tabla `contenido_embeddings`. Puedes hacerlo conectndote a la base de datos SQLite (ver seccin 4.1) y ejecutando el siguiente comando SQL: ```sql DELETE FROM contenido_embeddings; ``` Alternativamente, si el script `inicializar_db.py` o algn otro script de mantenimiento ofrece una opcin para vaciar o recrear especficamente la tabla `contenido_embeddings` sin afectar otros datos, podras usarlo. Una opcin ms drstica (si el esquema lo permite y no hay dependencias complejas) sera: ```sql DROP TABLE IF EXISTS contenido_embeddings; ``` Y luego recrear la tabla con su estructura original (a menudo, `inicializar_db.py` o un script similar de "inicializacin semntica" se encargara de recrearla si no existe). 2. **Ejecutar el Script de Procesamiento Semntico:** Una vez que la tabla `contenido_embeddings` est vaca, ejecuta el script como de costumbre: ```bash python procesar_semantica.py ``` Dado que no encontrar embeddings existentes, procesar todos los contenidos de la tabla `contenidos`. * **Nota Importante sobre el Tiempo de Procesamiento:** La generacin de embeddings es una tarea computacionalmente intensiva. Regenerar embeddings para una gran cantidad de contenidos puede llevar un tiempo considerable (desde minutos hasta varias horas), dependiendo del volumen de datos, la potencia de tu CPU/GPU y el modelo de embedding utilizado. --- ## 4. Consultas y Verificacin Directa en la Base de Datos Puedes interactuar directamente con la base de datos SQLite para verificar datos, realizar consultas de diagnstico o llevar a cabo tareas de mantenimiento manuales. ### 4.1. Acceso a la Terminal de SQLite 1. Abre una terminal o lnea de comandos. 2. Navega al directorio donde se encuentra tu archivo de base de datos (`backend/data/`). 3. Ejecuta el cliente de lnea de comandos de SQLite: ```bash sqlite3 biblioteca.db ``` (Si `sqlite3` no est en tu PATH, puede que necesites especificar la ruta completa al ejecutable de `sqlite3` o instalarlo). Esto te abrir el prompt de SQLite (`sqlite>`). ### 4.2. Ejemplos de Consultas tiles Una vez dentro del prompt de SQLite, puedes ejecutar comandos SQL. Algunos ejemplos: * **Listar todas las tablas:** ```sql .tables ``` * **Ver el esquema de una tabla (ej. `contenidos`):** ```sql .schema contenidos ``` * **Contar todos los registros en la tabla `contenidos`:** ```sql SELECT COUNT(*) FROM contenidos; ``` * **Ver los primeros 5 registros de la tabla `contenidos`:** ```sql SELECT id, texto_original, autor FROM contenidos LIMIT 5; ``` * **Contar cuntos contenidos tienen embeddings generados:** ```sql SELECT COUNT(DISTINCT contenido_id) FROM contenido_embeddings; ``` * **Listar autores nicos:** ```sql SELECT DISTINCT autor FROM contenidos WHERE autor IS NOT NULL AND autor != ''; ``` * **Contar contenidos por autor:** ```sql SELECT autor, COUNT(*) AS total_contenidos FROM contenidos GROUP BY autor ORDER BY total_contenidos DESC; ``` * **Salir de la terminal de SQLite:** ```sql .quit ``` --- ## 5. Preguntas Frecuentes sobre Gestin de Datos * **Puedo importar nuevos datos sin perder los anteriores?** S. Por defecto, el script `importar_completo.py` est diseado para aadir nuevos registros y actualizar los existentes si se encuentran por ID, pero no para borrar datos a menos que se use la opcin `--reiniciar-db`. * **Dnde deben estar los archivos de importacin NDJSON?** La ubicacin predeterminada que el script `importar_completo.py` puede usar (con la opcin `--solo-importar`) es `backend/data/import/`. Si proporcionas una ruta a un archivo NDJSON, el script tambin puede manejarlo. * **Cmo s si los embeddings estn actualizados para los nuevos contenidos?** El script `procesar_semantica.py` est diseado para procesar nicamente aquellos contenidos que an no tienen un embedding generado. Simplemente ejectalo despus de una importacin para poner al da los embeddings.
---

# tailwind.config

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** frontend

# tailwind.config

/** @type {import('tailwindcss').Config} */ export default { content: [ "./index.html", "./src/**/*.{js,ts,jsx,tsx}", ], theme: { extend: {}, }, plugins: [], }
---

# index

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** frontend

# index

<!doctype html> <html lang="en"> <head> <meta charset="UTF-8" /> <link rel="icon" type="image/svg+xml" href="/vite.svg" /> <meta name="viewport" content="width=device-width, initial-scale=1.0" /> <title>Biblioteca Personal</title> <script src="https://cdn.tailwindcss.com"></script> </head> <body class="bg-gray-100 text-gray-800"> <main class="container mx-auto p-6"> <div id="root"></div> </main> <script type="module" src="/src/main.tsx"></script> </body> </html>
---

# eslint.config

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** frontend

# eslint.config

import js from '@eslint/js' import globals from 'globals' import reactHooks from 'eslint-plugin-react-hooks' import reactRefresh from 'eslint-plugin-react-refresh' import tseslint from 'typescript-eslint' export default tseslint.config( { ignores: ['dist'] }, { extends: [js.configs.recommended, ...tseslint.configs.recommended], files: ['**/*.{ts,tsx}'], languageOptions: { ecmaVersion: 2020, globals: globals.browser, }, plugins: { 'react-hooks': reactHooks, 'react-refresh': reactRefresh, }, rules: { ...reactHooks.configs.recommended.rules, 'react-refresh/only-export-components': [ 'warn', { allowConstantExport: true }, ], }, }, )
---

# package lock

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** frontend

# package lock

{ "name": "frontend", "version": "0.0.0", "lockfileVersion": 3, "requires": true, "packages": { "": { "name": "frontend", "version": "0.0.0", "dependencies": { "@tanstack/react-query": "^5.75.2", "axios": "^1.9.0", "react": "^19.0.0", "react-dom": "^19.0.0", "react-router-dom": "^7.5.3", "react-select": "^5.10.1", "recharts": "^2.15.3" }, "devDependencies": { "@eslint/js": "^9.22.0", "@types/react": "^19.0.10", "@types/react-dom": "^19.0.4", "@vitejs/plugin-react": "^4.3.4", "autoprefixer": "^10.4.14", "eslint": "^9.22.0", "eslint-plugin-react-hooks": "^5.2.0", "eslint-plugin-react-refresh": "^0.4.19", "globals": "^16.0.0", "postcss": "^8.5.3", "tailwindcss": "^3.3.0", "typescript": "~5.7.2", "typescript-eslint": "^8.26.1", "vite": "^6.3.1" } }, "node_modules/@ampproject/remapping": { "version": "2.3.0", "resolved": "https://registry.npmjs.org/@ampproject/remapping/-/remapping-2.3.0.tgz", "integrity": "sha512-30iZtAPgz+LTIYoeivqYo853f02jBYSd5uGnGpkFV0M3xOt9aN73erkgYAmZU43x4VfqcnLxW9Kpg3R5LC4YYw==", "dev": true, "license": "Apache-2.0", "dependencies": { "@jridgewell/gen-mapping": "^0.3.5", "@jridgewell/trace-mapping": "^0.3.24" }, "engines": { "node": ">=6.0.0" } }, "node_modules/@babel/code-frame": { "version": "7.27.1", "resolved": "https://registry.npmjs.org/@babel/code-frame/-/code-frame-7.27.1.tgz", "integrity": "sha512-cjQ7ZlQ0Mv3b47hABuTevyTuYN4i+loJKGeV9flcCgIK37cCXRh+L1bd3iBHlynerhQ7BhCkn2BPbQUL+rGqFg==", "license": "MIT", "dependencies": { "@babel/helper-validator-identifier": "^7.27.1", "js-tokens": "^4.0.0", "picocolors": "^1.1.1" }, "engines": { "node": ">=6.9.0" } }, "node_modules/@babel/compat-data": { "version": "7.27.1", "resolved": "https://registry.npmjs.org/@babel/compat-data/-/compat-data-7.27.1.tgz", "integrity": "sha512-Q+E+rd/yBzNQhXkG+zQnF58e4zoZfBedaxwzPmicKsiK3nt8iJYrSrDbjwFFDGC4f+rPafqRaPH6TsDoSvMf7A==", "dev": true, "license": "MIT", "engines": { "node": ">=6.9.0" } }, "node_modules/@babel/core": { "version": "7.27.1", "resolved": "https://registry.npmjs.org/@babel/core/-/core-7.27.1.tgz", "integrity": "sha512-IaaGWsQqfsQWVLqMn9OB92MNN7zukfVA4s7KKAI0KfrrDsZ0yhi5uV4baBuLuN7n3vsZpwP8asPPcVwApxvjBQ==", "dev": true, "license": "MIT", "dependencies": { "@ampproject/remapping": "^2.2.0", "@babel/code-frame": "^7.27.1", "@babel/generator": "^7.27.1", "@babel/helper-compilation-targets": "^7.27.1", "@babel/helper-module-transforms": "^7.27.1", "@babel/helpers": "^7.27.1", "@babel/parser": "^7.27.1", "@babel/template": "^7.27.1", "@babel/traverse": "^7.27.1", "@babel/types": "^7.27.1", "convert-source-map": "^2.0.0", "debug": "^4.1.0", "gensync": "^1.0.0-beta.2", "json5": "^2.2.3", "semver": "^6.3.1" }, "engines": { "node": ">=6.9.0" }, "funding": { "type": "opencollective", "url": "https://opencollective.com/babel" } }, "node_modules/@babel/generator": { "version": "7.27.1", "resolved": "https://registry.npmjs.org/@babel/generator/-/generator-7.27.1.tgz", "integrity": "sha512-UnJfnIpc/+JO0/+KRVQNGU+y5taA5vCbwN8+azkX6beii/ZF+enZJSOKo11ZSzGJjlNfJHfQtmQT8H+9TXPG2w==", "license": "MIT", "dependencies": { "@babel/parser": "^7.27.1", "@babel/types": "^7.27.1", "@jridgewell/gen-mapping": "^0.3.5", "@jridgewell/trace-mapping": "^0.3.25", "jsesc": "^3.0.2" }, "engines": { "node": ">=6.9.0" } }, "node_modules/@babel/helper-compilation-targets": { "version": "7.27.1", "resolved": "https://registry.npmjs.org/@babel/helper-compilation-targets/-/helper-compilation-targets-7.27.1.tgz", "integrity": "sha512-2YaDd/Rd9E598B5+WIc8wJPmWETiiJXFYVE60oX8FDohv7rAUU3CQj+A1MgeEmcsk2+dQuEjIe/GDvig0SqL4g==", "dev": true, "license": "MIT", "dependencies": { "@babel/compat-data": "^7.27.1", "@babel/helper-validator-option": "^7.27.1", "browserslist": "^4.24.0", "lru-cache": "^5.1.1", "semver": "^6.3.1" }, "engines": { "node": ">=6.9.0" } }, "node_modules/@babel/helper-module-imports": { "version": "7.27.1", "resolved": "https://registry.npmjs.org/@babel/helper-module-imports/-/helper-module-imports-7.27.1.tgz", "integrity": "sha512-0gSFWUPNXNopqtIPQvlD5WgXYI5GY2kP2cCvoT8kczjbfcfuIljTbcWrulD1CIPIX2gt1wghbDy08yE1p+/r3w==", "license": "MIT", "dependencies": { "@babel/traverse": "^7.27.1", "@babel/types": "^7.27.1" }, "engines": { "node": ">=6.9.0" } }, "node_modules/@babel/helper-module-transforms": { "version": "7.27.1", "resolved": "https://registry.npmjs.org/@babel/helper-module-transforms/-/helper-module-transforms-7.27.1.tgz", "integrity": "sha512-9yHn519/8KvTU5BjTVEEeIM3w9/2yXNKoD82JifINImhpKkARMJKPP59kLo+BafpdN5zgNeIcS4jsGDmd3l58g==", "dev": true, "license": "MIT", "dependencies": { "@babel/helper-module-imports": "^7.27.1", "@babel/helper-validator-identifier": "^7.27.1", "@babel/traverse": "^7.27.1" }, "engines": { "node": ">=6.9.0" }, "peerDependencies": { "@babel/core": "^7.0.0" } }, "node_modules/@babel/helper-plugin-utils": { "version": "7.27.1", "resolved": "https://registry.npmjs.org/@babel/helper-plugin-utils/-/helper-plugin-utils-7.27.1.tgz", "integrity": "sha512-1gn1Up5YXka3YYAHGKpbideQ5Yjf1tDa9qYcgysz+cNCXukyLl6DjPXhD3VRwSb8c0J9tA4b2+rHEZtc6R0tlw==", "dev": true, "license": "MIT", "engines": { "node": ">=6.9.0" } }, "node_modules/@babel/helper-string-parser": { "version": "7.27.1", "resolved": "https://registry.npmjs.org/@babel/helper-string-parser/-/helper-string-parser-7.27.1.tgz", "integrity": "sha512-qMlSxKbpRlAridDExk92nSobyDdpPijUq2DW6oDnUqd0iOGxmQjyqhMIihI9+zv4LPyZdRje2cavWPbCbWm3eA==", "license": "MIT", "engines": { "node": ">=6.9.0" } }, "node_modules/@babel/helper-validator-identifier": { "version": "7.27.1", "resolved": "https://registry.npmjs.org/@babel/helper-validator-identifier/-/helper-validator-identifier-7.27.1.tgz", "integrity": "sha512-D2hP9eA+Sqx1kBZgzxZh0y1trbuU+JoDkiEwqhQ36nodYqJwyEIhPSdMNd7lOm/4io72luTPWH20Yda0xOuUow==", "license": "MIT", "engines": { "node": ">=6.9.0" } }, "node_modules/@babel/helper-validator-option": { "version": "7.27.1", "resolved": "https://registry.npmjs.org/@babel/helper-validator-option/-/helper-validator-option-7.27.1.tgz", "integrity": "sha512-YvjJow9FxbhFFKDSuFnVCe2WxXk1zWc22fFePVNEaWJEu8IrZVlda6N0uHwzZrUM1il7NC9Mlp4MaJYbYd9JSg==", "dev": true, "license": "MIT", "engines": { "node": ">=6.9.0" } }, "node_modules/@babel/helpers": { "version": "7.27.1", "resolved": "https://registry.npmjs.org/@babel/helpers/-/helpers-7.27.1.tgz", "integrity": "sha512-FCvFTm0sWV8Fxhpp2McP5/W53GPllQ9QeQ7SiqGWjMf/LVG07lFa5+pgK05IRhVwtvafT22KF+ZSnM9I545CvQ==", "dev": true, "license": "MIT", "dependencies": { "@babel/template": "^7.27.1", "@babel/types": "^7.27.1" }, "engines": { "node": ">=6.9.0" } }, "node_modules/@babel/parser": { "version": "7.27.1", "resolved": "https://registry.npmjs.org/@babel/parser/-/parser-7.27.1.tgz", "integrity": "sha512-I0dZ3ZpCrJ1c04OqlNsQcKiZlsrXf/kkE4FXzID9rIOYICsAbA8mMDzhW/luRNAHdCNt7os/u8wenklZDlUVUQ==", "license": "MIT", "dependencies": { "@babel/types": "^7.27.1" }, "bin": { "parser": "bin/babel-parser.js" }, "engines": { "node": ">=6.0.0" } }, "node_modules/@babel/plugin-transform-react-jsx-self": { "version": "7.27.1", "resolved": "https://registry.npmjs.org/@babel/plugin-transform-react-jsx-self/-/plugin-transform-react-jsx-self-7.27.1.tgz", "integrity": "sha512-6UzkCs+ejGdZ5mFFC/OCUrv028ab2fp1znZmCZjAOBKiBK2jXD1O+BPSfX8X2qjJ75fZBMSnQn3Rq2mrBJK2mw==", "dev": true, "license": "MIT", "dependencies": { "@babel/helper-plugin-utils": "^7.27.1" }, "engines": { "node": ">=6.9.0" }, "peerDependencies": { "@babel/core": "^7.0.0-0" } }, "node_modules/@babel/plugin-transform-react-jsx-source": { "version": "7.27.1", "resolved": "https://registry.npmjs.org/@babel/plugin-transform-react-jsx-source/-/plugin-transform-react-jsx-source-7.27.1.tgz", "integrity": "sha512-zbwoTsBruTeKB9hSq73ha66iFeJHuaFkUbwvqElnygoNbj/jHRsSeokowZFN3CZ64IvEqcmmkVe89OPXc7ldAw==", "dev": true, "license": "MIT", "dependencies": { "@babel/helper-plugin-utils": "^7.27.1" }, "engines": { "node": ">=6.9.0" }, "peerDependencies": { "@babel/core": "^7.0.0-0" } }, "node_modules/@babel/runtime": { "version": "7.27.1", "resolved": "https://registry.npmjs.org/@babel/runtime/-/runtime-7.27.1.tgz", "integrity": "sha512-1x3D2xEk2fRo3PAhwQwu5UubzgiVWSXTBfWpVd2Mx2AzRqJuDJCsgaDVZ7HB5iGzDW1Hl1sWN2mFyKjmR9uAog==", "license": "MIT", "engines": { "node": ">=6.9.0" } }, "node_modules/@babel/template": { "version": "7.27.1", "resolved": "https://registry.npmjs.org/@babel/template/-/template-7.27.1.tgz", "integrity": "sha512-Fyo3ghWMqkHHpHQCoBs2VnYjR4iWFFjguTDEqA5WgZDOrFesVjMhMM2FSqTKSoUSDO1VQtavj8NFpdRBEvJTtg==", "license": "MIT", "dependencies": { "@babel/code-frame": "^7.27.1", "@babel/parser": "^7.27.1", "@babel/types": "^7.27.1" }, "engines": { "node": ">=6.9.0" } }, "node_modules/@babel/traverse": { "version": "7.27.1", "resolved": "https://registry.npmjs.org/@babel/traverse/-/traverse-7.27.1.tgz", "integrity": "sha512-ZCYtZciz1IWJB4U61UPu4KEaqyfj+r5T1Q5mqPo+IBpcG9kHv30Z0aD8LXPgC1trYa6rK0orRyAhqUgk4MjmEg==", "license": "MIT", "dependencies": { "@babel/code-frame": "^7.27.1", "@babel/generator": "^7.27.1", "@babel/parser": "^7.27.1", "@babel/template": "^7.27.1", "@babel/types": "^7.27.1", "debug": "^4.3.1", "globals": "^11.1.0" }, "engines": { "node": ">=6.9.0" } }, "node_modules/@babel/traverse/node_modules/globals": { "version": "11.12.0", "resolved": "https://registry.npmjs.org/globals/-/globals-11.12.0.tgz", "integrity": "sha512-WOBp/EEGUiIsJSp7wcv/y6MO+lV9UoncWqxuFfm8eBwzWNgyfBd6Gz+IeKQ9jCmyhoH99g15M3T+QaVHFjizVA==", "license": "MIT", "engines": { "node": ">=4" } }, "node_modules/@babel/types": { "version": "7.27.1", "resolved": "https://registry.npmjs.org/@babel/types/-/types-7.27.1.tgz", "integrity": "sha512-+EzkxvLNfiUeKMgy/3luqfsCWFRXLb7U6wNQTk60tovuckwB15B191tJWvpp4HjiQWdJkCxO3Wbvc6jlk3Xb2Q==", "license": "MIT", "dependencies": { "@babel/helper-string-parser": "^7.27.1", "@babel/helper-validator-identifier": "^7.27.1" }, "engines": { "node": ">=6.9.0" } }, "node_modules/@emotion/babel-plugin": { "version": "11.13.5", "resolved": "https://registry.npmjs.org/@emotion/babel-plugin/-/babel-plugin-11.13.5.tgz", "integrity": "sha512-pxHCpT2ex+0q+HH91/zsdHkw/lXd468DIN2zvfvLtPKLLMo6gQj7oLObq8PhkrxOZb/gGCq03S3Z7PDhS8pduQ==", "license": "MIT", "dependencies": { "@babel/helper-module-imports": "^7.16.7", "@babel/runtime": "^7.18.3", "@emotion/hash": "^0.9.2", "@emotion/memoize": "^0.9.0", "@emotion/serialize": "^1.3.3", "babel-plugin-macros": "^3.1.0", "convert-source-map": "^1.5.0", "escape-string-regexp": "^4.0.0", "find-root": "^1.1.0", "source-map": "^0.5.7", "stylis": "4.2.0" } }, "node_modules/@emotion/babel-plugin/node_modules/convert-source-map": { "version": "1.9.0", "resolved": "https://registry.npmjs.org/convert-source-map/-/convert-source-map-1.9.0.tgz", "integrity": "sha512-ASFBup0Mz1uyiIjANan1jzLQami9z1PoYSZCiiYW2FczPbenXc45FZdBZLzOT+r6+iciuEModtmCti+hjaAk0A==", "license": "MIT" }, "node_modules/@emotion/cache": { "version": "11.14.0", "resolved": "https://registry.npmjs.org/@emotion/cache/-/cache-11.14.0.tgz", "integrity": "sha512-L/B1lc/TViYk4DcpGxtAVbx0ZyiKM5ktoIyafGkH6zg/tj+mA+NE//aPYKG0k8kCHSHVJrpLpcAlOBEXQ3SavA==", "license": "MIT", "dependencies": { "@emotion/memoize": "^0.9.0", "@emotion/sheet": "^1.4.0", "@emotion/utils": "^1.4.2", "@emotion/weak-memoize": "^0.4.0", "stylis": "4.2.0" } }, "node_modules/@emotion/hash": { "version": "0.9.2", "resolved": "https://registry.npmjs.org/@emotion/hash/-/hash-0.9.2.tgz", "integrity": "sha512-MyqliTZGuOm3+5ZRSaaBGP3USLw6+EGykkwZns2EPC5g8jJ4z9OrdZY9apkl3+UP9+sdz76YYkwCKP5gh8iY3g==", "license": "MIT" }, "node_modules/@emotion/memoize": { "version": "0.9.0", "resolved": "https://registry.npmjs.org/@emotion/memoize/-/memoize-0.9.0.tgz", "integrity": "sha512-30FAj7/EoJ5mwVPOWhAyCX+FPfMDrVecJAM+Iw9NRoSl4BBAQeqj4cApHHUXOVvIPgLVDsCFoz/hGD+5QQD1GQ==", "license": "MIT" }, "node_modules/@emotion/react": { "version": "11.14.0", "resolved": "https://registry.npmjs.org/@emotion/react/-/react-11.14.0.tgz", "integrity": "sha512-O000MLDBDdk/EohJPFUqvnp4qnHeYkVP5B0xEG0D/L7cOKP9kefu2DXn8dj74cQfsEzUqh+sr1RzFqiL1o+PpA==", "license": "MIT", "dependencies": { "@babel/runtime": "^7.18.3", "@emotion/babel-plugin": "^11.13.5", "@emotion/cache": "^11.14.0", "@emotion/serialize": "^1.3.3", "@emotion/use-insertion-effect-with-fallbacks": "^1.2.0", "@emotion/utils": "^1.4.2", "@emotion/weak-memoize": "^0.4.0", "hoist-non-react-statics": "^3.3.1" }, "peerDependencies": { "react": ">=16.8.0" }, "peerDependenciesMeta": { "@types/react": { "optional": true } } }, "node_modules/@emotion/serialize": { "version": "1.3.3", "resolved": "https://registry.npmjs.org/@emotion/serialize/-/serialize-1.3.3.tgz", "integrity": "sha512-EISGqt7sSNWHGI76hC7x1CksiXPahbxEOrC5RjmFRJTqLyEK9/9hZvBbiYn70dw4wuwMKiEMCUlR6ZXTSWQqxA==", "license": "MIT", "dependencies": { "@emotion/hash": "^0.9.2", "@emotion/memoize": "^0.9.0", "@emotion/unitless": "^0.10.0", "@emotion/utils": "^1.4.2", "csstype": "^3.0.2" } }, "node_modules/@emotion/sheet": { "version": "1.4.0", "resolved": "https://registry.npmjs.org/@emotion/sheet/-/sheet-1.4.0.tgz", "integrity": "sha512-fTBW9/8r2w3dXWYM4HCB1Rdp8NLibOw2+XELH5m5+AkWiL/KqYX6dc0kKYlaYyKjrQ6ds33MCdMPEwgs2z1rqg==", "license": "MIT" }, "node_modules/@emotion/unitless": { "version": "0.10.0", "resolved": "https://registry.npmjs.org/@emotion/unitless/-/unitless-0.10.0.tgz", "integrity": "sha512-dFoMUuQA20zvtVTuxZww6OHoJYgrzfKM1t52mVySDJnMSEa08ruEvdYQbhvyu6soU+NeLVd3yKfTfT0NeV6qGg==", "license": "MIT" }, "node_modules/@emotion/use-insertion-effect-with-fallbacks": { "version": "1.2.0", "resolved": "https://registry.npmjs.org/@emotion/use-insertion-effect-with-fallbacks/-/use-insertion-effect-with-fallbacks-1.2.0.tgz", "integrity": "sha512-yJMtVdH59sxi/aVJBpk9FQq+OR8ll5GT8oWd57UpeaKEVGab41JWaCFA7FRLoMLloOZF/c/wsPoe+bfGmRKgDg==", "license": "MIT", "peerDependencies": { "react": ">=16.8.0" } }, "node_modules/@emotion/utils": { "version": "1.4.2", "resolved": "https://registry.npmjs.org/@emotion/utils/-/utils-1.4.2.tgz", "integrity": "sha512-3vLclRofFziIa3J2wDh9jjbkUz9qk5Vi3IZ/FSTKViB0k+ef0fPV7dYrUIugbgupYDx7v9ud/SjrtEP8Y4xLoA==", "license": "MIT" }, "node_modules/@emotion/weak-memoize": { "version": "0.4.0", "resolved": "https://registry.npmjs.org/@emotion/weak-memoize/-/weak-memoize-0.4.0.tgz", "integrity": "sha512-snKqtPW01tN0ui7yu9rGv69aJXr/a/Ywvl11sUjNtEcRc+ng/mQriFL0wLXMef74iHa/EkftbDzU9F8iFbH+zg==", "license": "MIT" }, "node_modules/@esbuild/aix-ppc64": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/aix-ppc64/-/aix-ppc64-0.25.3.tgz", "integrity": "sha512-W8bFfPA8DowP8l//sxjJLSLkD8iEjMc7cBVyP+u4cEv9sM7mdUCkgsj+t0n/BWPFtv7WWCN5Yzj0N6FJNUUqBQ==", "cpu": [ "ppc64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "aix" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/android-arm": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/android-arm/-/android-arm-0.25.3.tgz", "integrity": "sha512-PuwVXbnP87Tcff5I9ngV0lmiSu40xw1At6i3GsU77U7cjDDB4s0X2cyFuBiDa1SBk9DnvWwnGvVaGBqoFWPb7A==", "cpu": [ "arm" ], "dev": true, "license": "MIT", "optional": true, "os": [ "android" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/android-arm64": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/android-arm64/-/android-arm64-0.25.3.tgz", "integrity": "sha512-XelR6MzjlZuBM4f5z2IQHK6LkK34Cvv6Rj2EntER3lwCBFdg6h2lKbtRjpTTsdEjD/WSe1q8UyPBXP1x3i/wYQ==", "cpu": [ "arm64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "android" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/android-x64": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/android-x64/-/android-x64-0.25.3.tgz", "integrity": "sha512-ogtTpYHT/g1GWS/zKM0cc/tIebFjm1F9Aw1boQ2Y0eUQ+J89d0jFY//s9ei9jVIlkYi8AfOjiixcLJSGNSOAdQ==", "cpu": [ "x64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "android" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/darwin-arm64": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/darwin-arm64/-/darwin-arm64-0.25.3.tgz", "integrity": "sha512-eESK5yfPNTqpAmDfFWNsOhmIOaQA59tAcF/EfYvo5/QWQCzXn5iUSOnqt3ra3UdzBv073ykTtmeLJZGt3HhA+w==", "cpu": [ "arm64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "darwin" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/darwin-x64": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/darwin-x64/-/darwin-x64-0.25.3.tgz", "integrity": "sha512-Kd8glo7sIZtwOLcPbW0yLpKmBNWMANZhrC1r6K++uDR2zyzb6AeOYtI6udbtabmQpFaxJ8uduXMAo1gs5ozz8A==", "cpu": [ "x64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "darwin" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/freebsd-arm64": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/freebsd-arm64/-/freebsd-arm64-0.25.3.tgz", "integrity": "sha512-EJiyS70BYybOBpJth3M0KLOus0n+RRMKTYzhYhFeMwp7e/RaajXvP+BWlmEXNk6uk+KAu46j/kaQzr6au+JcIw==", "cpu": [ "arm64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "freebsd" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/freebsd-x64": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/freebsd-x64/-/freebsd-x64-0.25.3.tgz", "integrity": "sha512-Q+wSjaLpGxYf7zC0kL0nDlhsfuFkoN+EXrx2KSB33RhinWzejOd6AvgmP5JbkgXKmjhmpfgKZq24pneodYqE8Q==", "cpu": [ "x64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "freebsd" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/linux-arm": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/linux-arm/-/linux-arm-0.25.3.tgz", "integrity": "sha512-dUOVmAUzuHy2ZOKIHIKHCm58HKzFqd+puLaS424h6I85GlSDRZIA5ycBixb3mFgM0Jdh+ZOSB6KptX30DD8YOQ==", "cpu": [ "arm" ], "dev": true, "license": "MIT", "optional": true, "os": [ "linux" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/linux-arm64": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/linux-arm64/-/linux-arm64-0.25.3.tgz", "integrity": "sha512-xCUgnNYhRD5bb1C1nqrDV1PfkwgbswTTBRbAd8aH5PhYzikdf/ddtsYyMXFfGSsb/6t6QaPSzxtbfAZr9uox4A==", "cpu": [ "arm64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "linux" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/linux-ia32": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/linux-ia32/-/linux-ia32-0.25.3.tgz", "integrity": "sha512-yplPOpczHOO4jTYKmuYuANI3WhvIPSVANGcNUeMlxH4twz/TeXuzEP41tGKNGWJjuMhotpGabeFYGAOU2ummBw==", "cpu": [ "ia32" ], "dev": true, "license": "MIT", "optional": true, "os": [ "linux" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/linux-loong64": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/linux-loong64/-/linux-loong64-0.25.3.tgz", "integrity": "sha512-P4BLP5/fjyihmXCELRGrLd793q/lBtKMQl8ARGpDxgzgIKJDRJ/u4r1A/HgpBpKpKZelGct2PGI4T+axcedf6g==", "cpu": [ "loong64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "linux" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/linux-mips64el": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/linux-mips64el/-/linux-mips64el-0.25.3.tgz", "integrity": "sha512-eRAOV2ODpu6P5divMEMa26RRqb2yUoYsuQQOuFUexUoQndm4MdpXXDBbUoKIc0iPa4aCO7gIhtnYomkn2x+bag==", "cpu": [ "mips64el" ], "dev": true, "license": "MIT", "optional": true, "os": [ "linux" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/linux-ppc64": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/linux-ppc64/-/linux-ppc64-0.25.3.tgz", "integrity": "sha512-ZC4jV2p7VbzTlnl8nZKLcBkfzIf4Yad1SJM4ZMKYnJqZFD4rTI+pBG65u8ev4jk3/MPwY9DvGn50wi3uhdaghg==", "cpu": [ "ppc64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "linux" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/linux-riscv64": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/linux-riscv64/-/linux-riscv64-0.25.3.tgz", "integrity": "sha512-LDDODcFzNtECTrUUbVCs6j9/bDVqy7DDRsuIXJg6so+mFksgwG7ZVnTruYi5V+z3eE5y+BJZw7VvUadkbfg7QA==", "cpu": [ "riscv64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "linux" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/linux-s390x": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/linux-s390x/-/linux-s390x-0.25.3.tgz", "integrity": "sha512-s+w/NOY2k0yC2p9SLen+ymflgcpRkvwwa02fqmAwhBRI3SC12uiS10edHHXlVWwfAagYSY5UpmT/zISXPMW3tQ==", "cpu": [ "s390x" ], "dev": true, "license": "MIT", "optional": true, "os": [ "linux" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/linux-x64": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/linux-x64/-/linux-x64-0.25.3.tgz", "integrity": "sha512-nQHDz4pXjSDC6UfOE1Fw9Q8d6GCAd9KdvMZpfVGWSJztYCarRgSDfOVBY5xwhQXseiyxapkiSJi/5/ja8mRFFA==", "cpu": [ "x64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "linux" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/netbsd-arm64": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/netbsd-arm64/-/netbsd-arm64-0.25.3.tgz", "integrity": "sha512-1QaLtOWq0mzK6tzzp0jRN3eccmN3hezey7mhLnzC6oNlJoUJz4nym5ZD7mDnS/LZQgkrhEbEiTn515lPeLpgWA==", "cpu": [ "arm64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "netbsd" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/netbsd-x64": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/netbsd-x64/-/netbsd-x64-0.25.3.tgz", "integrity": "sha512-i5Hm68HXHdgv8wkrt+10Bc50zM0/eonPb/a/OFVfB6Qvpiirco5gBA5bz7S2SHuU+Y4LWn/zehzNX14Sp4r27g==", "cpu": [ "x64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "netbsd" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/openbsd-arm64": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/openbsd-arm64/-/openbsd-arm64-0.25.3.tgz", "integrity": "sha512-zGAVApJEYTbOC6H/3QBr2mq3upG/LBEXr85/pTtKiv2IXcgKV0RT0QA/hSXZqSvLEpXeIxah7LczB4lkiYhTAQ==", "cpu": [ "arm64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "openbsd" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/openbsd-x64": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/openbsd-x64/-/openbsd-x64-0.25.3.tgz", "integrity": "sha512-fpqctI45NnCIDKBH5AXQBsD0NDPbEFczK98hk/aa6HJxbl+UtLkJV2+Bvy5hLSLk3LHmqt0NTkKNso1A9y1a4w==", "cpu": [ "x64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "openbsd" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/sunos-x64": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/sunos-x64/-/sunos-x64-0.25.3.tgz", "integrity": "sha512-ROJhm7d8bk9dMCUZjkS8fgzsPAZEjtRJqCAmVgB0gMrvG7hfmPmz9k1rwO4jSiblFjYmNvbECL9uhaPzONMfgA==", "cpu": [ "x64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "sunos" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/win32-arm64": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/win32-arm64/-/win32-arm64-0.25.3.tgz", "integrity": "sha512-YWcow8peiHpNBiIXHwaswPnAXLsLVygFwCB3A7Bh5jRkIBFWHGmNQ48AlX4xDvQNoMZlPYzjVOQDYEzWCqufMQ==", "cpu": [ "arm64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "win32" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/win32-ia32": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/win32-ia32/-/win32-ia32-0.25.3.tgz", "integrity": "sha512-qspTZOIGoXVS4DpNqUYUs9UxVb04khS1Degaw/MnfMe7goQ3lTfQ13Vw4qY/Nj0979BGvMRpAYbs/BAxEvU8ew==", "cpu": [ "ia32" ], "dev": true, "license": "MIT", "optional": true, "os": [ "win32" ], "engines": { "node": ">=18" } }, "node_modules/@esbuild/win32-x64": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/@esbuild/win32-x64/-/win32-x64-0.25.3.tgz", "integrity": "sha512-ICgUR+kPimx0vvRzf+N/7L7tVSQeE3BYY+NhHRHXS1kBuPO7z2+7ea2HbhDyZdTephgvNvKrlDDKUexuCVBVvg==", "cpu": [ "x64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "win32" ], "engines": { "node": ">=18" } }, "node_modules/@eslint-community/eslint-utils": { "version": "4.7.0", "resolved": "https://registry.npmjs.org/@eslint-community/eslint-utils/-/eslint-utils-4.7.0.tgz", "integrity": "sha512-dyybb3AcajC7uha6CvhdVRJqaKyn7w2YKqKyAN37NKYgZT36w+iRb0Dymmc5qEJ549c/S31cMMSFd75bteCpCw==", "dev": true, "license": "MIT", "dependencies": { "eslint-visitor-keys": "^3.4.3" }, "engines": { "node": "^12.22.0 || ^14.17.0 || >=16.0.0" }, "funding": { "url": "https://opencollective.com/eslint" }, "peerDependencies": { "eslint": "^6.0.0 || ^7.0.0 || >=8.0.0" } }, "node_modules/@eslint-community/eslint-utils/node_modules/eslint-visitor-keys": { "version": "3.4.3", "resolved": "https://registry.npmjs.org/eslint-visitor-keys/-/eslint-visitor-keys-3.4.3.tgz", "integrity": "sha512-wpc+LXeiyiisxPlEkUzU6svyS1frIO3Mgxj1fdy7Pm8Ygzguax2N3Fa/D/ag1WqbOprdI+uY6wMUl8/a2G+iag==", "dev": true, "license": "Apache-2.0", "engines": { "node": "^12.22.0 || ^14.17.0 || >=16.0.0" }, "funding": { "url": "https://opencollective.com/eslint" } }, "node_modules/@eslint-community/regexpp": { "version": "4.12.1", "resolved": "https://registry.npmjs.org/@eslint-community/regexpp/-/regexpp-4.12.1.tgz", "integrity": "sha512-CCZCDJuduB9OUkFkY2IgppNZMi2lBQgD2qzwXkEia16cge2pijY/aXi96CJMquDMn3nJdlPV1A5KrJEXwfLNzQ==", "dev": true, "license": "MIT", "engines": { "node": "^12.0.0 || ^14.0.0 || >=16.0.0" } }, "node_modules/@eslint/config-array": { "version": "0.20.0", "resolved": "https://registry.npmjs.org/@eslint/config-array/-/config-array-0.20.0.tgz", "integrity": "sha512-fxlS1kkIjx8+vy2SjuCB94q3htSNrufYTXubwiBFeaQHbH6Ipi43gFJq2zCMt6PHhImH3Xmr0NksKDvchWlpQQ==", "dev": true, "license": "Apache-2.0", "dependencies": { "@eslint/object-schema": "^2.1.6", "debug": "^4.3.1", "minimatch": "^3.1.2" }, "engines": { "node": "^18.18.0 || ^20.9.0 || >=21.1.0" } }, "node_modules/@eslint/config-helpers": { "version": "0.2.2", "resolved": "https://registry.npmjs.org/@eslint/config-helpers/-/config-helpers-0.2.2.tgz", "integrity": "sha512-+GPzk8PlG0sPpzdU5ZvIRMPidzAnZDl/s9L+y13iodqvb8leL53bTannOrQ/Im7UkpsmFU5Ily5U60LWixnmLg==", "dev": true, "license": "Apache-2.0", "engines": { "node": "^18.18.0 || ^20.9.0 || >=21.1.0" } }, "node_modules/@eslint/core": { "version": "0.13.0", "resolved": "https://registry.npmjs.org/@eslint/core/-/core-0.13.0.tgz", "integrity": "sha512-yfkgDw1KR66rkT5A8ci4irzDysN7FRpq3ttJolR88OqQikAWqwA8j5VZyas+vjyBNFIJ7MfybJ9plMILI2UrCw==", "dev": true, "license": "Apache-2.0", "dependencies": { "@types/json-schema": "^7.0.15" }, "engines": { "node": "^18.18.0 || ^20.9.0 || >=21.1.0" } }, "node_modules/@eslint/eslintrc": { "version": "3.3.1", "resolved": "https://registry.npmjs.org/@eslint/eslintrc/-/eslintrc-3.3.1.tgz", "integrity": "sha512-gtF186CXhIl1p4pJNGZw8Yc6RlshoePRvE0X91oPGb3vZ8pM3qOS9W9NGPat9LziaBV7XrJWGylNQXkGcnM3IQ==", "dev": true, "license": "MIT", "dependencies": { "ajv": "^6.12.4", "debug": "^4.3.2", "espree": "^10.0.1", "globals": "^14.0.0", "ignore": "^5.2.0", "import-fresh": "^3.2.1", "js-yaml": "^4.1.0", "minimatch": "^3.1.2", "strip-json-comments": "^3.1.1" }, "engines": { "node": "^18.18.0 || ^20.9.0 || >=21.1.0" }, "funding": { "url": "https://opencollective.com/eslint" } }, "node_modules/@eslint/eslintrc/node_modules/globals": { "version": "14.0.0", "resolved": "https://registry.npmjs.org/globals/-/globals-14.0.0.tgz", "integrity": "sha512-oahGvuMGQlPw/ivIYBjVSrWAfWLBeku5tpPE2fOPLi+WHffIWbuh2tCjhyQhTBPMf5E9jDEH4FOmTYgYwbKwtQ==", "dev": true, "license": "MIT", "engines": { "node": ">=18" }, "funding": { "url": "https://github.com/sponsors/sindresorhus" } }, "node_modules/@eslint/js": { "version": "9.26.0", "resolved": "https://registry.npmjs.org/@eslint/js/-/js-9.26.0.tgz", "integrity": "sha512-I9XlJawFdSMvWjDt6wksMCrgns5ggLNfFwFvnShsleWruvXM514Qxk8V246efTw+eo9JABvVz+u3q2RiAowKxQ==", "dev": true, "license": "MIT", "engines": { "node": "^18.18.0 || ^20.9.0 || >=21.1.0" } }, "node_modules/@eslint/object-schema": { "version": "2.1.6", "resolved": "https://registry.npmjs.org/@eslint/object-schema/-/object-schema-2.1.6.tgz", "integrity": "sha512-RBMg5FRL0I0gs51M/guSAj5/e14VQ4tpZnQNWwuDT66P14I43ItmPfIZRhO9fUVIPOAQXU47atlywZ/czoqFPA==", "dev": true, "license": "Apache-2.0", "engines": { "node": "^18.18.0 || ^20.9.0 || >=21.1.0" } }, "node_modules/@eslint/plugin-kit": { "version": "0.2.8", "resolved": "https://registry.npmjs.org/@eslint/plugin-kit/-/plugin-kit-0.2.8.tgz", "integrity": "sha512-ZAoA40rNMPwSm+AeHpCq8STiNAwzWLJuP8Xv4CHIc9wv/PSuExjMrmjfYNj682vW0OOiZ1HKxzvjQr9XZIisQA==", "dev": true, "license": "Apache-2.0", "dependencies": { "@eslint/core": "^0.13.0", "levn": "^0.4.1" }, "engines": { "node": "^18.18.0 || ^20.9.0 || >=21.1.0" } }, "node_modules/@floating-ui/core": { "version": "1.7.0", "resolved": "https://registry.npmjs.org/@floating-ui/core/-/core-1.7.0.tgz", "integrity": "sha512-FRdBLykrPPA6P76GGGqlex/e7fbe0F1ykgxHYNXQsH/iTEtjMj/f9bpY5oQqbjt5VgZvgz/uKXbGuROijh3VLA==", "license": "MIT", "dependencies": { "@floating-ui/utils": "^0.2.9" } }, "node_modules/@floating-ui/dom": { "version": "1.7.0", "resolved": "https://registry.npmjs.org/@floating-ui/dom/-/dom-1.7.0.tgz", "integrity": "sha512-lGTor4VlXcesUMh1cupTUTDoCxMb0V6bm3CnxHzQcw8Eaf1jQbgQX4i02fYgT0vJ82tb5MZ4CZk1LRGkktJCzg==", "license": "MIT", "dependencies": { "@floating-ui/core": "^1.7.0", "@floating-ui/utils": "^0.2.9" } }, "node_modules/@floating-ui/utils": { "version": "0.2.9", "resolved": "https://registry.npmjs.org/@floating-ui/utils/-/utils-0.2.9.tgz", "integrity": "sha512-MDWhGtE+eHw5JW7lq4qhc5yRLS11ERl1c7Z6Xd0a58DozHES6EnNNwUWbMiG4J9Cgj053Bhk8zvlhFYKVhULwg==", "license": "MIT" }, "node_modules/@humanfs/core": { "version": "0.19.1", "resolved": "https://registry.npmjs.org/@humanfs/core/-/core-0.19.1.tgz", "integrity": "sha512-5DyQ4+1JEUzejeK1JGICcideyfUbGixgS9jNgex5nqkW+cY7WZhxBigmieN5Qnw9ZosSNVC9KQKyb+GUaGyKUA==", "dev": true, "license": "Apache-2.0", "engines": { "node": ">=18.18.0" } }, "node_modules/@humanfs/node": { "version": "0.16.6", "resolved": "https://registry.npmjs.org/@humanfs/node/-/node-0.16.6.tgz", "integrity": "sha512-YuI2ZHQL78Q5HbhDiBA1X4LmYdXCKCMQIfw0pw7piHJwyREFebJUvrQN4cMssyES6x+vfUbx1CIpaQUKYdQZOw==", "dev": true, "license": "Apache-2.0", "dependencies": { "@humanfs/core": "^0.19.1", "@humanwhocodes/retry": "^0.3.0" }, "engines": { "node": ">=18.18.0" } }, "node_modules/@humanfs/node/node_modules/@humanwhocodes/retry": { "version": "0.3.1", "resolved": "https://registry.npmjs.org/@humanwhocodes/retry/-/retry-0.3.1.tgz", "integrity": "sha512-JBxkERygn7Bv/GbN5Rv8Ul6LVknS+5Bp6RgDC/O8gEBU/yeH5Ui5C/OlWrTb6qct7LjjfT6Re2NxB0ln0yYybA==", "dev": true, "license": "Apache-2.0", "engines": { "node": ">=18.18" }, "funding": { "type": "github", "url": "https://github.com/sponsors/nzakas" } }, "node_modules/@humanwhocodes/module-importer": { "version": "1.0.1", "resolved": "https://registry.npmjs.org/@humanwhocodes/module-importer/-/module-importer-1.0.1.tgz", "integrity": "sha512-bxveV4V8v5Yb4ncFTT3rPSgZBOpCkjfK0y4oVVVJwIuDVBRMDXrPyXRL988i5ap9m9bnyEEjWfm5WkBmtffLfA==", "dev": true, "license": "Apache-2.0", "engines": { "node": ">=12.22" }, "funding": { "type": "github", "url": "https://github.com/sponsors/nzakas" } }, "node_modules/@humanwhocodes/retry": { "version": "0.4.2", "resolved": "https://registry.npmjs.org/@humanwhocodes/retry/-/retry-0.4.2.tgz", "integrity": "sha512-xeO57FpIu4p1Ri3Jq/EXq4ClRm86dVF2z/+kvFnyqVYRavTZmaFaUBbWCOuuTh0o/g7DSsk6kc2vrS4Vl5oPOQ==", "dev": true, "license": "Apache-2.0", "engines": { "node": ">=18.18" }, "funding": { "type": "github", "url": "https://github.com/sponsors/nzakas" } }, "node_modules/@isaacs/cliui": { "version": "8.0.2", "resolved": "https://registry.npmjs.org/@isaacs/cliui/-/cliui-8.0.2.tgz", "integrity": "sha512-O8jcjabXaleOG9DQ0+ARXWZBTfnP4WNAqzuiJK7ll44AmxGKv/J2M4TPjxjY3znBCfvBXFzucm1twdyFybFqEA==", "dev": true, "license": "ISC", "dependencies": { "string-width": "^5.1.2", "string-width-cjs": "npm:string-width@^4.2.0", "strip-ansi": "^7.0.1", "strip-ansi-cjs": "npm:strip-ansi@^6.0.1", "wrap-ansi": "^8.1.0", "wrap-ansi-cjs": "npm:wrap-ansi@^7.0.0" }, "engines": { "node": ">=12" } }, "node_modules/@jridgewell/gen-mapping": { "version": "0.3.8", "resolved": "https://registry.npmjs.org/@jridgewell/gen-mapping/-/gen-mapping-0.3.8.tgz", "integrity": "sha512-imAbBGkb+ebQyxKgzv5Hu2nmROxoDOXHh80evxdoXNOrvAnVx7zimzc1Oo5h9RlfV4vPXaE2iM5pOFbvOCClWA==", "license": "MIT", "dependencies": { "@jridgewell/set-array": "^1.2.1", "@jridgewell/sourcemap-codec": "^1.4.10", "@jridgewell/trace-mapping": "^0.3.24" }, "engines": { "node": ">=6.0.0" } }, "node_modules/@jridgewell/resolve-uri": { "version": "3.1.2", "resolved": "https://registry.npmjs.org/@jridgewell/resolve-uri/-/resolve-uri-3.1.2.tgz", "integrity": "sha512-bRISgCIjP20/tbWSPWMEi54QVPRZExkuD9lJL+UIxUKtwVJA8wW1Trb1jMs1RFXo1CBTNZ/5hpC9QvmKWdopKw==", "license": "MIT", "engines": { "node": ">=6.0.0" } }, "node_modules/@jridgewell/set-array": { "version": "1.2.1", "resolved": "https://registry.npmjs.org/@jridgewell/set-array/-/set-array-1.2.1.tgz", "integrity": "sha512-R8gLRTZeyp03ymzP/6Lil/28tGeGEzhx1q2k703KGWRAI1VdvPIXdG70VJc2pAMw3NA6JKL5hhFu1sJX0Mnn/A==", "license": "MIT", "engines": { "node": ">=6.0.0" } }, "node_modules/@jridgewell/sourcemap-codec": { "version": "1.5.0", "resolved": "https://registry.npmjs.org/@jridgewell/sourcemap-codec/-/sourcemap-codec-1.5.0.tgz", "integrity": "sha512-gv3ZRaISU3fjPAgNsriBRqGWQL6quFx04YMPW/zD8XMLsU32mhCCbfbO6KZFLjvYpCZ8zyDEgqsgf+PwPaM7GQ==", "license": "MIT" }, "node_modules/@jridgewell/trace-mapping": { "version": "0.3.25", "resolved": "https://registry.npmjs.org/@jridgewell/trace-mapping/-/trace-mapping-0.3.25.tgz", "integrity": "sha512-vNk6aEwybGtawWmy/PzwnGDOjCkLWSD2wqvjGGAgOAwCGWySYXfYoxt00IJkTF+8Lb57DwOb3Aa0o9CApepiYQ==", "license": "MIT", "dependencies": { "@jridgewell/resolve-uri": "^3.1.0", "@jridgewell/sourcemap-codec": "^1.4.14" } }, "node_modules/@modelcontextprotocol/sdk": { "version": "1.11.0", "resolved": "https://registry.npmjs.org/@modelcontextprotocol/sdk/-/sdk-1.11.0.tgz", "integrity": "sha512-k/1pb70eD638anoi0e8wUGAlbMJXyvdV4p62Ko+EZ7eBe1xMx8Uhak1R5DgfoofsK5IBBnRwsYGTaLZl+6/+RQ==", "dev": true, "license": "MIT", "dependencies": { "content-type": "^1.0.5", "cors": "^2.8.5", "cross-spawn": "^7.0.3", "eventsource": "^3.0.2", "express": "^5.0.1", "express-rate-limit": "^7.5.0", "pkce-challenge": "^5.0.0", "raw-body": "^3.0.0", "zod": "^3.23.8", "zod-to-json-schema": "^3.24.1" }, "engines": { "node": ">=18" } }, "node_modules/@nodelib/fs.scandir": { "version": "2.1.5", "resolved": "https://registry.npmjs.org/@nodelib/fs.scandir/-/fs.scandir-2.1.5.tgz", "integrity": "sha512-vq24Bq3ym5HEQm2NKCr3yXDwjc7vTsEThRDnkp2DK9p1uqLR+DHurm/NOTo0KG7HYHU7eppKZj3MyqYuMBf62g==", "dev": true, "license": "MIT", "dependencies": { "@nodelib/fs.stat": "2.0.5", "run-parallel": "^1.1.9" }, "engines": { "node": ">= 8" } }, "node_modules/@nodelib/fs.stat": { "version": "2.0.5", "resolved": "https://registry.npmjs.org/@nodelib/fs.stat/-/fs.stat-2.0.5.tgz", "integrity": "sha512-RkhPPp2zrqDAQA/2jNhnztcPAlv64XdhIp7a7454A5ovI7Bukxgt7MX7udwAu3zg1DcpPU0rz3VV1SeaqvY4+A==", "dev": true, "license": "MIT", "engines": { "node": ">= 8" } }, "node_modules/@nodelib/fs.walk": { "version": "1.2.8", "resolved": "https://registry.npmjs.org/@nodelib/fs.walk/-/fs.walk-1.2.8.tgz", "integrity": "sha512-oGB+UxlgWcgQkgwo8GcEGwemoTFt3FIO9ababBmaGwXIoBKZ+GTy0pP185beGg7Llih/NSHSV2XAs1lnznocSg==", "dev": true, "license": "MIT", "dependencies": { "@nodelib/fs.scandir": "2.1.5", "fastq": "^1.6.0" }, "engines": { "node": ">= 8" } }, "node_modules/@pkgjs/parseargs": { "version": "0.11.0", "resolved": "https://registry.npmjs.org/@pkgjs/parseargs/-/parseargs-0.11.0.tgz", "integrity": "sha512-+1VkjdD0QBLPodGrJUeqarH8VAIvQODIbwh9XpP5Syisf7YoQgsJKPNFoqqLQlu+VQ/tVSshMR6loPMn8U+dPg==", "dev": true, "license": "MIT", "optional": true, "engines": { "node": ">=14" } }, "node_modules/@rollup/rollup-android-arm-eabi": { "version": "4.40.1", "resolved": "https://registry.npmjs.org/@rollup/rollup-android-arm-eabi/-/rollup-android-arm-eabi-4.40.1.tgz", "integrity": "sha512-kxz0YeeCrRUHz3zyqvd7n+TVRlNyTifBsmnmNPtk3hQURUyG9eAB+usz6DAwagMusjx/zb3AjvDUvhFGDAexGw==", "cpu": [ "arm" ], "dev": true, "license": "MIT", "optional": true, "os": [ "android" ] }, "node_modules/@rollup/rollup-android-arm64": { "version": "4.40.1", "resolved": "https://registry.npmjs.org/@rollup/rollup-android-arm64/-/rollup-android-arm64-4.40.1.tgz", "integrity": "sha512-PPkxTOisoNC6TpnDKatjKkjRMsdaWIhyuMkA4UsBXT9WEZY4uHezBTjs6Vl4PbqQQeu6oION1w2voYZv9yquCw==", "cpu": [ "arm64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "android" ] }, "node_modules/@rollup/rollup-darwin-arm64": { "version": "4.40.1", "resolved": "https://registry.npmjs.org/@rollup/rollup-darwin-arm64/-/rollup-darwin-arm64-4.40.1.tgz", "integrity": "sha512-VWXGISWFY18v/0JyNUy4A46KCFCb9NVsH+1100XP31lud+TzlezBbz24CYzbnA4x6w4hx+NYCXDfnvDVO6lcAA==", "cpu": [ "arm64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "darwin" ] }, "node_modules/@rollup/rollup-darwin-x64": { "version": "4.40.1", "resolved": "https://registry.npmjs.org/@rollup/rollup-darwin-x64/-/rollup-darwin-x64-4.40.1.tgz", "integrity": "sha512-nIwkXafAI1/QCS7pxSpv/ZtFW6TXcNUEHAIA9EIyw5OzxJZQ1YDrX+CL6JAIQgZ33CInl1R6mHet9Y/UZTg2Bw==", "cpu": [ "x64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "darwin" ] }, "node_modules/@rollup/rollup-freebsd-arm64": { "version": "4.40.1", "resolved": "https://registry.npmjs.org/@rollup/rollup-freebsd-arm64/-/rollup-freebsd-arm64-4.40.1.tgz", "integrity": "sha512-BdrLJ2mHTrIYdaS2I99mriyJfGGenSaP+UwGi1kB9BLOCu9SR8ZpbkmmalKIALnRw24kM7qCN0IOm6L0S44iWw==", "cpu": [ "arm64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "freebsd" ] }, "node_modules/@rollup/rollup-freebsd-x64": { "version": "4.40.1", "resolved": "https://registry.npmjs.org/@rollup/rollup-freebsd-x64/-/rollup-freebsd-x64-4.40.1.tgz", "integrity": "sha512-VXeo/puqvCG8JBPNZXZf5Dqq7BzElNJzHRRw3vjBE27WujdzuOPecDPc/+1DcdcTptNBep3861jNq0mYkT8Z6Q==", "cpu": [ "x64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "freebsd" ] }, "node_modules/@rollup/rollup-linux-arm-gnueabihf": { "version": "4.40.1", "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-arm-gnueabihf/-/rollup-linux-arm-gnueabihf-4.40.1.tgz", "integrity": "sha512-ehSKrewwsESPt1TgSE/na9nIhWCosfGSFqv7vwEtjyAqZcvbGIg4JAcV7ZEh2tfj/IlfBeZjgOXm35iOOjadcg==", "cpu": [ "arm" ], "dev": true, "license": "MIT", "optional": true, "os": [ "linux" ] }, "node_modules/@rollup/rollup-linux-arm-musleabihf": { "version": "4.40.1", "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-arm-musleabihf/-/rollup-linux-arm-musleabihf-4.40.1.tgz", "integrity": "sha512-m39iO/aaurh5FVIu/F4/Zsl8xppd76S4qoID8E+dSRQvTyZTOI2gVk3T4oqzfq1PtcvOfAVlwLMK3KRQMaR8lg==", "cpu": [ "arm" ], "dev": true, "license": "MIT", "optional": true, "os": [ "linux" ] }, "node_modules/@rollup/rollup-linux-arm64-gnu": { "version": "4.40.1", "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-arm64-gnu/-/rollup-linux-arm64-gnu-4.40.1.tgz", "integrity": "sha512-Y+GHnGaku4aVLSgrT0uWe2o2Rq8te9hi+MwqGF9r9ORgXhmHK5Q71N757u0F8yU1OIwUIFy6YiJtKjtyktk5hg==", "cpu": [ "arm64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "linux" ] }, "node_modules/@rollup/rollup-linux-arm64-musl": { "version": "4.40.1", "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-arm64-musl/-/rollup-linux-arm64-musl-4.40.1.tgz", "integrity": "sha512-jEwjn3jCA+tQGswK3aEWcD09/7M5wGwc6+flhva7dsQNRZZTe30vkalgIzV4tjkopsTS9Jd7Y1Bsj6a4lzz8gQ==", "cpu": [ "arm64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "linux" ] }, "node_modules/@rollup/rollup-linux-loongarch64-gnu": { "version": "4.40.1", "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-loongarch64-gnu/-/rollup-linux-loongarch64-gnu-4.40.1.tgz", "integrity": "sha512-ySyWikVhNzv+BV/IDCsrraOAZ3UaC8SZB67FZlqVwXwnFhPihOso9rPOxzZbjp81suB1O2Topw+6Ug3JNegejQ==", "cpu": [ "loong64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "linux" ] }, "node_modules/@rollup/rollup-linux-powerpc64le-gnu": { "version": "4.40.1", "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-powerpc64le-gnu/-/rollup-linux-powerpc64le-gnu-4.40.1.tgz", "integrity": "sha512-BvvA64QxZlh7WZWqDPPdt0GH4bznuL6uOO1pmgPnnv86rpUpc8ZxgZwcEgXvo02GRIZX1hQ0j0pAnhwkhwPqWg==", "cpu": [ "ppc64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "linux" ] }, "node_modules/@rollup/rollup-linux-riscv64-gnu": { "version": "4.40.1", "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-riscv64-gnu/-/rollup-linux-riscv64-gnu-4.40.1.tgz", "integrity": "sha512-EQSP+8+1VuSulm9RKSMKitTav89fKbHymTf25n5+Yr6gAPZxYWpj3DzAsQqoaHAk9YX2lwEyAf9S4W8F4l3VBQ==", "cpu": [ "riscv64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "linux" ] }, "node_modules/@rollup/rollup-linux-riscv64-musl": { "version": "4.40.1", "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-riscv64-musl/-/rollup-linux-riscv64-musl-4.40.1.tgz", "integrity": "sha512-n/vQ4xRZXKuIpqukkMXZt9RWdl+2zgGNx7Uda8NtmLJ06NL8jiHxUawbwC+hdSq1rrw/9CghCpEONor+l1e2gA==", "cpu": [ "riscv64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "linux" ] }, "node_modules/@rollup/rollup-linux-s390x-gnu": { "version": "4.40.1", "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-s390x-gnu/-/rollup-linux-s390x-gnu-4.40.1.tgz", "integrity": "sha512-h8d28xzYb98fMQKUz0w2fMc1XuGzLLjdyxVIbhbil4ELfk5/orZlSTpF/xdI9C8K0I8lCkq+1En2RJsawZekkg==", "cpu": [ "s390x" ], "dev": true, "license": "MIT", "optional": true, "os": [ "linux" ] }, "node_modules/@rollup/rollup-linux-x64-gnu": { "version": "4.40.1", "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-x64-gnu/-/rollup-linux-x64-gnu-4.40.1.tgz", "integrity": "sha512-XiK5z70PEFEFqcNj3/zRSz/qX4bp4QIraTy9QjwJAb/Z8GM7kVUsD0Uk8maIPeTyPCP03ChdI+VVmJriKYbRHQ==", "cpu": [ "x64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "linux" ] }, "node_modules/@rollup/rollup-linux-x64-musl": { "version": "4.40.1", "resolved": "https://registry.npmjs.org/@rollup/rollup-linux-x64-musl/-/rollup-linux-x64-musl-4.40.1.tgz", "integrity": "sha512-2BRORitq5rQ4Da9blVovzNCMaUlyKrzMSvkVR0D4qPuOy/+pMCrh1d7o01RATwVy+6Fa1WBw+da7QPeLWU/1mQ==", "cpu": [ "x64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "linux" ] }, "node_modules/@rollup/rollup-win32-arm64-msvc": { "version": "4.40.1", "resolved": "https://registry.npmjs.org/@rollup/rollup-win32-arm64-msvc/-/rollup-win32-arm64-msvc-4.40.1.tgz", "integrity": "sha512-b2bcNm9Kbde03H+q+Jjw9tSfhYkzrDUf2d5MAd1bOJuVplXvFhWz7tRtWvD8/ORZi7qSCy0idW6tf2HgxSXQSg==", "cpu": [ "arm64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "win32" ] }, "node_modules/@rollup/rollup-win32-ia32-msvc": { "version": "4.40.1", "resolved": "https://registry.npmjs.org/@rollup/rollup-win32-ia32-msvc/-/rollup-win32-ia32-msvc-4.40.1.tgz", "integrity": "sha512-DfcogW8N7Zg7llVEfpqWMZcaErKfsj9VvmfSyRjCyo4BI3wPEfrzTtJkZG6gKP/Z92wFm6rz2aDO7/JfiR/whA==", "cpu": [ "ia32" ], "dev": true, "license": "MIT", "optional": true, "os": [ "win32" ] }, "node_modules/@rollup/rollup-win32-x64-msvc": { "version": "4.40.1", "resolved": "https://registry.npmjs.org/@rollup/rollup-win32-x64-msvc/-/rollup-win32-x64-msvc-4.40.1.tgz", "integrity": "sha512-ECyOuDeH3C1I8jH2MK1RtBJW+YPMvSfT0a5NN0nHfQYnDSJ6tUiZH3gzwVP5/Kfh/+Tt7tpWVF9LXNTnhTJ3kA==", "cpu": [ "x64" ], "dev": true, "license": "MIT", "optional": true, "os": [ "win32" ] }, "node_modules/@tanstack/query-core": { "version": "5.75.0", "resolved": "https://registry.npmjs.org/@tanstack/query-core/-/query-core-5.75.0.tgz", "integrity": "sha512-rk8KQuCdhoRkzjRVF3QxLgAfFUyS0k7+GCQjlGEpEGco+qazJ0eMH6aO1DjDjibH7/ik383nnztua3BG+lOnwg==", "license": "MIT", "funding": { "type": "github", "url": "https://github.com/sponsors/tannerlinsley" } }, "node_modules/@tanstack/react-query": { "version": "5.75.2", "resolved": "https://registry.npmjs.org/@tanstack/react-query/-/react-query-5.75.2.tgz", "integrity": "sha512-8F8VOsWUfSkCFoi62O9HSZT9jDgg28Ln8Z2dYKfRo/O2A0sgvr0uxTuNoon3PPXoDuHofv5V3elBI1M2Gh1MPg==", "license": "MIT", "dependencies": { "@tanstack/query-core": "5.75.0" }, "funding": { "type": "github", "url": "https://github.com/sponsors/tannerlinsley" }, "peerDependencies": { "react": "^18 || ^19" } }, "node_modules/@types/babel__core": { "version": "7.20.5", "resolved": "https://registry.npmjs.org/@types/babel__core/-/babel__core-7.20.5.tgz", "integrity": "sha512-qoQprZvz5wQFJwMDqeseRXWv3rqMvhgpbXFfVyWhbx9X47POIA6i/+dXefEmZKoAgOaTdaIgNSMqMIU61yRyzA==", "dev": true, "license": "MIT", "dependencies": { "@babel/parser": "^7.20.7", "@babel/types": "^7.20.7", "@types/babel__generator": "*", "@types/babel__template": "*", "@types/babel__traverse": "*" } }, "node_modules/@types/babel__generator": { "version": "7.27.0", "resolved": "https://registry.npmjs.org/@types/babel__generator/-/babel__generator-7.27.0.tgz", "integrity": "sha512-ufFd2Xi92OAVPYsy+P4n7/U7e68fex0+Ee8gSG9KX7eo084CWiQ4sdxktvdl0bOPupXtVJPY19zk6EwWqUQ8lg==", "dev": true, "license": "MIT", "dependencies": { "@babel/types": "^7.0.0" } }, "node_modules/@types/babel__template": { "version": "7.4.4", "resolved": "https://registry.npmjs.org/@types/babel__template/-/babel__template-7.4.4.tgz", "integrity": "sha512-h/NUaSyG5EyxBIp8YRxo4RMe2/qQgvyowRwVMzhYhBCONbW8PUsg4lkFMrhgZhUe5z3L3MiLDuvyJ/CaPa2A8A==", "dev": true, "license": "MIT", "dependencies": { "@babel/parser": "^7.1.0", "@babel/types": "^7.0.0" } }, "node_modules/@types/babel__traverse": { "version": "7.20.7", "resolved": "https://registry.npmjs.org/@types/babel__traverse/-/babel__traverse-7.20.7.tgz", "integrity": "sha512-dkO5fhS7+/oos4ciWxyEyjWe48zmG6wbCheo/G2ZnHx4fs3EU6YC6UM8rk56gAjNJ9P3MTH2jo5jb92/K6wbng==", "dev": true, "license": "MIT", "dependencies": { "@babel/types": "^7.20.7" } }, "node_modules/@types/d3-array": { "version": "3.2.1", "resolved": "https://registry.npmjs.org/@types/d3-array/-/d3-array-3.2.1.tgz", "integrity": "sha512-Y2Jn2idRrLzUfAKV2LyRImR+y4oa2AntrgID95SHJxuMUrkNXmanDSed71sRNZysveJVt1hLLemQZIady0FpEg==", "license": "MIT" }, "node_modules/@types/d3-color": { "version": "3.1.3", "resolved": "https://registry.npmjs.org/@types/d3-color/-/d3-color-3.1.3.tgz", "integrity": "sha512-iO90scth9WAbmgv7ogoq57O9YpKmFBbmoEoCHDB2xMBY0+/KVrqAaCDyCE16dUspeOvIxFFRI+0sEtqDqy2b4A==", "license": "MIT" }, "node_modules/@types/d3-ease": { "version": "3.0.2", "resolved": "https://registry.npmjs.org/@types/d3-ease/-/d3-ease-3.0.2.tgz", "integrity": "sha512-NcV1JjO5oDzoK26oMzbILE6HW7uVXOHLQvHshBUW4UMdZGfiY6v5BeQwh9a9tCzv+CeefZQHJt5SRgK154RtiA==", "license": "MIT" }, "node_modules/@types/d3-interpolate": { "version": "3.0.4", "resolved": "https://registry.npmjs.org/@types/d3-interpolate/-/d3-interpolate-3.0.4.tgz", "integrity": "sha512-mgLPETlrpVV1YRJIglr4Ez47g7Yxjl1lj7YKsiMCb27VJH9W8NVM6Bb9d8kkpG/uAQS5AmbA48q2IAolKKo1MA==", "license": "MIT", "dependencies": { "@types/d3-color": "*" } }, "node_modules/@types/d3-path": { "version": "3.1.1", "resolved": "https://registry.npmjs.org/@types/d3-path/-/d3-path-3.1.1.tgz", "integrity": "sha512-VMZBYyQvbGmWyWVea0EHs/BwLgxc+MKi1zLDCONksozI4YJMcTt8ZEuIR4Sb1MMTE8MMW49v0IwI5+b7RmfWlg==", "license": "MIT" }, "node_modules/@types/d3-scale": { "version": "4.0.9", "resolved": "https://registry.npmjs.org/@types/d3-scale/-/d3-scale-4.0.9.tgz", "integrity": "sha512-dLmtwB8zkAeO/juAMfnV+sItKjlsw2lKdZVVy6LRr0cBmegxSABiLEpGVmSJJ8O08i4+sGR6qQtb6WtuwJdvVw==", "license": "MIT", "dependencies": { "@types/d3-time": "*" } }, "node_modules/@types/d3-shape": { "version": "3.1.7", "resolved": "https://registry.npmjs.org/@types/d3-shape/-/d3-shape-3.1.7.tgz", "integrity": "sha512-VLvUQ33C+3J+8p+Daf+nYSOsjB4GXp19/S/aGo60m9h1v6XaxjiT82lKVWJCfzhtuZ3yD7i/TPeC/fuKLLOSmg==", "license": "MIT", "dependencies": { "@types/d3-path": "*" } }, "node_modules/@types/d3-time": { "version": "3.0.4", "resolved": "https://registry.npmjs.org/@types/d3-time/-/d3-time-3.0.4.tgz", "integrity": "sha512-yuzZug1nkAAaBlBBikKZTgzCeA+k1uy4ZFwWANOfKw5z5LRhV0gNA7gNkKm7HoK+HRN0wX3EkxGk0fpbWhmB7g==", "license": "MIT" }, "node_modules/@types/d3-timer": { "version": "3.0.2", "resolved": "https://registry.npmjs.org/@types/d3-timer/-/d3-timer-3.0.2.tgz", "integrity": "sha512-Ps3T8E8dZDam6fUyNiMkekK3XUsaUEik+idO9/YjPtfj2qruF8tFBXS7XhtE4iIXBLxhmLjP3SXpLhVf21I9Lw==", "license": "MIT" }, "node_modules/@types/estree": { "version": "1.0.7", "resolved": "https://registry.npmjs.org/@types/estree/-/estree-1.0.7.tgz", "integrity": "sha512-w28IoSUCJpidD/TGviZwwMJckNESJZXFu7NBZ5YJ4mEUnNraUn9Pm8HSZm/jDF1pDWYKspWE7oVphigUPRakIQ==", "dev": true, "license": "MIT" }, "node_modules/@types/json-schema": { "version": "7.0.15", "resolved": "https://registry.npmjs.org/@types/json-schema/-/json-schema-7.0.15.tgz", "integrity": "sha512-5+fP8P8MFNC+AyZCDxrB2pkZFPGzqQWUzpSeuuVLvm8VMcorNYavBqoFcxK8bQz4Qsbn4oUEEem4wDLfcysGHA==", "dev": true, "license": "MIT" }, "node_modules/@types/parse-json": { "version": "4.0.2", "resolved": "https://registry.npmjs.org/@types/parse-json/-/parse-json-4.0.2.tgz", "integrity": "sha512-dISoDXWWQwUquiKsyZ4Ng+HX2KsPL7LyHKHQwgGFEA3IaKac4Obd+h2a/a6waisAoepJlBcx9paWqjA8/HVjCw==", "license": "MIT" }, "node_modules/@types/react": { "version": "19.1.2", "resolved": "https://registry.npmjs.org/@types/react/-/react-19.1.2.tgz", "integrity": "sha512-oxLPMytKchWGbnQM9O7D67uPa9paTNxO7jVoNMXgkkErULBPhPARCfkKL9ytcIJJRGjbsVwW4ugJzyFFvm/Tiw==", "license": "MIT", "dependencies": { "csstype": "^3.0.2" } }, "node_modules/@types/react-dom": { "version": "19.1.3", "resolved": "https://registry.npmjs.org/@types/react-dom/-/react-dom-19.1.3.tgz", "integrity": "sha512-rJXC08OG0h3W6wDMFxQrZF00Kq6qQvw0djHRdzl3U5DnIERz0MRce3WVc7IS6JYBwtaP/DwYtRRjVlvivNveKg==", "dev": true, "license": "MIT", "peerDependencies": { "@types/react": "^19.0.0" } }, "node_modules/@types/react-transition-group": { "version": "4.4.12", "resolved": "https://registry.npmjs.org/@types/react-transition-group/-/react-transition-group-4.4.12.tgz", "integrity": "sha512-8TV6R3h2j7a91c+1DXdJi3Syo69zzIZbz7Lg5tORM5LEJG7X/E6a1V3drRyBRZq7/utz7A+c4OgYLiLcYGHG6w==", "license": "MIT", "peerDependencies": { "@types/react": "*" } }, "node_modules/@typescript-eslint/eslint-plugin": { "version": "8.31.1", "resolved": "https://registry.npmjs.org/@typescript-eslint/eslint-plugin/-/eslint-plugin-8.31.1.tgz", "integrity": "sha512-oUlH4h1ABavI4F0Xnl8/fOtML/eu8nI2A1nYd+f+55XI0BLu+RIqKoCiZKNo6DtqZBEQm5aNKA20G3Z5w3R6GQ==", "dev": true, "license": "MIT", "dependencies": { "@eslint-community/regexpp": "^4.10.0", "@typescript-eslint/scope-manager": "8.31.1", "@typescript-eslint/type-utils": "8.31.1", "@typescript-eslint/utils": "8.31.1", "@typescript-eslint/visitor-keys": "8.31.1", "graphemer": "^1.4.0", "ignore": "^5.3.1", "natural-compare": "^1.4.0", "ts-api-utils": "^2.0.1" }, "engines": { "node": "^18.18.0 || ^20.9.0 || >=21.1.0" }, "funding": { "type": "opencollective", "url": "https://opencollective.com/typescript-eslint" }, "peerDependencies": { "@typescript-eslint/parser": "^8.0.0 || ^8.0.0-alpha.0", "eslint": "^8.57.0 || ^9.0.0", "typescript": ">=4.8.4 <5.9.0" } }, "node_modules/@typescript-eslint/parser": { "version": "8.31.1", "resolved": "https://registry.npmjs.org/@typescript-eslint/parser/-/parser-8.31.1.tgz", "integrity": "sha512-oU/OtYVydhXnumd0BobL9rkJg7wFJ9bFFPmSmB/bf/XWN85hlViji59ko6bSKBXyseT9V8l+CN1nwmlbiN0G7Q==", "dev": true, "license": "MIT", "dependencies": { "@typescript-eslint/scope-manager": "8.31.1", "@typescript-eslint/types": "8.31.1", "@typescript-eslint/typescript-estree": "8.31.1", "@typescript-eslint/visitor-keys": "8.31.1", "debug": "^4.3.4" }, "engines": { "node": "^18.18.0 || ^20.9.0 || >=21.1.0" }, "funding": { "type": "opencollective", "url": "https://opencollective.com/typescript-eslint" }, "peerDependencies": { "eslint": "^8.57.0 || ^9.0.0", "typescript": ">=4.8.4 <5.9.0" } }, "node_modules/@typescript-eslint/scope-manager": { "version": "8.31.1", "resolved": "https://registry.npmjs.org/@typescript-eslint/scope-manager/-/scope-manager-8.31.1.tgz", "integrity": "sha512-BMNLOElPxrtNQMIsFHE+3P0Yf1z0dJqV9zLdDxN/xLlWMlXK/ApEsVEKzpizg9oal8bAT5Sc7+ocal7AC1HCVw==", "dev": true, "license": "MIT", "dependencies": { "@typescript-eslint/types": "8.31.1", "@typescript-eslint/visitor-keys": "8.31.1" }, "engines": { "node": "^18.18.0 || ^20.9.0 || >=21.1.0" }, "funding": { "type": "opencollective", "url": "https://opencollective.com/typescript-eslint" } }, "node_modules/@typescript-eslint/type-utils": { "version": "8.31.1", "resolved": "https://registry.npmjs.org/@typescript-eslint/type-utils/-/type-utils-8.31.1.tgz", "integrity": "sha512-fNaT/m9n0+dpSp8G/iOQ05GoHYXbxw81x+yvr7TArTuZuCA6VVKbqWYVZrV5dVagpDTtj/O8k5HBEE/p/HM5LA==", "dev": true, "license": "MIT", "dependencies": { "@typescript-eslint/typescript-estree": "8.31.1", "@typescript-eslint/utils": "8.31.1", "debug": "^4.3.4", "ts-api-utils": "^2.0.1" }, "engines": { "node": "^18.18.0 || ^20.9.0 || >=21.1.0" }, "funding": { "type": "opencollective", "url": "https://opencollective.com/typescript-eslint" }, "peerDependencies": { "eslint": "^8.57.0 || ^9.0.0", "typescript": ">=4.8.4 <5.9.0" } }, "node_modules/@typescript-eslint/types": { "version": "8.31.1", "resolved": "https://registry.npmjs.org/@typescript-eslint/types/-/types-8.31.1.tgz", "integrity": "sha512-SfepaEFUDQYRoA70DD9GtytljBePSj17qPxFHA/h3eg6lPTqGJ5mWOtbXCk1YrVU1cTJRd14nhaXWFu0l2troQ==", "dev": true, "license": "MIT", "engines": { "node": "^18.18.0 || ^20.9.0 || >=21.1.0" }, "funding": { "type": "opencollective", "url": "https://opencollective.com/typescript-eslint" } }, "node_modules/@typescript-eslint/typescript-estree": { "version": "8.31.1", "resolved": "https://registry.npmjs.org/@typescript-eslint/typescript-estree/-/typescript-estree-8.31.1.tgz", "integrity": "sha512-kaA0ueLe2v7KunYOyWYtlf/QhhZb7+qh4Yw6Ni5kgukMIG+iP773tjgBiLWIXYumWCwEq3nLW+TUywEp8uEeag==", "dev": true, "license": "MIT", "dependencies": { "@typescript-eslint/types": "8.31.1", "@typescript-eslint/visitor-keys": "8.31.1", "debug": "^4.3.4", "fast-glob": "^3.3.2", "is-glob": "^4.0.3", "minimatch": "^9.0.4", "semver": "^7.6.0", "ts-api-utils": "^2.0.1" }, "engines": { "node": "^18.18.0 || ^20.9.0 || >=21.1.0" }, "funding": { "type": "opencollective", "url": "https://opencollective.com/typescript-eslint" }, "peerDependencies": { "typescript": ">=4.8.4 <5.9.0" } }, "node_modules/@typescript-eslint/typescript-estree/node_modules/brace-expansion": { "version": "2.0.1", "resolved": "https://registry.npmjs.org/brace-expansion/-/brace-expansion-2.0.1.tgz", "integrity": "sha512-XnAIvQ8eM+kC6aULx6wuQiwVsnzsi9d3WxzV3FpWTGA19F621kwdbsAcFKXgKUHZWsy+mY6iL1sHTxWEFCytDA==", "dev": true, "license": "MIT", "dependencies": { "balanced-match": "^1.0.0" } }, "node_modules/@typescript-eslint/typescript-estree/node_modules/minimatch": { "version": "9.0.5", "resolved": "https://registry.npmjs.org/minimatch/-/minimatch-9.0.5.tgz", "integrity": "sha512-G6T0ZX48xgozx7587koeX9Ys2NYy6Gmv//P89sEte9V9whIapMNF4idKxnW2QtCcLiTWlb/wfCabAtAFWhhBow==", "dev": true, "license": "ISC", "dependencies": { "brace-expansion": "^2.0.1" }, "engines": { "node": ">=16 || 14 >=14.17" }, "funding": { "url": "https://github.com/sponsors/isaacs" } }, "node_modules/@typescript-eslint/typescript-estree/node_modules/semver": { "version": "7.7.1", "resolved": "https://registry.npmjs.org/semver/-/semver-7.7.1.tgz", "integrity": "sha512-hlq8tAfn0m/61p4BVRcPzIGr6LKiMwo4VM6dGi6pt4qcRkmNzTcWq6eCEjEh+qXjkMDvPlOFFSGwQjoEa6gyMA==", "dev": true, "license": "ISC", "bin": { "semver": "bin/semver.js" }, "engines": { "node": ">=10" } }, "node_modules/@typescript-eslint/utils": { "version": "8.31.1", "resolved": "https://registry.npmjs.org/@typescript-eslint/utils/-/utils-8.31.1.tgz", "integrity": "sha512-2DSI4SNfF5T4oRveQ4nUrSjUqjMND0nLq9rEkz0gfGr3tg0S5KB6DhwR+WZPCjzkZl3cH+4x2ce3EsL50FubjQ==", "dev": true, "license": "MIT", "dependencies": { "@eslint-community/eslint-utils": "^4.4.0", "@typescript-eslint/scope-manager": "8.31.1", "@typescript-eslint/types": "8.31.1", "@typescript-eslint/typescript-estree": "8.31.1" }, "engines": { "node": "^18.18.0 || ^20.9.0 || >=21.1.0" }, "funding": { "type": "opencollective", "url": "https://opencollective.com/typescript-eslint" }, "peerDependencies": { "eslint": "^8.57.0 || ^9.0.0", "typescript": ">=4.8.4 <5.9.0" } }, "node_modules/@typescript-eslint/visitor-keys": { "version": "8.31.1", "resolved": "https://registry.npmjs.org/@typescript-eslint/visitor-keys/-/visitor-keys-8.31.1.tgz", "integrity": "sha512-I+/rgqOVBn6f0o7NDTmAPWWC6NuqhV174lfYvAm9fUaWeiefLdux9/YI3/nLugEn9L8fcSi0XmpKi/r5u0nmpw==", "dev": true, "license": "MIT", "dependencies": { "@typescript-eslint/types": "8.31.1", "eslint-visitor-keys": "^4.2.0" }, "engines": { "node": "^18.18.0 || ^20.9.0 || >=21.1.0" }, "funding": { "type": "opencollective", "url": "https://opencollective.com/typescript-eslint" } }, "node_modules/@vitejs/plugin-react": { "version": "4.4.1", "resolved": "https://registry.npmjs.org/@vitejs/plugin-react/-/plugin-react-4.4.1.tgz", "integrity": "sha512-IpEm5ZmeXAP/osiBXVVP5KjFMzbWOonMs0NaQQl+xYnUAcq4oHUBsF2+p4MgKWG4YMmFYJU8A6sxRPuowllm6w==", "dev": true, "license": "MIT", "dependencies": { "@babel/core": "^7.26.10", "@babel/plugin-transform-react-jsx-self": "^7.25.9", "@babel/plugin-transform-react-jsx-source": "^7.25.9", "@types/babel__core": "^7.20.5", "react-refresh": "^0.17.0" }, "engines": { "node": "^14.18.0 || >=16.0.0" }, "peerDependencies": { "vite": "^4.2.0 || ^5.0.0 || ^6.0.0" } }, "node_modules/accepts": { "version": "2.0.0", "resolved": "https://registry.npmjs.org/accepts/-/accepts-2.0.0.tgz", "integrity": "sha512-5cvg6CtKwfgdmVqY1WIiXKc3Q1bkRqGLi+2W/6ao+6Y7gu/RCwRuAhGEzh5B4KlszSuTLgZYuqFqo5bImjNKng==", "dev": true, "license": "MIT", "dependencies": { "mime-types": "^3.0.0", "negotiator": "^1.0.0" }, "engines": { "node": ">= 0.6" } }, "node_modules/acorn": { "version": "8.14.1", "resolved": "https://registry.npmjs.org/acorn/-/acorn-8.14.1.tgz", "integrity": "sha512-OvQ/2pUDKmgfCg++xsTX1wGxfTaszcHVcTctW4UJB4hibJx2HXxxO5UmVgyjMa+ZDsiaf5wWLXYpRWMmBI0QHg==", "dev": true, "license": "MIT", "bin": { "acorn": "bin/acorn" }, "engines": { "node": ">=0.4.0" } }, "node_modules/acorn-jsx": { "version": "5.3.2", "resolved": "https://registry.npmjs.org/acorn-jsx/-/acorn-jsx-5.3.2.tgz", "integrity": "sha512-rq9s+JNhf0IChjtDXxllJ7g41oZk5SlXtp0LHwyA5cejwn7vKmKp4pPri6YEePv2PU65sAsegbXtIinmDFDXgQ==", "dev": true, "license": "MIT", "peerDependencies": { "acorn": "^6.0.0 || ^7.0.0 || ^8.0.0" } }, "node_modules/ajv": { "version": "6.12.6", "resolved": "https://registry.npmjs.org/ajv/-/ajv-6.12.6.tgz", "integrity": "sha512-j3fVLgvTo527anyYyJOGTYJbG+vnnQYvE0m5mmkc1TK+nxAppkCLMIL0aZ4dblVCNoGShhm+kzE4ZUykBoMg4g==", "dev": true, "license": "MIT", "dependencies": { "fast-deep-equal": "^3.1.1", "fast-json-stable-stringify": "^2.0.0", "json-schema-traverse": "^0.4.1", "uri-js": "^4.2.2" }, "funding": { "type": "github", "url": "https://github.com/sponsors/epoberezkin" } }, "node_modules/ansi-regex": { "version": "6.1.0", "resolved": "https://registry.npmjs.org/ansi-regex/-/ansi-regex-6.1.0.tgz", "integrity": "sha512-7HSX4QQb4CspciLpVFwyRe79O3xsIZDDLER21kERQ71oaPodF8jL725AgJMFAYbooIqolJoRLuM81SpeUkpkvA==", "dev": true, "license": "MIT", "engines": { "node": ">=12" }, "funding": { "url": "https://github.com/chalk/ansi-regex?sponsor=1" } }, "node_modules/ansi-styles": { "version": "4.3.0", "resolved": "https://registry.npmjs.org/ansi-styles/-/ansi-styles-4.3.0.tgz", "integrity": "sha512-zbB9rCJAT1rbjiVDb2hqKFHNYLxgtk8NURxZ3IZwD3F6NtxbXZQCnnSi1Lkx+IDohdPlFp222wVALIheZJQSEg==", "dev": true, "license": "MIT", "dependencies": { "color-convert": "^2.0.1" }, "engines": { "node": ">=8" }, "funding": { "url": "https://github.com/chalk/ansi-styles?sponsor=1" } }, "node_modules/any-promise": { "version": "1.3.0", "resolved": "https://registry.npmjs.org/any-promise/-/any-promise-1.3.0.tgz", "integrity": "sha512-7UvmKalWRt1wgjL1RrGxoSJW/0QZFIegpeGvZG9kjp8vrRu55XTHbwnqq2GpXm9uLbcuhxm3IqX9OB4MZR1b2A==", "dev": true, "license": "MIT" }, "node_modules/anymatch": { "version": "3.1.3", "resolved": "https://registry.npmjs.org/anymatch/-/anymatch-3.1.3.tgz", "integrity": "sha512-KMReFUr0B4t+D+OBkjR3KYqvocp2XaSzO55UcB6mgQMd3KbcE+mWTyvVV7D/zsdEbNnV6acZUutkiHQXvTr1Rw==", "dev": true, "license": "ISC", "dependencies": { "normalize-path": "^3.0.0", "picomatch": "^2.0.4" }, "engines": { "node": ">= 8" } }, "node_modules/arg": { "version": "5.0.2", "resolved": "https://registry.npmjs.org/arg/-/arg-5.0.2.tgz", "integrity": "sha512-PYjyFOLKQ9y57JvQ6QLo8dAgNqswh8M1RMJYdQduT6xbWSgK36P/Z/v+p888pM69jMMfS8Xd8F6I1kQ/I9HUGg==", "dev": true, "license": "MIT" }, "node_modules/argparse": { "version": "2.0.1", "resolved": "https://registry.npmjs.org/argparse/-/argparse-2.0.1.tgz", "integrity": "sha512-8+9WqebbFzpX9OR+Wa6O29asIogeRMzcGtAINdpMHHyAg10f05aSFVBbcEqGf/PXw1EjAZ+q2/bEBg3DvurK3Q==", "dev": true, "license": "Python-2.0" }, "node_modules/asynckit": { "version": "0.4.0", "resolved": "https://registry.npmjs.org/asynckit/-/asynckit-0.4.0.tgz", "integrity": "sha512-Oei9OH4tRh0YqU3GxhX79dM/mwVgvbZJaSNaRk+bshkj0S5cfHcgYakreBjrHwatXKbz+IoIdYLxrKim2MjW0Q==", "license": "MIT" }, "node_modules/autoprefixer": { "version": "10.4.14", "resolved": "https://registry.npmjs.org/autoprefixer/-/autoprefixer-10.4.14.tgz", "integrity": "sha512-FQzyfOsTlwVzjHxKEqRIAdJx9niO6VCBCoEwax/VLSoQF29ggECcPuBqUMZ+u8jCZOPSy8b8/8KnuFbp0SaFZQ==", "dev": true, "funding": [ { "type": "opencollective", "url": "https://opencollective.com/postcss/" }, { "type": "tidelift", "url": "https://tidelift.com/funding/github/npm/autoprefixer" } ], "license": "MIT", "dependencies": { "browserslist": "^4.21.5", "caniuse-lite": "^1.0.30001464", "fraction.js": "^4.2.0", "normalize-range": "^0.1.2", "picocolors": "^1.0.0", "postcss-value-parser": "^4.2.0" }, "bin": { "autoprefixer": "bin/autoprefixer" }, "engines": { "node": "^10 || ^12 || >=14" }, "peerDependencies": { "postcss": "^8.1.0" } }, "node_modules/axios": { "version": "1.9.0", "resolved": "https://registry.npmjs.org/axios/-/axios-1.9.0.tgz", "integrity": "sha512-re4CqKTJaURpzbLHtIi6XpDv20/CnpXOtjRY5/CU32L8gU8ek9UIivcfvSWvmKEngmVbrUtPpdDwWDWL7DNHvg==", "license": "MIT", "dependencies": { "follow-redirects": "^1.15.6", "form-data": "^4.0.0", "proxy-from-env": "^1.1.0" } }, "node_modules/babel-plugin-macros": { "version": "3.1.0", "resolved": "https://registry.npmjs.org/babel-plugin-macros/-/babel-plugin-macros-3.1.0.tgz", "integrity": "sha512-Cg7TFGpIr01vOQNODXOOaGz2NpCU5gl8x1qJFbb6hbZxR7XrcE2vtbAsTAbJ7/xwJtUuJEw8K8Zr/AE0LHlesg==", "license": "MIT", "dependencies": { "@babel/runtime": "^7.12.5", "cosmiconfig": "^7.0.0", "resolve": "^1.19.0" }, "engines": { "node": ">=10", "npm": ">=6" } }, "node_modules/balanced-match": { "version": "1.0.2", "resolved": "https://registry.npmjs.org/balanced-match/-/balanced-match-1.0.2.tgz", "integrity": "sha512-3oSeUO0TMV67hN1AmbXsK4yaqU7tjiHlbxRDZOpH0KW9+CeX4bRAaX0Anxt0tx2MrpRpWwQaPwIlISEJhYU5Pw==", "dev": true, "license": "MIT" }, "node_modules/binary-extensions": { "version": "2.3.0", "resolved": "https://registry.npmjs.org/binary-extensions/-/binary-extensions-2.3.0.tgz", "integrity": "sha512-Ceh+7ox5qe7LJuLHoY0feh3pHuUDHAcRUeyL2VYghZwfpkNIy/+8Ocg0a3UuSoYzavmylwuLWQOf3hl0jjMMIw==", "dev": true, "license": "MIT", "engines": { "node": ">=8" }, "funding": { "url": "https://github.com/sponsors/sindresorhus" } }, "node_modules/body-parser": { "version": "2.2.0", "resolved": "https://registry.npmjs.org/body-parser/-/body-parser-2.2.0.tgz", "integrity": "sha512-02qvAaxv8tp7fBa/mw1ga98OGm+eCbqzJOKoRt70sLmfEEi+jyBYVTDGfCL/k06/4EMk/z01gCe7HoCH/f2LTg==", "dev": true, "license": "MIT", "dependencies": { "bytes": "^3.1.2", "content-type": "^1.0.5", "debug": "^4.4.0", "http-errors": "^2.0.0", "iconv-lite": "^0.6.3", "on-finished": "^2.4.1", "qs": "^6.14.0", "raw-body": "^3.0.0", "type-is": "^2.0.0" }, "engines": { "node": ">=18" } }, "node_modules/brace-expansion": { "version": "1.1.11", "resolved": "https://registry.npmjs.org/brace-expansion/-/brace-expansion-1.1.11.tgz", "integrity": "sha512-iCuPHDFgrHX7H2vEI/5xpz07zSHB00TpugqhmYtVmMO6518mCuRMoOYFldEBl0g187ufozdaHgWKcYFb61qGiA==", "dev": true, "license": "MIT", "dependencies": { "balanced-match": "^1.0.0", "concat-map": "0.0.1" } }, "node_modules/braces": { "version": "3.0.3", "resolved": "https://registry.npmjs.org/braces/-/braces-3.0.3.tgz", "integrity": "sha512-yQbXgO/OSZVD2IsiLlro+7Hf6Q18EJrKSEsdoMzKePKXct3gvD8oLcOQdIzGupr5Fj+EDe8gO/lxc1BzfMpxvA==", "dev": true, "license": "MIT", "dependencies": { "fill-range": "^7.1.1" }, "engines": { "node": ">=8" } }, "node_modules/browserslist": { "version": "4.24.5", "resolved": "https://registry.npmjs.org/browserslist/-/browserslist-4.24.5.tgz", "integrity": "sha512-FDToo4Wo82hIdgc1CQ+NQD0hEhmpPjrZ3hiUgwgOG6IuTdlpr8jdjyG24P6cNP1yJpTLzS5OcGgSw0xmDU1/Tw==", "dev": true, "funding": [ { "type": "opencollective", "url": "https://opencollective.com/browserslist" }, { "type": "tidelift", "url": "https://tidelift.com/funding/github/npm/browserslist" }, { "type": "github", "url": "https://github.com/sponsors/ai" } ], "license": "MIT", "dependencies": { "caniuse-lite": "^1.0.30001716", "electron-to-chromium": "^1.5.149", "node-releases": "^2.0.19", "update-browserslist-db": "^1.1.3" }, "bin": { "browserslist": "cli.js" }, "engines": { "node": "^6 || ^7 || ^8 || ^9 || ^10 || ^11 || ^12 || >=13.7" } }, "node_modules/bytes": { "version": "3.1.2", "resolved": "https://registry.npmjs.org/bytes/-/bytes-3.1.2.tgz", "integrity": "sha512-/Nf7TyzTx6S3yRJObOAV7956r8cr2+Oj8AC5dt8wSP3BQAoeX58NoHyCU8P8zGkNXStjTSi6fzO6F0pBdcYbEg==", "dev": true, "license": "MIT", "engines": { "node": ">= 0.8" } }, "node_modules/call-bind-apply-helpers": { "version": "1.0.2", "resolved": "https://registry.npmjs.org/call-bind-apply-helpers/-/call-bind-apply-helpers-1.0.2.tgz", "integrity": "sha512-Sp1ablJ0ivDkSzjcaJdxEunN5/XvksFJ2sMBFfq6x0ryhQV/2b/KwFe21cMpmHtPOSij8K99/wSfoEuTObmuMQ==", "license": "MIT", "dependencies": { "es-errors": "^1.3.0", "function-bind": "^1.1.2" }, "engines": { "node": ">= 0.4" } }, "node_modules/call-bound": { "version": "1.0.4", "resolved": "https://registry.npmjs.org/call-bound/-/call-bound-1.0.4.tgz", "integrity": "sha512-+ys997U96po4Kx/ABpBCqhA9EuxJaQWDQg7295H4hBphv3IZg0boBKuwYpt4YXp6MZ5AmZQnU/tyMTlRpaSejg==", "dev": true, "license": "MIT", "dependencies": { "call-bind-apply-helpers": "^1.0.2", "get-intrinsic": "^1.3.0" }, "engines": { "node": ">= 0.4" }, "funding": { "url": "https://github.com/sponsors/ljharb" } }, "node_modules/callsites": { "version": "3.1.0", "resolved": "https://registry.npmjs.org/callsites/-/callsites-3.1.0.tgz", "integrity": "sha512-P8BjAsXvZS+VIDUI11hHCQEv74YT67YUi5JJFNWIqL235sBmjX4+qx9Muvls5ivyNENctx46xQLQ3aTuE7ssaQ==", "license": "MIT", "engines": { "node": ">=6" } }, "node_modules/camelcase-css": { "version": "2.0.1", "resolved": "https://registry.npmjs.org/camelcase-css/-/camelcase-css-2.0.1.tgz", "integrity": "sha512-QOSvevhslijgYwRx6Rv7zKdMF8lbRmx+uQGx2+vDc+KI/eBnsy9kit5aj23AgGu3pa4t9AgwbnXWqS+iOY+2aA==", "dev": true, "license": "MIT", "engines": { "node": ">= 6" } }, "node_modules/caniuse-lite": { "version": "1.0.30001716", "resolved": "https://registry.npmjs.org/caniuse-lite/-/caniuse-lite-1.0.30001716.tgz", "integrity": "sha512-49/c1+x3Kwz7ZIWt+4DvK3aMJy9oYXXG6/97JKsnjdCk/6n9vVyWL8NAwVt95Lwt9eigI10Hl782kDfZUUlRXw==", "dev": true, "funding": [ { "type": "opencollective", "url": "https://opencollective.com/browserslist" }, { "type": "tidelift", "url": "https://tidelift.com/funding/github/npm/caniuse-lite" }, { "type": "github", "url": "https://github.com/sponsors/ai" } ], "license": "CC-BY-4.0" }, "node_modules/chalk": { "version": "4.1.2", "resolved": "https://registry.npmjs.org/chalk/-/chalk-4.1.2.tgz", "integrity": "sha512-oKnbhFyRIXpUuez8iBMmyEa4nbj4IOQyuhc/wy9kY7/WVPcwIO9VA668Pu8RkO7+0G76SLROeyw9CpQ061i4mA==", "dev": true, "license": "MIT", "dependencies": { "ansi-styles": "^4.1.0", "supports-color": "^7.1.0" }, "engines": { "node": ">=10" }, "funding": { "url": "https://github.com/chalk/chalk?sponsor=1" } }, "node_modules/chokidar": { "version": "3.6.0", "resolved": "https://registry.npmjs.org/chokidar/-/chokidar-3.6.0.tgz", "integrity": "sha512-7VT13fmjotKpGipCW9JEQAusEPE+Ei8nl6/g4FBAmIm0GOOLMua9NDDo/DWp0ZAxCr3cPq5ZpBqmPAQgDda2Pw==", "dev": true, "license": "MIT", "dependencies": { "anymatch": "~3.1.2", "braces": "~3.0.2", "glob-parent": "~5.1.2", "is-binary-path": "~2.1.0", "is-glob": "~4.0.1", "normalize-path": "~3.0.0", "readdirp": "~3.6.0" }, "engines": { "node": ">= 8.10.0" }, "funding": { "url": "https://paulmillr.com/funding/" }, "optionalDependencies": { "fsevents": "~2.3.2" } }, "node_modules/chokidar/node_modules/glob-parent": { "version": "5.1.2", "resolved": "https://registry.npmjs.org/glob-parent/-/glob-parent-5.1.2.tgz", "integrity": "sha512-AOIgSQCepiJYwP3ARnGx+5VnTu2HBYdzbGP45eLw1vr3zB3vZLeyed1sC9hnbcOc9/SrMyM5RPQrkGz4aS9Zow==", "dev": true, "license": "ISC", "dependencies": { "is-glob": "^4.0.1" }, "engines": { "node": ">= 6" } }, "node_modules/clsx": { "version": "2.1.1", "resolved": "https://registry.npmjs.org/clsx/-/clsx-2.1.1.tgz", "integrity": "sha512-eYm0QWBtUrBWZWG0d386OGAw16Z995PiOVo2B7bjWSbHedGl5e0ZWaq65kOGgUSNesEIDkB9ISbTg/JK9dhCZA==", "license": "MIT", "engines": { "node": ">=6" } }, "node_modules/color-convert": { "version": "2.0.1", "resolved": "https://registry.npmjs.org/color-convert/-/color-convert-2.0.1.tgz", "integrity": "sha512-RRECPsj7iu/xb5oKYcsFHSppFNnsj/52OVTRKb4zP5onXwVF3zVmmToNcOfGC+CRDpfK/U584fMg38ZHCaElKQ==", "dev": true, "license": "MIT", "dependencies": { "color-name": "~1.1.4" }, "engines": { "node": ">=7.0.0" } }, "node_modules/color-name": { "version": "1.1.4", "resolved": "https://registry.npmjs.org/color-name/-/color-name-1.1.4.tgz", "integrity": "sha512-dOy+3AuW3a2wNbZHIuMZpTcgjGuLU/uBL/ubcZF9OXbDo8ff4O8yVp5Bf0efS8uEoYo5q4Fx7dY9OgQGXgAsQA==", "dev": true, "license": "MIT" }, "node_modules/combined-stream": { "version": "1.0.8", "resolved": "https://registry.npmjs.org/combined-stream/-/combined-stream-1.0.8.tgz", "integrity": "sha512-FQN4MRfuJeHf7cBbBMJFXhKSDq+2kAArBlmRBvcvFE5BB1HZKXtSFASDhdlz9zOYwxh8lDdnvmMOe/+5cdoEdg==", "license": "MIT", "dependencies": { "delayed-stream": "~1.0.0" }, "engines": { "node": ">= 0.8" } }, "node_modules/commander": { "version": "4.1.1", "resolved": "https://registry.npmjs.org/commander/-/commander-4.1.1.tgz", "integrity": "sha512-NOKm8xhkzAjzFx8B2v5OAHT+u5pRQc2UCa2Vq9jYL/31o2wi9mxBA7LIFs3sV5VSC49z6pEhfbMULvShKj26WA==", "dev": true, "license": "MIT", "engines": { "node": ">= 6" } }, "node_modules/concat-map": { "version": "0.0.1", "resolved": "https://registry.npmjs.org/concat-map/-/concat-map-0.0.1.tgz", "integrity": "sha512-/Srv4dswyQNBfohGpz9o6Yb3Gz3SrUDqBH5rTuhGR7ahtlbYKnVxw2bCFMRljaA7EXHaXZ8wsHdodFvbkhKmqg==", "dev": true, "license": "MIT" }, "node_modules/content-disposition": { "version": "1.0.0", "resolved": "https://registry.npmjs.org/content-disposition/-/content-disposition-1.0.0.tgz", "integrity": "sha512-Au9nRL8VNUut/XSzbQA38+M78dzP4D+eqg3gfJHMIHHYa3bg067xj1KxMUWj+VULbiZMowKngFFbKczUrNJ1mg==", "dev": true, "license": "MIT", "dependencies": { "safe-buffer": "5.2.1" }, "engines": { "node": ">= 0.6" } }, "node_modules/content-type": { "version": "1.0.5", "resolved": "https://registry.npmjs.org/content-type/-/content-type-1.0.5.tgz", "integrity": "sha512-nTjqfcBFEipKdXCv4YDQWCfmcLZKm81ldF0pAopTvyrFGVbcR6P/VAAd5G7N+0tTr8QqiU0tFadD6FK4NtJwOA==", "dev": true, "license": "MIT", "engines": { "node": ">= 0.6" } }, "node_modules/convert-source-map": { "version": "2.0.0", "resolved": "https://registry.npmjs.org/convert-source-map/-/convert-source-map-2.0.0.tgz", "integrity": "sha512-Kvp459HrV2FEJ1CAsi1Ku+MY3kasH19TFykTz2xWmMeq6bk2NU3XXvfJ+Q61m0xktWwt+1HSYf3JZsTms3aRJg==", "dev": true, "license": "MIT" }, "node_modules/cookie": { "version": "0.7.2", "resolved": "https://registry.npmjs.org/cookie/-/cookie-0.7.2.tgz", "integrity": "sha512-yki5XnKuf750l50uGTllt6kKILY4nQ1eNIQatoXEByZ5dWgnKqbnqmTrBE5B4N7lrMJKQ2ytWMiTO2o0v6Ew/w==", "dev": true, "license": "MIT", "engines": { "node": ">= 0.6" } }, "node_modules/cookie-signature": { "version": "1.2.2", "resolved": "https://registry.npmjs.org/cookie-signature/-/cookie-signature-1.2.2.tgz", "integrity": "sha512-D76uU73ulSXrD1UXF4KE2TMxVVwhsnCgfAyTg9k8P6KGZjlXKrOLe4dJQKI3Bxi5wjesZoFXJWElNWBjPZMbhg==", "dev": true, "license": "MIT", "engines": { "node": ">=6.6.0" } }, "node_modules/cors": { "version": "2.8.5", "resolved": "https://registry.npmjs.org/cors/-/cors-2.8.5.tgz", "integrity": "sha512-KIHbLJqu73RGr/hnbrO9uBeixNGuvSQjul/jdFvS/KFSIH1hWVd1ng7zOHx+YrEfInLG7q4n6GHQ9cDtxv/P6g==", "dev": true, "license": "MIT", "dependencies": { "object-assign": "^4", "vary": "^1" }, "engines": { "node": ">= 0.10" } }, "node_modules/cosmiconfig": { "version": "7.1.0", "resolved": "https://registry.npmjs.org/cosmiconfig/-/cosmiconfig-7.1.0.tgz", "integrity": "sha512-AdmX6xUzdNASswsFtmwSt7Vj8po9IuqXm0UXz7QKPuEUmPB4XyjGfaAr2PSuELMwkRMVH1EpIkX5bTZGRB3eCA==", "license": "MIT", "dependencies": { "@types/parse-json": "^4.0.0", "import-fresh": "^3.2.1", "parse-json": "^5.0.0", "path-type": "^4.0.0", "yaml": "^1.10.0" }, "engines": { "node": ">=10" } }, "node_modules/cosmiconfig/node_modules/yaml": { "version": "1.10.2", "resolved": "https://registry.npmjs.org/yaml/-/yaml-1.10.2.tgz", "integrity": "sha512-r3vXyErRCYJ7wg28yvBY5VSoAF8ZvlcW9/BwUzEtUsjvX/DKs24dIkuwjtuprwJJHsbyUbLApepYTR1BN4uHrg==", "license": "ISC", "engines": { "node": ">= 6" } }, "node_modules/cross-spawn": { "version": "7.0.6", "resolved": "https://registry.npmjs.org/cross-spawn/-/cross-spawn-7.0.6.tgz", "integrity": "sha512-uV2QOWP2nWzsy2aMp8aRibhi9dlzF5Hgh5SHaB9OiTGEyDTiJJyx0uy51QXdyWbtAHNua4XJzUKca3OzKUd3vA==", "dev": true, "license": "MIT", "dependencies": { "path-key": "^3.1.0", "shebang-command": "^2.0.0", "which": "^2.0.1" }, "engines": { "node": ">= 8" } }, "node_modules/cssesc": { "version": "3.0.0", "resolved": "https://registry.npmjs.org/cssesc/-/cssesc-3.0.0.tgz", "integrity": "sha512-/Tb/JcjK111nNScGob5MNtsntNM1aCNUDipB/TkwZFhyDrrE47SOx/18wF2bbjgc3ZzCSKW1T5nt5EbFoAz/Vg==", "dev": true, "license": "MIT", "bin": { "cssesc": "bin/cssesc" }, "engines": { "node": ">=4" } }, "node_modules/csstype": { "version": "3.1.3", "resolved": "https://registry.npmjs.org/csstype/-/csstype-3.1.3.tgz", "integrity": "sha512-M1uQkMl8rQK/szD0LNhtqxIPLpimGm8sOBwU7lLnCpSbTyY3yeU1Vc7l4KT5zT4s/yOxHH5O7tIuuLOCnLADRw==", "license": "MIT" }, "node_modules/d3-array": { "version": "3.2.4", "resolved": "https://registry.npmjs.org/d3-array/-/d3-array-3.2.4.tgz", "integrity": "sha512-tdQAmyA18i4J7wprpYq8ClcxZy3SC31QMeByyCFyRt7BVHdREQZ5lpzoe5mFEYZUWe+oq8HBvk9JjpibyEV4Jg==", "license": "ISC", "dependencies": { "internmap": "1 - 2" }, "engines": { "node": ">=12" } }, "node_modules/d3-color": { "version": "3.1.0", "resolved": "https://registry.npmjs.org/d3-color/-/d3-color-3.1.0.tgz", "integrity": "sha512-zg/chbXyeBtMQ1LbD/WSoW2DpC3I0mpmPdW+ynRTj/x2DAWYrIY7qeZIHidozwV24m4iavr15lNwIwLxRmOxhA==", "license": "ISC", "engines": { "node": ">=12" } }, "node_modules/d3-ease": { "version": "3.0.1", "resolved": "https://registry.npmjs.org/d3-ease/-/d3-ease-3.0.1.tgz", "integrity": "sha512-wR/XK3D3XcLIZwpbvQwQ5fK+8Ykds1ip7A2Txe0yxncXSdq1L9skcG7blcedkOX+ZcgxGAmLX1FrRGbADwzi0w==", "license": "BSD-3-Clause", "engines": { "node": ">=12" } }, "node_modules/d3-format": { "version": "3.1.0", "resolved": "https://registry.npmjs.org/d3-format/-/d3-format-3.1.0.tgz", "integrity": "sha512-YyUI6AEuY/Wpt8KWLgZHsIU86atmikuoOmCfommt0LYHiQSPjvX2AcFc38PX0CBpr2RCyZhjex+NS/LPOv6YqA==", "license": "ISC", "engines": { "node": ">=12" } }, "node_modules/d3-interpolate": { "version": "3.0.1", "resolved": "https://registry.npmjs.org/d3-interpolate/-/d3-interpolate-3.0.1.tgz", "integrity": "sha512-3bYs1rOD33uo8aqJfKP3JWPAibgw8Zm2+L9vBKEHJ2Rg+viTR7o5Mmv5mZcieN+FRYaAOWX5SJATX6k1PWz72g==", "license": "ISC", "dependencies": { "d3-color": "1 - 3" }, "engines": { "node": ">=12" } }, "node_modules/d3-path": { "version": "3.1.0", "resolved": "https://registry.npmjs.org/d3-path/-/d3-path-3.1.0.tgz", "integrity": "sha512-p3KP5HCf/bvjBSSKuXid6Zqijx7wIfNW+J/maPs+iwR35at5JCbLUT0LzF1cnjbCHWhqzQTIN2Jpe8pRebIEFQ==", "license": "ISC", "engines": { "node": ">=12" } }, "node_modules/d3-scale": { "version": "4.0.2", "resolved": "https://registry.npmjs.org/d3-scale/-/d3-scale-4.0.2.tgz", "integrity": "sha512-GZW464g1SH7ag3Y7hXjf8RoUuAFIqklOAq3MRl4OaWabTFJY9PN/E1YklhXLh+OQ3fM9yS2nOkCoS+WLZ6kvxQ==", "license": "ISC", "dependencies": { "d3-array": "2.10.0 - 3", "d3-format": "1 - 3", "d3-interpolate": "1.2.0 - 3", "d3-time": "2.1.1 - 3", "d3-time-format": "2 - 4" }, "engines": { "node": ">=12" } }, "node_modules/d3-shape": { "version": "3.2.0", "resolved": "https://registry.npmjs.org/d3-shape/-/d3-shape-3.2.0.tgz", "integrity": "sha512-SaLBuwGm3MOViRq2ABk3eLoxwZELpH6zhl3FbAoJ7Vm1gofKx6El1Ib5z23NUEhF9AsGl7y+dzLe5Cw2AArGTA==", "license": "ISC", "dependencies": { "d3-path": "^3.1.0" }, "engines": { "node": ">=12" } }, "node_modules/d3-time": { "version": "3.1.0", "resolved": "https://registry.npmjs.org/d3-time/-/d3-time-3.1.0.tgz", "integrity": "sha512-VqKjzBLejbSMT4IgbmVgDjpkYrNWUYJnbCGo874u7MMKIWsILRX+OpX/gTk8MqjpT1A/c6HY2dCA77ZN0lkQ2Q==", "license": "ISC", "dependencies": { "d3-array": "2 - 3" }, "engines": { "node": ">=12" } }, "node_modules/d3-time-format": { "version": "4.1.0", "resolved": "https://registry.npmjs.org/d3-time-format/-/d3-time-format-4.1.0.tgz", "integrity": "sha512-dJxPBlzC7NugB2PDLwo9Q8JiTR3M3e4/XANkreKSUxF8vvXKqm1Yfq4Q5dl8budlunRVlUUaDUgFt7eA8D6NLg==", "license": "ISC", "dependencies": { "d3-time": "1 - 3" }, "engines": { "node": ">=12" } }, "node_modules/d3-timer": { "version": "3.0.1", "resolved": "https://registry.npmjs.org/d3-timer/-/d3-timer-3.0.1.tgz", "integrity": "sha512-ndfJ/JxxMd3nw31uyKoY2naivF+r29V+Lc0svZxe1JvvIRmi8hUsrMvdOwgS1o6uBHmiz91geQ0ylPP0aj1VUA==", "license": "ISC", "engines": { "node": ">=12" } }, "node_modules/debug": { "version": "4.4.0", "resolved": "https://registry.npmjs.org/debug/-/debug-4.4.0.tgz", "integrity": "sha512-6WTZ/IxCY/T6BALoZHaE4ctp9xm+Z5kY/pzYaCHRFeyVhojxlrm+46y68HA6hr0TcwEssoxNiDEUJQjfPZ/RYA==", "license": "MIT", "dependencies": { "ms": "^2.1.3" }, "engines": { "node": ">=6.0" }, "peerDependenciesMeta": { "supports-color": { "optional": true } } }, "node_modules/decimal.js-light": { "version": "2.5.1", "resolved": "https://registry.npmjs.org/decimal.js-light/-/decimal.js-light-2.5.1.tgz", "integrity": "sha512-qIMFpTMZmny+MMIitAB6D7iVPEorVw6YQRWkvarTkT4tBeSLLiHzcwj6q0MmYSFCiVpiqPJTJEYIrpcPzVEIvg==", "license": "MIT" }, "node_modules/deep-is": { "version": "0.1.4", "resolved": "https://registry.npmjs.org/deep-is/-/deep-is-0.1.4.tgz", "integrity": "sha512-oIPzksmTg4/MriiaYGO+okXDT7ztn/w3Eptv/+gSIdMdKsJo0u4CfYNFJPy+4SKMuCqGw2wxnA+URMg3t8a/bQ==", "dev": true, "license": "MIT" }, "node_modules/delayed-stream": { "version": "1.0.0", "resolved": "https://registry.npmjs.org/delayed-stream/-/delayed-stream-1.0.0.tgz", "integrity": "sha512-ZySD7Nf91aLB0RxL4KGrKHBXl7Eds1DAmEdcoVawXnLD7SDhpNgtuII2aAkg7a7QS41jxPSZ17p4VdGnMHk3MQ==", "license": "MIT", "engines": { "node": ">=0.4.0" } }, "node_modules/depd": { "version": "2.0.0", "resolved": "https://registry.npmjs.org/depd/-/depd-2.0.0.tgz", "integrity": "sha512-g7nH6P6dyDioJogAAGprGpCtVImJhpPk/roCzdb3fIh61/s/nPsfR6onyMwkCAR/OlC3yBC0lESvUoQEAssIrw==", "dev": true, "license": "MIT", "engines": { "node": ">= 0.8" } }, "node_modules/didyoumean": { "version": "1.2.2", "resolved": "https://registry.npmjs.org/didyoumean/-/didyoumean-1.2.2.tgz", "integrity": "sha512-gxtyfqMg7GKyhQmb056K7M3xszy/myH8w+B4RT+QXBQsvAOdc3XymqDDPHx1BgPgsdAA5SIifona89YtRATDzw==", "dev": true, "license": "Apache-2.0" }, "node_modules/dlv": { "version": "1.1.3", "resolved": "https://registry.npmjs.org/dlv/-/dlv-1.1.3.tgz", "integrity": "sha512-+HlytyjlPKnIG8XuRG8WvmBP8xs8P71y+SKKS6ZXWoEgLuePxtDoUEiH7WkdePWrQ5JBpE6aoVqfZfJUQkjXwA==", "dev": true, "license": "MIT" }, "node_modules/dom-helpers": { "version": "5.2.1", "resolved": "https://registry.npmjs.org/dom-helpers/-/dom-helpers-5.2.1.tgz", "integrity": "sha512-nRCa7CK3VTrM2NmGkIy4cbK7IZlgBE/PYMn55rrXefr5xXDP0LdtfPnblFDoVdcAfslJ7or6iqAUnx0CCGIWQA==", "license": "MIT", "dependencies": { "@babel/runtime": "^7.8.7", "csstype": "^3.0.2" } }, "node_modules/dunder-proto": { "version": "1.0.1", "resolved": "https://registry.npmjs.org/dunder-proto/-/dunder-proto-1.0.1.tgz", "integrity": "sha512-KIN/nDJBQRcXw0MLVhZE9iQHmG68qAVIBg9CqmUYjmQIhgij9U5MFvrqkUL5FbtyyzZuOeOt0zdeRe4UY7ct+A==", "license": "MIT", "dependencies": { "call-bind-apply-helpers": "^1.0.1", "es-errors": "^1.3.0", "gopd": "^1.2.0" }, "engines": { "node": ">= 0.4" } }, "node_modules/eastasianwidth": { "version": "0.2.0", "resolved": "https://registry.npmjs.org/eastasianwidth/-/eastasianwidth-0.2.0.tgz", "integrity": "sha512-I88TYZWc9XiYHRQ4/3c5rjjfgkjhLyW2luGIheGERbNQ6OY7yTybanSpDXZa8y7VUP9YmDcYa+eyq4ca7iLqWA==", "dev": true, "license": "MIT" }, "node_modules/ee-first": { "version": "1.1.1", "resolved": "https://registry.npmjs.org/ee-first/-/ee-first-1.1.1.tgz", "integrity": "sha512-WMwm9LhRUo+WUaRN+vRuETqG89IgZphVSNkdFgeb6sS/E4OrDIN7t48CAewSHXc6C8lefD8KKfr5vY61brQlow==", "dev": true, "license": "MIT" }, "node_modules/electron-to-chromium": { "version": "1.5.149", "resolved": "https://registry.npmjs.org/electron-to-chromium/-/electron-to-chromium-1.5.149.tgz", "integrity": "sha512-UyiO82eb9dVOx8YO3ajDf9jz2kKyt98DEITRdeLPstOEuTlLzDA4Gyq5K9he71TQziU5jUVu2OAu5N48HmQiyQ==", "dev": true, "license": "ISC" }, "node_modules/emoji-regex": { "version": "9.2.2", "resolved": "https://registry.npmjs.org/emoji-regex/-/emoji-regex-9.2.2.tgz", "integrity": "sha512-L18DaJsXSUk2+42pv8mLs5jJT2hqFkFE4j21wOmgbUqsZ2hL72NsUU785g9RXgo3s0ZNgVl42TiHp3ZtOv/Vyg==", "dev": true, "license": "MIT" }, "node_modules/encodeurl": { "version": "2.0.0", "resolved": "https://registry.npmjs.org/encodeurl/-/encodeurl-2.0.0.tgz", "integrity": "sha512-Q0n9HRi4m6JuGIV1eFlmvJB7ZEVxu93IrMyiMsGC0lrMJMWzRgx6WGquyfQgZVb31vhGgXnfmPNNXmxnOkRBrg==", "dev": true, "license": "MIT", "engines": { "node": ">= 0.8" } }, "node_modules/error-ex": { "version": "1.3.2", "resolved": "https://registry.npmjs.org/error-ex/-/error-ex-1.3.2.tgz", "integrity": "sha512-7dFHNmqeFSEt2ZBsCriorKnn3Z2pj+fd9kmI6QoWw4//DL+icEBfc0U7qJCisqrTsKTjw4fNFy2pW9OqStD84g==", "license": "MIT", "dependencies": { "is-arrayish": "^0.2.1" } }, "node_modules/es-define-property": { "version": "1.0.1", "resolved": "https://registry.npmjs.org/es-define-property/-/es-define-property-1.0.1.tgz", "integrity": "sha512-e3nRfgfUZ4rNGL232gUgX06QNyyez04KdjFrF+LTRoOXmrOgFKDg4BCdsjW8EnT69eqdYGmRpJwiPVYNrCaW3g==", "license": "MIT", "engines": { "node": ">= 0.4" } }, "node_modules/es-errors": { "version": "1.3.0", "resolved": "https://registry.npmjs.org/es-errors/-/es-errors-1.3.0.tgz", "integrity": "sha512-Zf5H2Kxt2xjTvbJvP2ZWLEICxA6j+hAmMzIlypy4xcBg1vKVnx89Wy0GbS+kf5cwCVFFzdCFh2XSCFNULS6csw==", "license": "MIT", "engines": { "node": ">= 0.4" } }, "node_modules/es-object-atoms": { "version": "1.1.1", "resolved": "https://registry.npmjs.org/es-object-atoms/-/es-object-atoms-1.1.1.tgz", "integrity": "sha512-FGgH2h8zKNim9ljj7dankFPcICIK9Cp5bm+c2gQSYePhpaG5+esrLODihIorn+Pe6FGJzWhXQotPv73jTaldXA==", "license": "MIT", "dependencies": { "es-errors": "^1.3.0" }, "engines": { "node": ">= 0.4" } }, "node_modules/es-set-tostringtag": { "version": "2.1.0", "resolved": "https://registry.npmjs.org/es-set-tostringtag/-/es-set-tostringtag-2.1.0.tgz", "integrity": "sha512-j6vWzfrGVfyXxge+O0x5sh6cvxAog0a/4Rdd2K36zCMV5eJ+/+tOAngRO8cODMNWbVRdVlmGZQL2YS3yR8bIUA==", "license": "MIT", "dependencies": { "es-errors": "^1.3.0", "get-intrinsic": "^1.2.6", "has-tostringtag": "^1.0.2", "hasown": "^2.0.2" }, "engines": { "node": ">= 0.4" } }, "node_modules/esbuild": { "version": "0.25.3", "resolved": "https://registry.npmjs.org/esbuild/-/esbuild-0.25.3.tgz", "integrity": "sha512-qKA6Pvai73+M2FtftpNKRxJ78GIjmFXFxd/1DVBqGo/qNhLSfv+G12n9pNoWdytJC8U00TrViOwpjT0zgqQS8Q==", "dev": true, "hasInstallScript": true, "license": "MIT", "bin": { "esbuild": "bin/esbuild" }, "engines": { "node": ">=18" }, "optionalDependencies": { "@esbuild/aix-ppc64": "0.25.3", "@esbuild/android-arm": "0.25.3", "@esbuild/android-arm64": "0.25.3", "@esbuild/android-x64": "0.25.3", "@esbuild/darwin-arm64": "0.25.3", "@esbuild/darwin-x64": "0.25.3", "@esbuild/freebsd-arm64": "0.25.3", "@esbuild/freebsd-x64": "0.25.3", "@esbuild/linux-arm": "0.25.3", "@esbuild/linux-arm64": "0.25.3", "@esbuild/linux-ia32": "0.25.3", "@esbuild/linux-loong64": "0.25.3", "@esbuild/linux-mips64el": "0.25.3", "@esbuild/linux-ppc64": "0.25.3", "@esbuild/linux-riscv64": "0.25.3", "@esbuild/linux-s390x": "0.25.3", "@esbuild/linux-x64": "0.25.3", "@esbuild/netbsd-arm64": "0.25.3", "@esbuild/netbsd-x64": "0.25.3", "@esbuild/openbsd-arm64": "0.25.3", "@esbuild/openbsd-x64": "0.25.3", "@esbuild/sunos-x64": "0.25.3", "@esbuild/win32-arm64": "0.25.3", "@esbuild/win32-ia32": "0.25.3", "@esbuild/win32-x64": "0.25.3" } }, "node_modules/escalade": { "version": "3.2.0", "resolved": "https://registry.npmjs.org/escalade/-/escalade-3.2.0.tgz", "integrity": "sha512-WUj2qlxaQtO4g6Pq5c29GTcWGDyd8itL8zTlipgECz3JesAiiOKotd8JU6otB3PACgG6xkJUyVhboMS+bje/jA==", "dev": true, "license": "MIT", "engines": { "node": ">=6" } }, "node_modules/escape-html": { "version": "1.0.3", "resolved": "https://registry.npmjs.org/escape-html/-/escape-html-1.0.3.tgz", "integrity": "sha512-NiSupZ4OeuGwr68lGIeym/ksIZMJodUGOSCZ/FSnTxcrekbvqrgdUxlJOMpijaKZVjAJrWrGs/6Jy8OMuyj9ow==", "dev": true, "license": "MIT" }, "node_modules/escape-string-regexp": { "version": "4.0.0", "resolved": "https://registry.npmjs.org/escape-string-regexp/-/escape-string-regexp-4.0.0.tgz", "integrity": "sha512-TtpcNJ3XAzx3Gq8sWRzJaVajRs0uVxA2YAkdb1jm2YkPz4G6egUFAyA3n5vtEIZefPk5Wa4UXbKuS5fKkJWdgA==", "license": "MIT", "engines": { "node": ">=10" }, "funding": { "url": "https://github.com/sponsors/sindresorhus" } }, "node_modules/eslint": { "version": "9.26.0", "resolved": "https://registry.npmjs.org/eslint/-/eslint-9.26.0.tgz", "integrity": "sha512-Hx0MOjPh6uK9oq9nVsATZKE/Wlbai7KFjfCuw9UHaguDW3x+HF0O5nIi3ud39TWgrTjTO5nHxmL3R1eANinWHQ==", "dev": true, "license": "MIT", "dependencies": { "@eslint-community/eslint-utils": "^4.2.0", "@eslint-community/regexpp": "^4.12.1", "@eslint/config-array": "^0.20.0", "@eslint/config-helpers": "^0.2.1", "@eslint/core": "^0.13.0", "@eslint/eslintrc": "^3.3.1", "@eslint/js": "9.26.0", "@eslint/plugin-kit": "^0.2.8", "@humanfs/node": "^0.16.6", "@humanwhocodes/module-importer": "^1.0.1", "@humanwhocodes/retry": "^0.4.2", "@modelcontextprotocol/sdk": "^1.8.0", "@types/estree": "^1.0.6", "@types/json-schema": "^7.0.15", "ajv": "^6.12.4", "chalk": "^4.0.0", "cross-spawn": "^7.0.6", "debug": "^4.3.2", "escape-string-regexp": "^4.0.0", "eslint-scope": "^8.3.0", "eslint-visitor-keys": "^4.2.0", "espree": "^10.3.0", "esquery": "^1.5.0", "esutils": "^2.0.2", "fast-deep-equal": "^3.1.3", "file-entry-cache": "^8.0.0", "find-up": "^5.0.0", "glob-parent": "^6.0.2", "ignore": "^5.2.0", "imurmurhash": "^0.1.4", "is-glob": "^4.0.0", "json-stable-stringify-without-jsonify": "^1.0.1", "lodash.merge": "^4.6.2", "minimatch": "^3.1.2", "natural-compare": "^1.4.0", "optionator": "^0.9.3", "zod": "^3.24.2" }, "bin": { "eslint": "bin/eslint.js" }, "engines": { "node": "^18.18.0 || ^20.9.0 || >=21.1.0" }, "funding": { "url": "https://eslint.org/donate" }, "peerDependencies": { "jiti": "*" }, "peerDependenciesMeta": { "jiti": { "optional": true } } }, "node_modules/eslint-plugin-react-hooks": { "version": "5.2.0", "resolved": "https://registry.npmjs.org/eslint-plugin-react-hooks/-/eslint-plugin-react-hooks-5.2.0.tgz", "integrity": "sha512-+f15FfK64YQwZdJNELETdn5ibXEUQmW1DZL6KXhNnc2heoy/sg9VJJeT7n8TlMWouzWqSWavFkIhHyIbIAEapg==", "dev": true, "license": "MIT", "engines": { "node": ">=10" }, "peerDependencies": { "eslint": "^3.0.0 || ^4.0.0 || ^5.0.0 || ^6.0.0 || ^7.0.0 || ^8.0.0-0 || ^9.0.0" } }, "node_modules/eslint-plugin-react-refresh": { "version": "0.4.20", "resolved": "https://registry.npmjs.org/eslint-plugin-react-refresh/-/eslint-plugin-react-refresh-0.4.20.tgz", "integrity": "sha512-XpbHQ2q5gUF8BGOX4dHe+71qoirYMhApEPZ7sfhF/dNnOF1UXnCMGZf79SFTBO7Bz5YEIT4TMieSlJBWhP9WBA==", "dev": true, "license": "MIT", "peerDependencies": { "eslint": ">=8.40" } }, "node_modules/eslint-scope": { "version": "8.3.0", "resolved": "https://registry.npmjs.org/eslint-scope/-/eslint-scope-8.3.0.tgz", "integrity": "sha512-pUNxi75F8MJ/GdeKtVLSbYg4ZI34J6C0C7sbL4YOp2exGwen7ZsuBqKzUhXd0qMQ362yET3z+uPwKeg/0C2XCQ==", "dev": true, "license": "BSD-2-Clause", "dependencies": { "esrecurse": "^4.3.0", "estraverse": "^5.2.0" }, "engines": { "node": "^18.18.0 || ^20.9.0 || >=21.1.0" }, "funding": { "url": "https://opencollective.com/eslint" } }, "node_modules/eslint-visitor-keys": { "version": "4.2.0", "resolved": "https://registry.npmjs.org/eslint-visitor-keys/-/eslint-visitor-keys-4.2.0.tgz", "integrity": "sha512-UyLnSehNt62FFhSwjZlHmeokpRK59rcz29j+F1/aDgbkbRTk7wIc9XzdoasMUbRNKDM0qQt/+BJ4BrpFeABemw==", "dev": true, "license": "Apache-2.0", "engines": { "node": "^18.18.0 || ^20.9.0 || >=21.1.0" }, "funding": { "url": "https://opencollective.com/eslint" } }, "node_modules/espree": { "version": "10.3.0", "resolved": "https://registry.npmjs.org/espree/-/espree-10.3.0.tgz", "integrity": "sha512-0QYC8b24HWY8zjRnDTL6RiHfDbAWn63qb4LMj1Z4b076A4une81+z03Kg7l7mn/48PUTqoLptSXez8oknU8Clg==", "dev": true, "license": "BSD-2-Clause", "dependencies": { "acorn": "^8.14.0", "acorn-jsx": "^5.3.2", "eslint-visitor-keys": "^4.2.0" }, "engines": { "node": "^18.18.0 || ^20.9.0 || >=21.1.0" }, "funding": { "url": "https://opencollective.com/eslint" } }, "node_modules/esquery": { "version": "1.6.0", "resolved": "https://registry.npmjs.org/esquery/-/esquery-1.6.0.tgz", "integrity": "sha512-ca9pw9fomFcKPvFLXhBKUK90ZvGibiGOvRJNbjljY7s7uq/5YO4BOzcYtJqExdx99rF6aAcnRxHmcUHcz6sQsg==", "dev": true, "license": "BSD-3-Clause", "dependencies": { "estraverse": "^5.1.0" }, "engines": { "node": ">=0.10" } }, "node_modules/esrecurse": { "version": "4.3.0", "resolved": "https://registry.npmjs.org/esrecurse/-/esrecurse-4.3.0.tgz", "integrity": "sha512-KmfKL3b6G+RXvP8N1vr3Tq1kL/oCFgn2NYXEtqP8/L3pKapUA4G8cFVaoF3SU323CD4XypR/ffioHmkti6/Tag==", "dev": true, "license": "BSD-2-Clause", "dependencies": { "estraverse": "^5.2.0" }, "engines": { "node": ">=4.0" } }, "node_modules/estraverse": { "version": "5.3.0", "resolved": "https://registry.npmjs.org/estraverse/-/estraverse-5.3.0.tgz", "integrity": "sha512-MMdARuVEQziNTeJD8DgMqmhwR11BRQ/cBP+pLtYdSTnf3MIO8fFeiINEbX36ZdNlfU/7A9f3gUw49B3oQsvwBA==", "dev": true, "license": "BSD-2-Clause", "engines": { "node": ">=4.0" } }, "node_modules/esutils": { "version": "2.0.3", "resolved": "https://registry.npmjs.org/esutils/-/esutils-2.0.3.tgz", "integrity": "sha512-kVscqXk4OCp68SZ0dkgEKVi6/8ij300KBWTJq32P/dYeWTSwK41WyTxalN1eRmA5Z9UU/LX9D7FWSmV9SAYx6g==", "dev": true, "license": "BSD-2-Clause", "engines": { "node": ">=0.10.0" } }, "node_modules/etag": { "version": "1.8.1", "resolved": "https://registry.npmjs.org/etag/-/etag-1.8.1.tgz", "integrity": "sha512-aIL5Fx7mawVa300al2BnEE4iNvo1qETxLrPI/o05L7z6go7fCw1J6EQmbK4FmJ2AS7kgVF/KEZWufBfdClMcPg==", "dev": true, "license": "MIT", "engines": { "node": ">= 0.6" } }, "node_modules/eventemitter3": { "version": "4.0.7", "resolved": "https://registry.npmjs.org/eventemitter3/-/eventemitter3-4.0.7.tgz", "integrity": "sha512-8guHBZCwKnFhYdHr2ysuRWErTwhoN2X8XELRlrRwpmfeY2jjuUN4taQMsULKUVo1K4DvZl+0pgfyoysHxvmvEw==", "license": "MIT" }, "node_modules/eventsource": { "version": "3.0.6", "resolved": "https://registry.npmjs.org/eventsource/-/eventsource-3.0.6.tgz", "integrity": "sha512-l19WpE2m9hSuyP06+FbuUUf1G+R0SFLrtQfbRb9PRr+oimOfxQhgGCbVaXg5IvZyyTThJsxh6L/srkMiCeBPDA==", "dev": true, "license": "MIT", "dependencies": { "eventsource-parser": "^3.0.1" }, "engines": { "node": ">=18.0.0" } }, "node_modules/eventsource-parser": { "version": "3.0.1", "resolved": "https://registry.npmjs.org/eventsource-parser/-/eventsource-parser-3.0.1.tgz", "integrity": "sha512-VARTJ9CYeuQYb0pZEPbzi740OWFgpHe7AYJ2WFZVnUDUQp5Dk2yJUgF36YsZ81cOyxT0QxmXD2EQpapAouzWVA==", "dev": true, "license": "MIT", "engines": { "node": ">=18.0.0" } }, "node_modules/express": { "version": "5.1.0", "resolved": "https://registry.npmjs.org/express/-/express-5.1.0.tgz", "integrity": "sha512-DT9ck5YIRU+8GYzzU5kT3eHGA5iL+1Zd0EutOmTE9Dtk+Tvuzd23VBU+ec7HPNSTxXYO55gPV/hq4pSBJDjFpA==", "dev": true, "license": "MIT", "dependencies": { "accepts": "^2.0.0", "body-parser": "^2.2.0", "content-disposition": "^1.0.0", "content-type": "^1.0.5", "cookie": "^0.7.1", "cookie-signature": "^1.2.1", "debug": "^4.4.0", "encodeurl": "^2.0.0", "escape-html": "^1.0.3", "etag": "^1.8.1", "finalhandler": "^2.1.0", "fresh": "^2.0.0", "http-errors": "^2.0.0", "merge-descriptors": "^2.0.0", "mime-types": "^3.0.0", "on-finished": "^2.4.1", "once": "^1.4.0", "parseurl": "^1.3.3", "proxy-addr": "^2.0.7", "qs": "^6.14.0", "range-parser": "^1.2.1", "router": "^2.2.0", "send": "^1.1.0", "serve-static": "^2.2.0", "statuses": "^2.0.1", "type-is": "^2.0.1", "vary": "^1.1.2" }, "engines": { "node": ">= 18" }, "funding": { "type": "opencollective", "url": "https://opencollective.com/express" } }, "node_modules/express-rate-limit": { "version": "7.5.0", "resolved": "https://registry.npmjs.org/express-rate-limit/-/express-rate-limit-7.5.0.tgz", "integrity": "sha512-eB5zbQh5h+VenMPM3fh+nw1YExi5nMr6HUCR62ELSP11huvxm/Uir1H1QEyTkk5QX6A58pX6NmaTMceKZ0Eodg==", "dev": true, "license": "MIT", "engines": { "node": ">= 16" }, "funding": { "url": "https://github.com/sponsors/express-rate-limit" }, "peerDependencies": { "express": "^4.11 || 5 || ^5.0.0-beta.1" } }, "node_modules/fast-deep-equal": { "version": "3.1.3", "resolved": "https://registry.npmjs.org/fast-deep-equal/-/fast-deep-equal-3.1.3.tgz", "integrity": "sha512-f3qQ9oQy9j2AhBe/H9VC91wLmKBCCU/gDOnKNAYG5hswO7BLKj09Hc5HYNz9cGI++xlpDCIgDaitVs03ATR84Q==", "dev": true, "license": "MIT" }, "node_modules/fast-equals": { "version": "5.2.2", "resolved": "https://registry.npmjs.org/fast-equals/-/fast-equals-5.2.2.tgz", "integrity": "sha512-V7/RktU11J3I36Nwq2JnZEM7tNm17eBJz+u25qdxBZeCKiX6BkVSZQjwWIr+IobgnZy+ag73tTZgZi7tr0LrBw==", "license": "MIT", "engines": { "node": ">=6.0.0" } }, "node_modules/fast-glob": { "version": "3.3.3", "resolved": "https://registry.npmjs.org/fast-glob/-/fast-glob-3.3.3.tgz", "integrity": "sha512-7MptL8U0cqcFdzIzwOTHoilX9x5BrNqye7Z/LuC7kCMRio1EMSyqRK3BEAUD7sXRq4iT4AzTVuZdhgQ2TCvYLg==", "dev": true, "license": "MIT", "dependencies": { "@nodelib/fs.stat": "^2.0.2", "@nodelib/fs.walk": "^1.2.3", "glob-parent": "^5.1.2", "merge2": "^1.3.0", "micromatch": "^4.0.8" }, "engines": { "node": ">=8.6.0" } }, "node_modules/fast-glob/node_modules/glob-parent": { "version": "5.1.2", "resolved": "https://registry.npmjs.org/glob-parent/-/glob-parent-5.1.2.tgz", "integrity": "sha512-AOIgSQCepiJYwP3ARnGx+5VnTu2HBYdzbGP45eLw1vr3zB3vZLeyed1sC9hnbcOc9/SrMyM5RPQrkGz4aS9Zow==", "dev": true, "license": "ISC", "dependencies": { "is-glob": "^4.0.1" }, "engines": { "node": ">= 6" } }, "node_modules/fast-json-stable-stringify": { "version": "2.1.0", "resolved": "https://registry.npmjs.org/fast-json-stable-stringify/-/fast-json-stable-stringify-2.1.0.tgz", "integrity": "sha512-lhd/wF+Lk98HZoTCtlVraHtfh5XYijIjalXck7saUtuanSDyLMxnHhSXEDJqHxD7msR8D0uCmqlkwjCV8xvwHw==", "dev": true, "license": "MIT" }, "node_modules/fast-levenshtein": { "version": "2.0.6", "resolved": "https://registry.npmjs.org/fast-levenshtein/-/fast-levenshtein-2.0.6.tgz", "integrity": "sha512-DCXu6Ifhqcks7TZKY3Hxp3y6qphY5SJZmrWMDrKcERSOXWQdMhU9Ig/PYrzyw/ul9jOIyh0N4M0tbC5hodg8dw==", "dev": true, "license": "MIT" }, "node_modules/fastq": { "version": "1.19.1", "resolved": "https://registry.npmjs.org/fastq/-/fastq-1.19.1.tgz", "integrity": "sha512-GwLTyxkCXjXbxqIhTsMI2Nui8huMPtnxg7krajPJAjnEG/iiOS7i+zCtWGZR9G0NBKbXKh6X9m9UIsYX/N6vvQ==", "dev": true, "license": "ISC", "dependencies": { "reusify": "^1.0.4" } }, "node_modules/file-entry-cache": { "version": "8.0.0", "resolved": "https://registry.npmjs.org/file-entry-cache/-/file-entry-cache-8.0.0.tgz", "integrity": "sha512-XXTUwCvisa5oacNGRP9SfNtYBNAMi+RPwBFmblZEF7N7swHYQS6/Zfk7SRwx4D5j3CH211YNRco1DEMNVfZCnQ==", "dev": true, "license": "MIT", "dependencies": { "flat-cache": "^4.0.0" }, "engines": { "node": ">=16.0.0" } }, "node_modules/fill-range": { "version": "7.1.1", "resolved": "https://registry.npmjs.org/fill-range/-/fill-range-7.1.1.tgz", "integrity": "sha512-YsGpe3WHLK8ZYi4tWDg2Jy3ebRz2rXowDxnld4bkQB00cc/1Zw9AWnC0i9ztDJitivtQvaI9KaLyKrc+hBW0yg==", "dev": true, "license": "MIT", "dependencies": { "to-regex-range": "^5.0.1" }, "engines": { "node": ">=8" } }, "node_modules/finalhandler": { "version": "2.1.0", "resolved": "https://registry.npmjs.org/finalhandler/-/finalhandler-2.1.0.tgz", "integrity": "sha512-/t88Ty3d5JWQbWYgaOGCCYfXRwV1+be02WqYYlL6h0lEiUAMPM8o8qKGO01YIkOHzka2up08wvgYD0mDiI+q3Q==", "dev": true, "license": "MIT", "dependencies": { "debug": "^4.4.0", "encodeurl": "^2.0.0", "escape-html": "^1.0.3", "on-finished": "^2.4.1", "parseurl": "^1.3.3", "statuses": "^2.0.1" }, "engines": { "node": ">= 0.8" } }, "node_modules/find-root": { "version": "1.1.0", "resolved": "https://registry.npmjs.org/find-root/-/find-root-1.1.0.tgz", "integrity": "sha512-NKfW6bec6GfKc0SGx1e07QZY9PE99u0Bft/0rzSD5k3sO/vwkVUpDUKVm5Gpp5Ue3YfShPFTX2070tDs5kB9Ng==", "license": "MIT" }, "node_modules/find-up": { "version": "5.0.0", "resolved": "https://registry.npmjs.org/find-up/-/find-up-5.0.0.tgz", "integrity": "sha512-78/PXT1wlLLDgTzDs7sjq9hzz0vXD+zn+7wypEe4fXQxCmdmqfGsEPQxmiCSQI3ajFV91bVSsvNtrJRiW6nGng==", "dev": true, "license": "MIT", "dependencies": { "locate-path": "^6.0.0", "path-exists": "^4.0.0" }, "engines": { "node": ">=10" }, "funding": { "url": "https://github.com/sponsors/sindresorhus" } }, "node_modules/flat-cache": { "version": "4.0.1", "resolved": "https://registry.npmjs.org/flat-cache/-/flat-cache-4.0.1.tgz", "integrity": "sha512-f7ccFPK3SXFHpx15UIGyRJ/FJQctuKZ0zVuN3frBo4HnK3cay9VEW0R6yPYFHC0AgqhukPzKjq22t5DmAyqGyw==", "dev": true, "license": "MIT", "dependencies": { "flatted": "^3.2.9", "keyv": "^4.5.4" }, "engines": { "node": ">=16" } }, "node_modules/flatted": { "version": "3.3.3", "resolved": "https://registry.npmjs.org/flatted/-/flatted-3.3.3.tgz", "integrity": "sha512-GX+ysw4PBCz0PzosHDepZGANEuFCMLrnRTiEy9McGjmkCQYwRq4A/X786G/fjM/+OjsWSU1ZrY5qyARZmO/uwg==", "dev": true, "license": "ISC" }, "node_modules/follow-redirects": { "version": "1.15.9", "resolved": "https://registry.npmjs.org/follow-redirects/-/follow-redirects-1.15.9.tgz", "integrity": "sha512-gew4GsXizNgdoRyqmyfMHyAmXsZDk6mHkSxZFCzW9gwlbtOW44CDtYavM+y+72qD/Vq2l550kMF52DT8fOLJqQ==", "funding": [ { "type": "individual", "url": "https://github.com/sponsors/RubenVerborgh" } ], "license": "MIT", "engines": { "node": ">=4.0" }, "peerDependenciesMeta": { "debug": { "optional": true } } }, "node_modules/foreground-child": { "version": "3.3.1", "resolved": "https://registry.npmjs.org/foreground-child/-/foreground-child-3.3.1.tgz", "integrity": "sha512-gIXjKqtFuWEgzFRJA9WCQeSJLZDjgJUOMCMzxtvFq/37KojM1BFGufqsCy0r4qSQmYLsZYMeyRqzIWOMup03sw==", "dev": true, "license": "ISC", "dependencies": { "cross-spawn": "^7.0.6", "signal-exit": "^4.0.1" }, "engines": { "node": ">=14" }, "funding": { "url": "https://github.com/sponsors/isaacs" } }, "node_modules/form-data": { "version": "4.0.2", "resolved": "https://registry.npmjs.org/form-data/-/form-data-4.0.2.tgz", "integrity": "sha512-hGfm/slu0ZabnNt4oaRZ6uREyfCj6P4fT/n6A1rGV+Z0VdGXjfOhVUpkn6qVQONHGIFwmveGXyDs75+nr6FM8w==", "license": "MIT", "dependencies": { "asynckit": "^0.4.0", "combined-stream": "^1.0.8", "es-set-tostringtag": "^2.1.0", "mime-types": "^2.1.12" }, "engines": { "node": ">= 6" } }, "node_modules/form-data/node_modules/mime-db": { "version": "1.52.0", "resolved": "https://registry.npmjs.org/mime-db/-/mime-db-1.52.0.tgz", "integrity": "sha512-sPU4uV7dYlvtWJxwwxHD0PuihVNiE7TyAbQ5SWxDCB9mUYvOgroQOwYQQOKPJ8CIbE+1ETVlOoK1UC2nU3gYvg==", "license": "MIT", "engines": { "node": ">= 0.6" } }, "node_modules/form-data/node_modules/mime-types": { "version": "2.1.35", "resolved": "https://registry.npmjs.org/mime-types/-/mime-types-2.1.35.tgz", "integrity": "sha512-ZDY+bPm5zTTF+YpCrAU9nK0UgICYPT0QtT1NZWFv4s++TNkcgVaT0g6+4R2uI4MjQjzysHB1zxuWL50hzaeXiw==", "license": "MIT", "dependencies": { "mime-db": "1.52.0" }, "engines": { "node": ">= 0.6" } }, "node_modules/forwarded": { "version": "0.2.0", "resolved": "https://registry.npmjs.org/forwarded/-/forwarded-0.2.0.tgz", "integrity": "sha512-buRG0fpBtRHSTCOASe6hD258tEubFoRLb4ZNA6NxMVHNw2gOcwHo9wyablzMzOA5z9xA9L1KNjk/Nt6MT9aYow==", "dev": true, "license": "MIT", "engines": { "node": ">= 0.6" } }, "node_modules/fraction.js": { "version": "4.3.7", "resolved": "https://registry.npmjs.org/fraction.js/-/fraction.js-4.3.7.tgz", "integrity": "sha512-ZsDfxO51wGAXREY55a7la9LScWpwv9RxIrYABrlvOFBlH/ShPnrtsXeuUIfXKKOVicNxQ+o8JTbJvjS4M89yew==", "dev": true, "license": "MIT", "engines": { "node": "*" }, "funding": { "type": "patreon", "url": "https://github.com/sponsors/rawify" } }, "node_modules/fresh": { "version": "2.0.0", "resolved": "https://registry.npmjs.org/fresh/-/fresh-2.0.0.tgz", "integrity": "sha512-Rx/WycZ60HOaqLKAi6cHRKKI7zxWbJ31MhntmtwMoaTeF7XFH9hhBp8vITaMidfljRQ6eYWCKkaTK+ykVJHP2A==", "dev": true, "license": "MIT", "engines": { "node": ">= 0.8" } }, "node_modules/fsevents": { "version": "2.3.3", "resolved": "https://registry.npmjs.org/fsevents/-/fsevents-2.3.3.tgz", "integrity": "sha512-5xoDfX+fL7faATnagmWPpbFtwh/R77WmMMqqHGS65C3vvB0YHrgF+B1YmZ3441tMj5n63k0212XNoJwzlhffQw==", "dev": true, "hasInstallScript": true, "license": "MIT", "optional": true, "os": [ "darwin" ], "engines": { "node": "^8.16.0 || ^10.6.0 || >=11.0.0" } }, "node_modules/function-bind": { "version": "1.1.2", "resolved": "https://registry.npmjs.org/function-bind/-/function-bind-1.1.2.tgz", "integrity": "sha512-7XHNxH7qX9xG5mIwxkhumTox/MIRNcOgDrxWsMt2pAr23WHp6MrRlN7FBSFpCpr+oVO0F744iUgR82nJMfG2SA==", "license": "MIT", "funding": { "url": "https://github.com/sponsors/ljharb" } }, "node_modules/gensync": { "version": "1.0.0-beta.2", "resolved": "https://registry.npmjs.org/gensync/-/gensync-1.0.0-beta.2.tgz", "integrity": "sha512-3hN7NaskYvMDLQY55gnW3NQ+mesEAepTqlg+VEbj7zzqEMBVNhzcGYYeqFo/TlYz6eQiFcp1HcsCZO+nGgS8zg==", "dev": true, "license": "MIT", "engines": { "node": ">=6.9.0" } }, "node_modules/get-intrinsic": { "version": "1.3.0", "resolved": "https://registry.npmjs.org/get-intrinsic/-/get-intrinsic-1.3.0.tgz", "integrity": "sha512-9fSjSaos/fRIVIp+xSJlE6lfwhES7LNtKaCBIamHsjr2na1BiABJPo0mOjjz8GJDURarmCPGqaiVg5mfjb98CQ==", "license": "MIT", "dependencies": { "call-bind-apply-helpers": "^1.0.2", "es-define-property": "^1.0.1", "es-errors": "^1.3.0", "es-object-atoms": "^1.1.1", "function-bind": "^1.1.2", "get-proto": "^1.0.1", "gopd": "^1.2.0", "has-symbols": "^1.1.0", "hasown": "^2.0.2", "math-intrinsics": "^1.1.0" }, "engines": { "node": ">= 0.4" }, "funding": { "url": "https://github.com/sponsors/ljharb" } }, "node_modules/get-proto": { "version": "1.0.1", "resolved": "https://registry.npmjs.org/get-proto/-/get-proto-1.0.1.tgz", "integrity": "sha512-sTSfBjoXBp89JvIKIefqw7U2CCebsc74kiY6awiGogKtoSGbgjYE/G/+l9sF3MWFPNc9IcoOC4ODfKHfxFmp0g==", "license": "MIT", "dependencies": { "dunder-proto": "^1.0.1", "es-object-atoms": "^1.0.0" }, "engines": { "node": ">= 0.4" } }, "node_modules/glob": { "version": "10.4.5", "resolved": "https://registry.npmjs.org/glob/-/glob-10.4.5.tgz", "integrity": "sha512-7Bv8RF0k6xjo7d4A/PxYLbUCfb6c+Vpd2/mB2yRDlew7Jb5hEXiCD9ibfO7wpk8i4sevK6DFny9h7EYbM3/sHg==", "dev": true, "license": "ISC", "dependencies": { "foreground-child": "^3.1.0", "jackspeak": "^3.1.2", "minimatch": "^9.0.4", "minipass": "^7.1.2", "package-json-from-dist": "^1.0.0", "path-scurry": "^1.11.1" }, "bin": { "glob": "dist/esm/bin.mjs" }, "funding": { "url": "https://github.com/sponsors/isaacs" } }, "node_modules/glob-parent": { "version": "6.0.2", "resolved": "https://registry.npmjs.org/glob-parent/-/glob-parent-6.0.2.tgz", "integrity": "sha512-XxwI8EOhVQgWp6iDL+3b0r86f4d6AX6zSU55HfB4ydCEuXLXc5FcYeOu+nnGftS4TEju/11rt4KJPTMgbfmv4A==", "dev": true, "license": "ISC", "dependencies": { "is-glob": "^4.0.3" }, "engines": { "node": ">=10.13.0" } }, "node_modules/glob/node_modules/brace-expansion": { "version": "2.0.1", "resolved": "https://registry.npmjs.org/brace-expansion/-/brace-expansion-2.0.1.tgz", "integrity": "sha512-XnAIvQ8eM+kC6aULx6wuQiwVsnzsi9d3WxzV3FpWTGA19F621kwdbsAcFKXgKUHZWsy+mY6iL1sHTxWEFCytDA==", "dev": true, "license": "MIT", "dependencies": { "balanced-match": "^1.0.0" } }, "node_modules/glob/node_modules/minimatch": { "version": "9.0.5", "resolved": "https://registry.npmjs.org/minimatch/-/minimatch-9.0.5.tgz", "integrity": "sha512-G6T0ZX48xgozx7587koeX9Ys2NYy6Gmv//P89sEte9V9whIapMNF4idKxnW2QtCcLiTWlb/wfCabAtAFWhhBow==", "dev": true, "license": "ISC", "dependencies": { "brace-expansion": "^2.0.1" }, "engines": { "node": ">=16 || 14 >=14.17" }, "funding": { "url": "https://github.com/sponsors/isaacs" } }, "node_modules/globals": { "version": "16.0.0", "resolved": "https://registry.npmjs.org/globals/-/globals-16.0.0.tgz", "integrity": "sha512-iInW14XItCXET01CQFqudPOWP2jYMl7T+QRQT+UNcR/iQncN/F0UNpgd76iFkBPgNQb4+X3LV9tLJYzwh+Gl3A==", "dev": true, "license": "MIT", "engines": { "node": ">=18" }, "funding": { "url": "https://github.com/sponsors/sindresorhus" } }, "node_modules/gopd": { "version": "1.2.0", "resolved": "https://registry.npmjs.org/gopd/-/gopd-1.2.0.tgz", "integrity": "sha512-ZUKRh6/kUFoAiTAtTYPZJ3hw9wNxx+BIBOijnlG9PnrJsCcSjs1wyyD6vJpaYtgnzDrKYRSqf3OO6Rfa93xsRg==", "license": "MIT", "engines": { "node": ">= 0.4" }, "funding": { "url": "https://github.com/sponsors/ljharb" } }, "node_modules/graphemer": { "version": "1.4.0", "resolved": "https://registry.npmjs.org/graphemer/-/graphemer-1.4.0.tgz", "integrity": "sha512-EtKwoO6kxCL9WO5xipiHTZlSzBm7WLT627TqC/uVRd0HKmq8NXyebnNYxDoBi7wt8eTWrUrKXCOVaFq9x1kgag==", "dev": true, "license": "MIT" }, "node_modules/has-flag": { "version": "4.0.0", "resolved": "https://registry.npmjs.org/has-flag/-/has-flag-4.0.0.tgz", "integrity": "sha512-EykJT/Q1KjTWctppgIAgfSO0tKVuZUjhgMr17kqTumMl6Afv3EISleU7qZUzoXDFTAHTDC4NOoG/ZxU3EvlMPQ==", "dev": true, "license": "MIT", "engines": { "node": ">=8" } }, "node_modules/has-symbols": { "version": "1.1.0", "resolved": "https://registry.npmjs.org/has-symbols/-/has-symbols-1.1.0.tgz", "integrity": "sha512-1cDNdwJ2Jaohmb3sg4OmKaMBwuC48sYni5HUw2DvsC8LjGTLK9h+eb1X6RyuOHe4hT0ULCW68iomhjUoKUqlPQ==", "license": "MIT", "engines": { "node": ">= 0.4" }, "funding": { "url": "https://github.com/sponsors/ljharb" } }, "node_modules/has-tostringtag": { "version": "1.0.2", "resolved": "https://registry.npmjs.org/has-tostringtag/-/has-tostringtag-1.0.2.tgz", "integrity": "sha512-NqADB8VjPFLM2V0VvHUewwwsw0ZWBaIdgo+ieHtK3hasLz4qeCRjYcqfB6AQrBggRKppKF8L52/VqdVsO47Dlw==", "license": "MIT", "dependencies": { "has-symbols": "^1.0.3" }, "engines": { "node": ">= 0.4" }, "funding": { "url": "https://github.com/sponsors/ljharb" } }, "node_modules/hasown": { "version": "2.0.2", "resolved": "https://registry.npmjs.org/hasown/-/hasown-2.0.2.tgz", "integrity": "sha512-0hJU9SCPvmMzIBdZFqNPXWa6dqh7WdH0cII9y+CyS8rG3nL48Bclra9HmKhVVUHyPWNH5Y7xDwAB7bfgSjkUMQ==", "license": "MIT", "dependencies": { "function-bind": "^1.1.2" }, "engines": { "node": ">= 0.4" } }, "node_modules/hoist-non-react-statics": { "version": "3.3.2", "resolved": "https://registry.npmjs.org/hoist-non-react-statics/-/hoist-non-react-statics-3.3.2.tgz", "integrity": "sha512-/gGivxi8JPKWNm/W0jSmzcMPpfpPLc3dY/6GxhX2hQ9iGj3aDfklV4ET7NjKpSinLpJ5vafa9iiGIEZg10SfBw==", "license": "BSD-3-Clause", "dependencies": { "react-is": "^16.7.0" } }, "node_modules/http-errors": { "version": "2.0.0", "resolved": "https://registry.npmjs.org/http-errors/-/http-errors-2.0.0.tgz", "integrity": "sha512-FtwrG/euBzaEjYeRqOgly7G0qviiXoJWnvEH2Z1plBdXgbyjv34pHTSb9zoeHMyDy33+DWy5Wt9Wo+TURtOYSQ==", "dev": true, "license": "MIT", "dependencies": { "depd": "2.0.0", "inherits": "2.0.4", "setprototypeof": "1.2.0", "statuses": "2.0.1", "toidentifier": "1.0.1" }, "engines": { "node": ">= 0.8" } }, "node_modules/iconv-lite": { "version": "0.6.3", "resolved": "https://registry.npmjs.org/iconv-lite/-/iconv-lite-0.6.3.tgz", "integrity": "sha512-4fCk79wshMdzMp2rH06qWrJE4iolqLhCUH+OiuIgU++RB0+94NlDL81atO7GX55uUKueo0txHNtvEyI6D7WdMw==", "dev": true, "license": "MIT", "dependencies": { "safer-buffer": ">= 2.1.2 < 3.0.0" }, "engines": { "node": ">=0.10.0" } }, "node_modules/ignore": { "version": "5.3.2", "resolved": "https://registry.npmjs.org/ignore/-/ignore-5.3.2.tgz", "integrity": "sha512-hsBTNUqQTDwkWtcdYI2i06Y/nUBEsNEDJKjWdigLvegy8kDuJAS8uRlpkkcQpyEXL0Z/pjDy5HBmMjRCJ2gq+g==", "dev": true, "license": "MIT", "engines": { "node": ">= 4" } }, "node_modules/import-fresh": { "version": "3.3.1", "resolved": "https://registry.npmjs.org/import-fresh/-/import-fresh-3.3.1.tgz", "integrity": "sha512-TR3KfrTZTYLPB6jUjfx6MF9WcWrHL9su5TObK4ZkYgBdWKPOFoSoQIdEuTuR82pmtxH2spWG9h6etwfr1pLBqQ==", "license": "MIT", "dependencies": { "parent-module": "^1.0.0", "resolve-from": "^4.0.0" }, "engines": { "node": ">=6" }, "funding": { "url": "https://github.com/sponsors/sindresorhus" } }, "node_modules/imurmurhash": { "version": "0.1.4", "resolved": "https://registry.npmjs.org/imurmurhash/-/imurmurhash-0.1.4.tgz", "integrity": "sha512-JmXMZ6wuvDmLiHEml9ykzqO6lwFbof0GG4IkcGaENdCRDDmMVnny7s5HsIgHCbaq0w2MyPhDqkhTUgS2LU2PHA==", "dev": true, "license": "MIT", "engines": { "node": ">=0.8.19" } }, "node_modules/inherits": { "version": "2.0.4", "resolved": "https://registry.npmjs.org/inherits/-/inherits-2.0.4.tgz", "integrity": "sha512-k/vGaX4/Yla3WzyMCvTQOXYeIHvqOKtnqBduzTHpzpQZzAskKMhZ2K+EnBiSM9zGSoIFeMpXKxa4dYeZIQqewQ==", "dev": true, "license": "ISC" }, "node_modules/internmap": { "version": "2.0.3", "resolved": "https://registry.npmjs.org/internmap/-/internmap-2.0.3.tgz", "integrity": "sha512-5Hh7Y1wQbvY5ooGgPbDaL5iYLAPzMTUrjMulskHLH6wnv/A+1q5rgEaiuqEjB+oxGXIVZs1FF+R/KPN3ZSQYYg==", "license": "ISC", "engines": { "node": ">=12" } }, "node_modules/ipaddr.js": { "version": "1.9.1", "resolved": "https://registry.npmjs.org/ipaddr.js/-/ipaddr.js-1.9.1.tgz", "integrity": "sha512-0KI/607xoxSToH7GjN1FfSbLoU0+btTicjsQSWQlh/hZykN8KpmMf7uYwPW3R+akZ6R/w18ZlXSHBYXiYUPO3g==", "dev": true, "license": "MIT", "engines": { "node": ">= 0.10" } }, "node_modules/is-arrayish": { "version": "0.2.1", "resolved": "https://registry.npmjs.org/is-arrayish/-/is-arrayish-0.2.1.tgz", "integrity": "sha512-zz06S8t0ozoDXMG+ube26zeCTNXcKIPJZJi8hBrF4idCLms4CG9QtK7qBl1boi5ODzFpjswb5JPmHCbMpjaYzg==", "license": "MIT" }, "node_modules/is-binary-path": { "version": "2.1.0", "resolved": "https://registry.npmjs.org/is-binary-path/-/is-binary-path-2.1.0.tgz", "integrity": "sha512-ZMERYes6pDydyuGidse7OsHxtbI7WVeUEozgR/g7rd0xUimYNlvZRE/K2MgZTjWy725IfelLeVcEM97mmtRGXw==", "dev": true, "license": "MIT", "dependencies": { "binary-extensions": "^2.0.0" }, "engines": { "node": ">=8" } }, "node_modules/is-core-module": { "version": "2.16.1", "resolved": "https://registry.npmjs.org/is-core-module/-/is-core-module-2.16.1.tgz", "integrity": "sha512-UfoeMA6fIJ8wTYFEUjelnaGI67v6+N7qXJEvQuIGa99l4xsCruSYOVSQ0uPANn4dAzm8lkYPaKLrrijLq7x23w==", "license": "MIT", "dependencies": { "hasown": "^2.0.2" }, "engines": { "node": ">= 0.4" }, "funding": { "url": "https://github.com/sponsors/ljharb" } }, "node_modules/is-extglob": { "version": "2.1.1", "resolved": "https://registry.npmjs.org/is-extglob/-/is-extglob-2.1.1.tgz", "integrity": "sha512-SbKbANkN603Vi4jEZv49LeVJMn4yGwsbzZworEoyEiutsN3nJYdbO36zfhGJ6QEDpOZIFkDtnq5JRxmvl3jsoQ==", "dev": true, "license": "MIT", "engines": { "node": ">=0.10.0" } }, "node_modules/is-fullwidth-code-point": { "version": "3.0.0", "resolved": "https://registry.npmjs.org/is-fullwidth-code-point/-/is-fullwidth-code-point-3.0.0.tgz", "integrity": "sha512-zymm5+u+sCsSWyD9qNaejV3DFvhCKclKdizYaJUuHA83RLjb7nSuGnddCHGv0hk+KY7BMAlsWeK4Ueg6EV6XQg==", "dev": true, "license": "MIT", "engines": { "node": ">=8" } }, "node_modules/is-glob": { "version": "4.0.3", "resolved": "https://registry.npmjs.org/is-glob/-/is-glob-4.0.3.tgz", "integrity": "sha512-xelSayHH36ZgE7ZWhli7pW34hNbNl8Ojv5KVmkJD4hBdD3th8Tfk9vYasLM+mXWOZhFkgZfxhLSnrwRr4elSSg==", "dev": true, "license": "MIT", "dependencies": { "is-extglob": "^2.1.1" }, "engines": { "node": ">=0.10.0" } }, "node_modules/is-number": { "version": "7.0.0", "resolved": "https://registry.npmjs.org/is-number/-/is-number-7.0.0.tgz", "integrity": "sha512-41Cifkg6e8TylSpdtTpeLVMqvSBEVzTttHvERD741+pnZ8ANv0004MRL43QKPDlK9cGvNp6NZWZUBlbGXYxxng==", "dev": true, "license": "MIT", "engines": { "node": ">=0.12.0" } }, "node_modules/is-promise": { "version": "4.0.0", "resolved": "https://registry.npmjs.org/is-promise/-/is-promise-4.0.0.tgz", "integrity": "sha512-hvpoI6korhJMnej285dSg6nu1+e6uxs7zG3BYAm5byqDsgJNWwxzM6z6iZiAgQR4TJ30JmBTOwqZUw3WlyH3AQ==", "dev": true, "license": "MIT" }, "node_modules/isexe": { "version": "2.0.0", "resolved": "https://registry.npmjs.org/isexe/-/isexe-2.0.0.tgz", "integrity": "sha512-RHxMLp9lnKHGHRng9QFhRCMbYAcVpn69smSGcq3f36xjgVVWThj4qqLbTLlq7Ssj8B+fIQ1EuCEGI2lKsyQeIw==", "dev": true, "license": "ISC" }, "node_modules/jackspeak": { "version": "3.4.3", "resolved": "https://registry.npmjs.org/jackspeak/-/jackspeak-3.4.3.tgz", "integrity": "sha512-OGlZQpz2yfahA/Rd1Y8Cd9SIEsqvXkLVoSw/cgwhnhFMDbsQFeZYoJJ7bIZBS9BcamUW96asq/npPWugM+RQBw==", "dev": true, "license": "BlueOak-1.0.0", "dependencies": { "@isaacs/cliui": "^8.0.2" }, "funding": { "url": "https://github.com/sponsors/isaacs" }, "optionalDependencies": { "@pkgjs/parseargs": "^0.11.0" } }, "node_modules/jiti": { "version": "1.21.7", "resolved": "https://registry.npmjs.org/jiti/-/jiti-1.21.7.tgz", "integrity": "sha512-/imKNG4EbWNrVjoNC/1H5/9GFy+tqjGBHCaSsN+P2RnPqjsLmv6UD3Ej+Kj8nBWaRAwyk7kK5ZUc+OEatnTR3A==", "dev": true, "license": "MIT", "bin": { "jiti": "bin/jiti.js" } }, "node_modules/js-tokens": { "version": "4.0.0", "resolved": "https://registry.npmjs.org/js-tokens/-/js-tokens-4.0.0.tgz", "integrity": "sha512-RdJUflcE3cUzKiMqQgsCu06FPu9UdIJO0beYbPhHN4k6apgJtifcoCtT9bcxOpYBtpD2kCM6Sbzg4CausW/PKQ==", "license": "MIT" }, "node_modules/js-yaml": { "version": "4.1.0", "resolved": "https://registry.npmjs.org/js-yaml/-/js-yaml-4.1.0.tgz", "integrity": "sha512-wpxZs9NoxZaJESJGIZTyDEaYpl0FKSA+FB9aJiyemKhMwkxQg63h4T1KJgUGHpTqPDNRcmmYLugrRjJlBtWvRA==", "dev": true, "license": "MIT", "dependencies": { "argparse": "^2.0.1" }, "bin": { "js-yaml": "bin/js-yaml.js" } }, "node_modules/jsesc": { "version": "3.1.0", "resolved": "https://registry.npmjs.org/jsesc/-/jsesc-3.1.0.tgz", "integrity": "sha512-/sM3dO2FOzXjKQhJuo0Q173wf2KOo8t4I8vHy6lF9poUp7bKT0/NHE8fPX23PwfhnykfqnC2xRxOnVw5XuGIaA==", "license": "MIT", "bin": { "jsesc": "bin/jsesc" }, "engines": { "node": ">=6" } }, "node_modules/json-buffer": { "version": "3.0.1", "resolved": "https://registry.npmjs.org/json-buffer/-/json-buffer-3.0.1.tgz", "integrity": "sha512-4bV5BfR2mqfQTJm+V5tPPdf+ZpuhiIvTuAB5g8kcrXOZpTT/QwwVRWBywX1ozr6lEuPdbHxwaJlm9G6mI2sfSQ==", "dev": true, "license": "MIT" }, "node_modules/json-parse-even-better-errors": { "version": "2.3.1", "resolved": "https://registry.npmjs.org/json-parse-even-better-errors/-/json-parse-even-better-errors-2.3.1.tgz", "integrity": "sha512-xyFwyhro/JEof6Ghe2iz2NcXoj2sloNsWr/XsERDK/oiPCfaNhl5ONfp+jQdAZRQQ0IJWNzH9zIZF7li91kh2w==", "license": "MIT" }, "node_modules/json-schema-traverse": { "version": "0.4.1", "resolved": "https://registry.npmjs.org/json-schema-traverse/-/json-schema-traverse-0.4.1.tgz", "integrity": "sha512-xbbCH5dCYU5T8LcEhhuh7HJ88HXuW3qsI3Y0zOZFKfZEHcpWiHU/Jxzk629Brsab/mMiHQti9wMP+845RPe3Vg==", "dev": true, "license": "MIT" }, "node_modules/json-stable-stringify-without-jsonify": { "version": "1.0.1", "resolved": "https://registry.npmjs.org/json-stable-stringify-without-jsonify/-/json-stable-stringify-without-jsonify-1.0.1.tgz", "integrity": "sha512-Bdboy+l7tA3OGW6FjyFHWkP5LuByj1Tk33Ljyq0axyzdk9//JSi2u3fP1QSmd1KNwq6VOKYGlAu87CisVir6Pw==", "dev": true, "license": "MIT" }, "node_modules/json5": { "version": "2.2.3", "resolved": "https://registry.npmjs.org/json5/-/json5-2.2.3.tgz", "integrity": "sha512-XmOWe7eyHYH14cLdVPoyg+GOH3rYX++KpzrylJwSW98t3Nk+U8XOl8FWKOgwtzdb8lXGf6zYwDUzeHMWfxasyg==", "dev": true, "license": "MIT", "bin": { "json5": "lib/cli.js" }, "engines": { "node": ">=6" } }, "node_modules/keyv": { "version": "4.5.4", "resolved": "https://registry.npmjs.org/keyv/-/keyv-4.5.4.tgz", "integrity": "sha512-oxVHkHR/EJf2CNXnWxRLW6mg7JyCCUcG0DtEGmL2ctUo1PNTin1PUil+r/+4r5MpVgC/fn1kjsx7mjSujKqIpw==", "dev": true, "license": "MIT", "dependencies": { "json-buffer": "3.0.1" } }, "node_modules/levn": { "version": "0.4.1", "resolved": "https://registry.npmjs.org/levn/-/levn-0.4.1.tgz", "integrity": "sha512-+bT2uH4E5LGE7h/n3evcS/sQlJXCpIp6ym8OWJ5eV6+67Dsql/LaaT7qJBAt2rzfoa/5QBGBhxDix1dMt2kQKQ==", "dev": true, "license": "MIT", "dependencies": { "prelude-ls": "^1.2.1", "type-check": "~0.4.0" }, "engines": { "node": ">= 0.8.0" } }, "node_modules/lilconfig": { "version": "2.1.0", "resolved": "https://registry.npmjs.org/lilconfig/-/lilconfig-2.1.0.tgz", "integrity": "sha512-utWOt/GHzuUxnLKxB6dk81RoOeoNeHgbrXiuGk4yyF5qlRz+iIVWu56E2fqGHFrXz0QNUhLB/8nKqvRH66JKGQ==", "dev": true, "license": "MIT", "engines": { "node": ">=10" } }, "node_modules/lines-and-columns": { "version": "1.2.4", "resolved": "https://registry.npmjs.org/lines-and-columns/-/lines-and-columns-1.2.4.tgz", "integrity": "sha512-7ylylesZQ/PV29jhEDl3Ufjo6ZX7gCqJr5F7PKrqc93v7fzSymt1BpwEU8nAUXs8qzzvqhbjhK5QZg6Mt/HkBg==", "license": "MIT" }, "node_modules/locate-path": { "version": "6.0.0", "resolved": "https://registry.npmjs.org/locate-path/-/locate-path-6.0.0.tgz", "integrity": "sha512-iPZK6eYjbxRu3uB4/WZ3EsEIMJFMqAoopl3R+zuq0UjcAm/MO6KCweDgPfP3elTztoKP3KtnVHxTn2NHBSDVUw==", "dev": true, "license": "MIT", "dependencies": { "p-locate": "^5.0.0" }, "engines": { "node": ">=10" }, "funding": { "url": "https://github.com/sponsors/sindresorhus" } }, "node_modules/lodash": { "version": "4.17.21", "resolved": "https://registry.npmjs.org/lodash/-/lodash-4.17.21.tgz", "integrity": "sha512-v2kDEe57lecTulaDIuNTPy3Ry4gLGJ6Z1O3vE1krgXZNrsQ+LFTGHVxVjcXPs17LhbZVGedAJv8XZ1tvj5FvSg==", "license": "MIT" }, "node_modules/lodash.merge": { "version": "4.6.2", "resolved": "https://registry.npmjs.org/lodash.merge/-/lodash.merge-4.6.2.tgz", "integrity": "sha512-0KpjqXRVvrYyCsX1swR/XTK0va6VQkQM6MNo7PqW77ByjAhoARA8EfrP1N4+KlKj8YS0ZUCtRT/YUuhyYDujIQ==", "dev": true, "license": "MIT" }, "node_modules/loose-envify": { "version": "1.4.0", "resolved": "https://registry.npmjs.org/loose-envify/-/loose-envify-1.4.0.tgz", "integrity": "sha512-lyuxPGr/Wfhrlem2CL/UcnUc1zcqKAImBDzukY7Y5F/yQiNdko6+fRLevlw1HgMySw7f611UIY408EtxRSoK3Q==", "license": "MIT", "dependencies": { "js-tokens": "^3.0.0 || ^4.0.0" }, "bin": { "loose-envify": "cli.js" } }, "node_modules/lru-cache": { "version": "5.1.1", "resolved": "https://registry.npmjs.org/lru-cache/-/lru-cache-5.1.1.tgz", "integrity": "sha512-KpNARQA3Iwv+jTA0utUVVbrh+Jlrr1Fv0e56GGzAFOXN7dk/FviaDW8LHmK52DlcH4WP2n6gI8vN1aesBFgo9w==", "dev": true, "license": "ISC", "dependencies": { "yallist": "^3.0.2" } }, "node_modules/math-intrinsics": { "version": "1.1.0", "resolved": "https://registry.npmjs.org/math-intrinsics/-/math-intrinsics-1.1.0.tgz", "integrity": "sha512-/IXtbwEk5HTPyEwyKX6hGkYXxM9nbj64B+ilVJnC/R6B0pH5G4V3b0pVbL7DBj4tkhBAppbQUlf6F6Xl9LHu1g==", "license": "MIT", "engines": { "node": ">= 0.4" } }, "node_modules/media-typer": { "version": "1.1.0", "resolved": "https://registry.npmjs.org/media-typer/-/media-typer-1.1.0.tgz", "integrity": "sha512-aisnrDP4GNe06UcKFnV5bfMNPBUw4jsLGaWwWfnH3v02GnBuXX2MCVn5RbrWo0j3pczUilYblq7fQ7Nw2t5XKw==", "dev": true, "license": "MIT", "engines": { "node": ">= 0.8" } }, "node_modules/memoize-one": { "version": "6.0.0", "resolved": "https://registry.npmjs.org/memoize-one/-/memoize-one-6.0.0.tgz", "integrity": "sha512-rkpe71W0N0c0Xz6QD0eJETuWAJGnJ9afsl1srmwPrI+yBCkge5EycXXbYRyvL29zZVUWQCY7InPRCv3GDXuZNw==", "license": "MIT" }, "node_modules/merge-descriptors": { "version": "2.0.0", "resolved": "https://registry.npmjs.org/merge-descriptors/-/merge-descriptors-2.0.0.tgz", "integrity": "sha512-Snk314V5ayFLhp3fkUREub6WtjBfPdCPY1Ln8/8munuLuiYhsABgBVWsozAG+MWMbVEvcdcpbi9R7ww22l9Q3g==", "dev": true, "license": "MIT", "engines": { "node": ">=18" }, "funding": { "url": "https://github.com/sponsors/sindresorhus" } }, "node_modules/merge2": { "version": "1.4.1", "resolved": "https://registry.npmjs.org/merge2/-/merge2-1.4.1.tgz", "integrity": "sha512-8q7VEgMJW4J8tcfVPy8g09NcQwZdbwFEqhe/WZkoIzjn/3TGDwtOCYtXGxA3O8tPzpczCCDgv+P2P5y00ZJOOg==", "dev": true, "license": "MIT", "engines": { "node": ">= 8" } }, "node_modules/micromatch": { "version": "4.0.8", "resolved": "https://registry.npmjs.org/micromatch/-/micromatch-4.0.8.tgz", "integrity": "sha512-PXwfBhYu0hBCPw8Dn0E+WDYb7af3dSLVWKi3HGv84IdF4TyFoC0ysxFd0Goxw7nSv4T/PzEJQxsYsEiFCKo2BA==", "dev": true, "license": "MIT", "dependencies": { "braces": "^3.0.3", "picomatch": "^2.3.1" }, "engines": { "node": ">=8.6" } }, "node_modules/mime-db": { "version": "1.54.0", "resolved": "https://registry.npmjs.org/mime-db/-/mime-db-1.54.0.tgz", "integrity": "sha512-aU5EJuIN2WDemCcAp2vFBfp/m4EAhWJnUNSSw0ixs7/kXbd6Pg64EmwJkNdFhB8aWt1sH2CTXrLxo/iAGV3oPQ==", "dev": true, "license": "MIT", "engines": { "node": ">= 0.6" } }, "node_modules/mime-types": { "version": "3.0.1", "resolved": "https://registry.npmjs.org/mime-types/-/mime-types-3.0.1.tgz", "integrity": "sha512-xRc4oEhT6eaBpU1XF7AjpOFD+xQmXNB5OVKwp4tqCuBpHLS/ZbBDrc07mYTDqVMg6PfxUjjNp85O6Cd2Z/5HWA==", "dev": true, "license": "MIT", "dependencies": { "mime-db": "^1.54.0" }, "engines": { "node": ">= 0.6" } }, "node_modules/minimatch": { "version": "3.1.2", "resolved": "https://registry.npmjs.org/minimatch/-/minimatch-3.1.2.tgz", "integrity": "sha512-J7p63hRiAjw1NDEww1W7i37+ByIrOWO5XQQAzZ3VOcL0PNybwpfmV/N05zFAzwQ9USyEcX6t3UO+K5aqBQOIHw==", "dev": true, "license": "ISC", "dependencies": { "brace-expansion": "^1.1.7" }, "engines": { "node": "*" } }, "node_modules/minipass": { "version": "7.1.2", "resolved": "https://registry.npmjs.org/minipass/-/minipass-7.1.2.tgz", "integrity": "sha512-qOOzS1cBTWYF4BH8fVePDBOO9iptMnGUEZwNc/cMWnTV2nVLZ7VoNWEPHkYczZA0pdoA7dl6e7FL659nX9S2aw==", "dev": true, "license": "ISC", "engines": { "node": ">=16 || 14 >=14.17" } }, "node_modules/ms": { "version": "2.1.3", "resolved": "https://registry.npmjs.org/ms/-/ms-2.1.3.tgz", "integrity": "sha512-6FlzubTLZG3J2a/NVCAleEhjzq5oxgHyaCU9yYXvcLsvoVaHJq/s5xXI6/XXP6tz7R9xAOtHnSO/tXtF3WRTlA==", "license": "MIT" }, "node_modules/mz": { "version": "2.7.0", "resolved": "https://registry.npmjs.org/mz/-/mz-2.7.0.tgz", "integrity": "sha512-z81GNO7nnYMEhrGh9LeymoE4+Yr0Wn5McHIZMK5cfQCl+NDX08sCZgUc9/6MHni9IWuFLm1Z3HTCXu2z9fN62Q==", "dev": true, "license": "MIT", "dependencies": { "any-promise": "^1.0.0", "object-assign": "^4.0.1", "thenify-all": "^1.0.0" } }, "node_modules/nanoid": { "version": "3.3.11", "resolved": "https://registry.npmjs.org/nanoid/-/nanoid-3.3.11.tgz", "integrity": "sha512-N8SpfPUnUp1bK+PMYW8qSWdl9U+wwNWI4QKxOYDy9JAro3WMX7p2OeVRF9v+347pnakNevPmiHhNmZ2HbFA76w==", "dev": true, "funding": [ { "type": "github", "url": "https://github.com/sponsors/ai" } ], "license": "MIT", "bin": { "nanoid": "bin/nanoid.cjs" }, "engines": { "node": "^10 || ^12 || ^13.7 || ^14 || >=15.0.1" } }, "node_modules/natural-compare": { "version": "1.4.0", "resolved": "https://registry.npmjs.org/natural-compare/-/natural-compare-1.4.0.tgz", "integrity": "sha512-OWND8ei3VtNC9h7V60qff3SVobHr996CTwgxubgyQYEpg290h9J0buyECNNJexkFm5sOajh5G116RYA1c8ZMSw==", "dev": true, "license": "MIT" }, "node_modules/negotiator": { "version": "1.0.0", "resolved": "https://registry.npmjs.org/negotiator/-/negotiator-1.0.0.tgz", "integrity": "sha512-8Ofs/AUQh8MaEcrlq5xOX0CQ9ypTF5dl78mjlMNfOK08fzpgTHQRQPBxcPlEtIw0yRpws+Zo/3r+5WRby7u3Gg==", "dev": true, "license": "MIT", "engines": { "node": ">= 0.6" } }, "node_modules/node-releases": { "version": "2.0.19", "resolved": "https://registry.npmjs.org/node-releases/-/node-releases-2.0.19.tgz", "integrity": "sha512-xxOWJsBKtzAq7DY0J+DTzuz58K8e7sJbdgwkbMWQe8UYB6ekmsQ45q0M/tJDsGaZmbC+l7n57UV8Hl5tHxO9uw==", "dev": true, "license": "MIT" }, "node_modules/normalize-path": { "version": "3.0.0", "resolved": "https://registry.npmjs.org/normalize-path/-/normalize-path-3.0.0.tgz", "integrity": "sha512-6eZs5Ls3WtCisHWp9S2GUy8dqkpGi4BVSz3GaqiE6ezub0512ESztXUwUB6C6IKbQkY2Pnb/mD4WYojCRwcwLA==", "dev": true, "license": "MIT", "engines": { "node": ">=0.10.0" } }, "node_modules/normalize-range": { "version": "0.1.2", "resolved": "https://registry.npmjs.org/normalize-range/-/normalize-range-0.1.2.tgz", "integrity": "sha512-bdok/XvKII3nUpklnV6P2hxtMNrCboOjAcyBuQnWEhO665FwrSNRxU+AqpsyvO6LgGYPspN+lu5CLtw4jPRKNA==", "dev": true, "license": "MIT", "engines": { "node": ">=0.10.0" } }, "node_modules/object-assign": { "version": "4.1.1", "resolved": "https://registry.npmjs.org/object-assign/-/object-assign-4.1.1.tgz", "integrity": "sha512-rJgTQnkUnH1sFw8yT6VSU3zD3sWmu6sZhIseY8VX+GRu3P6F7Fu+JNDoXfklElbLJSnc3FUQHVe4cU5hj+BcUg==", "license": "MIT", "engines": { "node": ">=0.10.0" } }, "node_modules/object-hash": { "version": "3.0.0", "resolved": "https://registry.npmjs.org/object-hash/-/object-hash-3.0.0.tgz", "integrity": "sha512-RSn9F68PjH9HqtltsSnqYC1XXoWe9Bju5+213R98cNGttag9q9yAOTzdbsqvIa7aNm5WffBZFpWYr2aWrklWAw==", "dev": true, "license": "MIT", "engines": { "node": ">= 6" } }, "node_modules/object-inspect": { "version": "1.13.4", "resolved": "https://registry.npmjs.org/object-inspect/-/object-inspect-1.13.4.tgz", "integrity": "sha512-W67iLl4J2EXEGTbfeHCffrjDfitvLANg0UlX3wFUUSTx92KXRFegMHUVgSqE+wvhAbi4WqjGg9czysTV2Epbew==", "dev": true, "license": "MIT", "engines": { "node": ">= 0.4" }, "funding": { "url": "https://github.com/sponsors/ljharb" } }, "node_modules/on-finished": { "version": "2.4.1", "resolved": "https://registry.npmjs.org/on-finished/-/on-finished-2.4.1.tgz", "integrity": "sha512-oVlzkg3ENAhCk2zdv7IJwd/QUD4z2RxRwpkcGY8psCVcCYZNq4wYnVWALHM+brtuJjePWiYF/ClmuDr8Ch5+kg==", "dev": true, "license": "MIT", "dependencies": { "ee-first": "1.1.1" }, "engines": { "node": ">= 0.8" } }, "node_modules/once": { "version": "1.4.0", "resolved": "https://registry.npmjs.org/once/-/once-1.4.0.tgz", "integrity": "sha512-lNaJgI+2Q5URQBkccEKHTQOPaXdUxnZZElQTZY0MFUAuaEqe1E+Nyvgdz/aIyNi6Z9MzO5dv1H8n58/GELp3+w==", "dev": true, "license": "ISC", "dependencies": { "wrappy": "1" } }, "node_modules/optionator": { "version": "0.9.4", "resolved": "https://registry.npmjs.org/optionator/-/optionator-0.9.4.tgz", "integrity": "sha512-6IpQ7mKUxRcZNLIObR0hz7lxsapSSIYNZJwXPGeF0mTVqGKFIXj1DQcMoT22S3ROcLyY/rz0PWaWZ9ayWmad9g==", "dev": true, "license": "MIT", "dependencies": { "deep-is": "^0.1.3", "fast-levenshtein": "^2.0.6", "levn": "^0.4.1", "prelude-ls": "^1.2.1", "type-check": "^0.4.0", "word-wrap": "^1.2.5" }, "engines": { "node": ">= 0.8.0" } }, "node_modules/p-limit": { "version": "3.1.0", "resolved": "https://registry.npmjs.org/p-limit/-/p-limit-3.1.0.tgz", "integrity": "sha512-TYOanM3wGwNGsZN2cVTYPArw454xnXj5qmWF1bEoAc4+cU/ol7GVh7odevjp1FNHduHc3KZMcFduxU5Xc6uJRQ==", "dev": true, "license": "MIT", "dependencies": { "yocto-queue": "^0.1.0" }, "engines": { "node": ">=10" }, "funding": { "url": "https://github.com/sponsors/sindresorhus" } }, "node_modules/p-locate": { "version": "5.0.0", "resolved": "https://registry.npmjs.org/p-locate/-/p-locate-5.0.0.tgz", "integrity": "sha512-LaNjtRWUBY++zB5nE/NwcaoMylSPk+S+ZHNB1TzdbMJMny6dynpAGt7X/tl/QYq3TIeE6nxHppbo2LGymrG5Pw==", "dev": true, "license": "MIT", "dependencies": { "p-limit": "^3.0.2" }, "engines": { "node": ">=10" }, "funding": { "url": "https://github.com/sponsors/sindresorhus" } }, "node_modules/package-json-from-dist": { "version": "1.0.1", "resolved": "https://registry.npmjs.org/package-json-from-dist/-/package-json-from-dist-1.0.1.tgz", "integrity": "sha512-UEZIS3/by4OC8vL3P2dTXRETpebLI2NiI5vIrjaD/5UtrkFX/tNbwjTSRAGC/+7CAo2pIcBaRgWmcBBHcsaCIw==", "dev": true, "license": "BlueOak-1.0.0" }, "node_modules/parent-module": { "version": "1.0.1", "resolved": "https://registry.npmjs.org/parent-module/-/parent-module-1.0.1.tgz", "integrity": "sha512-GQ2EWRpQV8/o+Aw8YqtfZZPfNRWZYkbidE9k5rpl/hC3vtHHBfGm2Ifi6qWV+coDGkrUKZAxE3Lot5kcsRlh+g==", "license": "MIT", "dependencies": { "callsites": "^3.0.0" }, "engines": { "node": ">=6" } }, "node_modules/parse-json": { "version": "5.2.0", "resolved": "https://registry.npmjs.org/parse-json/-/parse-json-5.2.0.tgz", "integrity": "sha512-ayCKvm/phCGxOkYRSCM82iDwct8/EonSEgCSxWxD7ve6jHggsFl4fZVQBPRNgQoKiuV/odhFrGzQXZwbifC8Rg==", "license": "MIT", "dependencies": { "@babel/code-frame": "^7.0.0", "error-ex": "^1.3.1", "json-parse-even-better-errors": "^2.3.0", "lines-and-columns": "^1.1.6" }, "engines": { "node": ">=8" }, "funding": { "url": "https://github.com/sponsors/sindresorhus" } }, "node_modules/parseurl": { "version": "1.3.3", "resolved": "https://registry.npmjs.org/parseurl/-/parseurl-1.3.3.tgz", "integrity": "sha512-CiyeOxFT/JZyN5m0z9PfXw4SCBJ6Sygz1Dpl0wqjlhDEGGBP1GnsUVEL0p63hoG1fcj3fHynXi9NYO4nWOL+qQ==", "dev": true, "license": "MIT", "engines": { "node": ">= 0.8" } }, "node_modules/path-exists": { "version": "4.0.0", "resolved": "https://registry.npmjs.org/path-exists/-/path-exists-4.0.0.tgz", "integrity": "sha512-ak9Qy5Q7jYb2Wwcey5Fpvg2KoAc/ZIhLSLOSBmRmygPsGwkVVt0fZa0qrtMz+m6tJTAHfZQ8FnmB4MG4LWy7/w==", "dev": true, "license": "MIT", "engines": { "node": ">=8" } }, "node_modules/path-key": { "version": "3.1.1", "resolved": "https://registry.npmjs.org/path-key/-/path-key-3.1.1.tgz", "integrity": "sha512-ojmeN0qd+y0jszEtoY48r0Peq5dwMEkIlCOu6Q5f41lfkswXuKtYrhgoTpLnyIcHm24Uhqx+5Tqm2InSwLhE6Q==", "dev": true, "license": "MIT", "engines": { "node": ">=8" } }, "node_modules/path-parse": { "version": "1.0.7", "resolved": "https://registry.npmjs.org/path-parse/-/path-parse-1.0.7.tgz", "integrity": "sha512-LDJzPVEEEPR+y48z93A0Ed0yXb8pAByGWo/k5YYdYgpY2/2EsOsksJrq7lOHxryrVOn1ejG6oAp8ahvOIQD8sw==", "license": "MIT" }, "node_modules/path-scurry": { "version": "1.11.1", "resolved": "https://registry.npmjs.org/path-scurry/-/path-scurry-1.11.1.tgz", "integrity": "sha512-Xa4Nw17FS9ApQFJ9umLiJS4orGjm7ZzwUrwamcGQuHSzDyth9boKDaycYdDcZDuqYATXw4HFXgaqWTctW/v1HA==", "dev": true, "license": "BlueOak-1.0.0", "dependencies": { "lru-cache": "^10.2.0", "minipass": "^5.0.0 || ^6.0.2 || ^7.0.0" }, "engines": { "node": ">=16 || 14 >=14.18" }, "funding": { "url": "https://github.com/sponsors/isaacs" } }, "node_modules/path-scurry/node_modules/lru-cache": { "version": "10.4.3", "resolved": "https://registry.npmjs.org/lru-cache/-/lru-cache-10.4.3.tgz", "integrity": "sha512-JNAzZcXrCt42VGLuYz0zfAzDfAvJWW6AfYlDBQyDV5DClI2m5sAmK+OIO7s59XfsRsWHp02jAJrRadPRGTt6SQ==", "dev": true, "license": "ISC" }, "node_modules/path-to-regexp": { "version": "8.2.0", "resolved": "https://registry.npmjs.org/path-to-regexp/-/path-to-regexp-8.2.0.tgz", "integrity": "sha512-TdrF7fW9Rphjq4RjrW0Kp2AW0Ahwu9sRGTkS6bvDi0SCwZlEZYmcfDbEsTz8RVk0EHIS/Vd1bv3JhG+1xZuAyQ==", "dev": true, "license": "MIT", "engines": { "node": ">=16" } }, "node_modules/path-type": { "version": "4.0.0", "resolved": "https://registry.npmjs.org/path-type/-/path-type-4.0.0.tgz", "integrity": "sha512-gDKb8aZMDeD/tZWs9P6+q0J9Mwkdl6xMV8TjnGP3qJVJ06bdMgkbBlLU8IdfOsIsFz2BW1rNVT3XuNEl8zPAvw==", "license": "MIT", "engines": { "node": ">=8" } }, "node_modules/picocolors": { "version": "1.1.1", "resolved": "https://registry.npmjs.org/picocolors/-/picocolors-1.1.1.tgz", "integrity": "sha512-xceH2snhtb5M9liqDsmEw56le376mTZkEX/jEb/RxNFyegNul7eNslCXP9FDj/Lcu0X8KEyMceP2ntpaHrDEVA==", "license": "ISC" }, "node_modules/picomatch": { "version": "2.3.1", "resolved": "https://registry.npmjs.org/picomatch/-/picomatch-2.3.1.tgz", "integrity": "sha512-JU3teHTNjmE2VCGFzuY8EXzCDVwEqB2a8fsIvwaStHhAWJEeVd1o1QD80CU6+ZdEXXSLbSsuLwJjkCBWqRQUVA==", "dev": true, "license": "MIT", "engines": { "node": ">=8.6" }, "funding": { "url": "https://github.com/sponsors/jonschlinkert" } }, "node_modules/pify": { "version": "2.3.0", "resolved": "https://registry.npmjs.org/pify/-/pify-2.3.0.tgz", "integrity": "sha512-udgsAY+fTnvv7kI7aaxbqwWNb0AHiB0qBO89PZKPkoTmGOgdbrHDKD+0B2X4uTfJ/FT1R09r9gTsjUjNJotuog==", "dev": true, "license": "MIT", "engines": { "node": ">=0.10.0" } }, "node_modules/pirates": { "version": "4.0.7", "resolved": "https://registry.npmjs.org/pirates/-/pirates-4.0.7.tgz", "integrity": "sha512-TfySrs/5nm8fQJDcBDuUng3VOUKsd7S+zqvbOTiGXHfxX4wK31ard+hoNuvkicM/2YFzlpDgABOevKSsB4G/FA==", "dev": true, "license": "MIT", "engines": { "node": ">= 6" } }, "node_modules/pkce-challenge": { "version": "5.0.0", "resolved": "https://registry.npmjs.org/pkce-challenge/-/pkce-challenge-5.0.0.tgz", "integrity": "sha512-ueGLflrrnvwB3xuo/uGob5pd5FN7l0MsLf0Z87o/UQmRtwjvfylfc9MurIxRAWywCYTgrvpXBcqjV4OfCYGCIQ==", "dev": true, "license": "MIT", "engines": { "node": ">=16.20.0" } }, "node_modules/postcss": { "version": "8.5.3", "resolved": "https://registry.npmjs.org/postcss/-/postcss-8.5.3.tgz", "integrity": "sha512-dle9A3yYxlBSrt8Fu+IpjGT8SY8hN0mlaA6GY8t0P5PjIOZemULz/E2Bnm/2dcUOena75OTNkHI76uZBNUUq3A==", "dev": true, "funding": [ { "type": "opencollective", "url": "https://opencollective.com/postcss/" }, { "type": "tidelift", "url": "https://tidelift.com/funding/github/npm/postcss" }, { "type": "github", "url": "https://github.com/sponsors/ai" } ], "license": "MIT", "dependencies": { "nanoid": "^3.3.8", "picocolors": "^1.1.1", "source-map-js": "^1.2.1" }, "engines": { "node": "^10 || ^12 || >=14" } }, "node_modules/postcss-import": { "version": "14.1.0", "resolved": "https://registry.npmjs.org/postcss-import/-/postcss-import-14.1.0.tgz", "integrity": "sha512-flwI+Vgm4SElObFVPpTIT7SU7R3qk2L7PyduMcokiaVKuWv9d/U+Gm/QAd8NDLuykTWTkcrjOeD2Pp1rMeBTGw==", "dev": true, "license": "MIT", "dependencies": { "postcss-value-parser": "^4.0.0", "read-cache": "^1.0.0", "resolve": "^1.1.7" }, "engines": { "node": ">=10.0.0" }, "peerDependencies": { "postcss": "^8.0.0" } }, "node_modules/postcss-js": { "version": "4.0.1", "resolved": "https://registry.npmjs.org/postcss-js/-/postcss-js-4.0.1.tgz", "integrity": "sha512-dDLF8pEO191hJMtlHFPRa8xsizHaM82MLfNkUHdUtVEV3tgTp5oj+8qbEqYM57SLfc74KSbw//4SeJma2LRVIw==", "dev": true, "license": "MIT", "dependencies": { "camelcase-css": "^2.0.1" }, "engines": { "node": "^12 || ^14 || >= 16" }, "funding": { "type": "opencollective", "url": "https://opencollective.com/postcss/" }, "peerDependencies": { "postcss": "^8.4.21" } }, "node_modules/postcss-load-config": { "version": "3.1.4", "resolved": "https://registry.npmjs.org/postcss-load-config/-/postcss-load-config-3.1.4.tgz", "integrity": "sha512-6DiM4E7v4coTE4uzA8U//WhtPwyhiim3eyjEMFCnUpzbrkK9wJHgKDT2mR+HbtSrd/NubVaYTOpSpjUl8NQeRg==", "dev": true, "license": "MIT", "dependencies": { "lilconfig": "^2.0.5", "yaml": "^1.10.2" }, "engines": { "node": ">= 10" }, "funding": { "type": "opencollective", "url": "https://opencollective.com/postcss/" }, "peerDependencies": { "postcss": ">=8.0.9", "ts-node": ">=9.0.0" }, "peerDependenciesMeta": { "postcss": { "optional": true }, "ts-node": { "optional": true } } }, "node_modules/postcss-load-config/node_modules/yaml": { "version": "1.10.2", "resolved": "https://registry.npmjs.org/yaml/-/yaml-1.10.2.tgz", "integrity": "sha512-r3vXyErRCYJ7wg28yvBY5VSoAF8ZvlcW9/BwUzEtUsjvX/DKs24dIkuwjtuprwJJHsbyUbLApepYTR1BN4uHrg==", "dev": true, "license": "ISC", "engines": { "node": ">= 6" } }, "node_modules/postcss-nested": { "version": "6.0.0", "resolved": "https://registry.npmjs.org/postcss-nested/-/postcss-nested-6.0.0.tgz", "integrity": "sha512-0DkamqrPcmkBDsLn+vQDIrtkSbNkv5AD/M322ySo9kqFkCIYklym2xEmWkwo+Y3/qZo34tzEPNUw4y7yMCdv5w==", "dev": true, "license": "MIT", "dependencies": { "postcss-selector-parser": "^6.0.10" }, "engines": { "node": ">=12.0" }, "funding": { "type": "opencollective", "url": "https://opencollective.com/postcss/" }, "peerDependencies": { "postcss": "^8.2.14" } }, "node_modules/postcss-selector-parser": { "version": "6.1.2", "resolved": "https://registry.npmjs.org/postcss-selector-parser/-/postcss-selector-parser-6.1.2.tgz", "integrity": "sha512-Q8qQfPiZ+THO/3ZrOrO0cJJKfpYCagtMUkXbnEfmgUjwXg6z/WBeOyS9APBBPCTSiDV+s4SwQGu8yFsiMRIudg==", "dev": true, "license": "MIT", "dependencies": { "cssesc": "^3.0.0", "util-deprecate": "^1.0.2" }, "engines": { "node": ">=4" } }, "node_modules/postcss-value-parser": { "version": "4.2.0", "resolved": "https://registry.npmjs.org/postcss-value-parser/-/postcss-value-parser-4.2.0.tgz", "integrity": "sha512-1NNCs6uurfkVbeXG4S8JFT9t19m45ICnif8zWLd5oPSZ50QnwMfK+H3jv408d4jw/7Bttv5axS5IiHoLaVNHeQ==", "dev": true, "license": "MIT" }, "node_modules/prelude-ls": { "version": "1.2.1", "resolved": "https://registry.npmjs.org/prelude-ls/-/prelude-ls-1.2.1.tgz", "integrity": "sha512-vkcDPrRZo1QZLbn5RLGPpg/WmIQ65qoWWhcGKf/b5eplkkarX0m9z8ppCat4mlOqUsWpyNuYgO3VRyrYHSzX5g==", "dev": true, "license": "MIT", "engines": { "node": ">= 0.8.0" } }, "node_modules/prop-types": { "version": "15.8.1", "resolved": "https://registry.npmjs.org/prop-types/-/prop-types-15.8.1.tgz", "integrity": "sha512-oj87CgZICdulUohogVAR7AjlC0327U4el4L6eAvOqCeudMDVU0NThNaV+b9Df4dXgSP1gXMTnPdhfe/2qDH5cg==", "license": "MIT", "dependencies": { "loose-envify": "^1.4.0", "object-assign": "^4.1.1", "react-is": "^16.13.1" } }, "node_modules/proxy-addr": { "version": "2.0.7", "resolved": "https://registry.npmjs.org/proxy-addr/-/proxy-addr-2.0.7.tgz", "integrity": "sha512-llQsMLSUDUPT44jdrU/O37qlnifitDP+ZwrmmZcoSKyLKvtZxpyV0n2/bD/N4tBAAZ/gJEdZU7KMraoK1+XYAg==", "dev": true, "license": "MIT", "dependencies": { "forwarded": "0.2.0", "ipaddr.js": "1.9.1" }, "engines": { "node": ">= 0.10" } }, "node_modules/proxy-from-env": { "version": "1.1.0", "resolved": "https://registry.npmjs.org/proxy-from-env/-/proxy-from-env-1.1.0.tgz", "integrity": "sha512-D+zkORCbA9f1tdWRK0RaCR3GPv50cMxcrz4X8k5LTSUD1Dkw47mKJEZQNunItRTkWwgtaUSo1RVFRIG9ZXiFYg==", "license": "MIT" }, "node_modules/punycode": { "version": "2.3.1", "resolved": "https://registry.npmjs.org/punycode/-/punycode-2.3.1.tgz", "integrity": "sha512-vYt7UD1U9Wg6138shLtLOvdAu+8DsC/ilFtEVHcH+wydcSpNE20AfSOduf6MkRFahL5FY7X1oU7nKVZFtfq8Fg==", "dev": true, "license": "MIT", "engines": { "node": ">=6" } }, "node_modules/qs": { "version": "6.14.0", "resolved": "https://registry.npmjs.org/qs/-/qs-6.14.0.tgz", "integrity": "sha512-YWWTjgABSKcvs/nWBi9PycY/JiPJqOD4JA6o9Sej2AtvSGarXxKC3OQSk4pAarbdQlKAh5D4FCQkJNkW+GAn3w==", "dev": true, "license": "BSD-3-Clause", "dependencies": { "side-channel": "^1.1.0" }, "engines": { "node": ">=0.6" }, "funding": { "url": "https://github.com/sponsors/ljharb" } }, "node_modules/queue-microtask": { "version": "1.2.3", "resolved": "https://registry.npmjs.org/queue-microtask/-/queue-microtask-1.2.3.tgz", "integrity": "sha512-NuaNSa6flKT5JaSYQzJok04JzTL1CA6aGhv5rfLW3PgqA+M2ChpZQnAC8h8i4ZFkBS8X5RqkDBHA7r4hej3K9A==", "dev": true, "funding": [ { "type": "github", "url": "https://github.com/sponsors/feross" }, { "type": "patreon", "url": "https://www.patreon.com/feross" }, { "type": "consulting", "url": "https://feross.org/support" } ], "license": "MIT" }, "node_modules/quick-lru": { "version": "5.1.1", "resolved": "https://registry.npmjs.org/quick-lru/-/quick-lru-5.1.1.tgz", "integrity": "sha512-WuyALRjWPDGtt/wzJiadO5AXY+8hZ80hVpe6MyivgraREW751X3SbhRvG3eLKOYN+8VEvqLcf3wdnt44Z4S4SA==", "dev": true, "license": "MIT", "engines": { "node": ">=10" }, "funding": { "url": "https://github.com/sponsors/sindresorhus" } }, "node_modules/range-parser": { "version": "1.2.1", "resolved": "https://registry.npmjs.org/range-parser/-/range-parser-1.2.1.tgz", "integrity": "sha512-Hrgsx+orqoygnmhFbKaHE6c296J+HTAQXoxEF6gNupROmmGJRoyzfG3ccAveqCBrwr/2yxQ5BVd/GTl5agOwSg==", "dev": true, "license": "MIT", "engines": { "node": ">= 0.6" } }, "node_modules/raw-body": { "version": "3.0.0", "resolved": "https://registry.npmjs.org/raw-body/-/raw-body-3.0.0.tgz", "integrity": "sha512-RmkhL8CAyCRPXCE28MMH0z2PNWQBNk2Q09ZdxM9IOOXwxwZbN+qbWaatPkdkWIKL2ZVDImrN/pK5HTRz2PcS4g==", "dev": true, "license": "MIT", "dependencies": { "bytes": "3.1.2", "http-errors": "2.0.0", "iconv-lite": "0.6.3", "unpipe": "1.0.0" }, "engines": { "node": ">= 0.8" } }, "node_modules/react": { "version": "19.1.0", "resolved": "https://registry.npmjs.org/react/-/react-19.1.0.tgz", "integrity": "sha512-FS+XFBNvn3GTAWq26joslQgWNoFu08F4kl0J4CgdNKADkdSGXQyTCnKteIAJy96Br6YbpEU1LSzV5dYtjMkMDg==", "license": "MIT", "engines": { "node": ">=0.10.0" } }, "node_modules/react-dom": { "version": "19.1.0", "resolved": "https://registry.npmjs.org/react-dom/-/react-dom-19.1.0.tgz", "integrity": "sha512-Xs1hdnE+DyKgeHJeJznQmYMIBG3TKIHJJT95Q58nHLSrElKlGQqDTR2HQ9fx5CN/Gk6Vh/kupBTDLU11/nDk/g==", "license": "MIT", "dependencies": { "scheduler": "^0.26.0" }, "peerDependencies": { "react": "^19.1.0" } }, "node_modules/react-is": { "version": "16.13.1", "resolved": "https://registry.npmjs.org/react-is/-/react-is-16.13.1.tgz", "integrity": "sha512-24e6ynE2H+OKt4kqsOvNd8kBpV65zoxbA4BVsEOB3ARVWQki/DHzaUoC5KuON/BiccDaCCTZBuOcfZs70kR8bQ==", "license": "MIT" }, "node_modules/react-refresh": { "version": "0.17.0", "resolved": "https://registry.npmjs.org/react-refresh/-/react-refresh-0.17.0.tgz", "integrity": "sha512-z6F7K9bV85EfseRCp2bzrpyQ0Gkw1uLoCel9XBVWPg/TjRj94SkJzUTGfOa4bs7iJvBWtQG0Wq7wnI0syw3EBQ==", "dev": true, "license": "MIT", "engines": { "node": ">=0.10.0" } }, "node_modules/react-router": { "version": "7.5.3", "resolved": "https://registry.npmjs.org/react-router/-/react-router-7.5.3.tgz", "integrity": "sha512-3iUDM4/fZCQ89SXlDa+Ph3MevBrozBAI655OAfWQlTm9nBR0IKlrmNwFow5lPHttbwvITZfkeeeZFP6zt3F7pw==", "license": "MIT", "dependencies": { "cookie": "^1.0.1", "set-cookie-parser": "^2.6.0", "turbo-stream": "2.4.0" }, "engines": { "node": ">=20.0.0" }, "peerDependencies": { "react": ">=18", "react-dom": ">=18" }, "peerDependenciesMeta": { "react-dom": { "optional": true } } }, "node_modules/react-router-dom": { "version": "7.5.3", "resolved": "https://registry.npmjs.org/react-router-dom/-/react-router-dom-7.5.3.tgz", "integrity": "sha512-cK0jSaTyW4jV9SRKAItMIQfWZ/D6WEZafgHuuCb9g+SjhLolY78qc+De4w/Cz9ybjvLzShAmaIMEXt8iF1Cm+A==", "license": "MIT", "dependencies": { "react-router": "7.5.3" }, "engines": { "node": ">=20.0.0" }, "peerDependencies": { "react": ">=18", "react-dom": ">=18" } }, "node_modules/react-router/node_modules/cookie": { "version": "1.0.2", "resolved": "https://registry.npmjs.org/cookie/-/cookie-1.0.2.tgz", "integrity": "sha512-9Kr/j4O16ISv8zBBhJoi4bXOYNTkFLOqSL3UDB0njXxCXNezjeyVrJyGOWtgfs/q2km1gwBcfH8q1yEGoMYunA==", "license": "MIT", "engines": { "node": ">=18" } }, "node_modules/react-select": { "version": "5.10.1", "resolved": "https://registry.npmjs.org/react-select/-/react-select-5.10.1.tgz", "integrity": "sha512-roPEZUL4aRZDx6DcsD+ZNreVl+fM8VsKn0Wtex1v4IazH60ILp5xhdlp464IsEAlJdXeD+BhDAFsBVMfvLQueA==", "license": "MIT", "dependencies": { "@babel/runtime": "^7.12.0", "@emotion/cache": "^11.4.0", "@emotion/react": "^11.8.1", "@floating-ui/dom": "^1.0.1", "@types/react-transition-group": "^4.4.0", "memoize-one": "^6.0.0", "prop-types": "^15.6.0", "react-transition-group": "^4.3.0", "use-isomorphic-layout-effect": "^1.2.0" }, "peerDependencies": { "react": "^16.8.0 || ^17.0.0 || ^18.0.0 || ^19.0.0", "react-dom": "^16.8.0 || ^17.0.0 || ^18.0.0 || ^19.0.0" } }, "node_modules/react-smooth": { "version": "4.0.4", "resolved": "https://registry.npmjs.org/react-smooth/-/react-smooth-4.0.4.tgz", "integrity": "sha512-gnGKTpYwqL0Iii09gHobNolvX4Kiq4PKx6eWBCYYix+8cdw+cGo3do906l1NBPKkSWx1DghC1dlWG9L2uGd61Q==", "license": "MIT", "dependencies": { "fast-equals": "^5.0.1", "prop-types": "^15.8.1", "react-transition-group": "^4.4.5" }, "peerDependencies": { "react": "^16.8.0 || ^17.0.0 || ^18.0.0 || ^19.0.0", "react-dom": "^16.8.0 || ^17.0.0 || ^18.0.0 || ^19.0.0" } }, "node_modules/react-transition-group": { "version": "4.4.5", "resolved": "https://registry.npmjs.org/react-transition-group/-/react-transition-group-4.4.5.tgz", "integrity": "sha512-pZcd1MCJoiKiBR2NRxeCRg13uCXbydPnmB4EOeRrY7480qNWO8IIgQG6zlDkm6uRMsURXPuKq0GWtiM59a5Q6g==", "license": "BSD-3-Clause", "dependencies": { "@babel/runtime": "^7.5.5", "dom-helpers": "^5.0.1", "loose-envify": "^1.4.0", "prop-types": "^15.6.2" }, "peerDependencies": { "react": ">=16.6.0", "react-dom": ">=16.6.0" } }, "node_modules/read-cache": { "version": "1.0.0", "resolved": "https://registry.npmjs.org/read-cache/-/read-cache-1.0.0.tgz", "integrity": "sha512-Owdv/Ft7IjOgm/i0xvNDZ1LrRANRfew4b2prF3OWMQLxLfu3bS8FVhCsrSCMK4lR56Y9ya+AThoTpDCTxCmpRA==", "dev": true, "license": "MIT", "dependencies": { "pify": "^2.3.0" } }, "node_modules/readdirp": { "version": "3.6.0", "resolved": "https://registry.npmjs.org/readdirp/-/readdirp-3.6.0.tgz", "integrity": "sha512-hOS089on8RduqdbhvQ5Z37A0ESjsqz6qnRcffsMU3495FuTdqSm+7bhJ29JvIOsBDEEnan5DPu9t3To9VRlMzA==", "dev": true, "license": "MIT", "dependencies": { "picomatch": "^2.2.1" }, "engines": { "node": ">=8.10.0" } }, "node_modules/recharts": { "version": "2.15.3", "resolved": "https://registry.npmjs.org/recharts/-/recharts-2.15.3.tgz", "integrity": "sha512-EdOPzTwcFSuqtvkDoaM5ws/Km1+WTAO2eizL7rqiG0V2UVhTnz0m7J2i0CjVPUCdEkZImaWvXLbZDS2H5t6GFQ==", "license": "MIT", "dependencies": { "clsx": "^2.0.0", "eventemitter3": "^4.0.1", "lodash": "^4.17.21", "react-is": "^18.3.1", "react-smooth": "^4.0.4", "recharts-scale": "^0.4.4", "tiny-invariant": "^1.3.1", "victory-vendor": "^36.6.8" }, "engines": { "node": ">=14" }, "peerDependencies": { "react": "^16.0.0 || ^17.0.0 || ^18.0.0 || ^19.0.0", "react-dom": "^16.0.0 || ^17.0.0 || ^18.0.0 || ^19.0.0" } }, "node_modules/recharts-scale": { "version": "0.4.5", "resolved": "https://registry.npmjs.org/recharts-scale/-/recharts-scale-0.4.5.tgz", "integrity": "sha512-kivNFO+0OcUNu7jQquLXAxz1FIwZj8nrj+YkOKc5694NbjCvcT6aSZiIzNzd2Kul4o4rTto8QVR9lMNtxD4G1w==", "license": "MIT", "dependencies": { "decimal.js-light": "^2.4.1" } }, "node_modules/recharts/node_modules/react-is": { "version": "18.3.1", "resolved": "https://registry.npmjs.org/react-is/-/react-is-18.3.1.tgz", "integrity": "sha512-/LLMVyas0ljjAtoYiPqYiL8VWXzUUdThrmU5+n20DZv+a+ClRoevUzw5JxU+Ieh5/c87ytoTBV9G1FiKfNJdmg==", "license": "MIT" }, "node_modules/resolve": { "version": "1.22.10", "resolved": "https://registry.npmjs.org/resolve/-/resolve-1.22.10.tgz", "integrity": "sha512-NPRy+/ncIMeDlTAsuqwKIiferiawhefFJtkNSW0qZJEqMEb+qBt/77B/jGeeek+F0uOeN05CDa6HXbbIgtVX4w==", "license": "MIT", "dependencies": { "is-core-module": "^2.16.0", "path-parse": "^1.0.7", "supports-preserve-symlinks-flag": "^1.0.0" }, "bin": { "resolve": "bin/resolve" }, "engines": { "node": ">= 0.4" }, "funding": { "url": "https://github.com/sponsors/ljharb" } }, "node_modules/resolve-from": { "version": "4.0.0", "resolved": "https://registry.npmjs.org/resolve-from/-/resolve-from-4.0.0.tgz", "integrity": "sha512-pb/MYmXstAkysRFx8piNI1tGFNQIFA3vkE3Gq4EuA1dF6gHp/+vgZqsCGJapvy8N3Q+4o7FwvquPJcnZ7RYy4g==", "license": "MIT", "engines": { "node": ">=4" } }, "node_modules/reusify": { "version": "1.1.0", "resolved": "https://registry.npmjs.org/reusify/-/reusify-1.1.0.tgz", "integrity": "sha512-g6QUff04oZpHs0eG5p83rFLhHeV00ug/Yf9nZM6fLeUrPguBTkTQOdpAWWspMh55TZfVQDPaN3NQJfbVRAxdIw==", "dev": true, "license": "MIT", "engines": { "iojs": ">=1.0.0", "node": ">=0.10.0" } }, "node_modules/rollup": { "version": "4.40.1", "resolved": "https://registry.npmjs.org/rollup/-/rollup-4.40.1.tgz", "integrity": "sha512-C5VvvgCCyfyotVITIAv+4efVytl5F7wt+/I2i9q9GZcEXW9BP52YYOXC58igUi+LFZVHukErIIqQSWwv/M3WRw==", "dev": true, "license": "MIT", "dependencies": { "@types/estree": "1.0.7" }, "bin": { "rollup": "dist/bin/rollup" }, "engines": { "node": ">=18.0.0", "npm": ">=8.0.0" }, "optionalDependencies": { "@rollup/rollup-android-arm-eabi": "4.40.1", "@rollup/rollup-android-arm64": "4.40.1", "@rollup/rollup-darwin-arm64": "4.40.1", "@rollup/rollup-darwin-x64": "4.40.1", "@rollup/rollup-freebsd-arm64": "4.40.1", "@rollup/rollup-freebsd-x64": "4.40.1", "@rollup/rollup-linux-arm-gnueabihf": "4.40.1", "@rollup/rollup-linux-arm-musleabihf": "4.40.1", "@rollup/rollup-linux-arm64-gnu": "4.40.1", "@rollup/rollup-linux-arm64-musl": "4.40.1", "@rollup/rollup-linux-loongarch64-gnu": "4.40.1", "@rollup/rollup-linux-powerpc64le-gnu": "4.40.1", "@rollup/rollup-linux-riscv64-gnu": "4.40.1", "@rollup/rollup-linux-riscv64-musl": "4.40.1", "@rollup/rollup-linux-s390x-gnu": "4.40.1", "@rollup/rollup-linux-x64-gnu": "4.40.1", "@rollup/rollup-linux-x64-musl": "4.40.1", "@rollup/rollup-win32-arm64-msvc": "4.40.1", "@rollup/rollup-win32-ia32-msvc": "4.40.1", "@rollup/rollup-win32-x64-msvc": "4.40.1", "fsevents": "~2.3.2" } }, "node_modules/router": { "version": "2.2.0", "resolved": "https://registry.npmjs.org/router/-/router-2.2.0.tgz", "integrity": "sha512-nLTrUKm2UyiL7rlhapu/Zl45FwNgkZGaCpZbIHajDYgwlJCOzLSk+cIPAnsEqV955GjILJnKbdQC1nVPz+gAYQ==", "dev": true, "license": "MIT", "dependencies": { "debug": "^4.4.0", "depd": "^2.0.0", "is-promise": "^4.0.0", "parseurl": "^1.3.3", "path-to-regexp": "^8.0.0" }, "engines": { "node": ">= 18" } }, "node_modules/run-parallel": { "version": "1.2.0", "resolved": "https://registry.npmjs.org/run-parallel/-/run-parallel-1.2.0.tgz", "integrity": "sha512-5l4VyZR86LZ/lDxZTR6jqL8AFE2S0IFLMP26AbjsLVADxHdhB/c0GUsH+y39UfCi3dzz8OlQuPmnaJOMoDHQBA==", "dev": true, "funding": [ { "type": "github", "url": "https://github.com/sponsors/feross" }, { "type": "patreon", "url": "https://www.patreon.com/feross" }, { "type": "consulting", "url": "https://feross.org/support" } ], "license": "MIT", "dependencies": { "queue-microtask": "^1.2.2" } }, "node_modules/safe-buffer": { "version": "5.2.1", "resolved": "https://registry.npmjs.org/safe-buffer/-/safe-buffer-5.2.1.tgz", "integrity": "sha512-rp3So07KcdmmKbGvgaNxQSJr7bGVSVk5S9Eq1F+ppbRo70+YeaDxkw5Dd8NPN+GD6bjnYm2VuPuCXmpuYvmCXQ==", "dev": true, "funding": [ { "type": "github", "url": "https://github.com/sponsors/feross" }, { "type": "patreon", "url": "https://www.patreon.com/feross" }, { "type": "consulting", "url": "https://feross.org/support" } ], "license": "MIT" }, "node_modules/safer-buffer": { "version": "2.1.2", "resolved": "https://registry.npmjs.org/safer-buffer/-/safer-buffer-2.1.2.tgz", "integrity": "sha512-YZo3K82SD7Riyi0E1EQPojLz7kpepnSQI9IyPbHHg1XXXevb5dJI7tpyN2ADxGcQbHG7vcyRHk0cbwqcQriUtg==", "dev": true, "license": "MIT" }, "node_modules/scheduler": { "version": "0.26.0", "resolved": "https://registry.npmjs.org/scheduler/-/scheduler-0.26.0.tgz", "integrity": "sha512-NlHwttCI/l5gCPR3D1nNXtWABUmBwvZpEQiD4IXSbIDq8BzLIK/7Ir5gTFSGZDUu37K5cMNp0hFtzO38sC7gWA==", "license": "MIT" }, "node_modules/semver": { "version": "6.3.1", "resolved": "https://registry.npmjs.org/semver/-/semver-6.3.1.tgz", "integrity": "sha512-BR7VvDCVHO+q2xBEWskxS6DJE1qRnb7DxzUrogb71CWoSficBxYsiAGd+Kl0mmq/MprG9yArRkyrQxTO6XjMzA==", "dev": true, "license": "ISC", "bin": { "semver": "bin/semver.js" } }, "node_modules/send": { "version": "1.2.0", "resolved": "https://registry.npmjs.org/send/-/send-1.2.0.tgz", "integrity": "sha512-uaW0WwXKpL9blXE2o0bRhoL2EGXIrZxQ2ZQ4mgcfoBxdFmQold+qWsD2jLrfZ0trjKL6vOw0j//eAwcALFjKSw==", "dev": true, "license": "MIT", "dependencies": { "debug": "^4.3.5", "encodeurl": "^2.0.0", "escape-html": "^1.0.3", "etag": "^1.8.1", "fresh": "^2.0.0", "http-errors": "^2.0.0", "mime-types": "^3.0.1", "ms": "^2.1.3", "on-finished": "^2.4.1", "range-parser": "^1.2.1", "statuses": "^2.0.1" }, "engines": { "node": ">= 18" } }, "node_modules/serve-static": { "version": "2.2.0", "resolved": "https://registry.npmjs.org/serve-static/-/serve-static-2.2.0.tgz", "integrity": "sha512-61g9pCh0Vnh7IutZjtLGGpTA355+OPn2TyDv/6ivP2h/AdAVX9azsoxmg2/M6nZeQZNYBEwIcsne1mJd9oQItQ==", "dev": true, "license": "MIT", "dependencies": { "encodeurl": "^2.0.0", "escape-html": "^1.0.3", "parseurl": "^1.3.3", "send": "^1.2.0" }, "engines": { "node": ">= 18" } }, "node_modules/set-cookie-parser": { "version": "2.7.1", "resolved": "https://registry.npmjs.org/set-cookie-parser/-/set-cookie-parser-2.7.1.tgz", "integrity": "sha512-IOc8uWeOZgnb3ptbCURJWNjWUPcO3ZnTTdzsurqERrP6nPyv+paC55vJM0LpOlT2ne+Ix+9+CRG1MNLlyZ4GjQ==", "license": "MIT" }, "node_modules/setprototypeof": { "version": "1.2.0", "resolved": "https://registry.npmjs.org/setprototypeof/-/setprototypeof-1.2.0.tgz", "integrity": "sha512-E5LDX7Wrp85Kil5bhZv46j8jOeboKq5JMmYM3gVGdGH8xFpPWXUMsNrlODCrkoxMEeNi/XZIwuRvY4XNwYMJpw==", "dev": true, "license": "ISC" }, "node_modules/shebang-command": { "version": "2.0.0", "resolved": "https://registry.npmjs.org/shebang-command/-/shebang-command-2.0.0.tgz", "integrity": "sha512-kHxr2zZpYtdmrN1qDjrrX/Z1rR1kG8Dx+gkpK1G4eXmvXswmcE1hTWBWYUzlraYw1/yZp6YuDY77YtvbN0dmDA==", "dev": true, "license": "MIT", "dependencies": { "shebang-regex": "^3.0.0" }, "engines": { "node": ">=8" } }, "node_modules/shebang-regex": { "version": "3.0.0", "resolved": "https://registry.npmjs.org/shebang-regex/-/shebang-regex-3.0.0.tgz", "integrity": "sha512-7++dFhtcx3353uBaq8DDR4NuxBetBzC7ZQOhmTQInHEd6bSrXdiEyzCvG07Z44UYdLShWUyXt5M/yhz8ekcb1A==", "dev": true, "license": "MIT", "engines": { "node": ">=8" } }, "node_modules/side-channel": { "version": "1.1.0", "resolved": "https://registry.npmjs.org/side-channel/-/side-channel-1.1.0.tgz", "integrity": "sha512-ZX99e6tRweoUXqR+VBrslhda51Nh5MTQwou5tnUDgbtyM0dBgmhEDtWGP/xbKn6hqfPRHujUNwz5fy/wbbhnpw==", "dev": true, "license": "MIT", "dependencies": { "es-errors": "^1.3.0", "object-inspect": "^1.13.3", "side-channel-list": "^1.0.0", "side-channel-map": "^1.0.1", "side-channel-weakmap": "^1.0.2" }, "engines": { "node": ">= 0.4" }, "funding": { "url": "https://github.com/sponsors/ljharb" } }, "node_modules/side-channel-list": { "version": "1.0.0", "resolved": "https://registry.npmjs.org/side-channel-list/-/side-channel-list-1.0.0.tgz", "integrity": "sha512-FCLHtRD/gnpCiCHEiJLOwdmFP+wzCmDEkc9y7NsYxeF4u7Btsn1ZuwgwJGxImImHicJArLP4R0yX4c2KCrMrTA==", "dev": true, "license": "MIT", "dependencies": { "es-errors": "^1.3.0", "object-inspect": "^1.13.3" }, "engines": { "node": ">= 0.4" }, "funding": { "url": "https://github.com/sponsors/ljharb" } }, "node_modules/side-channel-map": { "version": "1.0.1", "resolved": "https://registry.npmjs.org/side-channel-map/-/side-channel-map-1.0.1.tgz", "integrity": "sha512-VCjCNfgMsby3tTdo02nbjtM/ewra6jPHmpThenkTYh8pG9ucZ/1P8So4u4FGBek/BjpOVsDCMoLA/iuBKIFXRA==", "dev": true, "license": "MIT", "dependencies": { "call-bound": "^1.0.2", "es-errors": "^1.3.0", "get-intrinsic": "^1.2.5", "object-inspect": "^1.13.3" }, "engines": { "node": ">= 0.4" }, "funding": { "url": "https://github.com/sponsors/ljharb" } }, "node_modules/side-channel-weakmap": { "version": "1.0.2", "resolved": "https://registry.npmjs.org/side-channel-weakmap/-/side-channel-weakmap-1.0.2.tgz", "integrity": "sha512-WPS/HvHQTYnHisLo9McqBHOJk2FkHO/tlpvldyrnem4aeQp4hai3gythswg6p01oSoTl58rcpiFAjF2br2Ak2A==", "dev": true, "license": "MIT", "dependencies": { "call-bound": "^1.0.2", "es-errors": "^1.3.0", "get-intrinsic": "^1.2.5", "object-inspect": "^1.13.3", "side-channel-map": "^1.0.1" }, "engines": { "node": ">= 0.4" }, "funding": { "url": "https://github.com/sponsors/ljharb" } }, "node_modules/signal-exit": { "version": "4.1.0", "resolved": "https://registry.npmjs.org/signal-exit/-/signal-exit-4.1.0.tgz", "integrity": "sha512-bzyZ1e88w9O1iNJbKnOlvYTrWPDl46O1bG0D3XInv+9tkPrxrN8jUUTiFlDkkmKWgn1M6CfIA13SuGqOa9Korw==", "dev": true, "license": "ISC", "engines": { "node": ">=14" }, "funding": { "url": "https://github.com/sponsors/isaacs" } }, "node_modules/source-map": { "version": "0.5.7", "resolved": "https://registry.npmjs.org/source-map/-/source-map-0.5.7.tgz", "integrity": "sha512-LbrmJOMUSdEVxIKvdcJzQC+nQhe8FUZQTXQy6+I75skNgn3OoQ0DZA8YnFa7gp8tqtL3KPf1kmo0R5DoApeSGQ==", "license": "BSD-3-Clause", "engines": { "node": ">=0.10.0" } }, "node_modules/source-map-js": { "version": "1.2.1", "resolved": "https://registry.npmjs.org/source-map-js/-/source-map-js-1.2.1.tgz", "integrity": "sha512-UXWMKhLOwVKb728IUtQPXxfYU+usdybtUrK/8uGE8CQMvrhOpwvzDBwj0QhSL7MQc7vIsISBG8VQ8+IDQxpfQA==", "dev": true, "license": "BSD-3-Clause", "engines": { "node": ">=0.10.0" } }, "node_modules/statuses": { "version": "2.0.1", "resolved": "https://registry.npmjs.org/statuses/-/statuses-2.0.1.tgz", "integrity": "sha512-RwNA9Z/7PrK06rYLIzFMlaF+l73iwpzsqRIFgbMLbTcLD6cOao82TaWefPXQvB2fOC4AjuYSEndS7N/mTCbkdQ==", "dev": true, "license": "MIT", "engines": { "node": ">= 0.8" } }, "node_modules/string-width": { "version": "5.1.2", "resolved": "https://registry.npmjs.org/string-width/-/string-width-5.1.2.tgz", "integrity": "sha512-HnLOCR3vjcY8beoNLtcjZ5/nxn2afmME6lhrDrebokqMap+XbeW8n9TXpPDOqdGK5qcI3oT0GKTW6wC7EMiVqA==", "dev": true, "license": "MIT", "dependencies": { "eastasianwidth": "^0.2.0", "emoji-regex": "^9.2.2", "strip-ansi": "^7.0.1" }, "engines": { "node": ">=12" }, "funding": { "url": "https://github.com/sponsors/sindresorhus" } }, "node_modules/string-width-cjs": { "name": "string-width", "version": "4.2.3", "resolved": "https://registry.npmjs.org/string-width/-/string-width-4.2.3.tgz", "integrity": "sha512-wKyQRQpjJ0sIp62ErSZdGsjMJWsap5oRNihHhu6G7JVO/9jIB6UyevL+tXuOqrng8j/cxKTWyWUwvSTriiZz/g==", "dev": true, "license": "MIT", "dependencies": { "emoji-regex": "^8.0.0", "is-fullwidth-code-point": "^3.0.0", "strip-ansi": "^6.0.1" }, "engines": { "node": ">=8" } }, "node_modules/string-width-cjs/node_modules/ansi-regex": { "version": "5.0.1", "resolved": "https://registry.npmjs.org/ansi-regex/-/ansi-regex-5.0.1.tgz", "integrity": "sha512-quJQXlTSUGL2LH9SUXo8VwsY4soanhgo6LNSm84E1LBcE8s3O0wpdiRzyR9z/ZZJMlMWv37qOOb9pdJlMUEKFQ==", "dev": true, "license": "MIT", "engines": { "node": ">=8" } }, "node_modules/string-width-cjs/node_modules/emoji-regex": { "version": "8.0.0", "resolved": "https://registry.npmjs.org/emoji-regex/-/emoji-regex-8.0.0.tgz", "integrity": "sha512-MSjYzcWNOA0ewAHpz0MxpYFvwg6yjy1NG3xteoqz644VCo/RPgnr1/GGt+ic3iJTzQ8Eu3TdM14SawnVUmGE6A==", "dev": true, "license": "MIT" }, "node_modules/string-width-cjs/node_modules/strip-ansi": { "version": "6.0.1", "resolved": "https://registry.npmjs.org/strip-ansi/-/strip-ansi-6.0.1.tgz", "integrity": "sha512-Y38VPSHcqkFrCpFnQ9vuSXmquuv5oXOKpGeT6aGrr3o3Gc9AlVa6JBfUSOCnbxGGZF+/0ooI7KrPuUSztUdU5A==", "dev": true, "license": "MIT", "dependencies": { "ansi-regex": "^5.0.1" }, "engines": { "node": ">=8" } }, "node_modules/strip-ansi": { "version": "7.1.0", "resolved": "https://registry.npmjs.org/strip-ansi/-/strip-ansi-7.1.0.tgz", "integrity": "sha512-iq6eVVI64nQQTRYq2KtEg2d2uU7LElhTJwsH4YzIHZshxlgZms/wIc4VoDQTlG/IvVIrBKG06CrZnp0qv7hkcQ==", "dev": true, "license": "MIT", "dependencies": { "ansi-regex": "^6.0.1" }, "engines": { "node": ">=12" }, "funding": { "url": "https://github.com/chalk/strip-ansi?sponsor=1" } }, "node_modules/strip-ansi-cjs": { "name": "strip-ansi", "version": "6.0.1", "resolved": "https://registry.npmjs.org/strip-ansi/-/strip-ansi-6.0.1.tgz", "integrity": "sha512-Y38VPSHcqkFrCpFnQ9vuSXmquuv5oXOKpGeT6aGrr3o3Gc9AlVa6JBfUSOCnbxGGZF+/0ooI7KrPuUSztUdU5A==", "dev": true, "license": "MIT", "dependencies": { "ansi-regex": "^5.0.1" }, "engines": { "node": ">=8" } }, "node_modules/strip-ansi-cjs/node_modules/ansi-regex": { "version": "5.0.1", "resolved": "https://registry.npmjs.org/ansi-regex/-/ansi-regex-5.0.1.tgz", "integrity": "sha512-quJQXlTSUGL2LH9SUXo8VwsY4soanhgo6LNSm84E1LBcE8s3O0wpdiRzyR9z/ZZJMlMWv37qOOb9pdJlMUEKFQ==", "dev": true, "license": "MIT", "engines": { "node": ">=8" } }, "node_modules/strip-json-comments": { "version": "3.1.1", "resolved": "https://registry.npmjs.org/strip-json-comments/-/strip-json-comments-3.1.1.tgz", "integrity": "sha512-6fPc+R4ihwqP6N/aIv2f1gMH8lOVtWQHoqC4yK6oSDVVocumAsfCqjkXnqiYMhmMwS/mEHLp7Vehlt3ql6lEig==", "dev": true, "license": "MIT", "engines": { "node": ">=8" }, "funding": { "url": "https://github.com/sponsors/sindresorhus" } }, "node_modules/stylis": { "version": "4.2.0", "resolved": "https://registry.npmjs.org/stylis/-/stylis-4.2.0.tgz", "integrity": "sha512-Orov6g6BB1sDfYgzWfTHDOxamtX1bE/zo104Dh9e6fqJ3PooipYyfJ0pUmrZO2wAvO8YbEyeFrkV91XTsGMSrw==", "license": "MIT" }, "node_modules/sucrase": { "version": "3.35.0", "resolved": "https://registry.npmjs.org/sucrase/-/sucrase-3.35.0.tgz", "integrity": "sha512-8EbVDiu9iN/nESwxeSxDKe0dunta1GOlHufmSSXxMD2z2/tMZpDMpvXQGsc+ajGo8y2uYUmixaSRUc/QPoQ0GA==", "dev": true, "license": "MIT", "dependencies": { "@jridgewell/gen-mapping": "^0.3.2", "commander": "^4.0.0", "glob": "^10.3.10", "lines-and-columns": "^1.1.6", "mz": "^2.7.0", "pirates": "^4.0.1", "ts-interface-checker": "^0.1.9" }, "bin": { "sucrase": "bin/sucrase", "sucrase-node": "bin/sucrase-node" }, "engines": { "node": ">=16 || 14 >=14.17" } }, "node_modules/supports-color": { "version": "7.2.0", "resolved": "https://registry.npmjs.org/supports-color/-/supports-color-7.2.0.tgz", "integrity": "sha512-qpCAvRl9stuOHveKsn7HncJRvv501qIacKzQlO/+Lwxc9+0q2wLyv4Dfvt80/DPn2pqOBsJdDiogXGR9+OvwRw==", "dev": true, "license": "MIT", "dependencies": { "has-flag": "^4.0.0" }, "engines": { "node": ">=8" } }, "node_modules/supports-preserve-symlinks-flag": { "version": "1.0.0", "resolved": "https://registry.npmjs.org/supports-preserve-symlinks-flag/-/supports-preserve-symlinks-flag-1.0.0.tgz", "integrity": "sha512-ot0WnXS9fgdkgIcePe6RHNk1WA8+muPa6cSjeR3V8K27q9BB1rTE3R1p7Hv0z1ZyAc8s6Vvv8DIyWf681MAt0w==", "license": "MIT", "engines": { "node": ">= 0.4" }, "funding": { "url": "https://github.com/sponsors/ljharb" } }, "node_modules/tailwindcss": { "version": "3.3.0", "resolved": "https://registry.npmjs.org/tailwindcss/-/tailwindcss-3.3.0.tgz", "integrity": "sha512-hOXlFx+YcklJ8kXiCAfk/FMyr4Pm9ck477G0m/us2344Vuj355IpoEDB5UmGAsSpTBmr+4ZhjzW04JuFXkb/fw==", "dev": true, "license": "MIT", "dependencies": { "arg": "^5.0.2", "chokidar": "^3.5.3", "color-name": "^1.1.4", "didyoumean": "^1.2.2", "dlv": "^1.1.3", "fast-glob": "^3.2.12", "glob-parent": "^6.0.2", "is-glob": "^4.0.3", "jiti": "^1.17.2", "lilconfig": "^2.0.6", "micromatch": "^4.0.5", "normalize-path": "^3.0.0", "object-hash": "^3.0.0", "picocolors": "^1.0.0", "postcss": "^8.0.9", "postcss-import": "^14.1.0", "postcss-js": "^4.0.0", "postcss-load-config": "^3.1.4", "postcss-nested": "6.0.0", "postcss-selector-parser": "^6.0.11", "postcss-value-parser": "^4.2.0", "quick-lru": "^5.1.1", "resolve": "^1.22.1", "sucrase": "^3.29.0" }, "bin": { "tailwind": "lib/cli.js", "tailwindcss": "lib/cli.js" }, "engines": { "node": ">=12.13.0" }, "peerDependencies": { "postcss": "^8.0.9" } }, "node_modules/thenify": { "version": "3.3.1", "resolved": "https://registry.npmjs.org/thenify/-/thenify-3.3.1.tgz", "integrity": "sha512-RVZSIV5IG10Hk3enotrhvz0T9em6cyHBLkH/YAZuKqd8hRkKhSfCGIcP2KUY0EPxndzANBmNllzWPwak+bheSw==", "dev": true, "license": "MIT", "dependencies": { "any-promise": "^1.0.0" } }, "node_modules/thenify-all": { "version": "1.6.0", "resolved": "https://registry.npmjs.org/thenify-all/-/thenify-all-1.6.0.tgz", "integrity": "sha512-RNxQH/qI8/t3thXJDwcstUO4zeqo64+Uy/+sNVRBx4Xn2OX+OZ9oP+iJnNFqplFra2ZUVeKCSa2oVWi3T4uVmA==", "dev": true, "license": "MIT", "dependencies": { "thenify": ">= 3.1.0 < 4" }, "engines": { "node": ">=0.8" } }, "node_modules/tiny-invariant": { "version": "1.3.3", "resolved": "https://registry.npmjs.org/tiny-invariant/-/tiny-invariant-1.3.3.tgz", "integrity": "sha512-+FbBPE1o9QAYvviau/qC5SE3caw21q3xkvWKBtja5vgqOWIHHJ3ioaq1VPfn/Szqctz2bU/oYeKd9/z5BL+PVg==", "license": "MIT" }, "node_modules/tinyglobby": { "version": "0.2.13", "resolved": "https://registry.npmjs.org/tinyglobby/-/tinyglobby-0.2.13.tgz", "integrity": "sha512-mEwzpUgrLySlveBwEVDMKk5B57bhLPYovRfPAXD5gA/98Opn0rCDj3GtLwFvCvH5RK9uPCExUROW5NjDwvqkxw==", "dev": true, "license": "MIT", "dependencies": { "fdir": "^6.4.4", "picomatch": "^4.0.2" }, "engines": { "node": ">=12.0.0" }, "funding": { "url": "https://github.com/sponsors/SuperchupuDev" } }, "node_modules/tinyglobby/node_modules/fdir": { "version": "6.4.4", "resolved": "https://registry.npmjs.org/fdir/-/fdir-6.4.4.tgz", "integrity": "sha512-1NZP+GK4GfuAv3PqKvxQRDMjdSRZjnkq7KfhlNrCNNlZ0ygQFpebfrnfnq/W7fpUnAv9aGWmY1zKx7FYL3gwhg==", "dev": true, "license": "MIT", "peerDependencies": { "picomatch": "^3 || ^4" }, "peerDependenciesMeta": { "picomatch": { "optional": true } } }, "node_modules/tinyglobby/node_modules/picomatch": { "version": "4.0.2", "resolved": "https://registry.npmjs.org/picomatch/-/picomatch-4.0.2.tgz", "integrity": "sha512-M7BAV6Rlcy5u+m6oPhAPFgJTzAioX/6B0DxyvDlo9l8+T3nLKbrczg2WLUyzd45L8RqfUMyGPzekbMvX2Ldkwg==", "dev": true, "license": "MIT", "engines": { "node": ">=12" }, "funding": { "url": "https://github.com/sponsors/jonschlinkert" } }, "node_modules/to-regex-range": { "version": "5.0.1", "resolved": "https://registry.npmjs.org/to-regex-range/-/to-regex-range-5.0.1.tgz", "integrity": "sha512-65P7iz6X5yEr1cwcgvQxbbIw7Uk3gOy5dIdtZ4rDveLqhrdJP+Li/Hx6tyK0NEb+2GCyneCMJiGqrADCSNk8sQ==", "dev": true, "license": "MIT", "dependencies": { "is-number": "^7.0.0" }, "engines": { "node": ">=8.0" } }, "node_modules/toidentifier": { "version": "1.0.1", "resolved": "https://registry.npmjs.org/toidentifier/-/toidentifier-1.0.1.tgz", "integrity": "sha512-o5sSPKEkg/DIQNmH43V0/uerLrpzVedkUh8tGNvaeXpfpuwjKenlSox/2O/BTlZUtEe+JG7s5YhEz608PlAHRA==", "dev": true, "license": "MIT", "engines": { "node": ">=0.6" } }, "node_modules/ts-api-utils": { "version": "2.1.0", "resolved": "https://registry.npmjs.org/ts-api-utils/-/ts-api-utils-2.1.0.tgz", "integrity": "sha512-CUgTZL1irw8u29bzrOD/nH85jqyc74D6SshFgujOIA7osm2Rz7dYH77agkx7H4FBNxDq7Cjf+IjaX/8zwFW+ZQ==", "dev": true, "license": "MIT", "engines": { "node": ">=18.12" }, "peerDependencies": { "typescript": ">=4.8.4" } }, "node_modules/ts-interface-checker": { "version": "0.1.13", "resolved": "https://registry.npmjs.org/ts-interface-checker/-/ts-interface-checker-0.1.13.tgz", "integrity": "sha512-Y/arvbn+rrz3JCKl9C4kVNfTfSm2/mEp5FSz5EsZSANGPSlQrpRI5M4PKF+mJnE52jOO90PnPSc3Ur3bTQw0gA==", "dev": true, "license": "Apache-2.0" }, "node_modules/turbo-stream": { "version": "2.4.0", "resolved": "https://registry.npmjs.org/turbo-stream/-/turbo-stream-2.4.0.tgz", "integrity": "sha512-FHncC10WpBd2eOmGwpmQsWLDoK4cqsA/UT/GqNoaKOQnT8uzhtCbg3EoUDMvqpOSAI0S26mr0rkjzbOO6S3v1g==", "license": "ISC" }, "node_modules/type-check": { "version": "0.4.0", "resolved": "https://registry.npmjs.org/type-check/-/type-check-0.4.0.tgz", "integrity": "sha512-XleUoc9uwGXqjWwXaUTZAmzMcFZ5858QA2vvx1Ur5xIcixXIP+8LnFDgRplU30us6teqdlskFfu+ae4K79Ooew==", "dev": true, "license": "MIT", "dependencies": { "prelude-ls": "^1.2.1" }, "engines": { "node": ">= 0.8.0" } }, "node_modules/type-is": { "version": "2.0.1", "resolved": "https://registry.npmjs.org/type-is/-/type-is-2.0.1.tgz", "integrity": "sha512-OZs6gsjF4vMp32qrCbiVSkrFmXtG/AZhY3t0iAMrMBiAZyV9oALtXO8hsrHbMXF9x6L3grlFuwW2oAz7cav+Gw==", "dev": true, "license": "MIT", "dependencies": { "content-type": "^1.0.5", "media-typer": "^1.1.0", "mime-types": "^3.0.0" }, "engines": { "node": ">= 0.6" } }, "node_modules/typescript": { "version": "5.7.3", "resolved": "https://registry.npmjs.org/typescript/-/typescript-5.7.3.tgz", "integrity": "sha512-84MVSjMEHP+FQRPy3pX9sTVV/INIex71s9TL2Gm5FG/WG1SqXeKyZ0k7/blY/4FdOzI12CBy1vGc4og/eus0fw==", "dev": true, "license": "Apache-2.0", "bin": { "tsc": "bin/tsc", "tsserver": "bin/tsserver" }, "engines": { "node": ">=14.17" } }, "node_modules/typescript-eslint": { "version": "8.31.1", "resolved": "https://registry.npmjs.org/typescript-eslint/-/typescript-eslint-8.31.1.tgz", "integrity": "sha512-j6DsEotD/fH39qKzXTQRwYYWlt7D+0HmfpOK+DVhwJOFLcdmn92hq3mBb7HlKJHbjjI/gTOqEcc9d6JfpFf/VA==", "dev": true, "license": "MIT", "dependencies": { "@typescript-eslint/eslint-plugin": "8.31.1", "@typescript-eslint/parser": "8.31.1", "@typescript-eslint/utils": "8.31.1" }, "engines": { "node": "^18.18.0 || ^20.9.0 || >=21.1.0" }, "funding": { "type": "opencollective", "url": "https://opencollective.com/typescript-eslint" }, "peerDependencies": { "eslint": "^8.57.0 || ^9.0.0", "typescript": ">=4.8.4 <5.9.0" } }, "node_modules/unpipe": { "version": "1.0.0", "resolved": "https://registry.npmjs.org/unpipe/-/unpipe-1.0.0.tgz", "integrity": "sha512-pjy2bYhSsufwWlKwPc+l3cN7+wuJlK6uz0YdJEOlQDbl6jo/YlPi4mb8agUkVC8BF7V8NuzeyPNqRksA3hztKQ==", "dev": true, "license": "MIT", "engines": { "node": ">= 0.8" } }, "node_modules/update-browserslist-db": { "version": "1.1.3", "resolved": "https://registry.npmjs.org/update-browserslist-db/-/update-browserslist-db-1.1.3.tgz", "integrity": "sha512-UxhIZQ+QInVdunkDAaiazvvT/+fXL5Osr0JZlJulepYu6Jd7qJtDZjlur0emRlT71EN3ScPoE7gvsuIKKNavKw==", "dev": true, "funding": [ { "type": "opencollective", "url": "https://opencollective.com/browserslist" }, { "type": "tidelift", "url": "https://tidelift.com/funding/github/npm/browserslist" }, { "type": "github", "url": "https://github.com/sponsors/ai" } ], "license": "MIT", "dependencies": { "escalade": "^3.2.0", "picocolors": "^1.1.1" }, "bin": { "update-browserslist-db": "cli.js" }, "peerDependencies": { "browserslist": ">= 4.21.0" } }, "node_modules/uri-js": { "version": "4.4.1", "resolved": "https://registry.npmjs.org/uri-js/-/uri-js-4.4.1.tgz", "integrity": "sha512-7rKUyy33Q1yc98pQ1DAmLtwX109F7TIfWlW1Ydo8Wl1ii1SeHieeh0HHfPeL2fMXK6z0s8ecKs9frCuLJvndBg==", "dev": true, "license": "BSD-2-Clause", "dependencies": { "punycode": "^2.1.0" } }, "node_modules/use-isomorphic-layout-effect": { "version": "1.2.0", "resolved": "https://registry.npmjs.org/use-isomorphic-layout-effect/-/use-isomorphic-layout-effect-1.2.0.tgz", "integrity": "sha512-q6ayo8DWoPZT0VdG4u3D3uxcgONP3Mevx2i2b0434cwWBoL+aelL1DzkXI6w3PhTZzUeR2kaVlZn70iCiseP6w==", "license": "MIT", "peerDependencies": { "react": "^16.8.0 || ^17.0.0 || ^18.0.0 || ^19.0.0" }, "peerDependenciesMeta": { "@types/react": { "optional": true } } }, "node_modules/util-deprecate": { "version": "1.0.2", "resolved": "https://registry.npmjs.org/util-deprecate/-/util-deprecate-1.0.2.tgz", "integrity": "sha512-EPD5q1uXyFxJpCrLnCc1nHnq3gOa6DZBocAIiI2TaSCA7VCJ1UJDMagCzIkXNsUYfD1daK//LTEQ8xiIbrHtcw==", "dev": true, "license": "MIT" }, "node_modules/vary": { "version": "1.1.2", "resolved": "https://registry.npmjs.org/vary/-/vary-1.1.2.tgz", "integrity": "sha512-BNGbWLfd0eUPabhkXUVm0j8uuvREyTh5ovRa/dyow/BqAbZJyC+5fU+IzQOzmAKzYqYRAISoRhdQr3eIZ/PXqg==", "dev": true, "license": "MIT", "engines": { "node": ">= 0.8" } }, "node_modules/victory-vendor": { "version": "36.9.2", "resolved": "https://registry.npmjs.org/victory-vendor/-/victory-vendor-36.9.2.tgz", "integrity": "sha512-PnpQQMuxlwYdocC8fIJqVXvkeViHYzotI+NJrCuav0ZYFoq912ZHBk3mCeuj+5/VpodOjPe1z0Fk2ihgzlXqjQ==", "license": "MIT AND ISC", "dependencies": { "@types/d3-array": "^3.0.3", "@types/d3-ease": "^3.0.0", "@types/d3-interpolate": "^3.0.1", "@types/d3-scale": "^4.0.2", "@types/d3-shape": "^3.1.0", "@types/d3-time": "^3.0.0", "@types/d3-timer": "^3.0.0", "d3-array": "^3.1.6", "d3-ease": "^3.0.1", "d3-interpolate": "^3.0.1", "d3-scale": "^4.0.2", "d3-shape": "^3.1.0", "d3-time": "^3.0.0", "d3-timer": "^3.0.1" } }, "node_modules/vite": { "version": "6.3.4", "resolved": "https://registry.npmjs.org/vite/-/vite-6.3.4.tgz", "integrity": "sha512-BiReIiMS2fyFqbqNT/Qqt4CVITDU9M9vE+DKcVAsB+ZV0wvTKd+3hMbkpxz1b+NmEDMegpVbisKiAZOnvO92Sw==", "dev": true, "license": "MIT", "dependencies": { "esbuild": "^0.25.0", "fdir": "^6.4.4", "picomatch": "^4.0.2", "postcss": "^8.5.3", "rollup": "^4.34.9", "tinyglobby": "^0.2.13" }, "bin": { "vite": "bin/vite.js" }, "engines": { "node": "^18.0.0 || ^20.0.0 || >=22.0.0" }, "funding": { "url": "https://github.com/vitejs/vite?sponsor=1" }, "optionalDependencies": { "fsevents": "~2.3.3" }, "peerDependencies": { "@types/node": "^18.0.0 || ^20.0.0 || >=22.0.0", "jiti": ">=1.21.0", "less": "*", "lightningcss": "^1.21.0", "sass": "*", "sass-embedded": "*", "stylus": "*", "sugarss": "*", "terser": "^5.16.0", "tsx": "^4.8.1", "yaml": "^2.4.2" }, "peerDependenciesMeta": { "@types/node": { "optional": true }, "jiti": { "optional": true }, "less": { "optional": true }, "lightningcss": { "optional": true }, "sass": { "optional": true }, "sass-embedded": { "optional": true }, "stylus": { "optional": true }, "sugarss": { "optional": true }, "terser": { "optional": true }, "tsx": { "optional": true }, "yaml": { "optional": true } } }, "node_modules/vite/node_modules/fdir": { "version": "6.4.4", "resolved": "https://registry.npmjs.org/fdir/-/fdir-6.4.4.tgz", "integrity": "sha512-1NZP+GK4GfuAv3PqKvxQRDMjdSRZjnkq7KfhlNrCNNlZ0ygQFpebfrnfnq/W7fpUnAv9aGWmY1zKx7FYL3gwhg==", "dev": true, "license": "MIT", "peerDependencies": { "picomatch": "^3 || ^4" }, "peerDependenciesMeta": { "picomatch": { "optional": true } } }, "node_modules/vite/node_modules/picomatch": { "version": "4.0.2", "resolved": "https://registry.npmjs.org/picomatch/-/picomatch-4.0.2.tgz", "integrity": "sha512-M7BAV6Rlcy5u+m6oPhAPFgJTzAioX/6B0DxyvDlo9l8+T3nLKbrczg2WLUyzd45L8RqfUMyGPzekbMvX2Ldkwg==", "dev": true, "license": "MIT", "engines": { "node": ">=12" }, "funding": { "url": "https://github.com/sponsors/jonschlinkert" } }, "node_modules/which": { "version": "2.0.2", "resolved": "https://registry.npmjs.org/which/-/which-2.0.2.tgz", "integrity": "sha512-BLI3Tl1TW3Pvl70l3yq3Y64i+awpwXqsGBYWkkqMtnbXgrMD+yj7rhW0kuEDxzJaYXGjEW5ogapKNMEKNMjibA==", "dev": true, "license": "ISC", "dependencies": { "isexe": "^2.0.0" }, "bin": { "node-which": "bin/node-which" }, "engines": { "node": ">= 8" } }, "node_modules/word-wrap": { "version": "1.2.5", "resolved": "https://registry.npmjs.org/word-wrap/-/word-wrap-1.2.5.tgz", "integrity": "sha512-BN22B5eaMMI9UMtjrGd5g5eCYPpCPDUy0FJXbYsaT5zYxjFOckS53SQDE3pWkVoWpHXVb3BrYcEN4Twa55B5cA==", "dev": true, "license": "MIT", "engines": { "node": ">=0.10.0" } }, "node_modules/wrap-ansi": { "version": "8.1.0", "resolved": "https://registry.npmjs.org/wrap-ansi/-/wrap-ansi-8.1.0.tgz", "integrity": "sha512-si7QWI6zUMq56bESFvagtmzMdGOtoxfR+Sez11Mobfc7tm+VkUckk9bW2UeffTGVUbOksxmSw0AA2gs8g71NCQ==", "dev": true, "license": "MIT", "dependencies": { "ansi-styles": "^6.1.0", "string-width": "^5.0.1", "strip-ansi": "^7.0.1" }, "engines": { "node": ">=12" }, "funding": { "url": "https://github.com/chalk/wrap-ansi?sponsor=1" } }, "node_modules/wrap-ansi-cjs": { "name": "wrap-ansi", "version": "7.0.0", "resolved": "https://registry.npmjs.org/wrap-ansi/-/wrap-ansi-7.0.0.tgz", "integrity": "sha512-YVGIj2kamLSTxw6NsZjoBxfSwsn0ycdesmc4p+Q21c5zPuZ1pl+NfxVdxPtdHvmNVOQ6XSYG4AUtyt/Fi7D16Q==", "dev": true, "license": "MIT", "dependencies": { "ansi-styles": "^4.0.0", "string-width": "^4.1.0", "strip-ansi": "^6.0.0" }, "engines": { "node": ">=10" }, "funding": { "url": "https://github.com/chalk/wrap-ansi?sponsor=1" } }, "node_modules/wrap-ansi-cjs/node_modules/ansi-regex": { "version": "5.0.1", "resolved": "https://registry.npmjs.org/ansi-regex/-/ansi-regex-5.0.1.tgz", "integrity": "sha512-quJQXlTSUGL2LH9SUXo8VwsY4soanhgo6LNSm84E1LBcE8s3O0wpdiRzyR9z/ZZJMlMWv37qOOb9pdJlMUEKFQ==", "dev": true, "license": "MIT", "engines": { "node": ">=8" } }, "node_modules/wrap-ansi-cjs/node_modules/emoji-regex": { "version": "8.0.0", "resolved": "https://registry.npmjs.org/emoji-regex/-/emoji-regex-8.0.0.tgz", "integrity": "sha512-MSjYzcWNOA0ewAHpz0MxpYFvwg6yjy1NG3xteoqz644VCo/RPgnr1/GGt+ic3iJTzQ8Eu3TdM14SawnVUmGE6A==", "dev": true, "license": "MIT" }, "node_modules/wrap-ansi-cjs/node_modules/string-width": { "version": "4.2.3", "resolved": "https://registry.npmjs.org/string-width/-/string-width-4.2.3.tgz", "integrity": "sha512-wKyQRQpjJ0sIp62ErSZdGsjMJWsap5oRNihHhu6G7JVO/9jIB6UyevL+tXuOqrng8j/cxKTWyWUwvSTriiZz/g==", "dev": true, "license": "MIT", "dependencies": { "emoji-regex": "^8.0.0", "is-fullwidth-code-point": "^3.0.0", "strip-ansi": "^6.0.1" }, "engines": { "node": ">=8" } }, "node_modules/wrap-ansi-cjs/node_modules/strip-ansi": { "version": "6.0.1", "resolved": "https://registry.npmjs.org/strip-ansi/-/strip-ansi-6.0.1.tgz", "integrity": "sha512-Y38VPSHcqkFrCpFnQ9vuSXmquuv5oXOKpGeT6aGrr3o3Gc9AlVa6JBfUSOCnbxGGZF+/0ooI7KrPuUSztUdU5A==", "dev": true, "license": "MIT", "dependencies": { "ansi-regex": "^5.0.1" }, "engines": { "node": ">=8" } }, "node_modules/wrap-ansi/node_modules/ansi-styles": { "version": "6.2.1", "resolved": "https://registry.npmjs.org/ansi-styles/-/ansi-styles-6.2.1.tgz", "integrity": "sha512-bN798gFfQX+viw3R7yrGWRqnrN2oRkEkUjjl4JNn4E8GxxbjtG3FbrEIIY3l8/hrwUwIeCZvi4QuOTP4MErVug==", "dev": true, "license": "MIT", "engines": { "node": ">=12" }, "funding": { "url": "https://github.com/chalk/ansi-styles?sponsor=1" } }, "node_modules/wrappy": { "version": "1.0.2", "resolved": "https://registry.npmjs.org/wrappy/-/wrappy-1.0.2.tgz", "integrity": "sha512-l4Sp/DRseor9wL6EvV2+TuQn63dMkPjZ/sp9XkghTEbV9KlPS1xUsZ3u7/IQO4wxtcFB4bgpQPRcR3QCvezPcQ==", "dev": true, "license": "ISC" }, "node_modules/yallist": { "version": "3.1.1", "resolved": "https://registry.npmjs.org/yallist/-/yallist-3.1.1.tgz", "integrity": "sha512-a4UGQaWPH59mOXUYnAG2ewncQS4i4F43Tv3JoAM+s2VDAmS9NsK8GpDMLrCHPksFT7h3K6TOoUNn2pb7RoXx4g==", "dev": true, "license": "ISC" }, "node_modules/yaml": { "version": "2.7.1", "resolved": "https://registry.npmjs.org/yaml/-/yaml-2.7.1.tgz", "integrity": "sha512-10ULxpnOCQXxJvBgxsn9ptjq6uviG/htZKk9veJGhlqn3w/DxQ631zFF+nlQXLwmImeS5amR2dl2U8sg6U9jsQ==", "dev": true, "license": "ISC", "optional": true, "peer": true, "bin": { "yaml": "bin.mjs" }, "engines": { "node": ">= 14" } }, "node_modules/yocto-queue": { "version": "0.1.0", "resolved": "https://registry.npmjs.org/yocto-queue/-/yocto-queue-0.1.0.tgz", "integrity": "sha512-rVksvsnNCdJ/ohGc6xgPwyN8eheCxsiLM8mxuE/t/mOVqJewPuO1miLpTHQiRgTKCLexL4MeAFVagts7HmNZ2Q==", "dev": true, "license": "MIT", "engines": { "node": ">=10" }, "funding": { "url": "https://github.com/sponsors/sindresorhus" } }, "node_modules/zod": { "version": "3.24.3", "resolved": "https://registry.npmjs.org/zod/-/zod-3.24.3.tgz", "integrity": "sha512-HhY1oqzWCQWuUqvBFnsyrtZRhyPeR7SUGv+C4+MsisMuVfSPx8HpwWqH8tRahSlt6M3PiFAcoeFhZAqIXTxoSg==", "dev": true, "license": "MIT", "funding": { "url": "https://github.com/sponsors/colinhacks" } }, "node_modules/zod-to-json-schema": { "version": "3.24.5", "resolved": "https://registry.npmjs.org/zod-to-json-schema/-/zod-to-json-schema-3.24.5.tgz", "integrity": "sha512-/AuWwMP+YqiPbsJx5D6TfgRTc4kTLjsh5SOcd4bLsfUg2RcEXrFMJl1DGgdHy2aCfsIA/cr/1JM0xcB2GZji8g==", "dev": true, "license": "ISC", "peerDependencies": { "zod": "^3.24.1" } } } }
---

# postcss.config

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** frontend

# postcss.config

export default { plugins: { 'tailwindcss/plugin': {}, autoprefixer: {}, }, }
---

# vite.config

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** frontend

# vite.config

import { defineConfig } from 'vite' import react from '@vitejs/plugin-react' // https://vite.dev/config/ export default defineConfig({ plugins: [react()], })
---

# tsconfig.app

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** frontend

# tsconfig.app

{ "compilerOptions": { "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.app.tsbuildinfo", "target": "ES2020", "useDefineForClassFields": true, "lib": ["ES2020", "DOM", "DOM.Iterable"], "module": "ESNext", "skipLibCheck": true, /* Bundler mode */ "moduleResolution": "bundler", "allowImportingTsExtensions": true, "isolatedModules": true, "moduleDetection": "force", "noEmit": true, "jsx": "react-jsx", /* Linting */ "strict": true, "noUnusedLocals": true, "noUnusedParameters": true, "noFallthroughCasesInSwitch": true, "noUncheckedSideEffectImports": true }, "include": ["src"] }
---

# tsconfig.node

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** frontend

# tsconfig.node

{ "compilerOptions": { "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.node.tsbuildinfo", "target": "ES2022", "lib": ["ES2023"], "module": "ESNext", "skipLibCheck": true, /* Bundler mode */ "moduleResolution": "bundler", "allowImportingTsExtensions": true, "isolatedModules": true, "moduleDetection": "force", "noEmit": true, /* Linting */ "strict": true, "noUnusedLocals": true, "noUnusedParameters": true, "noFallthroughCasesInSwitch": true, "noUncheckedSideEffectImports": true }, "include": ["vite.config.ts"] }
---

# README

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** frontend

# README

# Biblioteca de Conocimiento Personal (Biblioperson) **ltima actualizacin:** 9 de mayo de 2025 Sistema para gestionar, analizar y realizar bsquedas avanzadas sobre contenido personal proveniente de diferentes plataformas y fuentes documentales. ## ndice del README - [Biblioteca de Conocimiento Personal (Biblioperson)](#biblioteca-de-conocimiento-personal-biblioperson) - [ndice del README](#ndice-del-readme) - [Caractersticas Principales](#caractersticas-principales) - [Requisitos Previos](#requisitos-previos) - [Instalacin](#instalacin) - [Estructura del Proyecto](#estructura-del-proyecto) - [Cmo Ejecutar la Aplicacin](#cmo-ejecutar-la-aplicacin) --- ## Caractersticas Principales * Importacin de contenido desde mltiples fuentes (formatos DOCX, PDF, TXT, NDJSON, etc.). * Procesamiento y normalizacin de textos para anlisis. * Base de datos centralizada para el contenido (SQLite). * Generacin de embeddings semnticos para bsqueda por significado. * Motor de bsqueda avanzado (Meilisearch) para full-text y bsqueda vectorial. * API REST (Flask) para acceder a los datos y funcionalidades. * Interfaz web interactiva (React) para exploracin y anlisis. --- ## Requisitos Previos Asegrate de tener instalados los siguientes componentes en tu sistema: * **Python:** Versin 3.8 o superior. * **Node.js:** Versin 16 o superior (incluye npm). * **SQLite3:** Generalmente viene preinstalado en muchos sistemas operativos o se instala fcilmente. * **Git:** Para clonar el repositorio. * **Meilisearch:** Opcional para la instalacin inicial, pero necesario para la funcionalidad de bsqueda. Consulta la [Gua de Meilisearch](docs/GUIA_MEILISEARCH.md) para su instalacin y configuracin. --- ## Instalacin Sigue estos pasos para configurar el proyecto en tu entorno local: 1. **Clonar el Repositorio:** ```bash git clone [URL_DEL_REPOSITORIO_AQUI] cd biblioperson ``` *(Reemplaza `[URL_DEL_REPOSITORIO_AQUI]` con la URL real de tu repositorio)* 2. **Configurar Entorno Virtual de Python y Activar:** Se recomienda usar un entorno virtual para aislar las dependencias del proyecto. ```bash python -m venv venv ``` * En Windows: ```bash .\venv\Scripts\activate ``` * En Linux/macOS: ```bash source venv/bin/activate ``` 3. **Instalar Dependencias de Python:** El proyecto tiene diferentes conjuntos de dependencias. Instala las que necesites: * Para el **backend y la API Flask**: ```bash pip install -r backend/requirements.txt ``` * Para el **procesamiento de datos/datasets** (si vas a trabajar con la carpeta `dataset/`): ```bash pip install -r dataset/requirements.txt ``` *(Considera tener un `requirements.txt` general en la raz si las dependencias son compartidas o si deseas simplificar este paso).* 4. **Instalar Dependencias del Frontend (Node.js):** ```bash cd frontend npm install cd .. ``` 5. **Configurar la Base de Datos Inicial:** Si es la primera vez que configuras el proyecto, o si necesitas una base de datos limpia, puedes inicializarla: ```bash cd backend/scripts python inicializar_db.py cd ../.. ``` Para ms detalles sobre la gestin de datos y la importacin, consulta la [Gua de Gestin de Datos](docs/GUIA_GESTION_DATOS.md). --- ## Estructura del Proyecto El proyecto est organizado en varias carpetas principales: * `backend/`: Contiene la lgica del servidor API (Flask), scripts de procesamiento, y la base de datos. * `frontend/`: Alberga la aplicacin de interfaz de usuario (React). * `dataset/`: Scripts y herramientas para la ingesta, limpieza y normalizacin de datos fuente. * `docs/`: Documentacin detallada del proyecto. Para una descripcin exhaustiva de la arquitectura y la estructura de carpetas, consulta el documento: **[ Arquitectura del Proyecto Biblioperson](docs/BIBLIOPERSON_ARQUITECTURA.md)** --- ## Cmo Ejecutar la Aplicacin Una vez completada la instalacin y configuracin (incluyendo la inicializacin de la base de datos y la configuracin de Meilisearch si se va a usar la bsqueda), puedes ejecutar la aplicacin completa (backend y frontend) con un solo comando desde la raz del proyecto: ```bash Esto debera: Iniciar el servidor backend de Flask (generalmente en http://localhost:5000). Iniciar el servidor de desarrollo del frontend React (generalmente en http://localhost:5173 o el puerto que indique la consola). Nota: Asegrate de que Meilisearch est en ejecucin si deseas utilizar las funcionalidades de bsqueda. Consulta la Gua de Meilisearch. Para levantar los componentes individualmente (solo backend, solo frontend) o para flujos de trabajo ms especficos como la importacin de datos o la generacin de embeddings, consulta las guas detalladas en la seccin de Documentacin. Documentacin Detallada Para profundizar en aspectos especficos del proyecto, consulta las siguientes guas en la carpeta docs/:  Arquitectura del Proyecto Biblioperson: Visin general de la arquitectura, componentes principales, flujos de datos y decisiones de diseo.  Gua de Gestin de Datos: Instrucciones detalladas sobre la preparacin de la base de datos, importacin de contenidos, generacin y regeneracin de embeddings semnticos, y consultas directas a la base de datos.  Gua de Meilisearch: Todo sobre la instalacin, configuracin, integracin, indexacin (completa e incremental), verificacin, depuracin y mantenimiento de Meilisearch en Biblioperson. Licencia [ESPECIFICAR LICENCIA AQU - Ej: MIT, Apache 2.0, GPLv3, etc.] **Recordatorios:** 1. **Reemplaza `[URL_DEL_REPOSITORIO_AQUI]`** con la URL real de tu repositorio Git. 2. **Asegrate de que los nombres de archivo y las rutas en los enlaces** (ej. `docs/GUIA_MEILISEARCH.md`) coincidan exactamente con cmo has nombrado y ubicado tus archivos de documentacin. 3. **Actualiza la seccin de "Licencia"** con la licencia que hayas elegido para tu proyecto. 4. Considera si necesitas un archivo `requirements.txt` general en la raz del proyecto adems de los especficos en `backend/` y `dataset/` para simplificar la instalacin de todas las dependencias Python de una vez. Espero que esta versin completa te sea de utilidad. Hemos logrado una buena reestructuracin de tu documentacin! Fuentes y contenido relacionado
---

# package

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** frontend

# package

{ "name": "frontend", "private": true, "version": "0.0.0", "type": "module", "scripts": { "dev": "vite", "build": "tsc -b && vite build", "lint": "eslint .", "preview": "vite preview" }, "dependencies": { "@tanstack/react-query": "^5.75.2", "axios": "^1.9.0", "react": "^19.0.0", "react-dom": "^19.0.0", "react-router-dom": "^7.5.3", "react-select": "^5.10.1", "recharts": "^2.15.3" }, "devDependencies": { "@eslint/js": "^9.22.0", "@types/react": "^19.0.10", "@types/react-dom": "^19.0.4", "@vitejs/plugin-react": "^4.3.4", "autoprefixer": "^10.4.14", "eslint": "^9.22.0", "eslint-plugin-react-hooks": "^5.2.0", "eslint-plugin-react-refresh": "^0.4.19", "globals": "^16.0.0", "postcss": "^8.5.3", "tailwindcss": "^3.3.0", "typescript": "~5.7.2", "typescript-eslint": "^8.26.1", "vite": "^6.3.1" } }
---

# tsconfig

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** frontend

# tsconfig

{ "files": [], "references": [ { "path": "./tsconfig.app.json" }, { "path": "./tsconfig.node.json" } ] }
---

# mcp

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** .cursor

# mcp

{ "mcpServers": { "task-master-ai": { "command": "npx", "args": [ "-y", "--package=task-master-ai", "task-master-ai" ], "env": { "ANTHROPIC_API_KEY": "ANTHROPIC_API_KEY_HERE", "PERPLEXITY_API_KEY": "PERPLEXITY_API_KEY_HERE", "OPENAI_API_KEY": "OPENAI_API_KEY_HERE", "GOOGLE_API_KEY": "GOOGLE_API_KEY_HERE", "XAI_API_KEY": "XAI_API_KEY_HERE", "OPENROUTER_API_KEY": "OPENROUTER_API_KEY_HERE", "MISTRAL_API_KEY": "MISTRAL_API_KEY_HERE", "AZURE_OPENAI_API_KEY": "AZURE_OPENAI_API_KEY_HERE", "OLLAMA_API_KEY": "OLLAMA_API_KEY_HERE" } } } }
---

# data models

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# data models

from dataclasses import dataclass, field from typing import Optional, Dict, Any, List @dataclass class ProcessedContentItem: """ Dataclass representing a single processed content item (segment), ready for output to NDJSON. This structure aligns with the fields defined in docs/NDJSON_ESPECIFICACION.md. """ # --- Campos Obligatorios sin valor por defecto --- (Deben ir primero) id_segmento: str # UUID string, generado durante el proceso ETL. id_documento_fuente: str # Identificador nico para el documento original. idioma_documento: str # Cdigo ISO 639-1 (ej. "es", "en"). texto_segmento: str tipo_segmento: str # Vocabulario controlado (ej. "parrafo", "titulo_h1"). orden_segmento_documento: int # Nmero secuencial global del segmento. longitud_caracteres_segmento: int timestamp_procesamiento: str # Fecha y hora ISO 8601 del procesamiento. # --- Campos Opcionales o con valor por defecto --- # Metadatos del Documento Fuente ruta_archivo_original: Optional[str] = None hash_documento_original: Optional[str] = None # SHA256 del archivo original. titulo_documento: Optional[str] = None autor_documento: Optional[str] = None fecha_publicacion_documento: Optional[str] = None # Formato YYYY-MM-DD o YYYY. editorial_documento: Optional[str] = None isbn_documento: Optional[str] = None # Para metadatos adicionales del documento fuente. metadatos_adicionales_fuente: Dict[str, Any] = field(default_factory=dict) # Metadatos del Segmento # Objeto JSON que describe la posicin del segmento en la estructura. jerarquia_contextual: Dict[str, Any] = field(default_factory=dict) embedding_vectorial: Optional[List[float]] = None # Podra generarse en un paso posterior. # Metadatos del Proceso ETL version_pipeline_etl: Optional[str] = None nombre_segmentador_usado: Optional[str] = None # Notas o advertencias especficas generadas. notas_procesamiento_segmento: Optional[str] = None @dataclass class BatchContext: """ Contextual information for a batch of files being processed, typically related to a single job defined in jobs_config.json. This information is generally constant for all items within that job. """ author_name: str language_code: str origin_type_name: str # General type for the source of content in this batch (e.g., "Telegram Archive", "EGW Books") acquisition_date: Optional[str] = None # Expected format: YYYY-MM-DD force_null_publication_date: bool = False # If true, publication_date for all items in this batch will be set to None
---

# probar loaders

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# probar loaders

#!/usr/bin/env python3 """ Script para probar los loaders de diferentes formatos de archivo. Este script muestra cmo cargar cada tipo de archivo soportado. """ import os import sys import logging import json from pathlib import Path # Asegurar que los mdulos se encuentren en el path sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))) from dataset.processing.profile_manager import ProfileManager from dataset.processing.loaders import ( txtLoader, MarkdownLoader, NDJSONLoader, DocxLoader, PDFLoader, ExcelLoader, CSVLoader ) def mostrar_documento(doc): """Muestra un documento procesado de forma legible.""" # Filtrar campos internos campos_a_mostrar = {k: v for k, v in doc.items() if not k.startswith('_')} return json.dumps(campos_a_mostrar, ensure_ascii=False, indent=2) def probar_loader(loader_class, archivo, tipo='escritos'): """Prueba un loader especfico y muestra los resultados.""" print(f"\n{'='*80}") print(f"Probando {loader_class.__name__} con {archivo}") print(f"{'='*80}") try: loader = loader_class(archivo, tipo=tipo) for i, doc in enumerate(loader.load(), 1): if i <= 3: # Mostrar solo los primeros 3 documentos para no saturar la salida print(f"\nDocumento {i}:") print(mostrar_documento(doc)) else: print(f"\n... y {i-3} documentos ms") break print(f"\nProcesamiento completado. Total documentos: {i}") except Exception as e: print(f"Error al procesar: {e}") def probar_gestor_perfiles(archivo, perfil='book_structure'): """Prueba el gestor de perfiles con un archivo dado.""" print(f"\n{'='*80}") print(f"Probando ProfileManager con {archivo} usando perfil '{perfil}'") print(f"{'='*80}") try: manager = ProfileManager() resultados = manager.process_file(archivo, perfil) if resultados: for i, doc in enumerate(resultados[:3], 1): # Mostrar solo los 3 primeros print(f"\nResultado {i}:") print(mostrar_documento(doc)) if len(resultados) > 3: print(f"\n... y {len(resultados)-3} resultados ms") print(f"\nProcesamiento completado. Total resultados: {len(resultados)}") else: print("No se obtuvieron resultados.") except Exception as e: print(f"Error en el gestor de perfiles: {e}") def main(): """Funcin principal de prueba.""" # Configurar logging logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s') # Directorio de pruebas (ajustar segn estructura del proyecto) test_dir = Path(__file__).parent / "prueba_loaders" test_dir.mkdir(exist_ok=True) # Crear archivos de ejemplo para pruebas si no existen crear_archivos_ejemplo(test_dir) # Probar cada loader print("\nPRUEBAS DE LOADERS INDIVIDUALES") print("-"*50) # Archivos de ejemplo para cada loader archivos_ejemplo = { txtLoader: test_dir / "ejemplo.txt", MarkdownLoader: test_dir / "ejemplo.md", NDJSONLoader: test_dir / "ejemplo.ndjson", DocxLoader: test_dir / "ejemplo.docx", PDFLoader: test_dir / "ejemplo.pdf", ExcelLoader: test_dir / "ejemplo.xlsx", CSVLoader: test_dir / "ejemplo.csv" } # Probar cada loader que tenga su archivo de ejemplo for loader_class, archivo in archivos_ejemplo.items(): if archivo.exists(): probar_loader(loader_class, archivo) # Probar el gestor de perfiles print("\nPRUEBAS DEL GESTOR DE PERFILES") print("-"*50) for archivo in archivos_ejemplo.values(): if archivo.exists(): probar_gestor_perfiles(archivo) def crear_archivos_ejemplo(directorio): """Crea archivos de ejemplo para pruebas si no existen.""" # Ejemplo de texto archivo_txt = directorio / "ejemplo.txt" if not archivo_txt.exists(): with open(archivo_txt, 'w', encoding='utf-8') as f: f.write("Ttulo del documento\n\n") f.write("Este es un ejemplo de archivo de texto.\n") f.write("Contiene mltiples lneas para probar el txtLoader.\n\n") f.write("Y tambin tiene mltiples prrafos separados por lneas en blanco.") # Ejemplo de Markdown archivo_md = directorio / "ejemplo.md" if not archivo_md.exists(): with open(archivo_md, 'w', encoding='utf-8') as f: f.write("# Ttulo del documento Markdown\n\n") f.write("Este es un **ejemplo** de archivo *Markdown*.\n\n") f.write("## Una seccin\n\n") f.write("- Elemento 1\n") f.write("- Elemento 2\n\n") f.write("### Subseccin\n\n") f.write("Texto de la subseccin.") # Ejemplo de NDJSON archivo_ndjson = directorio / "ejemplo.ndjson" if not archivo_ndjson.exists(): with open(archivo_ndjson, 'w', encoding='utf-8') as f: f.write('{"id": 1, "texto": "Primer registro en NDJSON"}\n') f.write('{"id": 2, "texto": "Segundo registro"}\n') f.write('{"id": 3, "texto": "Tercer registro", "metadatos": {"fecha": "2023-05-15"}}\n') # Ejemplo de CSV archivo_csv = directorio / "ejemplo.csv" if not archivo_csv.exists(): with open(archivo_csv, 'w', encoding='utf-8') as f: f.write('Nombre,Edad,Ciudad\n') f.write('Juan Prez,32,Madrid\n') f.write('Mara Lpez,28,Barcelona\n') f.write('Carlos Ruiz,45,Valencia\n') # Informar sobre los archivos que requieren creacin manual print(f"Los siguientes archivos deben crearse manualmente en {directorio}:") for ext in ['.docx', '.pdf', '.xlsx']: print(f" - ejemplo{ext}") if __name__ == "__main__": main()
---

# processors

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# processors

import logging import re import os import uuid # Importado from datetime import datetime, timezone from typing import Optional, Dict, Any, Tuple, List # List importado por si acaso, aunque no se usa directamente aqu # Importar el nuevo ProcessedContentItem from .data_models import ProcessedContentItem # Para parsear fechas de manera flexible try: from dateutil import parser as date_parser DATEUTIL_AVAILABLE = True except ImportError: DATEUTIL_AVAILABLE = False logging.warning("python-dateutil not found. Date parsing will be limited.") # Para extraer frontmatter de archivos Markdown try: import frontmatter FRONTMATTER_AVAILABLE = True except ImportError: FRONTMATTER_AVAILABLE = False logging.warning("python-frontmatter not found. Frontmatter parsing from .md files will be skipped.") # Para metadatos de archivos (si se decide usar como fallback extremo para fechas) # from stat import ST_CTIME, ST_MTIME logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') # El dataclass ProcessedTextContent ha sido eliminado. Usaremos ProcessedContentItem directamente. def parse_publication_date( raw_date_str: Optional[Any] = None, metadata_from_file: Optional[Dict[str, Any]] = None, # e.g., source_doc_metadata force_null: bool = False ) -> Optional[str]: """ Tries to parse a publication date from various sources and returns it in 'YYYY-MM-DD' or 'YYYY' format. Order of preference: 1. Explicit raw_date_str (e.g., from JSON field, or a specific metadata field). 2. 'fecha_publicacion_documento', 'date', 'publish_date', 'created' fields in metadata_from_file. If force_null is True, returns None immediately. """ if force_null: return None date_candidates_str: List[str] = [] if raw_date_str: if isinstance(raw_date_str, datetime): # Si ya es datetime, formatearlo directamente try: return raw_date_str.strftime('%Y-%m-%d') if raw_date_str.year > 1 else str(raw_date_str.year) except ValueError: # En caso de aos muy lejanos que strftime no maneje return str(raw_date_str.year) elif isinstance(raw_date_str, str): date_candidates_str.append(raw_date_str) if metadata_from_file: # Claves ordenadas por preferencia for key in ['fecha_publicacion_documento', 'date', 'publish_date', 'publication_date', 'created', 'creation_date']: val = metadata_from_file.get(key) if val: if isinstance(val, datetime): try: return val.strftime('%Y-%m-%d') if val.year > 1 else str(val.year) except ValueError: return str(val.year) elif isinstance(val, str): date_candidates_str.append(val) break # Tomar el primer metadato de fecha encontrado parsed_date: Optional[datetime] = None if DATEUTIL_AVAILABLE: for candidate_str_item in date_candidates_str: try: parsed_date = date_parser.parse(str(candidate_str_item)) if parsed_date: break except (ValueError, TypeError, OverflowError) as e: logging.debug(f"Could not parse date string '{candidate_str_item}' with dateutil: {e}") else: # Fallback si dateutil no est for candidate_str_item in date_candidates_str: try: parsed_date = datetime.fromisoformat(str(candidate_str_item).replace('Z', '+00:00')) if parsed_date: break except ValueError: # Intentar otros formatos comunes si es necesario pass # Por ahora, simplificado if parsed_date: try: # Devolver YYYY-MM-DD si es posible, o solo YYYY como fallback return parsed_date.strftime('%Y-%m-%d') if parsed_date.year > 1 else str(parsed_date.year) except ValueError: # Para aos muy fuera de rango para strftime return str(parsed_date.year) if hasattr(parsed_date, 'year') and parsed_date.year > 1 else None # Si solo se encontr una cadena que parece ao (YYYY) for candidate in date_candidates_str: if re.fullmatch(r'\d{4}', candidate): return candidate # Asumir que es un ao si es la nica info return None def extract_title_from_markdown(markdown_text: str, source_doc_metadata: Optional[Dict[str, Any]] = None) -> Optional[str]: """ Extracts a title. Priority: 1. 'titulo_documento' from source_doc_metadata. 2. First H1 header in Markdown. """ if source_doc_metadata and isinstance(source_doc_metadata.get('titulo_documento'), str) and source_doc_metadata['titulo_documento'].strip(): return source_doc_metadata['titulo_documento'].strip() h1_match = re.search(r"^\s*#\s+(.+)", markdown_text, re.MULTILINE) if h1_match: return h1_match.group(1).strip() return None def extract_frontmatter_and_text(markdown_content: str) -> Tuple[Dict[str, Any], str]: """ Extracts frontmatter (if present) from Markdown content. Returns (frontmatter_dict, text_content_without_frontmatter). """ if FRONTMATTER_AVAILABLE: try: post = frontmatter.loads(markdown_content) return post.metadata, post.content except Exception as e: logging.debug(f"Could not parse frontmatter: {e}. Treating content as plain markdown.") return {}, markdown_content return {}, markdown_content def process_raw_ndjson_data( raw_data: Dict[str, Any], parser_config: Optional[Dict[str, Any]] = None ) -> Tuple[str, Dict[str, Any]]: """ Extracts key fields from a raw NDJSON data object and attempts to structure them into markdown_text and a source_doc_metadata dictionary compatible with ProcessedContentItem. Returns: - markdown_text (str): The main text content. - source_doc_metadata (Dict[str, Any]): Metadata dictionary. """ final_text_content = "" extracted_meta: Dict[str, Any] = {"metadatos_adicionales_fuente": {}} # Lgica para extraer texto (simplificada, asume que hay una clave principal de texto) # Debera ser configurable a travs de parser_config si es ms complejo text_keys = parser_config.get("text_property_paths", ["text", "content", "body"]) if parser_config else ["text", "content", "body"] for key in text_keys: if key in raw_data and isinstance(raw_data[key], str): final_text_content = raw_data[key] break # Extraer y mapear otros campos conocidos # El ID original del NDJSON podra ser el id_documento_fuente id_keys = parser_config.get("id_property_paths", ["id", "_id", "message_id"]) if parser_config else ["id", "_id", "message_id"] for key in id_keys: if key in raw_data: extracted_meta["id_documento_fuente_original_ndjson"] = str(raw_data[key]) # Se usar para id_documento_fuente o en metadatos adicionales break title_keys = parser_config.get("title_property_paths", ["title", "subject"]) if parser_config else ["title", "subject"] for key in title_keys: if key in raw_data and isinstance(raw_data[key], str): extracted_meta["titulo_documento"] = raw_data[key] break # Para fecha de publicacin y autor, se pueden usar claves comunes o configurables date_val = raw_data.get("date") or raw_data.get("timestamp") or raw_data.get("created_at") if date_val: extracted_meta["fecha_publicacion_documento_raw"] = date_val # parse_publication_date lo procesar author_val = raw_data.get("author") or raw_data.get("user") or raw_data.get("from") if author_val: extracted_meta["autor_documento"] = str(author_val) if not isinstance(author_val, dict) else author_val.get("name") # Todos los dems campos del NDJSON van a metadatos_adicionales_fuente for key, value in raw_data.items(): if key not in [text_keys, id_keys, title_keys, "date", "timestamp", "created_at", "author", "user", "from"]: # Evitar duplicados si ya se mapearon extracted_meta.setdefault("metadatos_adicionales_fuente", {})[key] = value return final_text_content, extracted_meta def process_text_content( markdown_text: Optional[str] = None, file_path: Optional[str] = None, # Usado principalmente para logs o como fallback source_doc_metadata: Optional[Dict[str, Any]] = None, # Metadatos del conversor raw_ndjson_object: Optional[Dict[str, Any]] = None, # Si la entrada es un objeto NDJSON force_null_publication_date: bool = False, parser_config: Optional[Dict[str, Any]] = None, # Para guiar extraccin de NDJSON pipeline_version: str = "0.1.0-default" # Versin del pipeline ) -> Optional[ProcessedContentItem]: """ Processes Markdown text (or raw NDJSON) to create a ProcessedContentItem. Handles frontmatter if present in Markdown. Returns: An instance of ProcessedContentItem, or None if essential data is missing. """ current_timestamp_iso = datetime.now(timezone.utc).isoformat() final_source_metadata = source_doc_metadata.copy() if source_doc_metadata else {} final_markdown_content = markdown_text if markdown_text is not None else "" # Si la entrada es un objeto NDJSON, procesarlo primero if raw_ndjson_object: ndjson_text, ndjson_meta = process_raw_ndjson_data(raw_ndjson_object, parser_config) final_markdown_content = ndjson_text # Sobrescribir markdown_text # Fusionar metadatos, dando preferencia a los del NDJSON si entran en conflicto # o decidiendo una estrategia (aqu ndjson_meta puede tener campos ms especficos) for key, value in ndjson_meta.items(): if key == "metadatos_adicionales_fuente": final_source_metadata.setdefault("metadatos_adicionales_fuente", {}).update(value) else: final_source_metadata[key] = value # Extraer frontmatter del contenido Markdown y fusionar con metadatos existentes frontmatter_meta, content_after_frontmatter = extract_frontmatter_and_text(final_markdown_content) final_markdown_content = content_after_frontmatter # Actualizar texto sin frontmatter if frontmatter_meta: for key, value in frontmatter_meta.items(): # Mapear claves comunes de frontmatter a campos de ProcessedContentItem si es posible if key.lower() == "title": final_source_metadata["titulo_documento"] = final_source_metadata.get("titulo_documento", value) elif key.lower() == "author": final_source_metadata["autor_documento"] = final_source_metadata.get("autor_documento", value) elif key.lower() in ["date", "published", "creation_date"]: # Dar preferencia a la fecha del frontmatter si no hay una ya de una fuente "superior" if "fecha_publicacion_documento_raw" not in final_source_metadata: # O alguna lgica de preferencia final_source_metadata["fecha_publicacion_documento_raw"] = value elif key.lower() == "tags" and isinstance(value, list): final_source_metadata.setdefault("metadatos_adicionales_fuente", {})["tags_frontmatter"] = value else: # Otros campos del frontmatter van a metadatos adicionales final_source_metadata.setdefault("metadatos_adicionales_fuente", {})[f"fm_{key}"] = value # --- Construir ProcessedContentItem --- item_id_segmento = str(uuid.uuid4()) # Determinar id_documento_fuente (prioridad: hash, luego ruta, luego ID de NDJSON original) item_id_documento_fuente = final_source_metadata.get("hash_documento_original") if not item_id_documento_fuente: item_id_documento_fuente = final_source_metadata.get("id_documento_fuente_original_ndjson") if not item_id_documento_fuente and file_path: item_id_documento_fuente = str(Path(file_path).resolve()) # Como fallback si no hay hash if not item_id_documento_fuente: # Si sigue sin ID (ej. texto puro sin archivo) item_id_documento_fuente = str(uuid.uuid4()) # Un UUID como ltimo recurso para el documento final_source_metadata.setdefault("metadatos_adicionales_fuente", {})["id_documento_generado"] = True item_titulo = extract_title_from_markdown(final_markdown_content, final_source_metadata) or \ final_source_metadata.get("titulo_documento") or \ (Path(file_path).stem if file_path else "Sin Ttulo") item_fecha_pub_str = parse_publication_date( raw_date_str=final_source_metadata.pop("fecha_publicacion_documento_raw", None), # Tomar y remover la versin raw metadata_from_file=final_source_metadata, # Pasar el resto de metadatos force_null=force_null_publication_date ) # Asegurar que los campos principales de metadatos del documento estn en el nivel superior si existen item_ruta_original = final_source_metadata.get("ruta_archivo_original", file_path) item_hash_original = final_source_metadata.get("hash_documento_original") item_autor = final_source_metadata.get("autor_documento") item_editorial = final_source_metadata.get("editorial_documento") item_isbn = final_source_metadata.get("isbn_documento") item_idioma = final_source_metadata.get("idioma_documento", "und") # Consolidar metadatos_adicionales_fuente additional_meta = final_source_metadata.get("metadatos_adicionales_fuente", {}) # Mover campos no mapeados directamente de final_source_metadata a additional_meta # para no perderlos y evitar redundancia. campos_directos_item = [ "ruta_archivo_original", "hash_documento_original", "titulo_documento", "autor_documento", "fecha_publicacion_documento", "editorial_documento", "isbn_documento", "idioma_documento", "metadatos_adicionales_fuente", "id_documento_fuente_original_ndjson", "fecha_publicacion_documento_raw" # Claves temporales o ya usadas ] for k, v in final_source_metadata.items(): if k not in campos_directos_item and k not in additional_meta : additional_meta[f"orig_{k}"] = v # Prefijo para indicar su origen si hay colisin # Placeholder para campos de segmentacin (el documento entero es un segmento) item_tipo_segmento = "documento_completo" item_orden_segmento = 0 item_jerarquia = {} # Vaco por ahora item_notas_procesamiento = additional_meta.pop("converter_notes", None) # Mover notas del conversor try: content_item = ProcessedContentItem( id_segmento=item_id_segmento, id_documento_fuente=item_id_documento_fuente, ruta_archivo_original=item_ruta_original, hash_documento_original=item_hash_original, titulo_documento=item_titulo, autor_documento=item_autor, fecha_publicacion_documento=item_fecha_pub_str, editorial_documento=item_editorial, isbn_documento=item_isbn, idioma_documento=item_idioma, metadatos_adicionales_fuente=additional_meta, texto_segmento=final_markdown_content, tipo_segmento=item_tipo_segmento, # Placeholder orden_segmento_documento=item_orden_segmento, # Placeholder jerarquia_contextual=item_jerarquia, # Placeholder longitud_caracteres_segmento=len(final_markdown_content), embedding_vectorial=None, # Se generar despus timestamp_procesamiento=current_timestamp_iso, version_pipeline_etl=pipeline_version, nombre_segmentador_usado="ninguno_aplicado_en_procesador", # Placeholder notas_procesamiento_segmento=item_notas_procesamiento ) return content_item except Exception as e: logging.error(f"Error creando ProcessedContentItem para {file_path or 'datos NDJSON'}: {e}", exc_info=True) return None if __name__ == '__main__': # --- Ejemplos de prueba --- print("--- Test 1: Markdown con Frontmatter ---") md_with_fm = """--- title: Ttulo desde Frontmatter author: Autor FM date: 2023-03-15 tags: [test, fm] custom_field: valor_custom --- Este es el contenido principal del markdown. # Un subttulo H1 aqu Otro prrafo. """ # Simular metadatos del conversor mock_converter_meta_fm = { "ruta_archivo_original": "/path/to/test_fm.md", "hash_documento_original": "fm_hash_123", "idioma_documento": "es", "metadatos_adicionales_fuente": {"converter_notes": "Conversion OK"} } processed_fm = process_text_content(markdown_text=md_with_fm, file_path="/path/to/test_fm.md", source_doc_metadata=mock_converter_meta_fm) if processed_fm: print(f"ID Segmento: {processed_fm.id_segmento}") print(f"ID Documento Fuente: {processed_fm.id_documento_fuente}") print(f"Ttulo: {processed_fm.titulo_documento}") print(f"Autor: {processed_fm.autor_documento}") print(f"Fecha Pub: {processed_fm.fecha_publicacion_documento}") print(f"Idioma: {processed_fm.idioma_documento}") print(f"Texto (primeros 50): {processed_fm.texto_segmento[:50]}...") print(f"Metadatos Adicionales Fuente: {processed_fm.metadatos_adicionales_fuente}") print(f"Notas Procesamiento: {processed_fm.notas_procesamiento_segmento}") print("-" * 20) print("\\n--- Test 2: Markdown simple sin Frontmatter ---") md_simple = """# Ttulo Principal H1 Este es el contenido. Proviene de un archivo simple. """ mock_converter_meta_simple = { "ruta_archivo_original": "/path/to/simple.md", "hash_documento_original": "simple_hash_456", "idioma_documento": "en", "titulo_documento": "Ttulo desde Conversor", # Conversor ya extrajo un ttulo "autor_documento": "Autor del Conversor", "fecha_publicacion_documento": "2022" # Conversor pudo extraer solo ao } processed_simple = process_text_content(markdown_text=md_simple, file_path="/path/to/simple.md", source_doc_metadata=mock_converter_meta_simple) if processed_simple: print(f"Ttulo: {processed_simple.titulo_documento}") print(f"Autor: {processed_simple.autor_documento}") print(f"Fecha Pub: {processed_simple.fecha_publicacion_documento}") print(f"Texto (primeros 50): {processed_simple.texto_segmento[:50]}...") print(f"Metadatos Adicionales Fuente: {processed_simple.metadatos_adicionales_fuente}") print("-" * 20) print("\\n--- Test 3: Objeto NDJSON ---") ndjson_obj = { "id": "msg123", "user": "Usuario NDJSON", "text": "Este es un mensaje desde un objeto NDJSON. Contiene informacin til.", "timestamp": "2023-01-01T12:00:00Z", "channel": "general", "custom_ndjson_field": "valor_ndjson" } # Para NDJSON, source_doc_metadata podra estar vaco o tener info general del batch processed_ndjson = process_text_content(raw_ndjson_object=ndjson_obj, parser_config={"id_property_paths": ["id"], "text_property_paths": ["text"]}) if processed_ndjson: print(f"ID Documento Fuente: {processed_ndjson.id_documento_fuente}") # Debera ser 'msg123' si se mapea print(f"Ttulo: {processed_ndjson.titulo_documento}") # Podra ser None o parte del texto print(f"Autor: {processed_ndjson.autor_documento}") print(f"Fecha Pub: {processed_ndjson.fecha_publicacion_documento}") print(f"Texto: {processed_ndjson.texto_segmento}") print(f"Metadatos Adicionales Fuente: {processed_ndjson.metadatos_adicionales_fuente}") print("-" * 20) print("\\n--- Test 4: Markdown sin info de ttulo ni fecha ---") md_no_info = "Solo un prrafo de texto.\nSin mucha estructura." mock_converter_meta_no_info = { "ruta_archivo_original": "test_no_info.md", "hash_documento_original": "no_info_hash", "idioma_documento": "es" } processed_no_info = process_text_content(markdown_text=md_no_info, file_path="test_no_info.md", source_doc_metadata=mock_converter_meta_no_info) if processed_no_info: print(f"Ttulo: {processed_no_info.titulo_documento}") # Debera ser 'test_no_info' print(f"Fecha Pub: {processed_no_info.fecha_publicacion_documento}") # Debera ser None print(f"Texto: {processed_no_info.texto_segmento}") print("-" * 20)
---

# process file

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# process file

#!/usr/bin/env python3 """ Herramienta de lnea de comandos para procesar archivos con el sistema de perfiles. Uso: python process_file.py mi_archivo.docx --profile poem_or_lyrics --output resultados.ndjson python process_file.py --list-profiles """ import os import sys import argparse import logging import signal # Nueva importacin from typing import List, Dict, Any, Optional from pathlib import Path # Asegurar que el paquete 'dataset' est en el PYTHONPATH sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))) from dataset.processing.profile_manager import ProfileManager # Definiciones para salida de consola mejorada class ConsoleStyle: """Estilos ANSI y emojis para la consola.""" HEADER = '\033[95m' BLUE = '\033[94m' CYAN = '\033[96m' GREEN = '\033[92m' YELLOW = '\033[93m' RED = '\033[91m' ENDC = '\033[0m' BOLD = '\033[1m' UNDERLINE = '\033[4m' INFO_EMOJI = "" SUCCESS_EMOJI = "" WARNING_EMOJI = "" ERROR_EMOJI = "" DEBUG_EMOJI = "" FILE_EMOJI = "" PROFILE_EMOJI = "" LIST_EMOJI = "" SAVE_EMOJI = "" def cprint(message: str, level: str = "INFO", bold: bool = False, emoji: str = None): """Imprime mensajes con estilo en la consola.""" color = "" base_emoji = "" if level == "INFO": color = ConsoleStyle.BLUE base_emoji = ConsoleStyle.INFO_EMOJI elif level == "SUCCESS": color = ConsoleStyle.GREEN base_emoji = ConsoleStyle.SUCCESS_EMOJI elif level == "WARNING": color = ConsoleStyle.YELLOW base_emoji = ConsoleStyle.WARNING_EMOJI elif level == "ERROR": color = ConsoleStyle.RED base_emoji = ConsoleStyle.ERROR_EMOJI elif level == "DEBUG": # Los mensajes de debug del logger ya tienen formato, no aplicar color aqu # o hacerlo de forma sutil si se desea. # Por ahora, los mensajes de debug directos de cprint s tendrn emoji. base_emoji = ConsoleStyle.DEBUG_EMOJI print(f"{base_emoji} DEBUG: {message}") return elif level == "HEADER": color = ConsoleStyle.HEADER # No base_emoji for header, emoji can be passed directly final_emoji = emoji if emoji else base_emoji styled_message = f"{color}{ConsoleStyle.BOLD if bold else ''}{final_emoji} {message}{ConsoleStyle.ENDC}" print(styled_message) def setup_logging(verbose: bool = False): """Configura el sistema de logging.""" # Configuracin del logger principal (raz) root_logger = logging.getLogger() # Eliminar handlers preexistentes para evitar duplicados si esta funcin se llama varias veces for handler in root_logger.handlers[:]: root_logger.removeHandler(handler) base_level = logging.DEBUG if verbose else logging.INFO # El logger raz debera tener el nivel ms permisivo que cualquiera de sus handlers. # Si queremos que FileHandler capture DEBUG pero ConsoleHandler solo INFO, el raz debe ser DEBUG. root_logger.setLevel(logging.DEBUG) # Establecer en DEBUG para permitir que los handlers filtren log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s' formatter = logging.Formatter(log_format) # Handler para la consola (stdout) console_handler = logging.StreamHandler(sys.stdout) console_handler.setLevel(base_level) # Nivel basado en verbose para la consola console_handler.setFormatter(formatter) root_logger.addHandler(console_handler) # Handler para el archivo de errores (processing_errors.log) # Capturar WARNING, ERROR, CRITICAL error_log_file = Path.cwd() / 'processing_errors.log' file_handler = logging.FileHandler(error_log_file, mode='a', encoding='utf-8') # 'a' para append file_handler.setLevel(logging.WARNING) # Solo WARNING y superiores en el archivo file_handler.setFormatter(formatter) root_logger.addHandler(file_handler) if verbose: cprint("Logging detallado activado. Los errores tambin se guardarn en processing_errors.log", level="DEBUG") else: # Incluso si no es verbose, informar sobre el archivo de log de errores si se crea/usa. # Podramos verificar si el handler se aadi o si el archivo existe. cprint(f"Los warnings y errores se guardarn en: {error_log_file}", level="INFO") # Configurar loggers especficos para debug si es verbose (esto ya estaba) if verbose: logging.getLogger("dataset.processing.segmenters.heading_segmenter").setLevel(logging.DEBUG) logging.getLogger("dataset.processing.profile_manager").setLevel(logging.DEBUG) def list_profiles(manager: ProfileManager): """Lista todos los perfiles disponibles.""" profiles = manager.list_profiles() if not profiles: cprint("No se encontraron perfiles. Verifique el directorio de perfiles.", level="WARNING") return cprint(f"Perfiles disponibles ({len(profiles)}):", level="HEADER", emoji=ConsoleStyle.LIST_EMOJI, bold=True) for profile in profiles: print(f"{ConsoleStyle.PROFILE_EMOJI} {ConsoleStyle.BOLD}{profile['name']}{ConsoleStyle.ENDC}: {profile['description']}") print(f" {ConsoleStyle.CYAN}Segmentador:{ConsoleStyle.ENDC} {profile['segmenter']}") print(f" {ConsoleStyle.CYAN}Formatos:{ConsoleStyle.ENDC} {', '.join(profile['file_types'])}") print() # Nueva estructura para las estadsticas class ProcessingStats: def __init__(self): self.total_files_attempted = 0 self.success_with_units = 0 self.success_no_units = 0 # Casos donde el procesamiento es OK pero no hay segmentos (ej. doc vaco bien manejado) self.loader_errors = 0 # Errores especficos del loader (archivo corrupto, no encontrado por loader) self.config_errors = 0 # Errores de configuracin (perfil no encontrado, etc.) self.processing_exceptions = 0 # Otras excepciones durante el pipeline (segmentador, preprocesador) self.failed_files_details = [] # Lista de tuplas (filepath, error_type, message) def add_failure(self, filepath: str, error_type: str, message: str): self.failed_files_details.append((filepath, error_type, message)) def _process_single_file(manager: ProfileManager, file_path: Path, args, base_output_path: Path = None) -> tuple[str, Optional[str]]: """Procesa un nico archivo y guarda el resultado. Args: manager: Instancia de ProfileManager. file_path: Ruta al archivo a procesar. args: Argumentos de lnea de comandos. base_output_path: Directorio base para la salida si se procesa un directorio. Returns: Tuple con (cdigo de resultado: str, mensaje de error/advertencia opcional: str) """ # DEBUG: Imprimir args.input_path y la evaluacin de is_input_dir_mode resolved_input_path_for_mode_check = Path(args.input_path).resolve() is_input_dir_mode_eval = resolved_input_path_for_mode_check.is_dir() logging.debug(f"DEBUG_PATH: args.input_path = {args.input_path}") logging.debug(f"DEBUG_PATH: resolved_input_path_for_mode_check = {resolved_input_path_for_mode_check}") logging.debug(f"DEBUG_PATH: is_input_dir_mode_eval = {is_input_dir_mode_eval}") logging.debug(f"DEBUG_PATH: file_path (actual) = {file_path}") logging.debug(f"DEBUG_PATH: args.output = {args.output}") if not file_path.exists() or not file_path.is_file(): cprint(f"Archivo no encontrado o no es un archivo vlido: {file_path}", level="ERROR") return 'CONFIG_ERROR', f"Archivo no encontrado o no es un archivo vlido: {file_path}" cprint(f"Procesando archivo: {file_path}", level="INFO", emoji=ConsoleStyle.FILE_EMOJI, bold=True) profile_name = args.profile if not profile_name: profile_name = manager.get_profile_for_file(file_path) if profile_name: cprint(f"Perfil detectado automticamente: {profile_name}", level="INFO", emoji=ConsoleStyle.PROFILE_EMOJI) else: cprint(f"No se pudo detectar un perfil adecuado para {file_path.name}. Especifique uno con --profile o verifique las extensiones.", level="ERROR") # No listamos perfiles aqu para no inundar la consola si son muchos archivos return 'CONFIG_ERROR', f"No se pudo detectar un perfil adecuado para {file_path.name}" else: cprint(f"Usando perfil: {profile_name}", level="INFO", emoji=ConsoleStyle.PROFILE_EMOJI) # Manejo del archivo de salida output_file_path: Path is_input_dir_mode = Path(args.input_path).resolve().is_dir() # Verifica si la entrada original a process_path era un dir if args.output: output_arg_path = Path(args.output).resolve() if output_arg_path.is_dir(): # CASO: --output es un directorio output_arg_path.mkdir(parents=True, exist_ok=True) if is_input_dir_mode: # Entrada original fue un dir, salida es un dir: replicar estructura relative_path = file_path.relative_to(Path(args.input_path).resolve()) output_file_path = output_arg_path / relative_path.parent / f"{file_path.stem}.ndjson" else: # Entrada original fue un archivo, salida es un dir: archivo plano en dir de salida output_file_path = output_arg_path / f"{file_path.stem}.ndjson" output_file_path.parent.mkdir(parents=True, exist_ok=True) # Asegurar que el subdirectorio tambin exista else: # CASO: --output es un nombre de archivo explcito if is_input_dir_mode: # Este caso es manejado como error en process_path, _process_single_file no debera llegar aqu con esta combinacin. # Si llega, es un error de lgica, pero para evitar un crash, se usa un fallback. cprint(f"Advertencia: Se especific --output como archivo pero la entrada es un directorio. Usando CWD para {file_path.name}", level="WARNING") output_file_path = Path.cwd() / f"{file_path.stem}.ndjson" else: # Entrada original fue archivo, salida es archivo: usar el nombre de archivo de salida provisto output_file_path = output_arg_path output_file_path.parent.mkdir(parents=True, exist_ok=True) else: # CASO: --output NO se especific # Siempre guardar en CWD si no hay --output, sin importar si es archivo o dir. output_file_path = Path.cwd() / f"{file_path.stem}.ndjson" cprint(f"Archivo de salida: {output_file_path}", level="INFO", emoji=ConsoleStyle.SAVE_EMOJI) try: segments, segmenter_stats, document_metadata = manager.process_file( file_path=str(file_path), profile_name=profile_name, output_path=str(output_file_path), encoding=args.encoding, force_content_type=args.force_type, confidence_threshold=args.confidence_threshold ) if isinstance(segments, tuple) and len(segments) == 3: segments, segmenter_stats, document_metadata = segments elif isinstance(segments, list): cprint(f"No se recibieron estadsticas del segmentador para {file_path.name}.", level="WARNING") # No consideramos esto un error fatal para la tupla de retorno, pero el log lo capturar. else: error_msg = f"Resultado inesperado del procesamiento para {file_path.name}: {type(segments)}" cprint(error_msg, level="ERROR") return 'PROCESSING_EXCEPTION', error_msg if segments: cprint(f"Se encontraron {len(segments)} unidades en {file_path.name}.", level="SUCCESS", emoji=ConsoleStyle.SUCCESS_EMOJI) if segmenter_stats: # No imprimir stats detalladas por archivo en modo directorio para no ser muy verboso # A menos que args.verbose est activado if args.verbose or not base_output_path: cprint(f"Estadsticas del Segmentador ({file_path.name}):", level="INFO", emoji=ConsoleStyle.LIST_EMOJI) for key, value in segmenter_stats.items(): cprint(f" - {key.replace('_', ' ').capitalize()}: {value}", level="INFO") if args.verbose and document_metadata: cprint(f"Metadatos del Documento ({file_path.name}):", level="DEBUG", emoji="") excluded_meta_keys = ['source_file_path', 'original_contexto', 'blocks', 'error'] for key, value in document_metadata.items(): if key not in excluded_meta_keys and value is not None: value_str = str(value) if len(value_str) > 100: value_str = value_str[:97] + "..." cprint(f" - {key.replace('_', ' ').capitalize()}: {value_str}", level="DEBUG") if args.verbose: cprint(f"Ejemplos de segmentos ({file_path.name}, primeros 12):", level="DEBUG", emoji="") for i, segment in enumerate(segments[:12]): text_preview = segment.get('title', segment.get('text', ''))[:50] print(f"{ConsoleStyle.BLUE}[{i+1}]{ConsoleStyle.ENDC} {segment.get('type', 'unknown')}: {text_preview}...") return 'SUCCESS_WITH_UNITS', None else: # Revisar si el loader report un error o advertencia especfica loader_error = document_metadata.get('error') loader_warning = document_metadata.get('warning') if loader_error: # Si el loader tuvo un error (ej. archivo corrupto), este es el mensaje principal. cprint(f"Error del cargador: {loader_error}", level="ERROR", emoji=ConsoleStyle.ERROR_EMOJI) return 'LOADER_ERROR', loader_error elif document_metadata.get('error'): # Otro tipo de error en metadata (ej. preprocesador, segmentador) # Esto podra ser un error del preprocesador o segmentador capturado en ProfileManager processing_error = document_metadata.get('error') cprint(f"Error de procesamiento: {processing_error}", level="ERROR", emoji=ConsoleStyle.ERROR_EMOJI) return 'PROCESSING_EXCEPTION', processing_error elif loader_warning: cprint(f"Advertencia: {loader_warning} (No se encontraron unidades en {file_path.name})", level="WARNING", emoji=ConsoleStyle.WARNING_EMOJI) return 'SUCCESS_NO_UNITS', loader_warning # Considerado un xito, pero sin unidades debido a una advertencia else: cprint(f"No se encontraron unidades procesables en {file_path.name} con el perfil '{profile_name}'.", level="WARNING", emoji=ConsoleStyle.WARNING_EMOJI) return 'SUCCESS_NO_UNITS', f"No se encontraron unidades con perfil '{profile_name}'" except Exception as e: # Captura de excepciones generales que ocurran dentro de _process_single_file # (antes o despus de la llamada a ProfileManager.process_file si algo ms falla aqu) error_msg = f"Excepcin inesperada en _process_single_file para {file_path.name}: {str(e)}" cprint(error_msg, level="ERROR", emoji=ConsoleStyle.ERROR_EMOJI) logging.exception(f"Detalles de la excepcin inesperada en _process_single_file para {file_path.name}:") return 'PROCESSING_EXCEPTION', str(e) def process_path(manager: ProfileManager, args: argparse.Namespace, stats: ProcessingStats) -> None: """Procesa un archivo o todos los archivos de un directorio.""" input_path = Path(args.input_path).resolve() if input_path.is_file(): files_to_process = [input_path] base_output_for_relative_path = None elif input_path.is_dir(): cprint(f"Procesando directorio: {input_path}", level="INFO", bold=True) files_to_process = sorted(list(input_path.rglob('*'))) files_to_process = [f for f in files_to_process if f.is_file()] # Solo archivos base_output_for_relative_path = Path(args.input_path).resolve() # Para calcular paths relativos if args.output and Path(args.output).is_file(): cprint(f"Error: La entrada es un directorio ('{args.input_path}') pero la salida ('{args.output}') es un archivo. Use --output con un directorio o no lo especifique.", level="ERROR") stats.config_errors += 1 # Contar como error de configuracin stats.add_failure(str(input_path), "CONFIG_ERROR", "Entrada es directorio, salida es archivo") return else: cprint(f"Ruta de entrada no vlida: {input_path}", level="ERROR") stats.config_errors += 1 stats.add_failure(str(input_path), "CONFIG_ERROR", "Ruta de entrada no vlida") return if not files_to_process: cprint("No se encontraron archivos para procesar.", level="WARNING") return cprint(f"Archivos encontrados para procesar: {len(files_to_process)}", level="INFO") stats.total_files_attempted = len(files_to_process) for file_path in files_to_process: # Aqu no incrementamos total_files_attempted porque ya se hizo con len(files_to_process) # _process_single_file se encarga de la lgica de un archivo result_code, message = _process_single_file(manager, file_path, args, base_output_for_relative_path) if result_code == 'SUCCESS_WITH_UNITS': stats.success_with_units += 1 elif result_code == 'SUCCESS_NO_UNITS': stats.success_no_units += 1 if message: # Guardar la advertencia si existe stats.add_failure(str(file_path), "SUCCESS_NO_UNITS_WARN", message) elif result_code == 'LOADER_ERROR': stats.loader_errors += 1 stats.add_failure(str(file_path), "LOADER_ERROR", message) elif result_code == 'CONFIG_ERROR': stats.config_errors += 1 stats.add_failure(str(file_path), "CONFIG_ERROR", message) elif result_code == 'PROCESSING_EXCEPTION': stats.processing_exceptions += 1 stats.add_failure(str(file_path), "PROCESSING_EXCEPTION", message) # No hay 'else' explcito; si se aade un nuevo cdigo, se ignorara aqu hasta que se maneje. # El resumen se imprimir en main despus de que esto termine. def _print_summary(stats: ProcessingStats): """Imprime el resumen del procesamiento.""" cprint("Resumen del Procesamiento:", level="HEADER", bold=True, emoji="") cprint(f"Total de archivos intentados: {stats.total_files_attempted}", level="INFO") cprint(f" {ConsoleStyle.SUCCESS_EMOJI} Con unidades: {stats.success_with_units}", level="SUCCESS") cprint(f" {ConsoleStyle.WARNING_EMOJI} Sin unidades (OK): {stats.success_no_units}", level="WARNING") cprint(f" {ConsoleStyle.ERROR_EMOJI} Errores de cargador: {stats.loader_errors}", level="ERROR") cprint(f" {ConsoleStyle.ERROR_EMOJI} Errores de configuracin: {stats.config_errors}", level="ERROR") cprint(f" {ConsoleStyle.ERROR_EMOJI} Errores de procesamiento: {stats.processing_exceptions}", level="ERROR") total_fallos = stats.loader_errors + stats.config_errors + stats.processing_exceptions if total_fallos > 0: cprint(f"Detalle de archivos con errores/advertencias ({len(stats.failed_files_details)} entradas):", level="HEADER", bold=True, emoji=" ") # Imprimir primero errores, luego advertencias de 'no unidades' for filepath, err_type, msg in sorted(stats.failed_files_details, key=lambda x: x[1] == 'SUCCESS_NO_UNITS_WARN'): if err_type == "SUCCESS_NO_UNITS_WARN": cprint(f"- {filepath}: {msg} (No se encontraron unidades)", level="WARNING") else: cprint(f"- {filepath}: [{err_type}] {msg}", level="ERROR") if stats.total_files_attempted == 0: cprint("No se proces ningn archivo.", level="INFO") def signal_handler(sig, frame): """Manejador para la seal SIGINT (Ctrl+C).""" cprint("\nProcesamiento interrumpido por el usuario (Ctrl+C). Saliendo...", level="WARNING", bold=True) # Intentar imprimir un resumen parcial si las estadsticas estn disponibles y tienen datos # Esto es un 'mejor esfuerzo' y puede que no siempre funcione dependiendo de dnde se interrumpi. # Necesitaramos que 'processing_stats' sea accesible globalmente o pasada aqu, lo cual complica. # Por ahora, solo salimos limpiamente. # Alternativamente, se podra levantar una excepcin personalizada que el bucle principal capture. sys.exit(130) # Cdigo de salida comn para interrupcin por Ctrl+C def main(): # Registrar el manejador de seal para SIGINT (Ctrl+C) signal.signal(signal.SIGINT, signal_handler) parser = argparse.ArgumentParser(description="Procesa archivos usando perfiles de segmentacin.") parser.add_argument("input_path", nargs='?', help="Ruta al archivo o directorio a procesar.") parser.add_argument("--list-profiles", action="store_true", help="Muestra los perfiles de procesamiento disponibles.") processing_options = parser.add_argument_group(f'{ConsoleStyle.GREEN}Opciones de Procesamiento{ConsoleStyle.ENDC}') processing_options.add_argument("--profile", "-p", help="Nombre del perfil a utilizar (si se omite, se intentar detectar automticamente).") processing_options.add_argument("--output", "-o", help="Ruta del archivo de salida NDJSON (si la entrada es un archivo) o directorio de salida (si la entrada es un directorio). " "Si se omite, la salida se genera junto al archivo de entrada o en el directorio de entrada respectivo.") processing_options.add_argument("--force-type", choices=["poemas", "escritos", "canciones", "capitulos"], help="Forzar un tipo de contenido especfico para el loader (ignora la deteccin automtica del loader).") processing_options.add_argument("--confidence-threshold", type=float, default=0.5, help="Umbral de confianza para segmentadores que lo soporten (ej. detector de poemas) (0.0-1.0, default: 0.5).") # Opciones generales general_options = parser.add_argument_group(f'{ConsoleStyle.YELLOW}Opciones Generales{ConsoleStyle.ENDC}') general_options.add_argument("--verbose", "-v", action="store_true", help="Muestra informacin de depuracin detallada durante el procesamiento.") general_options.add_argument("--profiles-dir", help="Ruta a un directorio de perfiles personalizado.") general_options.add_argument("--encoding", default="utf-8", help="Codificacin de caracteres del archivo de entrada (default: utf-8).") args = parser.parse_args() # Configurar logging # setup_logging se encarga de los mensajes de logging, cprint para mensajes directos del script. setup_logging(args.verbose) # Inicializar gestor de perfiles try: manager = ProfileManager(args.profiles_dir) except Exception as e: cprint(f"Error al inicializar ProfileManager: {e}", level="ERROR", bold=True) sys.exit(1) # Indica fallo if args.list_profiles: list_profiles(manager) sys.exit(0) if not args.input_path: parser.print_help() cprint("Error: Debe especificar una ruta de entrada o usar --list-profiles.", level="ERROR") sys.exit(1) # Inicializar estadsticas processing_stats = ProcessingStats() # Procesar el archivo o directorio # La funcin process_path ahora actualizar processing_stats directamente process_path(manager, args, processing_stats) # Imprimir el resumen _print_summary(processing_stats) # Cdigo de salida basado en si hubo errores graves if processing_stats.loader_errors > 0 or \ processing_stats.config_errors > 0 or \ processing_stats.processing_exceptions > 0: sys.exit(1) else: sys.exit(0) if __name__ == "__main__": main()
---

# utils

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# utils

import logging import json from typing import Any, Optional, Dict, List from pathlib import Path import re logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') logger = logging.getLogger(__name__) def get_nested_value(data: Dict[str, Any], path: str, default: Optional[Any] = None) -> Optional[Any]: """ Retrieves a value from a nested dictionary using a dot-separated path. Handles basic list indexing if specified in path (e.g., 'items[0].name'). Args: data: The dictionary to navigate. path: A dot-separated string representing the path to the desired value. List indices can be specified like 'list_name[index]'. default: The value to return if the path is not found or an error occurs. Returns: The value at the specified path, or the default value. """ if not isinstance(data, dict): return default keys = path.split('.') current_value = data for key_part in keys: if isinstance(current_value, dict): # Check for list index in key_part (e.g., "my_list[0]") if '[' in key_part and key_part.endswith(']'): list_key, index_str = key_part.split('[', 1) index_str = index_str[:-1] # Remove trailing ']' if not list_key in current_value: logging.debug(f"List key '{list_key}' not found in current dict part of path '{path}'.") return default list_val = current_value.get(list_key) if not isinstance(list_val, list): logging.debug(f"Value for key '{list_key}' is not a list in path '{path}'.") return default try: index = int(index_str) if 0 <= index < len(list_val): current_value = list_val[index] else: logging.debug(f"Index {index} out of bounds for list '{list_key}' in path '{path}'.") return default except ValueError: logging.debug(f"Invalid index '{index_str}' for list '{list_key}' in path '{path}'.") return default # Regular dictionary key elif key_part in current_value: current_value = current_value[key_part] else: logging.debug(f"Key '{key_part}' not found in current dict part of path '{path}'.") return default elif isinstance(current_value, list): # This case would typically be handled by the list index logic above. # If we reach here with a list and key_part is not an index, it's likely an error in path. logging.debug(f"Expected a dictionary to access key '{key_part}', but found a list in path '{path}'.") return default else: # Reached a non-dictionary, non-list value before exhausting the path logging.debug(f"Path traversal failed at key '{key_part}'; encountered non-dict/list value in path '{path}'.") return default return current_value def clean_filename(filename: str) -> str: """ Cleans a filename by removing or replacing invalid characters for most filesystems. This is a basic version; more robust cleaning might be needed depending on targets. """ # Remove characters that are problematic in filenames on many OSes # (except path separators if they are part of a relative path passed by mistake) # For a simple filename (not path), we can be more aggressive. cleaned = re.sub(r'[\\/*?:"<>|]',"", filename) # Replace sequences of whitespace with a single underscore cleaned = re.sub(r'\s+', '_', cleaned) # Remove leading/trailing underscores/hyphens that might result cleaned = cleaned.strip('_-') return cleaned if cleaned else "unnamed_file" def save_to_ndjson(data_list: List[Dict[str, Any]], output_file_path: str) -> None: """ Saves a list of dictionaries to an NDJSON file. Each dictionary is written as a JSON object on a new line. """ try: with open(output_file_path, 'w', encoding='utf-8') as f: for item in data_list: json.dump(item, f, ensure_ascii=False) f.write('\n') logging.info(f"Successfully saved {len(data_list)} items to NDJSON file: {output_file_path}") except IOError as e: logging.error(f"Error writing to NDJSON file {output_file_path}: {e}") except TypeError as e: logging.error(f"TypeError during NDJSON serialization (check data types): {e}") if __name__ == '__main__': print("--- Testing get_nested_value ---") test_dict = { "name": "Test Item", "details": { "type": "A", "metadata": { "id": 123, "status": "active" } }, "tags": ["tag1", "tag2", "tag3"], "items": [ {"item_id": "i1", "value": 100}, {"item_id": "i2", "value": 200, "sub_item": {"name": "SubI2"}} ] } print(f"Value for 'name': {get_nested_value(test_dict, 'name')}") # Expected: Test Item print(f"Value for 'details.metadata.id': {get_nested_value(test_dict, 'details.metadata.id')}") # Expected: 123 print(f"Value for 'details.type': {get_nested_value(test_dict, 'details.type')}") # Expected: A print(f"Value for 'tags[1]': {get_nested_value(test_dict, 'tags[1]')}") # Expected: tag2 print(f"Value for 'items[0].value': {get_nested_value(test_dict, 'items[0].value')}") # Expected: 100 print(f"Value for 'items[1].sub_item.name': {get_nested_value(test_dict, 'items[1].sub_item.name')}") # Expected: SubI2 print(f"Value for 'non.existent.path': {get_nested_value(test_dict, 'non.existent.path')}") # Expected: None print(f"Value for 'details.non_existent': {get_nested_value(test_dict, 'details.non_existent', default='Not Found')}") # Expected: Not Found print(f"Value for 'items[2].value' (out of bounds): {get_nested_value(test_dict, 'items[2].value')}") # Expected: None print(f"Value for 'items[foo]' (invalid index): {get_nested_value(test_dict, 'items[foo]')}") # Expected: None print(f"Value for 'name.non_dict_access': {get_nested_value(test_dict, 'name.non_dict_access')}") # Expected: None print("\n--- Testing clean_filename ---") filenames_to_test = [ "My Document: Final Version?.docx", "file/with/slashes.txt", # Slashes might be an issue if not part of actual path " leading and trailing spaces ", "file*with<bad>chars|pipe.pdf", " multiple spaces here " ] for fn in filenames_to_test: print(f"Original: '{fn}' -> Cleaned: '{clean_filename(fn)}'") print("\n--- Testing save_to_ndjson ---") sample_data_for_ndjson = [ {"id": 1, "name": "Alice", "city": "Wonderland"}, {"id": 2, "name": "Bob", "occupation": "Builder"}, {"id": 3, "name": "Charlie", "hobbies": ["chess", "cycling"], "meta":{"source":"test"}} ] ndjson_test_path = "test_output.ndjson" save_to_ndjson(sample_data_for_ndjson, ndjson_test_path) # Verificar manualmente el contenido de test_output.ndjson print(f"Sample NDJSON saved to {ndjson_test_path}. Please verify its content.") # os.remove(ndjson_test_path) # Opcional: limpiar archivo de prueba # --------------------------------------------------------------------------- # # GENERIC RULE EVALUATOR (JSON) # # --------------------------------------------------------------------------- # OPS = { "eq": lambda a, b: a == b, "neq": lambda a, b: a != b, "contains": lambda a, b: isinstance(a, str) and b in a, "regex": lambda a, b: isinstance(a, str) and re.search(b, a) is not None, "gte": lambda a, b: (a is not None) and (a >= b), "lte": lambda a, b: (a is not None) and (a <= b), } def _rule_passes(actual: Any, rule: Dict[str, Any]) -> bool: """ Return **True** if *actual* satisfies *rule*. rule := {path, op, value} """ op_fn = OPS.get(rule.get("op", "eq"), OPS["eq"]) return op_fn(actual, rule.get("value")) # --------------------------------------------------------------------------- # # FILTER & EXTRACT FROM A JSON/NDJSON OBJECT # # --------------------------------------------------------------------------- # def filter_and_extract_from_json_object( json_object: Dict[str, Any], text_property_paths: List[str], filter_rules: Optional[List[Dict[str, Any]]] = None, pointer_path: Optional[str] = None, date_path: Optional[str] = None, ) -> Optional[Dict[str, Any]]: """ Apply *filter_rules* and extract text/pointer/date from one JSON object. Returns **None** if the object is filtered out; otherwise a dict with: { "text", "pointer", "date_candidate", "raw_data" } """ # 1) apply include / exclude rules if filter_rules: for rule in filter_rules: actual_val = get_nested_value(json_object, rule.get("path")) passed = _rule_passes(actual_val, rule) if rule.get("exclude", False): # exclude when rule passes if passed: logger.debug(f"JSON object filtered out by EXCLUDE rule: {rule} (actual: {actual_val})") return None else: # include rule if not passed: logger.debug(f"JSON object filtered out by INCLUDE rule: {rule} (actual: {actual_val})") return None # 2) find text (first path that yields nonempty string) extracted_text: Optional[str] = None for p in text_property_paths: candidate = get_nested_value(json_object, p) if isinstance(candidate, str) and candidate.strip(): extracted_text = candidate.strip() break # if candidate is list/dict, concatenate all strings inside if isinstance(candidate, (list, dict)): temp: List[str] = [] def _dig(x): if isinstance(x, str): temp.append(x.strip()) elif isinstance(x, list): for y in x: _dig(y) elif isinstance(x, dict): for y in x.values(): _dig(y) _dig(candidate) if temp: extracted_text = "\\n\\n".join(filter(None, temp)) break if not extracted_text: logger.debug("No text content found in JSON object after checking all paths.") return None pointer = str(get_nested_value(json_object, pointer_path)) if pointer_path else None date_candidate = get_nested_value(json_object, date_path) if date_path else None logger.debug(f"JSON object passed filters. Extracted text: {'Yes' if extracted_text else 'No'}, Pointer: {pointer}, Date Candidate: {date_candidate}") return { "text": extracted_text, "pointer": pointer, "date_candidate": date_candidate, "raw_data": json_object, }
---

# app depuracion

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# app depuracion

import os import json import logging from pathlib import Path from typing import List, Dict, Any, Optional, Iterator # Importaciones locales from dataset.processing.profile_manager import ProfileManager from .data_models import ProcessedContentItem, BatchContext from .utils import get_nested_value, clean_filename, filter_and_extract_from_json_object # Para validacin de esquemas try: import jsonschema JSONSCHEMA_AVAILABLE = True except ImportError: JSONSCHEMA_AVAILABLE = False logging.warning("jsonschema library not found. Configuration validation will be skipped.") logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') logger = logging.getLogger(__name__) # --- Rutas de Configuracin y Esquemas --- project_root = Path(__file__).resolve().parents[2] config_dir = project_root / "dataset" / "config" schema_dir = config_dir / "schema" profiles_dir = config_dir / "profiles" RAW_DATA_BASE_DIR = project_root / "dataset" / "raw_data" OUTPUT_BASE_DIR = project_root / "dataset" / "output" CONTENT_PROFILES_FILE = config_dir / "content_profiles.json" JOBS_CONFIG_FILE = config_dir / "jobs_config.json" CONTENT_PROFILES_SCHEMA_FILE = schema_dir / "content_profiles.schema.json" JOBS_CONFIG_SCHEMA_FILE = schema_dir / "jobs_config.schema.json" # --- Funciones de Carga y Validacin de Configuracin --- def load_config(config_path: Path = Path(__file__).parent / "config.json") -> Dict[str, Any]: """Carga la configuracin JSON.""" try: with open(config_path, 'r', encoding='utf-8') as f: config = json.load(f) logger.info(f"Configuration loaded from {config_path}") return config except FileNotFoundError: logger.warning(f"Configuration file {config_path} not found. Using empty config.") return {} except json.JSONDecodeError: logger.error(f"Error decoding JSON from {config_path}. Using empty config.") return {} def load_json_file(file_path: Path, description: str = "JSON file") -> Optional[Any]: """Carga un archivo JSON y devuelve su contenido, o None si hay un error.""" if not file_path.exists(): logger.error(f"{description.capitalize()} not found at {file_path}") return None try: with open(file_path, 'r', encoding='utf-8') as f: data = json.load(f) logger.info(f"{description.capitalize()} loaded successfully from {file_path}") return data except json.JSONDecodeError: logger.error(f"Error decoding JSON from {file_path}. Please ensure it's valid JSON.") return None except Exception as e: logger.error(f"An unexpected error occurred while loading {file_path}: {e}") return None def validate_config(data: Any, schema: Dict[str, Any], config_name: str) -> bool: """Valida los datos de configuracin contra un esquema JSON.""" if not JSONSCHEMA_AVAILABLE: logger.warning(f"Skipping validation for {config_name} as jsonschema library is not available.") return True if data is None or schema is None: logger.error(f"Cannot validate {config_name}: data or schema is missing.") return False try: jsonschema.validate(instance=data, schema=schema) logger.info(f"{config_name} is valid according to the schema.") return True except jsonschema.exceptions.ValidationError as e: logger.error(f"Validation error in {config_name}: {e.message} (Path: {list(e.path)})") return False except Exception as e: logger.error(f"An unexpected error occurred during {config_name} validation: {e}") return False # --- Lgica de Procesamiento Principal Refactorizada --- def main(): """ Funcin principal para ejecutar el pipeline de procesamiento utilizando ProfileManager. """ logger.info("Iniciando el proceso de depuracin y datos...") content_profiles_schema = load_json_file(CONTENT_PROFILES_SCHEMA_FILE, "Content Profiles Schema") jobs_config_schema = load_json_file(JOBS_CONFIG_SCHEMA_FILE, "Jobs Config Schema") content_profiles_data = load_json_file(CONTENT_PROFILES_FILE, "Content Profiles") jobs_config_data = load_json_file(JOBS_CONFIG_FILE, "Jobs Config") if jobs_config_data is None: return profile_manager = ProfileManager(profiles_dir) jobs_to_process = [job for job in jobs_config_data if job.get("enabled", True)] if not jobs_to_process: logger.info("No hay trabajos configurados o habilitados para procesar en jobs_config.json.") return for job_config in jobs_to_process: job_id = job_config.get("job_id") author_name = job_config.get("author_name") origin_type_name = job_config.get("origin_type_name") source_directory_name = job_config.get("source_directory_name") profile_name_from_job = job_config.get("content_profile_name") if not all([job_id, author_name, origin_type_name, source_directory_name is not None, profile_name_from_job]): logger.error(f"Configuracin de job incompleta para job_id '{job_id}'. Faltan campos clave (author_name, origin_type_name, source_directory_name, content_profile_name). Saltando job.") continue logger.info(f"Procesando job: {job_id} - Perfil: {profile_name_from_job}") input_dir = RAW_DATA_BASE_DIR / source_directory_name output_subdir = OUTPUT_BASE_DIR / author_name / origin_type_name / job_id output_subdir.mkdir(parents=True, exist_ok=True) if not input_dir.is_dir(): logger.warning(f"Directorio de entrada no encontrado para el job '{job_id}': {input_dir}. Saltando job.") continue processed_files_count = 0 for input_file_path in input_dir.glob("*"): if input_file_path.is_file(): logger.info(f"Procesando archivo: {input_file_path.name} para el job {job_id}") try: profile_manager.process_file( file_path=str(input_file_path), profile_name=profile_name_from_job, job_config_dict=job_config, output_dir=str(output_subdir) ) processed_files_count += 1 except Exception as e: logger.error(f"Error procesando el archivo {input_file_path.name} para el job {job_id}: {e}", exc_info=True) if processed_files_count == 0: logger.info(f"No se encontraron archivos para procesar en {input_dir} para el job {job_id}.") else: logger.info(f"Job {job_id} completado. {processed_files_count} archivo(s) procesado(s).") logger.info("Todos los jobs han finalizado.") if __name__ == "__main__": for config_file_path in [CONTENT_PROFILES_FILE, JOBS_CONFIG_FILE, CONTENT_PROFILES_SCHEMA_FILE, JOBS_CONFIG_SCHEMA_FILE]: if not config_file_path.exists(): config_file_path.parent.mkdir(parents=True, exist_ok=True) if "schema" in config_file_path.name: config_file_path.write_text("{}", encoding='utf-8') elif "jobs_config" in config_file_path.name: config_file_path.write_text("{\"jobs\": []}", encoding='utf-8') else: config_file_path.write_text("{}", encoding='utf-8') logger.info(f"Creado archivo de configuracin vaco: {config_file_path}") OUTPUT_BASE_DIR.mkdir(parents=True, exist_ok=True) main()
---

# config

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# config

{ "carpeta_fuentes": "../fuentes", "carpeta_salidas": "../salidas", "categorias": { "poemas": { "segmentacion": "archivo" }, "canciones": { "segmentacion": "archivo" }, "mensajes": { "segmentacion": "mensaje" }, "escritos": { "segmentacion": "parrafo" }, "libros": { "segmentacion": "parrafo" } }, "formato_salida": "ndjson", "last_input_folder": "E:\\dev-projects\\biblioperson\\dataset\\fuentes", "last_output_folder": "E:\\dev-projects\\biblioperson\\dataset\\salidas", "last_unify_input_folder": "" }
---

# test heading segmenter

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# test heading segmenter

#!/usr/bin/env python3 """ Script para probar el HeadingSegmenter con un ejemplo. Uso: python test_heading_segmenter.py """ import os import sys import json from pathlib import Path # Asegurar que el paquete 'dataset' est en el PYTHONPATH sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))) from dataset.processing.segmenters.heading_segmenter import HeadingSegmenter def create_test_document(): """Crea un documento de prueba con estructura jerrquica.""" return [ # Documento de prueba con estructura de libro {"text": "# Ttulo del Libro", "is_heading": True, "heading_level": 1}, {"text": ""}, {"text": "Descripcin inicial del libro que sirve como introduccin."}, {"text": ""}, {"text": "## Captulo 1: Introduccin", "is_heading": True, "heading_level": 2}, {"text": ""}, {"text": "Este es el primer prrafo del captulo 1."}, {"text": "Este es el segundo prrafo con ms contenido para el ejemplo."}, {"text": ""}, {"text": "### 1.1 Primera Seccin", "is_heading": True, "heading_level": 3}, {"text": ""}, {"text": "Contenido de la subseccin 1.1 que explica conceptos bsicos."}, {"text": "Ms texto para esta subseccin."}, {"text": ""}, {"text": "### 1.2 Segunda Seccin", "is_heading": True, "heading_level": 3}, {"text": ""}, {"text": "Contenido de la subseccin 1.2 con informacin importante."}, {"text": ""}, {"text": "## Captulo 2: Desarrollo", "is_heading": True, "heading_level": 2}, {"text": ""}, {"text": "Este captulo contiene el desarrollo principal del tema."}, {"text": ""}, {"text": "### 2.1 Primera parte", "is_heading": True, "heading_level": 3}, {"text": ""}, {"text": "Detalle de la primera parte del desarrollo."}, {"text": ""}, {"text": "#### 2.1.1 Subseccin detallada", "is_heading": True, "heading_level": 4}, {"text": ""}, {"text": "Informacin muy especfica sobre este subtema."}, {"text": ""}, {"text": "### 2.2 Segunda parte", "is_heading": True, "heading_level": 3}, {"text": ""}, {"text": "Explicacin de la segunda parte con ms detalles."}, {"text": ""}, {"text": "## Captulo 3: Conclusiones", "is_heading": True, "heading_level": 2}, {"text": ""}, {"text": "Este es el captulo final que resume las ideas principales."}, {"text": "Contiene las conclusiones y recomendaciones finales."} ] def print_section_tree(section, level=0): """Imprime la estructura jerrquica de manera visual.""" indent = " " * level title = section.get("title", "Sin ttulo") print(f"{indent}{'' if level > 0 else ''} {title} (Nivel: {section.get('level', '?')})") # Imprimir contenido resumido content = section.get("content", []) if content: content_count = len(content) print(f"{indent}  {content_count} prrafo(s)") if content_count <= 2: # Mostrar solo si hay pocos prrafos for item in content: text = item.get("text", "") if len(text) > 50: text = text[:47] + "..." print(f"{indent}  \"{text}\"") # Procesar subsecciones recursivamente subsections = section.get("subsections", []) for subsection in subsections: print_section_tree(subsection, level + 1) def main(): # Crear segmentador con configuracin predeterminada segmenter = HeadingSegmenter() # Crear documento de prueba blocks = create_test_document() print(f"Documento de prueba creado con {len(blocks)} bloques\n") # Segmentar documento print("Procesando documento...") segments = segmenter.segment(blocks) # Analizar resultados sections = [s for s in segments if s.get("type") == "section"] paragraphs = [s for s in segments if s.get("type") == "paragraph"] headings = [s for s in segments if s.get("type") == "heading"] print(f"\nResultados de la segmentacin:") print(f"- Secciones: {len(sections)}") print(f"- Prrafos sueltos: {len(paragraphs)}") print(f"- Encabezados sueltos: {len(headings)}") # Imprimir estructura jerrquica print("\nEstructura del documento:") for section in sections: print_section_tree(section) # Guardar resultados en JSON para anlisis output_dir = Path(__file__).parent / "output" output_dir.mkdir(exist_ok=True) output_file = output_dir / "heading_segmenter_test.json" with open(output_file, 'w', encoding='utf-8') as f: json.dump(segments, f, ensure_ascii=False, indent=2) print(f"\nResultados guardados en: {output_file}") # Probar la versin plana flat_sections = segmenter._flatten_sections(sections) flat_output = output_dir / "heading_segmenter_flat.json" with open(flat_output, 'w', encoding='utf-8') as f: json.dump(flat_sections, f, ensure_ascii=False, indent=2) print(f"Versin plana guardada en: {flat_output}") return 0 if __name__ == "__main__": sys.exit(main())
---

# converters

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# converters

""" dataset/scripts/converters.py Convierte archivos de distintos formatos (DOCX, PDF, HTML, TXT, MD) a Markdown.  DOCX  pandoc  PDF  pdfminer.six  texto plano  Markdown simple  HTML  BeautifulSoup4 + markdownify  TXT/MD  lectura directa La funcin principal devuelve: (markdown_text: str, metadata: Dict[str, Any]) `metadata` incluye, cuando es posible: - 'source_title' (ttulo del documento) - 'num_pages' (PDF) - 'num_chars' (conteo caracteres del Markdown) - 'converter_notes' (warnings, etc.) """ from __future__ import annotations import subprocess import tempfile import logging import hashlib from pathlib import Path from typing import Any, Dict, Tuple import re # PDF try: from pdfminer.high_level import extract_text as pdf_extract_text from pdfminer.pdfparser import PDFParser from pdfminer.pdfdocument import PDFDocument from pdfminer.psparser import PSLiteral from pdfminer.utils import PDFDate from pdfminer.layout import LAParams from pdfminer.pdfpage import PDFPage except ImportError: pdf_extract_text = None # type: ignore PDFParser = None # type: ignore PDFDocument = None # type: ignore PSLiteral = None # type: ignore PDFDate = None # type: ignore PDFPage = None # type: ignore LAParams = None # type: ignore # HTML try: from bs4 import BeautifulSoup from markdownify import markdownify as mdify except ImportError: BeautifulSoup = None # type: ignore mdify = None # type: ignore logger = logging.getLogger(__name__) # --------------------------------------------------------------------------- # # CORE FUNCTION # # --------------------------------------------------------------------------- # def convert_file_to_markdown( file_path: str | Path, converter_config: Dict[str, Any] | None = None, ) -> Tuple[str, Dict[str, Any]]: """ Convierte `file_path` a Markdown y extrae metadatos del documento fuente. Args: file_path: ruta al archivo de entrada. converter_config: diccionario opcional para override de opciones (por ahora solo se pasa a pandoc). Returns: Tuple[str, Dict[str, Any]]: - markdown_text (str): Contenido convertido a Markdown. - source_doc_metadata (Dict[str, Any]): Diccionario con metadatos alineados con ProcessedContentItem (ej. 'titulo_documento', 'hash_documento_original', 'metadatos_adicionales_fuente'). """ path = Path(file_path) if not path.exists(): logger.error("Archivo no encontrado: %s", path) # Devolver estructura de metadatos vaca pero con nota de error return "", { "ruta_archivo_original": str(path.resolve()), "hash_documento_original": None, "titulo_documento": None, "autor_documento": None, "fecha_publicacion_documento": None, "editorial_documento": None, "isbn_documento": None, "idioma_documento": "und", # Indeterminado "metadatos_adicionales_fuente": { "converter_notes": "file_not_found", "num_chars_markdown": 0 } } ext = path.suffix.lower() # Inicializar metadatos del documento fuente source_doc_metadata: Dict[str, Any] = { "ruta_archivo_original": str(path.resolve()), "hash_documento_original": _calculate_sha256(path), "titulo_documento": path.stem, # Fallback inicial "autor_documento": None, "fecha_publicacion_documento": None, "editorial_documento": None, "isbn_documento": None, "idioma_documento": "und", # Default, loaders/processors deben verificar/establecer. "metadatos_adicionales_fuente": { "converter_notes": "", "num_chars_markdown": 0, "file_extension": ext } } md = "" # -- DOCX ---------------------------------------------------------------- if ext == ".docx": md, docx_meta = _docx_to_markdown(path, converter_config or {}) source_doc_metadata.update(docx_meta) # Actualiza con lo que pandoc pudo extraer if not source_doc_metadata.get("titulo_documento"): source_doc_metadata["titulo_documento"] = path.stem # -- PDF ----------------------------------------------------------------- elif ext == ".pdf": md, pdf_meta = _pdf_to_markdown(path) source_doc_metadata.update(pdf_meta) if not source_doc_metadata.get("titulo_documento"): source_doc_metadata["titulo_documento"] = path.stem # -- HTML / HTM ---------------------------------------------------------- elif ext in (".html", ".htm"): md = _html_to_markdown_text(path) source_doc_metadata["titulo_documento"] = _guess_html_title(path) or path.stem # -- MARKDOWN ------------------------------------------------------------ elif ext in (".md", ".markdown"): md = path.read_text(encoding="utf-8", errors="ignore") source_doc_metadata["titulo_documento"] = path.stem # -- PLAIN TEXT ---------------------------------------------------------- elif ext in (".txt", ".text", ".log"): md = path.read_text(encoding="utf-8", errors="ignore") source_doc_metadata["titulo_documento"] = path.stem else: logger.warning("Extensin %s no soportada an  se devuelve vaco.", ext) source_doc_metadata["metadatos_adicionales_fuente"]["converter_notes"] = "unsupported_extension" return "", source_doc_metadata source_doc_metadata["metadatos_adicionales_fuente"]["num_chars_markdown"] = len(md) if source_doc_metadata["idioma_documento"] == "und": current_notes = source_doc_metadata["metadatos_adicionales_fuente"].get("converter_notes", "") if current_notes: current_notes += "; idioma_documento set to 'und', requires verification" else: current_notes = "idioma_documento set to 'und', requires verification" source_doc_metadata["metadatos_adicionales_fuente"]["converter_notes"] = current_notes return md, source_doc_metadata # --------------------------------------------------------------------------- # # FORMATSPECIFIC HELPERS # # --------------------------------------------------------------------------- # def _calculate_sha256(file_path: Path) -> str: """Calcula el hash SHA256 de un archivo.""" sha256_hash = hashlib.sha256() try: with open(file_path, "rb") as f: for byte_block in iter(lambda: f.read(4096), b""): sha256_hash.update(byte_block) return sha256_hash.hexdigest() except IOError as e: logger.error("No se pudo leer el archivo para hashing %s: %s", file_path, e) return "" def _docx_to_markdown(path: Path, config: Dict[str, Any]) -> Tuple[str, Dict[str, Any]]: """Usa pandoc para convertir DOCX a Markdown y extraer metadatos bsicos.""" meta: Dict[str, Any] = {} # TODO: Investigar cmo extraer metadatos de DOCX (autor, fecha_publicacion) con pandoc. # Pandoc puede extraerlos a un AST JSON, pero no directamente con la conversin a markdown simple. # Por ahora, se usar el nombre de archivo como ttulo y otros campos quedarn vacos o None. # Considerar parsear docProps/core.xml si es necesario y no hay opcin pandoc directa. pandoc_cmd = ["pandoc", "-f", "docx", "-t", "markdown", str(path)] extra = config.get("pandoc_extra_args", []) if extra: pandoc_cmd[1:1] = extra try: completed = subprocess.run( pandoc_cmd, text=True, check=False, capture_output=True, encoding='utf-8' # Asegurar UTF-8 ) if completed.returncode != 0: logger.error("pandoc error (%s): %s", completed.returncode, completed.stderr[:200]) meta.setdefault("metadatos_adicionales_fuente", {}).setdefault("converter_notes", "") meta["metadatos_adicionales_fuente"]["converter_notes"] += f"pandoc_error: {completed.stderr[:100]}; " return "", meta return completed.stdout, meta except FileNotFoundError: logger.error("pandoc no est instalado o no est en PATH.") meta.setdefault("metadatos_adicionales_fuente", {}).setdefault("converter_notes", "") meta["metadatos_adicionales_fuente"]["converter_notes"] += "pandoc_not_found; " return "", meta except Exception as e: logger.error(f"Error inesperado al ejecutar pandoc: {e}") meta.setdefault("metadatos_adicionales_fuente", {}).setdefault("converter_notes", "") meta["metadatos_adicionales_fuente"]["converter_notes"] += f"pandoc_unexpected_error: {str(e)[:100]}; " return "", meta def _decode_pdf_string(s, default_encoding='utf-8') -> str: """Decodifica una cadena de PDF (puede ser bytes o PSLiteral).""" if isinstance(s, bytes): try: return s.decode(default_encoding, 'surrogateescape') except UnicodeDecodeError: return s.decode('latin-1', 'replace') # Fallback a latin-1 if PSLiteral is not None and isinstance(s, PSLiteral): # Los PSLiteral a menudo ya estn en una forma cercana a una cadena Python # pero su .name podra ser la representacin str que queremos. # Esto es heurstico y puede necesitar ajustes segn el contenido. s_name = getattr(s, 'name', str(s)) if isinstance(s_name, bytes): try: return s_name.decode(default_encoding, 'surrogateescape') except UnicodeDecodeError: return s_name.decode('latin-1', 'replace') return str(s_name) # Convertir a str como ltimo recurso return str(s) # Si no es bytes ni PSLiteral conocido, intentar convertir a str def _pdf_to_markdown(path: Path) -> Tuple[str, Dict[str, Any]]: """Extrae texto de PDF y lo convierte a Markdown simple. Intenta extraer metadatos.""" extracted_metadata: Dict[str, Any] = { "metadatos_adicionales_fuente": {} } num_pages = 0 if pdf_extract_text is None or PDFParser is None or PDFDocument is None: logger.error("pdfminer.six no instalado o componentes faltantes; no se puede procesar PDF.") extracted_metadata["metadatos_adicionales_fuente"]["converter_notes"] = "pdfminer_not_installed_or_incomplete; " return "", extracted_metadata text_content = "" try: with open(path, 'rb') as fp: # Extraer texto # Usar LAParams para mejorar el layout si es necesario, por defecto es simple text_content = pdf_extract_text(fp) # Extraer metadatos fp.seek(0) # Volver al inicio para el parser de metadatos parser = PDFParser(fp) doc = PDFDocument(parser) if doc.info: # doc.info es una lista de diccionarios de metadatos doc_info = doc.info[0] # Usualmente solo hay un diccionario title = doc_info.get('Title') if title: extracted_metadata['titulo_documento'] = _decode_pdf_string(title) author = doc_info.get('Author') if author: extracted_metadata['autor_documento'] = _decode_pdf_string(author) # Fechas pueden ser CreationDate o ModDate. Usaremos CreationDate si est. # El formato es D:YYYYMMDDHHMMSSOHH'mm' creation_date_str = doc_info.get('CreationDate') if PDFDate is not None and creation_date_str: try: # PDFDate.decode maneja la conversin de formato PDF Date string dt_obj = PDFDate.decode(creation_date_str) # Formatear a YYYY-MM-DD o YYYY # strftime puede fallar si el ao es muy antiguo/grande, as que proteger try: if dt_obj.year > 1: # Ao vlido extracted_metadata['fecha_publicacion_documento'] = dt_obj.strftime('%Y-%m-%d') except ValueError: if hasattr(dt_obj, 'year') and dt_obj.year > 1: extracted_metadata['fecha_publicacion_documento'] = str(dt_obj.year) # Solo ao como fallback except Exception as e_date: logger.warning(f"Error decodificando fecha PDF '{creation_date_str}': {e_date}") extracted_metadata['metadatos_adicionales_fuente']["converter_notes"] = extracted_metadata['metadatos_adicionales_fuente'].get("converter_notes","") + f"pdf_date_parse_error: {e_date}; " # Contar pginas fp.seek(0) if PDFPage is not None: try: num_pages = sum(1 for _ in PDFPage.get_pages(fp, check_extractable=True)) except Exception as e_page_count: logger.warning(f"Error contando pginas PDF: {e_page_count}") num_pages = 0 # O manejar el error como se prefiera extracted_metadata["metadatos_adicionales_fuente"]["num_pages"] = num_pages except Exception as e: logger.error("Error al procesar PDF %s: %s", path, e) extracted_metadata["metadatos_adicionales_fuente"]["converter_notes"] = extracted_metadata['metadatos_adicionales_fuente'].get("converter_notes","") + f"pdf_processing_error: {e}; " return "", extracted_metadata # Separar prrafos dobles text_content = re.sub(r"\n\s*\n", "\n\n", text_content.strip()) return text_content, extracted_metadata def _html_to_markdown_text(path: Path) -> str: """Convierte HTML  Markdown usando BeautifulSoup + markdownify.""" if BeautifulSoup is None or mdify is None: logger.error("bs4/markdownify no instalados; no se puede procesar HTML.") return "" html_content = path.read_text(encoding="utf-8", errors="ignore") soup = BeautifulSoup(html_content, "html.parser") # Eliminar scripts y estilos for tag in soup(["script", "style"]): tag.decompose() markdown_text = mdify(str(soup), heading_style="ATX") return markdown_text def _guess_html_title(path: Path) -> str | None: if BeautifulSoup is None: return None html_content = path.read_text(encoding="utf-8", errors="ignore") soup = BeautifulSoup(html_content, "html.parser") title_tag = soup.find("title") return title_tag.text.strip() if title_tag else None if __name__ == '__main__': # Crear directorio de prueba si no existe test_dir = "test_conversion_files_output" os.makedirs(test_dir, exist_ok=True) # Para probar, necesitas colocar archivos de ejemplo en un directorio, por ejemplo "sample_files" sample_files_dir = "sample_files_for_conversion" os.makedirs(sample_files_dir, exist_ok=True) print(f"Place your sample DOCX, PDF, HTML, TXT, MD files in '{sample_files_dir}' to test conversion.") # Archivo de prueba TXT (creado automticamente) sample_txt_path = os.path.join(sample_files_dir, "sample.txt") with open(sample_txt_path, "w", encoding="utf-8") as f: f.write("This is the first line of the text file.\n\nThis is a second paragraph after a blank line.\nThis line directly follows.") # Archivo de prueba HTML (creado automticamente) sample_html_path = os.path.join(sample_files_dir, "sample.html") with open(sample_html_path, "w", encoding="utf-8") as f: f.write("<html><head><title>Sample HTML Title</title></head><body><h1>Main Heading</h1><p>This is a paragraph with <b>bold</b> and <i>italic</i> text.</p><ul><li>Item 1</li><li>Item 2</li></ul></body></html>") # Lista de archivos a probar (aade tus archivos DOCX y PDF aqu manualmente) # Ejemplo: # test_file_paths = [ # os.path.join(sample_files_dir, "my_document.docx"), # os.path.join(sample_files_dir, "my_presentation.pdf"), # sample_txt_path, # sample_html_path, # os.path.join(sample_files_dir, "my_notes.md") # Asumiendo que tienes un my_notes.md # ] # Para una prueba rpida solo con los creados automticamente: test_file_paths = [sample_txt_path, sample_html_path] if DOCX_PYTHON_AVAILABLE: # Solo aadir si python-docx est para metadatos, pandoc es externo # Necesitars un DOCX de prueba real llamado "sample.docx" en sample_files_dir sample_docx_path = os.path.join(sample_files_dir, "sample.docx") if os.path.exists(sample_docx_path): test_file_paths.append(sample_docx_path) else: print(f"WARNING: DOCX test file '{sample_docx_path}' not found. Skipping DOCX test.") if pdfminer_extract_text: # Necesitars un PDF de prueba real llamado "sample.pdf" en sample_files_dir sample_pdf_path = os.path.join(sample_files_dir, "sample.pdf") if os.path.exists(sample_pdf_path): test_file_paths.append(sample_pdf_path) else: print(f"WARNING: PDF test file '{sample_pdf_path}' not found. Skipping PDF test.") for f_path in test_file_paths: if not os.path.exists(f_path): logging.warning(f"Test file {f_path} not found. Skipping.") continue print(f"\n--- Converting: {os.path.basename(f_path)} ---") markdown_content, metadata = convert_file_to_markdown(f_path) if markdown_content: print(f"Successfully converted {os.path.basename(f_path)}.") print("Extracted Metadata:", metadata) # Guardar el markdown resultante para inspeccin output_md_filename = os.path.join(test_dir, os.path.basename(f_path) + ".md") with open(output_md_filename, "w", encoding="utf-8") as out_f: out_f.write(f"# METADATA: {metadata}\n\n---\n\n{markdown_content}") print(f"Output saved to: {output_md_filename}") else: print(f"Failed to convert {os.path.basename(f_path)}.") print(f"\nCheck all converted Markdown files in the '{test_dir}' directory.") print("Ensure Pandoc is installed if you are testing DOCX files: https://pandoc.org/installing.html")
---

# chunking strategies

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# chunking strategies

import logging import re # Para CustomEGWSplitterStrategy from abc import ABC, abstractmethod from typing import Iterator, Dict, Any, Optional, Union logger = logging.getLogger(__name__) class ChunkingStrategy(ABC): """ Clase base abstracta para diferentes estrategias de divisin de contenido (chunking). """ def __init__(self, chunking_config: Optional[Dict[str, Any]] = None): """ Inicializa la estrategia con su configuracin especfica. Args: chunking_config: Diccionario de configuracin para la estrategia de chunking (ej. min_size, regex, etc.), proveniente del perfil de contenido. """ self.config = chunking_config or {} # Ejemplo de un parmetro comn que podra estar en chunking_config self.min_chunk_size = self.config.get("min_chunk_size", 50) @abstractmethod def chunk(self, content: Union[str, Dict[str, Any]], file_path: Optional[str] = None) -> Iterator[Dict[str, Any]]: """ Divide el contenido (texto o un objeto JSON) en chunks. Cada chunk se devuelve como un diccionario. - Para estrategias basadas en texto, el diccionario podra ser {'text_content': str_chunk}. - Para estrategias basadas en objetos (como JsonObjectAsItemStrategy), el diccionario devuelto es el objeto JSON en s mismo. Args: content: El contenido a dividir. Puede ser una cadena de texto completa (ej. Markdown) o un nico objeto JSON (si la fuente es json_like). file_path: Ruta opcional al archivo original, para logging o contexto. Yields: Iterador de diccionarios, donde cada diccionario representa un chunk. """ pass class ParagraphChunkerStrategy(ChunkingStrategy): """Divide el texto en prrafos (separados por lneas en blanco).""" def chunk(self, content: Union[str, Dict[str, Any]], file_path: Optional[str] = None) -> Iterator[Dict[str, Any]]: if not isinstance(content, str): logger.error(f"ParagraphChunkerStrategy expects string content, but received {type(content)} for file {file_path}. Skipping chunking for this content.") return lines = content.splitlines() current_paragraph_lines = [] for line in lines: stripped_line = line.strip() if not stripped_line: if current_paragraph_lines: paragraph_text = "\n".join(current_paragraph_lines).strip() if len(paragraph_text) >= self.min_chunk_size: yield {"text_content": paragraph_text} elif paragraph_text: logger.debug(f"Short paragraph (len {len(paragraph_text)}) found in {file_path}, smaller than min_chunk_size {self.min_chunk_size}.") yield {"text_content": paragraph_text} current_paragraph_lines = [] else: current_paragraph_lines.append(line) if current_paragraph_lines: paragraph_text = "\n".join(current_paragraph_lines).strip() if len(paragraph_text) >= self.min_chunk_size: yield {"text_content": paragraph_text} elif paragraph_text: logger.debug(f"Short paragraph (len {len(paragraph_text)}) at end of file {file_path}, smaller than min_chunk_size {self.min_chunk_size}.") yield {"text_content": paragraph_text} class WholeDocumentAsItemStrategy(ChunkingStrategy): """Trata todo el contenido del documento (texto) como un nico chunk.""" def chunk(self, content: Union[str, Dict[str, Any]], file_path: Optional[str] = None) -> Iterator[Dict[str, Any]]: if not isinstance(content, str): logger.error(f"WholeDocumentAsItemStrategy expects string content, but received {type(content)} for file {file_path}. Skipping.") return trimmed_content = content.strip() if trimmed_content: yield {"text_content": trimmed_content} else: logger.info(f"WholeDocumentAsItemStrategy found no content after stripping for file {file_path}.") class JsonObjectAsItemStrategy(ChunkingStrategy): """ Trata un objeto JSON (ya parseado) como un nico item/chunk. Esta estrategia no divide texto, sino que pasa el objeto JSON. """ def chunk(self, content: Union[str, Dict[str, Any]], file_path: Optional[str] = None, profile_cfg: Optional[Dict[str, Any]] = None) -> Iterator[Dict[str, Any]]: if not isinstance(content, dict): logger.error(f"JsonObjectAsItemStrategy expects a dictionary (JSON object), but received {type(content)} for file {file_path}. Skipping.") return yield content class CustomEGWSplitterStrategy(ChunkingStrategy): """ Divide un texto de Ellen G. White en secciones basadas en referencias tipo {PVGM 12.3}. Cada referencia se detecta con un regex configurable. chunking_config opcional: { "reference_regex": r"{\([A-Z]{2,5})\\\\s+(\\\\d{1,3}\\\\. \\\\d{1,3})}\", "keep_reference_in_text": true } """ DEFAULT_REGEX = r"{\\\\s*([A-Z]{2,5})\\\\s+(\\\\d{1,3}\\\\. \\\\d{1,3})\\\\s*}" def __init__(self, chunking_config: Optional[Dict[str, Any]] = None): super().__init__(chunking_config) pattern = self.config.get("reference_regex", self.DEFAULT_REGEX) self.keep_reference = self.config.get("keep_reference_in_text", True) try: self._ref_re = re.compile(pattern, re.MULTILINE) logger.info(f"CustomEGWSplitterStrategy initialized with regex: {pattern}") except re.error as e: logger.error(f"Invalid regex pattern for CustomEGWSplitterStrategy: {pattern}. Error: {e}. Using default regex.") self._ref_re = re.compile(self.DEFAULT_REGEX, re.MULTILINE) def chunk(self, content: Union[str, Dict[str, Any]], file_path: Optional[str] = None, profile_cfg: Optional[Dict[str, Any]] = None) -> Iterator[Dict[str, Any]]: """ Devuelve dicts con:  text_content  source_document_pointer_hint (p.ej. PVGM 12.3) """ if not isinstance(content, str): logger.error(f"CustomEGWSplitterStrategy expects string content, but received {type(content)} for file {file_path}. Skipping.") return if not content.strip(): logger.debug(f"CustomEGWSplitterStrategy received empty content for {file_path}. No chunks produced.") return matches = list(self._ref_re.finditer(content)) if not matches: # sin referencias  todo el documento como un chunk logger.debug(f"No EGW references found in {file_path} with regex. Yielding whole content as one chunk.") yield {"text_content": content.strip()} return # Recorremos las coincidencias y cortamos de ref a ref logger.debug(f"Found {len(matches)} EGW references in {file_path}.") for idx, match in enumerate(matches): ref_code = f"{match.group(1)} {match.group(2)}" # ej. PVGM 12.3 # Determinar el inicio del texto del chunk # Si keep_reference es True, el chunk comienza CON la referencia. # Si es False, comienza DESPUS de la referencia. chunk_start_pos = match.start() if self.keep_reference else match.end() # Determinar el final del texto del chunk # Es el inicio de la SIGUIENTE referencia, o el final del contenido si es la ltima ref. chunk_end_pos = matches[idx + 1].start() if idx + 1 < len(matches) else len(content) chunk_text = content[chunk_start_pos:chunk_end_pos].strip() if not chunk_text: logger.debug(f"Empty chunk generated for reference '{ref_code}' in {file_path}. Skipping.") continue yield { "text_content": chunk_text, "source_document_pointer_hint": ref_code, } # --- Registro y Factory para Estrategias --- STRATEGY_REGISTRY: Dict[str, type[ChunkingStrategy]] = { "ParagraphChunkerStrategy": ParagraphChunkerStrategy, "WholeDocumentAsItemStrategy": WholeDocumentAsItemStrategy, "JsonObjectAsItemStrategy": JsonObjectAsItemStrategy, "CustomEGWSplitterStrategy": CustomEGWSplitterStrategy, } def get_chunking_strategy(strategy_name: str, chunking_config: Optional[Dict[str, Any]] = None) -> ChunkingStrategy: """ Factory function para obtener una instancia de una ChunkingStrategy. """ strategy_class = STRATEGY_REGISTRY.get(strategy_name) if not strategy_class: logger.error(f"Unknown chunking strategy name: '{strategy_name}'. Available strategies: {list(STRATEGY_REGISTRY.keys())}") raise ValueError(f"Unknown chunking strategy: {strategy_name}") logger.info(f"Instantiating chunking strategy: {strategy_name}") return strategy_class(chunking_config)
---

# cli

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# cli

"""CLI wizard for managing Biblioperson profiles and jobs. Author: Biblioperson """ from __future__ import annotations import argparse import json import logging import re import sys from pathlib import Path from typing import Any, Dict, List, Optional # optional dependency try: import questionary except ImportError: questionary = None # type: ignore from .utils import OPS # ruleengine operators logger = logging.getLogger("biblioperson.cli") logging.basicConfig(level=logging.INFO, format="%(levelname)s - %(message)s") SCRIPT_DIR = Path(__file__).resolve().parent CONFIG_DIR = SCRIPT_DIR.parent / "config" SCHEMA_DIR = CONFIG_DIR / "schema" CONTENT_PROFILES_PATH = CONFIG_DIR / "content_profiles.json" JOBS_CONFIG_PATH = CONFIG_DIR / "jobs_config.json" CONTENT_PROFILES_SCHEMA = json.loads((SCHEMA_DIR / "content_profiles.schema.json").read_text()) JOBS_CONFIG_SCHEMA = json.loads((SCHEMA_DIR / "jobs_config.schema.json").read_text()) # --------------------------------------------------------------------------- # # helpers # # --------------------------------------------------------------------------- # def load_json(path: Path, default: Any) -> Any: if not path.exists(): save_json(path, default) try: return json.loads(path.read_text(encoding="utf-8")) except json.JSONDecodeError as err: logger.error("Invalid JSON in %s: %s", path, err) sys.exit(1) def save_json(path: Path, data: Any) -> None: path.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding="utf-8") logger.info("Saved %s", path.name) def ask(prompt: str, default: str | None = None, *, validate: str | None = None) -> str: if questionary: q = questionary.text(prompt, default=default) if validate: q = q.validate(lambda x: bool(re.match(validate, x)) or "Invalid format") return q.ask() while True: raw = input(f"{prompt} [{default or ''}]: ").strip() or (default or "") if not validate or re.match(validate, raw): return raw print("Invalid format  try again.") def yes_no(prompt: str, *, default: bool = False) -> bool: d = "y" if default else "n" resp = ask(f"{prompt} [y/n]", default=d, validate=r"^[yYnN]$") return resp.lower() == "y" def menu_choice(title: str, options: List[str], *, default: str | None = None) -> str: if questionary: return questionary.select(title, choices=options, default=default).ask() print(f"\n{title}") for i, opt in enumerate(options, 1): print(f" {i}. {opt}") while True: sel = input("Select option #: ").strip() if sel.isdigit() and 1 <= int(sel) <= len(options): return options[int(sel) - 1] # --------------------------------------------------------------------------- # # filterrule interactive wizard # # --------------------------------------------------------------------------- # def ask_filter_rules() -> Optional[List[Dict[str, Any]]]: if not yes_no("Add filtering rules for this JSON job?", default=False): return None rules: List[Dict[str, Any]] = [] while True: path = ask("JSON path (dotnotation):") op = menu_choice("Operator", list(OPS.keys())) value_str = ask("Value (will autocast bool/num/null if possible):") try: value = json.loads(value_str) except Exception: value = value_str exclude = yes_no("Exclude when rule PASSES?", default=False) rules.append({"path": path, "op": op, "value": value, "exclude": exclude}) if not yes_no("Add another rule?", default=False): break return rules or None # --------------------------------------------------------------------------- # # profile management functions # # --------------------------------------------------------------------------- # def menu_profiles(profiles: Dict[str, Any]) -> Dict[str, Any]: while True: choice = menu_choice("Content profiles", ["Create", "Edit", "Delete", "Back"]) if choice == "Create": profiles = create_profile(profiles) elif choice == "Edit": profiles = edit_profile(profiles) elif choice == "Delete": profiles = delete_profile(profiles) else: return profiles def create_profile(profiles: Dict[str, Any]) -> Dict[str, Any]: pid = ask("Internal profile ID (snakecase):", validate=r"^[a-z0-9_-]+$") if pid in profiles: logger.warning("Profile %s already exists", pid) return profiles display = ask("Display name:", default=pid) group = menu_choice("Source format group", ["json_like", "document", "presentation"]) chunk = ask("Default chunking strategy (class name):", default="ParagraphChunkerStrategy") profiles[pid] = { "profile_display_name": display, "source_format_group": group, "content_kind": "generic", "chunking_strategy_name": chunk, } logger.info("Profile %s created", pid) return profiles def select_profile(profiles: Dict[str, Any]) -> str | None: if not profiles: logger.warning("No profiles available") return None opt = menu_choice("Choose profile", list(profiles.keys()) + ["Cancel"]) return None if opt == "Cancel" else opt def edit_profile(profiles: Dict[str, Any]) -> Dict[str, Any]: pid = select_profile(profiles) if not pid: return profiles prof = profiles[pid] prof["profile_display_name"] = ask("Display name:", default=prof.get("profile_display_name")) prof["chunking_strategy_name"] = ask("Chunking strategy:", default=prof.get("chunking_strategy_name")) logger.info("Profile %s updated", pid) return profiles def delete_profile(profiles: Dict[str, Any]) -> Dict[str, Any]: pid = select_profile(profiles) if pid and yes_no(f"Really delete profile {pid}?"): profiles.pop(pid) logger.info("Profile %s removed", pid) return profiles # --------------------------------------------------------------------------- # # job management functions # # --------------------------------------------------------------------------- # def menu_jobs(jobs: List[Dict[str, Any]], profiles: Dict[str, Any]) -> List[Dict[str, Any]]: while True: choice = menu_choice("Processing jobs", ["Create", "Edit", "Delete", "Back"]) if choice == "Create": jobs = create_job(jobs, profiles) elif choice == "Edit": jobs = edit_job(jobs, profiles) elif choice == "Delete": jobs = delete_job(jobs) else: return jobs def select_job(jobs: List[Dict[str, Any]]) -> int | None: if not jobs: logger.warning("No jobs defined") return None titles = [f"{i}. {j['job_id']} ({j['author_name']})" for i, j in enumerate(jobs)] sel = menu_choice("Select job", titles + ["Cancel"]) if sel == "Cancel": return None return int(sel.split(".")[0]) def create_job(jobs: List[Dict[str, Any]], profiles: Dict[str, Any]) -> List[Dict[str, Any]]: jid = ask("Job ID (snakecase):", validate=r"^[a-z0-9_-]+$") if any(j["job_id"] == jid for j in jobs): logger.warning("Job %s already exists", jid) return jobs author = ask("Author name:") lang = ask("Language code (ISO6391):", default="es", validate=r"^[a-z]{2}$") origin = ask("Origin type (e.g. Telegram Archive):") source_dir = ask("Source directory inside raw_data/:") profile_name = menu_choice("Content profile", list(profiles.keys())) job: Dict[str, Any] = { "job_id": jid, "author_name": author, "language_code": lang, "origin_type_name": origin, "source_directory_name": source_dir, "content_profile_name": profile_name, } # extra parser rules for json_like if profiles[profile_name]["source_format_group"] == "json_like": rules = ask_filter_rules() if rules: if "parser_config" not in job: job["parser_config"] = {} job["parser_config"]["filter_rules"] = rules jobs.append(job) logger.info("Job %s created", jid) return jobs def edit_job(jobs: List[Dict[str, Any]], profiles: Dict[str, Any]) -> List[Dict[str, Any]]: idx = select_job(jobs) if idx is None: return jobs job = jobs[idx] job["author_name"] = ask("Author name:", default=job.get("author_name")) job["language_code"] = ask("Language code:", default=job.get("language_code"), validate=r"^[a-z]{2}$") profile_name = menu_choice("Content profile", list(profiles.keys()), default=job.get("content_profile_name")) job["content_profile_name"] = profile_name # Update filter rules if json_like profile if profiles[profile_name]["source_format_group"] == "json_like": if "parser_config" in job and "filter_rules" in job["parser_config"]: print("Current filter rules:", job["parser_config"]["filter_rules"]) if yes_no("Edit filter rules?"): rules = ask_filter_rules() if rules: if "parser_config" not in job: job["parser_config"] = {} job["parser_config"]["filter_rules"] = rules elif "parser_config" in job: job["parser_config"].pop("filter_rules", None) logger.info("Job %s updated", job["job_id"]) return jobs def delete_job(jobs: List[Dict[str, Any]]) -> List[Dict[str, Any]]: idx = select_job(jobs) if idx is None: return jobs job_id = jobs[idx]["job_id"] if yes_no(f"Really delete job {job_id}?"): jobs.pop(idx) logger.info("Job %s deleted", job_id) return jobs # --------------------------------------------------------------------------- # # Main entry point # # --------------------------------------------------------------------------- # def main(): parser = argparse.ArgumentParser(description="Biblioperson ETL configuration CLI") parser.add_argument("--validate", action="store_true", help="Validate existing configurations") args = parser.parse_args() profiles = load_json(CONTENT_PROFILES_PATH, {}) jobs = load_json(JOBS_CONFIG_PATH, []) if args.validate: # TODO: implement validation against schemas print("Validation not yet implemented") return while True: choice = menu_choice( "Biblioperson ETL Configuration", ["Content Profiles", "Processing Jobs", "Save and Exit"] ) if choice == "Content Profiles": profiles = menu_profiles(profiles) elif choice == "Processing Jobs": jobs = menu_jobs(jobs, profiles) else: # Save and Exit save_json(CONTENT_PROFILES_PATH, profiles) save_json(JOBS_CONFIG_PATH, jobs) logger.info("Configuration saved. Goodbye!") break if __name__ == "__main__": main()
---

#   init  

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** processing

#   init  

# Biblioperson - Dataset Processing # Mdulo para procesamiento de textos en diferentes formatos from . import segmenters __all__ = ['segmenters']
---

# profile manager

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** processing

# profile manager

import os import yaml import logging from typing import Dict, List, Any, Optional, Type from pathlib import Path import dataclasses from .segmenters.base import BaseSegmenter from .segmenters.verse_segmenter import VerseSegmenter from .segmenters.heading_segmenter import HeadingSegmenter from .loaders import BaseLoader, MarkdownLoader, NDJSONLoader, DocxLoader, txtLoader, PDFLoader, ExcelLoader, CSVLoader from .pre_processors import CommonBlockPreprocessor # A medida que se implementen, importar otros componentes: # from .loaders.base import BaseLoader # from .exporters.base import BaseExporter # etc. class ProfileManager: """ Gestor del sistema de perfiles de procesamiento. Esta clase coordina todo el pipeline de procesamiento: - Carga de perfiles YAML - Seleccin del loader adecuado - Inicializacin de segmentadores con sus configuraciones - Ejecucin de post-procesadores - Exportacin de resultados """ def __init__(self, profiles_dir: str = None): """ Inicializa el gestor de perfiles. Args: profiles_dir: Directorio donde se encuentran los perfiles YAML """ self.logger = logging.getLogger(__name__) # Directorio de perfiles por defecto if profiles_dir is None: module_dir = os.path.dirname(os.path.abspath(__file__)) profiles_dir = os.path.join(os.path.dirname(module_dir), 'config', 'profiles') self.profiles_dir = profiles_dir self.profiles = {} self._segmenter_registry = {} self._loader_registry = {} # Registrar componentes disponibles self.register_default_components() # Cargar perfiles desde directorio self.load_profiles() def register_default_components(self): """Registra los componentes por defecto del sistema.""" # Registrar segmentadores self.register_segmenter('verse', VerseSegmenter) self.register_segmenter('heading', HeadingSegmenter) # Registrar loaders self.register_loader('.md', MarkdownLoader) self.register_loader('.markdown', MarkdownLoader) self.register_loader('.ndjson', NDJSONLoader) self.register_loader('.docx', DocxLoader) self.register_loader('.txt', txtLoader) self.register_loader('.pdf', PDFLoader) self.register_loader('.xls', ExcelLoader) self.register_loader('.xlsx', ExcelLoader) self.register_loader('.xlsm', ExcelLoader) self.register_loader('.csv', CSVLoader) # Usando CSVLoader especfico para CSV self.register_loader('.tsv', CSVLoader) # Tambin para archivos TSV (valores separados por tabulaciones) def register_segmenter(self, name: str, segmenter_class: Type[BaseSegmenter]): """ Registra un segmentador en el sistema. Args: name: Nombre para referencia en perfiles segmenter_class: Clase del segmentador """ self._segmenter_registry[name] = segmenter_class self.logger.debug(f"Registrado segmentador '{name}'") def register_loader(self, extension: str, loader_class: Type[BaseLoader]): """ Registra un loader para una extensin de archivo. Args: extension: Extensin del archivo (con punto) loader_class: Clase del loader """ self._loader_registry[extension.lower()] = loader_class self.logger.debug(f"Registrado loader para {extension}") def get_loader_for_file(self, file_path: str, profile_name: str = None) -> Optional[tuple]: """ Obtiene el loader apropiado para un archivo y determina el tipo de contenido. Args: file_path: Ruta al archivo profile_name: Nombre del perfil (opcional) Returns: Tuple con (clase del loader, tipo de contenido) o None si no hay loader registrado """ extension = Path(file_path).suffix.lower() loader_class = self._loader_registry.get(extension) if not loader_class: self.logger.error(f"No hay loader registrado para extensin: {extension}") return None # Determinar el tipo de contenido basado en el perfil si est disponible content_type = 'escritos' # Valor por defecto if profile_name and profile_name in self.profiles: profile = self.profiles[profile_name] # Si el perfil especifica un tipo de contenido por defecto, usarlo if 'default_content_type' in profile: content_type = profile['default_content_type'] # Si el perfil tiene mapeo de extensiones a tipos, usarlo elif 'extension_types' in profile and extension in profile['extension_types']: content_type = profile['extension_types'][extension] # O inferir basado en el nombre del perfil elif 'poem' in profile_name or 'lyric' in profile_name: content_type = 'poemas' if 'poem' in profile_name else 'canciones' # Tambin podemos intentar inferir del nombre del archivo filename = Path(file_path).stem.lower() if not profile_name or content_type == 'escritos': if any(word in filename for word in ['poema', 'poem', 'verso']): content_type = 'poemas' elif any(word in filename for word in ['cancion', 'song', 'lyric']): content_type = 'canciones' self.logger.debug(f"Loader para {extension}: {loader_class.__name__}, tipo: {content_type}") return (loader_class, content_type) def load_profiles(self): """Carga todos los perfiles YAML del directorio configurado.""" if not os.path.exists(self.profiles_dir): self.logger.warning(f"Directorio de perfiles no encontrado: {self.profiles_dir}") return for filename in os.listdir(self.profiles_dir): if filename.endswith(('.yaml', '.yml')): profile_path = os.path.join(self.profiles_dir, filename) try: with open(profile_path, 'r', encoding='utf-8') as f: profile = yaml.safe_load(f) if 'name' in profile: self.profiles[profile['name']] = profile self.logger.info(f"Cargado perfil: {profile['name']}") else: self.logger.warning(f"Perfil sin nombre en {filename}") except Exception as e: self.logger.error(f"Error al cargar perfil {filename}: {str(e)}") def get_profile(self, profile_name: str) -> Optional[Dict[str, Any]]: """ Obtiene un perfil por su nombre. Args: profile_name: Nombre del perfil Returns: Diccionario con la configuracin del perfil o None si no existe """ return self.profiles.get(profile_name) def list_profiles(self) -> List[Dict[str, Any]]: """ Lista todos los perfiles disponibles. Returns: Lista con informacin bsica de cada perfil """ return [ { 'name': profile['name'], 'description': profile.get('description', ''), 'segmenter': profile.get('segmenter', 'unknown'), 'file_types': profile.get('file_types', []) } for profile in self.profiles.values() ] def create_segmenter(self, profile_name: str) -> Optional[BaseSegmenter]: """ Crea una instancia de segmentador segn el perfil. Args: profile_name: Nombre del perfil a usar Returns: Instancia del segmentador configurada o None si hay error """ profile = self.get_profile(profile_name) if not profile: self.logger.error(f"Perfil no encontrado: {profile_name}") return None segmenter_type = profile.get('segmenter') if not segmenter_type: self.logger.error(f"Tipo de segmentador no especificado en perfil: {profile_name}") return None if segmenter_type not in self._segmenter_registry: self.logger.error(f"Segmentador '{segmenter_type}' no registrado") return None # Configuracin para el segmentador config = {} # Copiar thresholds del perfil if 'thresholds' in profile: config['thresholds'] = profile['thresholds'] # Copiar patrones especficos for key in ['title_patterns', 'paragraph_patterns', 'section_patterns']: if key in profile: config[key] = profile[key] # Crear instancia del segmentador try: segmenter_class = self._segmenter_registry[segmenter_type] return segmenter_class(config) except Exception as e: self.logger.error(f"Error al crear segmentador '{segmenter_type}': {str(e)}") return None def process_file(self, file_path: str, profile_name: str, output_dir: Optional[str] = None, encoding: str = 'utf-8', force_content_type: Optional[str] = None, confidence_threshold: float = 0.5, job_config_dict: Optional[Dict[str, Any]] = None) -> List[Any]: """ Procesa un archivo completo usando un perfil. Args: file_path: Ruta al archivo a procesar profile_name: Nombre del perfil a usar output_dir: Directorio para guardar resultados (opcional) encoding: Codificacin para abrir el archivo (por defecto utf-8) force_content_type: Forzar un tipo especfico de contenido (ignora deteccin automtica) confidence_threshold: Umbral de confianza para deteccin de poemas (0.0-1.0) job_config_dict: Diccionario con la configuracin del job actual (opcional) Returns: Tuple con: (Lista de unidades procesadas, Estadsticas del segmentador, Metadatos del documento) """ if not os.path.exists(file_path): self.logger.error(f"Archivo no encontrado: {file_path}") # Devolver la estructura de tupla esperada por process_file.py return [], {}, {'error': f"Archivo no encontrado: {file_path}"} # 1. Obtener loader apropiado y tipo de contenido loader_result = self.get_loader_for_file(file_path, profile_name) if not loader_result: return [], {}, {'error': f"No se pudo obtener el loader para: {file_path}"} loader_class, content_type = loader_result # Si el usuario forz un tipo especfico, usarlo if force_content_type: self.logger.info(f"Forzando tipo de contenido: {force_content_type}") content_type = force_content_type # 2. Crear loader e intentar cargar el archivo raw_blocks: List[Dict[str, Any]] = [] raw_document_metadata: Dict[str, Any] = {} try: self.logger.info(f"Usando loader: {loader_class.__name__}") loader = loader_class(file_path, encoding=encoding) loaded_data = loader.load() # Asegurar que las claves existan, incluso si estn vacas raw_blocks = loaded_data.get('blocks', []) raw_document_metadata = loaded_data.get('document_metadata', {}) raw_document_metadata.setdefault('source_file_path', str(Path(file_path).absolute())) raw_document_metadata.setdefault('file_format', Path(file_path).suffix.lower()) # Si el loader report un error, lo propagamos antes del pre-procesamiento if raw_document_metadata.get('error'): self.logger.error(f"Error reportado por loader {loader_class.__name__} para {file_path}: {raw_document_metadata['error']}") return [], {}, raw_document_metadata if not raw_blocks and not raw_document_metadata.get('warning'): # Si no hay bloques y no es una advertencia de archivo vaco self.logger.info(f"Loader {loader_class.__name__} no devolvi bloques para {file_path} y no hay advertencia de archivo vaco.") except Exception as e: self.logger.error(f"Excepcin al instanciar o usar loader {loader_class.__name__} para archivo {file_path}: {str(e)}", exc_info=True) error_metadata = { 'source_file_path': str(Path(file_path).absolute()), 'file_format': Path(file_path).suffix.lower(), 'error': f"Excepcin en ProfileManager durante la carga con loader: {str(e)}" } return [], {}, error_metadata # 2.5 Aplicar Pre-procesador Comn # TODO: Considerar si la config del preprocesador debe venir del perfil YAML preprocessor_config = self.get_profile(profile_name).get('common_preprocessor_config') if self.get_profile(profile_name) else None common_preprocessor = CommonBlockPreprocessor(config=preprocessor_config) try: self.logger.info(f"Aplicando CommonBlockPreprocessor a {len(raw_blocks)} bloques.") processed_blocks, processed_document_metadata = common_preprocessor.process(raw_blocks, raw_document_metadata) self.logger.debug(f"CommonBlockPreprocessor finalizado. Bloques resultantes: {len(processed_blocks)}.") except Exception as e: self.logger.error(f"Excepcin durante CommonBlockPreprocessor para archivo {file_path}: {str(e)}", exc_info=True) # Si el preprocesador falla, usamos los datos crudos del loader pero aadimos un error raw_document_metadata['error'] = (raw_document_metadata.get('error', '') + f"; Excepcin en CommonBlockPreprocessor: {str(e)}").strip('; ') # Devolver datos crudos del loader con el error del preprocesador aadido return raw_blocks, {}, raw_document_metadata # Si el preprocesador marc un error, lo propagamos. # Los errores del loader ya se manejaron, as que esto sera un error del preprocesador. if processed_document_metadata.get('error'): self.logger.error(f"Error reportado por CommonBlockPreprocessor para {file_path}: {processed_document_metadata['error']}") return [], {}, processed_document_metadata # 3. Crear segmentador segn perfil profile = self.get_profile(profile_name) # Recargar perfil por si se modific segmenter = self.create_segmenter(profile_name) if not segmenter: # Si no se pudo crear el segmentador, devolver los bloques pre-procesados con un error. processed_document_metadata['error'] = (processed_document_metadata.get('error', '') + f"; No se pudo crear el segmentador '{profile.get('segmenter') if profile else 'desconocido'}' para el perfil '{profile_name}'").strip('; ') return processed_blocks, {}, processed_document_metadata # Configurar umbral de confianza si el segmentador lo soporta if hasattr(segmenter, 'set_confidence_threshold'): segmenter.set_confidence_threshold(confidence_threshold) self.logger.debug(f"Umbral de confianza establecido a: {confidence_threshold}") # 4. Segmentar contenido (usando los bloques pre-procesados) self.logger.info(f"Segmentando archivo: {file_path} con {len(processed_blocks)} bloques pre-procesados.") segments = segmenter.segment(blocks=processed_blocks, document_metadata_from_loader=processed_document_metadata) segmenter_stats = segmenter.get_stats() if hasattr(segmenter, 'get_stats') else {} # 5. TODO: Aplicar post-procesador si est configurado # 6. Exportar si se especific ruta de salida # Usar processed_document_metadata para la exportacin if output_dir and segments: self._export_results(segments, output_dir, processed_document_metadata) elif output_dir and not segments: # Tambin exportar si no hay segmentos pero s un output_path self.logger.info(f"No se encontraron segmentos para {file_path}, pero se exportarn metadatos a {output_dir}") self._export_results([], output_dir, processed_document_metadata) # Asegurar que se exporta metadata con error/warning # Devolver la tupla completa como espera process_file.py return segments, segmenter_stats, processed_document_metadata def _export_results(self, segments: List[Any], output_dir: str, document_metadata: Optional[Dict[str, Any]] = None): """ Exporta los resultados al formato especificado. Si segments est vaco, exporta document_metadata con un campo segments: []. Args: segments: Lista de segmentos a exportar (ahora List[ProcessedContentItem] o similar) output_dir: Directorio donde guardar los resultados document_metadata: Metadatos del documento """ import json try: with open(os.path.join(output_dir, 'results.ndjson'), 'w', encoding='utf-8') as f: if segments: # Si hay segmentos, escribir cada uno como una lnea (NDJSON) for segment in segments: f.write(json.dumps(dataclasses.asdict(segment), ensure_ascii=False) + '\n') else: # Si no hay segmentos, escribir los metadatos del documento y segments: [] output_data = document_metadata.copy() if document_metadata else {} output_data['segments'] = [] # Asegurar que el campo segments exista f.write(json.dumps(output_data, ensure_ascii=False) + '\n') self.logger.info(f"Resultados guardados en: {output_dir}") except Exception as e: self.logger.error(f"Error al exportar resultados: {str(e)}") def get_profile_for_file(self, file_path: str) -> Optional[str]: """ Sugiere el perfil ms adecuado para un archivo basado en su nombre y extensin. Args: file_path: Ruta al archivo Returns: Nombre del perfil sugerido o None si no hay sugerencia """ file_path = Path(file_path) extension = file_path.suffix.lower() filename = file_path.stem.lower() # Palabras clave que sugieren tipos de contenido poem_keywords = ['poema', 'poemas', 'poesa', 'poesas', 'versos', 'verso', 'estrofa', 'poeta', 'poem', 'poetry', 'lyric', 'lyrics', 'cancin', 'canciones'] book_keywords = ['libro', 'capitulo', 'captulos', 'book', 'chapter', 'section', 'manual', 'gua', 'documento', 'texto', 'escrito'] # Comprobar primero en el nombre del archivo for keyword in poem_keywords: if keyword in filename: self.logger.debug(f"Perfil poem_or_lyrics detectado por palabra clave en el nombre: {keyword}") return 'poem_or_lyrics' for keyword in book_keywords: if keyword in filename: self.logger.debug(f"Perfil book_structure detectado por palabra clave en el nombre: {keyword}") return 'book_structure' # Verificar si algn perfil tiene configuracin especfica para esta extensin for profile_name, profile in self.profiles.items(): if 'file_types' in profile and extension in profile['file_types']: # Si ambos perfiles soportan el tipo, poem_or_lyrics tiene preferencia para documentos personales if extension in ['.txt', '.md', '.docx']: if 'poem_or_lyrics' in self.profiles: self.logger.debug(f"Perfil poem_or_lyrics sugerido para extensin personal: {extension}") return 'poem_or_lyrics' self.logger.debug(f"Perfil {profile_name} sugerido por tipo de archivo: {extension}") return profile_name # Si todo lo dems falla, poem_or_lyrics es una buena opcin predeterminada para archivos personales if extension in ['.txt', '.md', '.docx']: if 'poem_or_lyrics' in self.profiles: self.logger.debug(f"Perfil poem_or_lyrics sugerido por defecto para archivos personales") return 'poem_or_lyrics' return None if __name__ == "__main__": # Configuracin bsica de logging logging.basicConfig(level=logging.INFO) # Ejemplo de uso manager = ProfileManager() print("Perfiles disponibles:") for profile in manager.list_profiles(): print(f"- {profile['name']}: {profile['description']}")
---

# README

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** processing

# README

# Sistema de Perfiles de Procesamiento de Documentos Este mdulo implementa un sistema modular y extensible para procesar diversos tipos de documentos (texto, JSON, DOCX, PDF, etc.) en unidades semnticas significativas mediante perfiles configurables. ## Arquitectura La arquitectura sigue un patrn de pipeline con cuatro componentes principales: ```Documento  [LOADER]  bloques  [SEGMENTER]  unidades  [POST-PROCESSOR]  [EXPORTER]  NDJSON/DB ``` ### 1. Loader Responsable de cargar cualquier formato de archivo y convertirlo en una secuencia de bloques de texto con metadatos de formato preservados: - Convierte documentos estructurados en bloques uniformes - Preserva metadatos crticos (estilo, nivel de encabezado, formato, etc.) - Hace las transformaciones formato-especficas necesarias **Loaders disponibles:** - `DocxLoader`: Archivos Microsoft Word (.docx) - `txtLoader`: Archivos de texto plano (.txt, .md) - *Prximamente:* `PdfLoader`, `JsonLoader`, etc. ### 2. Segmenter Define qu constituye una "unidad semntica" segn el perfil, aplicando reglas especficas para: - Detectar lmites de unidades (poemas, prrafos, captulos, mensajes) - Procesar bloques agrupndolos en unidades coherentes - Generar metadatos adicionales (estrofas, versos, nivel jerrquico) Implementa algoritmos como mquinas de estado para seguir los patrones definidos en `dataset/docs/ALGORITMOS_PROPUESTOS.md`. **Segmenters disponibles:** - `VerseSegmenter`: Detecta poemas y canciones - `HeadingSegmenter`: Detecta estructura jerrquica de libros y documentos - *Prximamente:* `ParagraphSegmenter`, `MessageSegmenter` ### 3. Post-Processor Aplica filtros y transformaciones genricas a las unidades detectadas: - Filtrado por longitud mnima - Normalizacin de texto (Unicode, espacios, etc.) - Deteccin de idioma - Deduplicacin - Enriquecimiento de metadatos ### 4. Exporter Exporta las unidades procesadas a diversos formatos: - NDJSON (formato histrico de Biblioperson) - SQLite - Meilisearch - Otros destinos configurables ## Perfiles y Configuracin Los perfiles de procesamiento se definen en archivos YAML en `dataset/config/profiles/`. Cada perfil especifica: ```yaml name: poem_or_lyrics description: "Detecta poemas y canciones en archivos de texto" segmenter: verse file_types: [".txt", ".md", ".docx", ".pdf"] thresholds: max_verse_length: 120 # Otros umbrales configurables title_patterns: - "^# " # Patrones regex para deteccin post_processor: text_normalizer post_processor_config: min_length: 30 metadata_map: titulo: title versos: verses_count exporter: ndjson ``` ## Mquinas de Estados Los segmentadores utilizan mquinas de estado claras para modelar el procesamiento lnea a lnea. ### VerseSegmenter Estados para la deteccin de poemas: - `SEARCH_TITLE`: Buscando ttulo de poema - `TITLE_FOUND`: Ttulo encontrado, esperando versos - `COLLECTING_VERSE`: Recolectando versos - `STANZA_GAP`: En hueco entre estrofas - `END_POEM`: Finalizando poema actual - `OUTSIDE_POEM`: Procesando otro tipo de contenido ### HeadingSegmenter Estados para la deteccin de estructura jerrquica: - `INITIAL`: Estado inicial buscando cualquier contenido - `HEADING_FOUND`: Encabezado encontrado, procesando seccin - `COLLECTING_CONTENT`: Recolectando contenido de la seccin - `SECTION_END`: Finalizando seccin actual - `OUTSIDE_SECTION`: Procesando contenido fuera de la jerarqua ## Uso Para procesar un archivo con un perfil: ```python from dataset.processing.profile_manager import ProfileManager # Inicializar gestor de perfiles profile_manager = ProfileManager("/ruta/a/perfiles") # Procesar archivo units = profile_manager.process_file( file_path="mi_documento.docx", profile_name="poem_or_lyrics", output_path="resultados.ndjson" # Opcional ) # Acceder a unidades procesadas for unit in units: print(f"Tipo: {unit['type']}") if unit['type'] == 'poem': print(f"Ttulo: {unit.get('title')}") print(f"Estrofas: {unit.get('stanzas')}") ``` ## Extensibilidad El sistema est diseado para ser fcilmente extensible: 1. **Agregar un nuevo loader**: Crear una subclase de `BaseLoader` e implementar `load()` 2. **Agregar un nuevo segmenter**: Crear una subclase de `BaseSegmenter` e implementar `segment()` 3. **Agregar un nuevo perfil**: Crear un archivo YAML en `dataset/config/profiles/` ## Ventajas sobre el sistema anterior - Separacin completa entre I/O, lgica y configuracin - Umbrales 100% parametrizables en YAML (no hard-code) - Mquinas de estado explcitas en lugar de ifs anidados - Preservacin de metadatos de formato desde el origen - Mtricas de confianza para cada unidad detectada - Pipeline modular que permite intercambiar componentes - Fcil extensin con nuevos perfiles sin modificar cdigo ## Prximos pasos - Implementar `PdfLoader` con extraccin de formato - Aadir soporte para documentos HTML - Implementar deteccin de idioma en post-procesador - Crear interfaz de usuario para ajuste de perfiles
---

# content profiles.example

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** config

# content profiles.example

{ "prose_document_docx": { "description": "Perfil para documentos de prosa general en formato DOCX (ej. libros, artculos).", "source_format_group": "document", "content_kind": "prose", "parser_config": null, "converter_config": { "docx_to_markdown_options": ["--wrap=none"] }, "chunking_strategy_name": "ParagraphChunkerStrategy", "chunking_config": { "min_chunk_size": 100, "split_on_headings": true }, "post_chunk_processors": [ "normalize_whitespace", "clean_html_remnants" ] }, "telegram_messages_json": { "description": "Perfil para exportaciones de chats de Telegram en formato JSON.", "source_format_group": "json_like", "content_kind": "messages", "parser_config": { "json_item_prefix_ijson": "messages.item", "text_property_paths": ["text_entities.item.text", "text"], "metadata_paths": { "publication_date": "date", "author_name": "from", "title": "message_id" }, "filter_rules_schema": { "type": "object", "properties": { "min_text_length": {"type": "integer"}, "required_fields": {"type": "array", "items": {"type": "string"}} } }, "deep_text_config": { "target_keys": ["text", "content", "message"], "ignore_keys": ["meta", "debug"], "min_length": 20 } }, "converter_config": null, "chunking_strategy_name": "JsonObjectAsItemStrategy", "chunking_config": {}, "post_chunk_processors": ["normalize_whitespace"] }, "egw_references_txt": { "description": "Perfil para textos de EGW en formato TXT con referencias especficas.", "source_format_group": "text_plain", "content_kind": "reference_material", "parser_config": null, "converter_config": { "text_encoding": "utf-8" }, "chunking_strategy_name": "CustomEGWSplitterStrategy", "chunking_config": { "reference_regex": "^\\s*([A-Z0-9]+)\\s+([0-9]+)\\.([0-9]+)\\s*$" }, "post_chunk_processors": ["normalize_whitespace"] } }
---

# content profiles

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** config

# content profiles

{ "perfil_docx_heading": { "description": "Procesa archivos DOCX usando DocxLoader y HeadingSegmenter", "source_format_group": "document", "content_kind": "prose", "chunking_strategy_name": "ParagraphChunkerStrategy", "chunking_config": { "min_chunk_size": 10 } } }
---

# jobs config

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** config

# jobs config

[ { "job_id": "prueba_docx_headings_01", "author_name": "autor_prueba", "language_code": "es", "source_directory_name": "autor_prueba/documentos_generales", "content_profile_name": "perfil_docx_heading", "origin_type_name": "documentos_generales", "acquisition_date": null, "force_null_publication_date": false } ]
---

# ejemplo

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** prueba_loaders

# ejemplo

# Ttulo del documento Markdown Este es un **ejemplo** de archivo *Markdown*. ## Una seccin - Elemento 1 - Elemento 2 ### Subseccin Texto de la subseccin.
---

# ejemplo

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** prueba_loaders

# ejemplo

Ttulo del documento Este es un ejemplo de archivo de texto. Contiene mltiples lneas para probar el txtLoader. Y tambin tiene mltiples prrafos separados por lneas en blanco.
---

# ejemplo

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** prueba_loaders

# ejemplo

Nombre,Edad,Ciudad Juan Prez,32,Madrid Mara Lpez,28,Barcelona Carlos Ruiz,45,Valencia
---

# heading segmenter test

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** output

# heading segmenter test

[ { "type": "section", "title": "Ttulo del Libro", "level": 1, "content": [ { "type": "paragraph", "text": "Descripcin inicial del libro que sirve como introduccin." } ], "subsections": [ { "type": "section", "title": "Captulo 1: Introduccin", "level": 2, "content": [ { "type": "paragraph", "text": "Este es el primer prrafo del captulo 1." }, { "type": "paragraph", "text": "Este es el segundo prrafo con ms contenido para el ejemplo." } ], "subsections": [ { "type": "section", "title": "Primera Seccin", "level": 3, "content": [ { "type": "paragraph", "text": "Contenido de la subseccin 1.1 que explica conceptos bsicos." }, { "type": "paragraph", "text": "Ms texto para esta subseccin." } ], "subsections": [] } ] } ] }, { "type": "section", "title": "Captulo 2: Desarrollo", "level": 2, "content": [ { "type": "paragraph", "text": "Este captulo contiene el desarrollo principal del tema." } ], "subsections": [ { "type": "section", "title": "Primera parte", "level": 3, "content": [ { "type": "paragraph", "text": "Detalle de la primera parte del desarrollo." } ], "subsections": [ { "type": "section", "title": "Subseccin detallada", "level": 4, "content": [ { "type": "paragraph", "text": "Informacin muy especfica sobre este subtema." } ], "subsections": [] } ] } ] }, { "type": "section", "title": "Captulo 3: Conclusiones", "level": 2, "content": [ { "type": "paragraph", "text": "Este es el captulo final que resume las ideas principales." }, { "type": "paragraph", "text": "Contiene las conclusiones y recomendaciones finales." } ], "subsections": [] } ]
---

# heading segmenter flat

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** output

# heading segmenter flat

[ { "id": 1, "type": "section", "title": "Ttulo del Libro", "level": 1, "parent_id": null, "content": [ { "type": "paragraph", "text": "Descripcin inicial del libro que sirve como introduccin." } ] }, { "id": 2, "type": "section", "title": "Captulo 1: Introduccin", "level": 2, "parent_id": 1, "content": [ { "type": "paragraph", "text": "Este es el primer prrafo del captulo 1." }, { "type": "paragraph", "text": "Este es el segundo prrafo con ms contenido para el ejemplo." } ] }, { "id": 3, "type": "section", "title": "Primera Seccin", "level": 3, "parent_id": 2, "content": [ { "type": "paragraph", "text": "Contenido de la subseccin 1.1 que explica conceptos bsicos." }, { "type": "paragraph", "text": "Ms texto para esta subseccin." } ] }, { "id": 4, "type": "section", "title": "Captulo 2: Desarrollo", "level": 2, "parent_id": null, "content": [ { "type": "paragraph", "text": "Este captulo contiene el desarrollo principal del tema." } ] }, { "id": 5, "type": "section", "title": "Primera parte", "level": 3, "parent_id": 4, "content": [ { "type": "paragraph", "text": "Detalle de la primera parte del desarrollo." } ] }, { "id": 6, "type": "section", "title": "Subseccin detallada", "level": 4, "parent_id": 5, "content": [ { "type": "paragraph", "text": "Informacin muy especfica sobre este subtema." } ] }, { "id": 7, "type": "section", "title": "Captulo 3: Conclusiones", "level": 2, "parent_id": null, "content": [ { "type": "paragraph", "text": "Este es el captulo final que resume las ideas principales." }, { "type": "paragraph", "text": "Contiene las conclusiones y recomendaciones finales." } ] } ]
---

# heading segmenter

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** segmenters

# heading segmenter

import re from enum import Enum from typing import List, Dict, Any, Optional import logging from .base import BaseSegmenter class HeadingState(Enum): """Estados posibles durante el procesamiento de estructura jerrquica.""" INITIAL = 1 # Estado inicial, buscando cualquier contenido HEADING_FOUND = 2 # Encabezado encontrado, procesando seccin COLLECTING_CONTENT = 3 # Recolectando contenido de la seccin SECTION_END = 4 # Finalizando seccin actual OUTSIDE_SECTION = 5 # Fuera de seccin estructurada class HeadingSegmenter(BaseSegmenter): """ Segmentador para estructuras jerrquicas como libros y documentos con captulos/secciones. Este segmentador implementa las reglas descritas en ALGORITMOS_PROPUESTOS.md para la deteccin de encabezados y estructura jerrquica en documentos. """ def __init__(self, config: Optional[Dict[str, Any]] = None): super().__init__(config or {}) # Configurar thresholds con valores por defecto si no estn en config self.thresholds = self.config.get('thresholds', {}) self.max_heading_length = self.thresholds.get('max_heading_length', 150) self.max_section_depth = self.thresholds.get('max_section_depth', 3) self.min_heading_content = self.thresholds.get('min_heading_content', 2) self.max_empty_after_heading = self.thresholds.get('max_empty_after_heading', 3) # Patrones para detectar numeracin self.number_pattern = re.compile(r'^(\d+)(\.\d+)*\.?\s+') # Logger para depuracin self.logger = logging.getLogger(__name__) # Configuracin extra para debug self.debug_mode = self.config.get('debug', False) # Contador para estadsticas self.stats = { "blocks_total": 0, "headings_detected": 0, "headings_by_pattern": {}, "headings_by_format": 0, "headings_by_heuristic": 0 } def is_heading(self, block: Dict[str, Any]) -> bool: """ Determina si un bloque es un encabezado SOLO usando seales estructurales. Args: block: Bloque de texto con metadatos Returns: True si parece un encabezado, False en caso contrario """ self.stats["blocks_total"] += 1 # Si ya viene marcado como heading desde el loader, es encabezado if block.get('is_heading', False): self.stats["headings_by_format"] += 1 return True text = block.get('text', '').strip() if not text: return False # Seal estructural: longitud mxima if len(text) > self.max_heading_length: if self.debug_mode: self.logger.debug(f"Rechazado por longitud: {text[:50]}...") return False # Seal estructural: formato visual if block.get('is_bold', False) or block.get('is_centered', False): self.stats["headings_by_format"] += 1 self.logger.debug(f"Encabezado detectado por formato visual: {text[:80]}...") return True # Seal estructural: todo en maysculas y corto if text.isupper() and len(text) < 100: self.stats["headings_detected"] += 1 self.stats["headings_by_heuristic"] += 1 self.logger.debug(f"Encabezado detectado por maysculas: {text}") return True # Seal estructural: lneas cortas sin punto final if len(text) < 80 and not text.endswith('.') and not text.endswith(','): words = text.split() if 2 <= len(words) <= 8 and words[0][0].isupper(): # Todas las palabras capitalizadas all_capitalized = all(w[0].isupper() for w in words if len(w) > 2) if all_capitalized: self.stats["headings_detected"] += 1 self.stats["headings_by_heuristic"] += 1 self.logger.debug(f"Encabezado detectado por capitalizacin: {text}") return True # Formato "Tema - Explicacin" if " - " in text and text.index(" - ") < 40: self.stats["headings_detected"] += 1 self.stats["headings_by_heuristic"] += 1 self.logger.debug(f"Encabezado detectado por guion: {text}") return True # Termina con dos puntos y es corto if text.endswith(':') and len(text) < 40: self.stats["headings_detected"] += 1 self.stats["headings_by_heuristic"] += 1 self.logger.debug(f"Encabezado detectado por dos puntos: {text}") return True return False def get_heading_level(self, block: Dict[str, Any]) -> int: """ Determina el nivel jerrquico de un encabezado. Args: block: Bloque de texto con metadatos Returns: Nivel jerrquico (1-6, donde 1 es el nivel superior) """ # Si el loader ya determin el nivel, usarlo if 'heading_level' in block and block['heading_level'] > 0: return min(block['heading_level'], 6) text = block.get('text', '').strip() # Contar '#' para headings de Markdown if text.startswith('#'): level = 0 for char in text: if char == '#': level += 1 else: break return min(level, 6) # Contar niveles en numeracin decimal match = self.number_pattern.match(text) if match: # Contar nmero de puntos en la numeracin numbering = match.group(0) level = numbering.count('.') + 1 return min(level, 6) # Todo en maysculas suele ser de nivel superior if text.isupper() or (text == text.upper() and len(text) > 3): return 1 # Si empieza con artculos en maysculas (LA, EL, LOS, LAS) if re.match(r'^(LA|EL|LOS|LAS)\s', text): return 1 # Ttulos ms cortos suelen ser de mayor nivel if len(text) < 30: return 1 elif len(text) < 50: return 2 else: return 3 # Por defecto, nivel 1 (ttulo principal) return 1 def extract_title(self, block: Dict[str, Any]) -> str: """ Extrae el ttulo limpio de un bloque de encabezado. Args: block: Bloque de texto con metadatos Returns: Texto del ttulo sin marcadores ni numeracin """ text = block.get('text', '').strip() # Eliminar marcadores de Markdown if text.startswith('#'): # Contar '#' y eliminarlos count = 0 for char in text: if char == '#': count += 1 else: break text = text[count:].lstrip() # Eliminar numeracin match = self.number_pattern.match(text) if match: text = text[len(match.group(0)):].lstrip() # Eliminar puntos suspensivos al final (comunes en el documento de ejemplo) if text.endswith('...'): text = text[:-3].rstrip() return text def segment(self, blocks: List[Dict[str, Any]]) -> List[Dict[str, Any]]: """ Segmenta bloques en estructura jerrquica usando una mquina de estados. Args: blocks: Lista de bloques de texto con metadatos Returns: Lista de unidades semnticas (secciones, subsecciones, prrafos) """ self.logger.info(f"HeadingSegmenter.segment received {len(blocks)} blocks to process.") # Log added for debugging # Reiniciar estadsticas para este procesamiento self.stats = { "blocks_total": 0, "headings_detected": 0, "headings_by_pattern": {}, "headings_by_format": 0, "headings_by_heuristic": 0 } # Imprimir informacin de bloques para debug if self.debug_mode: self.logger.debug(f"Procesando {len(blocks)} bloques") for i, block in enumerate(blocks[:20]): # Slo imprimir los primeros 20 para no saturar text = block.get('text', '').strip() self.logger.debug(f"Bloque {i+1}: {text[:100]}...") segments = [] # Variables para la seccin actual current_section = None current_content = [] section_stack = [] # Para mantener jerarqua consecutive_empty = 0 # Estado inicial state = HeadingState.INITIAL # Procesamiento basado en estados for i, block in enumerate(blocks): text = block.get('text', '').strip() is_empty = not text # Actualizar contador de lneas vacas if is_empty: consecutive_empty += 1 # No procesar ms lgica para lneas vacas, solo actualizar contador continue else: consecutive_empty = 0 # Resetear contador de lneas vacas is_heading = self.is_heading(block) # Depuracin self.logger.debug(f"Procesando bloque {i}: {'HEADING' if is_heading else 'CONTENT'} - {text[:80]}") # Transiciones de estado y acciones if state == HeadingState.INITIAL: if is_heading: # Encabezado encontrado, iniciar nueva seccin level = self.get_heading_level(block) title = self.extract_title(block) current_section = { "type": "section", "title": title, "level": level, "content": [], "subsections": [] } state = HeadingState.HEADING_FOUND else: # Texto normal fuera de seccin estructurada segments.append({ "type": "paragraph", "text": text }) elif state == HeadingState.HEADING_FOUND: if is_heading: # Otro encabezado justo despus de un encabezado (sin contenido) level = self.get_heading_level(block) prev_level = current_section["level"] if level > prev_level: # Es una subseccin del encabezado anterior section_stack.append(current_section) # Crear nueva subseccin title = self.extract_title(block) current_section = { "type": "section", "title": title, "level": level, "content": [], "subsections": [] } else: # Es un encabezado al mismo nivel o superior # El anterior era solo un ttulo sin contenido if section_stack: # Verificar y ajustar la jerarqua while section_stack and level <= section_stack[-1]["level"]: parent = section_stack.pop() if not parent["content"] and not parent["subsections"]: # Seccin vaca, aadir como prrafo segments.append({ "type": "heading", "text": parent["title"], "level": parent["level"] }) else: # Aadir seccin completa parent["subsections"].append(current_section) current_section = parent if section_stack: # Si queda un padre vlido, aadir seccin actual parent = section_stack[-1] parent["subsections"].append(current_section) else: # No hay padre, aadir a segmentos principales segments.append(current_section) else: # No hay jerarqua, aadir como encabezado simple segments.append({ "type": "heading", "text": current_section["title"], "level": current_section["level"] }) # Crear nueva seccin con el encabezado actual title = self.extract_title(block) level = self.get_heading_level(block) current_section = { "type": "section", "title": title, "level": level, "content": [], "subsections": [] } else: # Contenido despus de un encabezado, comenzar a recolectar current_section["content"].append(text) state = HeadingState.COLLECTING_CONTENT elif state == HeadingState.COLLECTING_CONTENT: if is_heading: # Nuevo encabezado encontrado mientras recolectbamos contenido level = self.get_heading_level(block) prev_level = current_section["level"] # Finalizar seccin actual min_content_required = max(1, self.min_heading_content) # Usar al menos 1 if len(current_section["content"]) >= min_content_required or current_section["subsections"]: # La seccin tiene suficiente contenido, procesarla normalmente if level > prev_level: # Es una subseccin section_stack.append(current_section) title = self.extract_title(block) current_section = { "type": "section", "title": title, "level": level, "content": [], "subsections": [] } else: # Mismo nivel o superior, cerrar secciones segn sea necesario while section_stack and level <= section_stack[-1]["level"]: parent = section_stack.pop() parent["subsections"].append(current_section) current_section = parent # Aadir la seccin actual a donde corresponda if section_stack: parent = section_stack[-1] parent["subsections"].append(current_section) # Crear nueva seccin con el encabezado actual title = self.extract_title(block) current_section = { "type": "section", "title": title, "level": level, "content": [], "subsections": [] } else: # No hay padre, aadir a segmentos principales segments.append(current_section) # Crear nueva seccin con el encabezado actual title = self.extract_title(block) current_section = { "type": "section", "title": title, "level": level, "content": [], "subsections": [] } else: # Seccin con poco contenido, tratar el contenido como prrafos independientes for content in current_section["content"]: segments.append({ "type": "paragraph", "text": content }) # Crear nueva seccin con el encabezado actual title = self.extract_title(block) current_section = { "type": "section", "title": title, "level": level, "content": [], "subsections": [] } state = HeadingState.HEADING_FOUND else: # Seguir recolectando contenido current_section["content"].append(text) # Procesar cualquier seccin pendiente if current_section: # Cerrar todas las secciones abiertas while section_stack: parent = section_stack.pop() parent["subsections"].append(current_section) current_section = parent # Aadir la seccin final segments.append(current_section) # Mostrar estadsticas de procesamiento self.logger.info(f"Estadsticas de procesamiento:") self.logger.info(f" Total bloques procesados: {self.stats['blocks_total']}") self.logger.info(f" Encabezados detectados: {self.stats['headings_detected']}") self.logger.info(f" Por formato visual: {self.stats['headings_by_format']}") self.logger.info(f" Por heursticas: {self.stats['headings_by_heuristic']}") if self.stats['headings_by_pattern']: self.logger.info(" Por patrones:") for pattern, count in self.stats['headings_by_pattern'].items(): if count > 0: pattern_short = pattern[:30] + "..." if len(pattern) > 30 else pattern self.logger.info(f" - {pattern_short}: {count}") if not segments: self.logger.warning("No se encontraron secciones ni prrafos. Verificar la configuracin.") # Como fallback, si no hay secciones, tratar cada bloque como prrafo if len(blocks) > 0: self.logger.info("Aplicando fallback: tratando cada bloque como prrafo") for block in blocks: text = block.get('text', '').strip() if text: segments.append({ "type": "paragraph", "text": text }) # Procesar las secciones y convertirlas al formato final return self._post_process_segments(segments) def _post_process_segments(self, segments: List[Dict[str, Any]]) -> List[Dict[str, Any]]: """ Procesa las secciones despus de la segmentacin inicial. Args: segments: Lista de segmentos iniciales Returns: Lista de segmentos procesados """ # Aplanar la estructura si se configur como flat=True if self.config.get("flat", False): return self._flatten_sections(segments) # Convertir contenido en texto for segment in segments: if segment["type"] == "section": if "content" in segment and isinstance(segment["content"], list): segment["content"] = "\n\n".join(segment["content"]) # Procesar subsecciones recursivamente if "subsections" in segment: segment["subsections"] = self._post_process_segments(segment["subsections"]) return segments def _flatten_sections(self, sections: List[Dict[str, Any]]) -> List[Dict[str, Any]]: """ Aplana la estructura jerrquica de secciones a una lista plana. Args: sections: Lista de secciones Returns: Lista plana de secciones """ flat_sections = [] section_id = 0 # Funcin recursiva para procesar secciones def process_section(section, parent_id=None): nonlocal section_id current_id = section_id section_id += 1 # Crear entrada plana flat_section = { "id": current_id, "parent_id": parent_id, "type": section["type"] } # Copiar atributos relevantes for key in ["title", "level", "content", "text"]: if key in section: flat_section[key] = section[key] flat_sections.append(flat_section) # Procesar subsecciones if "subsections" in section: for subsection in section["subsections"]: process_section(subsection, current_id) # Procesar todas las secciones de primer nivel for section in sections: if section["type"] == "section": process_section(section) else: # Para elementos no-seccin (prrafos, etc.), aadirlos directamente flat_sections.append(section) return flat_sections
---

# base

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** segmenters

# base

from abc import ABC, abstractmethod from typing import Dict, List, Any, Optional class BaseSegmenter(ABC): """ Clase base para todos los segmentadores. Los segmentadores implementan algoritmos para dividir bloques de texto en unidades semnticas coherentes (poemas, prrafos, captulos, etc.). """ def __init__(self, config: Optional[Dict[str, Any]] = None): """ Inicializa el segmentador con su configuracin. Args: config: Diccionario de configuracin con thresholds y otros parmetros """ self.config = config if config is not None else {} @abstractmethod def segment(self, data: Any) -> List[Dict[str, Any]]: """ Segmenta bloques de texto en unidades semnticas. Este mtodo debe ser implementado por todas las subclases. Args: data: Datos de entrada para la segmentacin Returns: Lista de unidades semnticas (poemas, prrafos, etc.) """ raise NotImplementedError("Las subclases deben implementar segment()") def get_stats(self) -> Dict[str, Any]: """Devuelve estadsticas de la ltima operacin de segmentacin.""" # Las clases hijas deben sobrescribir esto si tienen estadsticas especficas. return {}
---

#   init  

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** segmenters

#   init  

from .base import BaseSegmenter from .verse_segmenter import VerseSegmenter from .heading_segmenter import HeadingSegmenter __all__ = ['BaseSegmenter', 'VerseSegmenter', 'HeadingSegmenter']
---

# verse segmenter

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** segmenters

# verse segmenter

import re from enum import Enum from typing import List, Dict, Any, Optional import math import logging from .base import BaseSegmenter class VerseState(Enum): """Estados posibles durante el procesamiento de poemas.""" SEARCH_TITLE = 1 # Buscando ttulo de poema TITLE_FOUND = 2 # Ttulo encontrado, esperando versos iniciales COLLECTING_VERSE = 3 # Recolectando versos de un poema STANZA_GAP = 4 # En hueco entre estrofas END_POEM = 5 # Finalizando poema actual OUTSIDE_POEM = 6 # Fuera de poema (procesando otro tipo de contenido) class VerseSegmenter(BaseSegmenter): """ Segmentador para poemas y canciones basado en una mquina de estados. Este segmentador implementa las reglas descritas en ALGORITMOS_PROPUESTOS.md para la deteccin de poemas, versos y estrofas. """ def __init__(self, config: Optional[Dict[str, Any]] = None): super().__init__(config or {}) # Configurar logger self.logger = logging.getLogger(__name__) # Configurar thresholds con valores por defecto si no estn en config self.thresholds = self.config.get('thresholds', {}) self.max_verse_length = self.thresholds.get('max_verse_length', 120) self.max_title_length = self.thresholds.get('max_title_length', 80) self.min_consecutive_verses = self.thresholds.get('min_consecutive_verses', 3) self.min_stanza_verses = self.thresholds.get('min_stanza_verses', 2) self.max_empty_in_stanza = self.thresholds.get('max_empty_in_stanza', 2) self.min_empty_between_stanzas = self.thresholds.get('min_empty_between_stanzas', 2) self.max_empty_between_stanzas = self.thresholds.get('max_empty_between_stanzas', 3) self.max_empty_end_poem = self.thresholds.get('max_empty_end_poem', 3) self.max_space_ratio = self.thresholds.get('max_space_ratio', 0.35) # Umbral de confianza para poemas self.confidence_threshold = self.thresholds.get('confidence_threshold', 0.5) # Patrones para deteccin de ttulo y estilos self.title_patterns = self.config.get('title_patterns', [ r'^# ', # Markdown H1 r'^\* ', # Asterisco inicial r'^> ', # Cita r'^[A-Z ]{4,}:$' # TTULO: (todo maysculas seguido de dos puntos) ]) self.title_regex = re.compile('|'.join(self.title_patterns)) # Patrn para acordes self.chord_pattern = re.compile(r'^\s*\[[A-G][#b]?m?[0-9]?(\([0-9]\))?\]\s*$') def set_confidence_threshold(self, threshold: float): """ Establece el umbral de confianza para deteccin de poemas. Args: threshold: Valor entre 0.0 y 1.0 """ self.confidence_threshold = max(0.0, min(1.0, threshold)) self.logger.debug(f"Umbral de confianza establecido a: {self.confidence_threshold}") def is_verse(self, text: str) -> bool: """ Determina si una lnea es un verso segn heursticas. Args: text: Texto de la lnea Returns: True si parece un verso, False en caso contrario """ stripped = text.strip() if not stripped: return False # Chequear longitud if len(stripped) <= self.max_verse_length: # Si es una lnea de acordes, no es un verso if self.chord_pattern.match(stripped): return False # Si termina en puntuacin final, podra ser dilogo, no verso if stripped[-1] in '.!?' and not self._is_verse_by_context(stripped): return False return True # Chequear ratio de espacios (para versos con formato especial) space_count = stripped.count(' ') if space_count / len(stripped) >= self.max_space_ratio: return True return False def _is_verse_by_context(self, text: str) -> bool: """ Anlisis adicional para lneas que podran ser dilogo. (Implementacin simplificada, se expandira con anlisis de contexto) """ # Si empieza con guin de dilogo y termina en puntuacin, probablemente es dilogo if text.startswith('') and text[-1] in '.!?': return False return True def has_title_format(self, block: Dict[str, Any]) -> bool: """ Determina si un bloque tiene formato de ttulo. Args: block: Bloque de texto con metadatos Returns: True si parece un ttulo, False en caso contrario """ # Si ya viene marcado como heading desde el loader, es ttulo if block.get('is_heading', False): return True text = block.get('text', '').strip() if not text: return False # Verificar longitud mxima if len(text) > self.max_title_length: return False # Verificar patrones de formato if self.title_regex.search(text): return True # Verificar maysculas (para ttulos en maysculas) if len(text) >= 4: # Solo para textos no muy cortos uppercase_ratio = sum(1 for c in text if c.isupper()) / len(text) if uppercase_ratio > 0.7: # >70% maysculas return True # Verificar si est en negrita o centrado (si hay metadatos disponibles) if block.get('is_bold', False) or block.get('is_centered', False): return True return False def count_stanzas(self, verses: List[str]) -> int: """ Cuenta el nmero de estrofas en un conjunto de versos. Args: verses: Lista de versos incluyendo lneas vacas Returns: Nmero de estrofas detectadas """ stanzas = 1 empty_count = 0 for verse in verses: if not verse.strip(): empty_count += 1 if empty_count >= self.min_empty_between_stanzas: stanzas += 1 empty_count = 0 else: empty_count = 0 return stanzas def _create_numbered_verses(self, verses: List[str]) -> List[Dict[str, Any]]: """ Convierte una lista de versos a una lista de objetos con numeracin. Args: verses: Lista de versos incluyendo lneas vacas Returns: Lista de objetos con numeracin para cada verso """ numbered_verses = [] verse_number = 1 for i, verse in enumerate(verses): if not verse.strip(): # Para lneas vacas, no incrementar nmero pero mantener en estructura numbered_verses.append({ "text": "", "line_number": i + 1, # Nmero de lnea en el poema "verse_number": None # No es un verso numerado }) else: # Para versos con contenido, incrementar numeracin numbered_verses.append({ "text": verse, "line_number": i + 1, "verse_number": verse_number }) verse_number += 1 return numbered_verses def segment(self, blocks: List[Dict[str, Any]]) -> List[Dict[str, Any]]: """ Segmenta bloques en poemas usando una mquina de estados. Args: blocks: Lista de bloques de texto con metadatos Returns: Lista de unidades semnticas (poemas, prrafos, etc.) """ segments = [] # Primero procesamos bloques que ya vienen marcados como poemas for block in blocks: # Si el loader ya detect que es un poema (DocxLoader por ejemplo) if block.get('is_poem', False): text = block.get('texto', '') title = block.get('titulo') verses = text.split('\n') if text else [] # Calculamos algunos metadatos adicionales stanzas = self.count_stanzas(verses) confidence = self._calculate_confidence(verses) verses_count = block.get('verse_count', len([v for v in verses if v.strip()])) # Solo incluir poemas que superen el umbral de confianza if confidence >= self.confidence_threshold: # Crear segmento de poema con versos numerados numbered_verses = self._create_numbered_verses(verses) poem = { "type": "poem", "title": title or "Sin ttulo", "numbered_verses": numbered_verses, "verses_count": verses_count, "stanzas": stanzas, "confidence": confidence, # Copiar metadatos adicionales "fuente": block.get('fuente'), "contexto": block.get('contexto'), "fecha": block.get('fecha') } segments.append(poem) else: self.logger.debug(f"Poema descartado por baja confianza: {confidence} < {self.confidence_threshold}") continue # Variables para el algoritmo de deteccin de poemas current_title = None current_verses = [] consecutive_empty = 0 consecutive_verses = 0 # Estado inicial state = VerseState.SEARCH_TITLE # Procesamiento basado en estados para bloques sin marcar for i, block in enumerate(blocks): # Skip bloques que ya se procesaron como poemas if block.get('is_poem', False): continue text = block.get('text', '').strip() is_empty = not text # Actualizar contador de lneas vacas if is_empty: consecutive_empty += 1 # No procesar ms lgica para lneas vacas, solo actualizar contador continue else: # Lnea no vaca, evaluar segn estado actual is_potential_title = self.has_title_format(block) is_verse = self.is_verse(text) # Transiciones de estado y acciones if state == VerseState.SEARCH_TITLE: if is_potential_title: # Posible ttulo encontrado, cambiar estado y verificar lo que sigue current_title = text consecutive_empty = 0 state = VerseState.TITLE_FOUND elif is_verse: # Verso sin ttulo previo, podra ser poema sin ttulo consecutive_verses = 1 current_verses.append(text) if self._peek_ahead_for_verses(blocks, i, 2): # Si vienen ms versos, tratarlo como inicio de poema sin ttulo state = VerseState.COLLECTING_VERSE else: # Lnea suelta, no es poema segments.append({ "type": "paragraph", "text": text }) current_verses = [] consecutive_verses = 0 else: # Texto normal, no poema segments.append({ "type": "paragraph", "text": text }) elif state == VerseState.TITLE_FOUND: # Despus de un ttulo, esperamos versos if is_verse: current_verses.append(text) consecutive_verses = 1 consecutive_empty = 0 # Si vienen ms versos, confirmar poema if self._peek_ahead_for_verses(blocks, i, self.min_consecutive_verses - 1): state = VerseState.COLLECTING_VERSE elif is_potential_title: # Otro ttulo despus de un ttulo, sin versos intermedios # El ttulo anterior era falso positivo segments.append({ "type": "heading", "text": current_title, "level": block.get('heading_level', 1) }) current_title = text consecutive_empty = 0 # Seguimos en estado TITLE_FOUND con el nuevo ttulo else: # Texto normal despus de ttulo, no es poema segments.append({ "type": "heading", "text": current_title, "level": block.get('heading_level', 1) }) segments.append({ "type": "paragraph", "text": text }) current_title = None state = VerseState.SEARCH_TITLE elif state == VerseState.COLLECTING_VERSE: if is_verse: # Continuar recogiendo versos current_verses.append(text) consecutive_verses += 1 consecutive_empty = 0 elif is_potential_title: # Ttulo despus de recoger algunos versos = fin de poema state = VerseState.END_POEM # Procesaremos el ttulo en la siguiente iteracin, tras aadir el poema i -= 1 # Retroceder para reprocesar este bloque else: # Texto normal despus de versos = fin de poema state = VerseState.END_POEM # Retroceder para procesar este bloque despus i -= 1 elif state == VerseState.STANZA_GAP: if is_verse: # Verso despus de hueco = nueva estrofa del mismo poema current_verses.append("") # Aadir marcador de separacin de estrofa current_verses.append(text) consecutive_verses += 1 consecutive_empty = 0 state = VerseState.COLLECTING_VERSE elif is_potential_title: # Ttulo despus de hueco = fin del poema anterior state = VerseState.END_POEM # Retroceder para procesar ttulo en siguiente iteracin i -= 1 else: # Texto normal despus de hueco = fin de poema state = VerseState.END_POEM # Retroceder para procesar texto en siguiente iteracin i -= 1 elif state == VerseState.END_POEM: # Finalizar poema actual if current_verses: if consecutive_verses >= self.min_consecutive_verses: # Calcular confianza confidence = self._calculate_confidence(current_verses) # Solo agregar como poema si supera el umbral de confianza if confidence >= self.confidence_threshold: # Agregar versos numerados numbered_verses = self._create_numbered_verses(current_verses) segments.append({ "type": "poem", "title": current_title or "Sin ttulo", "numbered_verses": numbered_verses, "verses_count": len([v for v in current_verses if v.strip()]), "stanzas": self.count_stanzas(current_verses), "confidence": confidence }) else: self.logger.debug(f"Poema final descartado por baja confianza: {confidence} < {self.confidence_threshold}") # Tratar como prrafos normales if current_title: segments.append({ "type": "heading", "text": current_title, "level": 1 }) for verse in current_verses: if verse.strip(): segments.append({ "type": "paragraph", "text": verse }) else: # Pocos versos, tratar como prrafos normales if current_title: segments.append({ "type": "heading", "text": current_title, "level": 1 }) for verse in current_verses: if verse.strip(): segments.append({ "type": "paragraph", "text": verse }) # Reiniciar variables para siguiente poema current_title = None current_verses = [] consecutive_verses = 0 consecutive_empty = 0 # Volver a estado inicial y reprocesar el bloque actual state = VerseState.SEARCH_TITLE i -= 1 # Retroceder para procesar este bloque desde inicio # Transiciones basadas en contador de lneas vacas if consecutive_empty > 0: if state == VerseState.COLLECTING_VERSE: if consecutive_empty <= self.max_empty_in_stanza: # Pocas vacas: sigue siendo misma estrofa pass elif consecutive_empty <= self.max_empty_between_stanzas: # Vacas intermedias: posible separacin de estrofas state = VerseState.STANZA_GAP else: # Demasiadas vacas: fin de poema state = VerseState.END_POEM elif state == VerseState.TITLE_FOUND: if consecutive_empty > self.max_empty_in_stanza: # Demasiadas vacas tras ttulo, no parece inicio de poema segments.append({ "type": "heading", "text": current_title, "level": block.get('heading_level', 1) }) current_title = None state = VerseState.SEARCH_TITLE # Procesar estado final - si qued un poema sin cerrar if state in (VerseState.COLLECTING_VERSE, VerseState.STANZA_GAP) and current_verses: if consecutive_verses >= self.min_consecutive_verses: # Calcular confianza confidence = self._calculate_confidence(current_verses) # Solo agregar como poema si supera el umbral de confianza if confidence >= self.confidence_threshold: # Agregar versos numerados numbered_verses = self._create_numbered_verses(current_verses) segments.append({ "type": "poem", "title": current_title or "Sin ttulo", "numbered_verses": numbered_verses, "verses_count": len([v for v in current_verses if v.strip()]), "stanzas": self.count_stanzas(current_verses), "confidence": confidence }) else: self.logger.debug(f"Poema final descartado por baja confianza: {confidence} < {self.confidence_threshold}") # Tratar como prrafos normales if current_title: segments.append({ "type": "heading", "text": current_title, "level": 1 }) for verse in current_verses: if verse.strip(): segments.append({ "type": "paragraph", "text": verse }) else: # Pocos versos, tratar como prrafos normales if current_title: segments.append({ "type": "heading", "text": current_title, "level": 1 }) for verse in current_verses: if verse.strip(): segments.append({ "type": "paragraph", "text": verse }) return segments def _peek_ahead_for_verses(self, blocks: List[Dict[str, Any]], current_idx: int, count: int) -> bool: """ Mira adelante en los bloques para verificar si hay suficientes versos. Args: blocks: Lista completa de bloques current_idx: ndice actual count: Nmero de versos a buscar Returns: True si hay al menos 'count' versos en los bloques siguientes """ if current_idx >= len(blocks) - 1: return False verse_count = 0 empty_count = 0 for i in range(current_idx + 1, min(len(blocks), current_idx + count*2 + 5)): text = blocks[i].get('text', '').strip() if not text: empty_count += 1 if empty_count > self.max_empty_in_stanza: # Demasiadas lneas vacas, corta la bsqueda break continue # Reiniciar contador de vacas si encontramos texto empty_count = 0 # Si es verso, incrementar contador if self.is_verse(text): verse_count += 1 if verse_count >= count: return True else: # Si encontramos algo que no es verso, cortar bsqueda break return False def _calculate_confidence(self, verses: List[str]) -> float: """ Calcula un valor de confianza para el poema detectado con anlisis avanzado. Args: verses: Lista de versos incluyendo lneas vacas Returns: Valor de confianza de 0.0 a 1.0 """ if not verses: return 0.0 # Filtrar versos no vacos non_empty = [v for v in verses if v.strip()] if not non_empty: return 0.0 # Factores que aumentan la confianza factors = { "min_verses": 0.2, # Base por tener versos suficientes "verse_regularity": 0.0, # Consistencia en longitud de versos "stanza_structure": 0.0, # Estructura clara de estrofas "rhyme_patterns": 0.0, # Patrones de rima (anlisis avanzado) "line_brevity": 0.0, # Versos cortos (caracterstica potica) "metric_patterns": 0.0, # Patrones mtricos "linguistic_features": 0.0, # Caractersticas lingsticas poticas } # 1. Calcular regularidad de versos (longitudes similares) if len(non_empty) >= self.min_consecutive_verses: lengths = [len(v) for v in non_empty] if lengths: avg_len = sum(lengths) / len(lengths) # Desviacin estndar simplificada variance = sum((l - avg_len) ** 2 for l in lengths) / len(lengths) std_dev = variance ** 0.5 # Menor desviacin = ms regular = ms confianza if avg_len > 0: regularity = max(0, 1 - min(std_dev / avg_len, 1.0)) factors["verse_regularity"] = regularity * 0.2 # Bonus por versos cortos (poemas suelen tener versos ms breves que prosa) if avg_len < 50: # Menor longitud promedio = ms probable que sea poema brevity_score = max(0, 1 - (avg_len / 100)) factors["line_brevity"] = brevity_score * 0.1 # 2. Estructura de estrofas stanzas = self.count_stanzas(verses) if stanzas > 1: # Calculamos el tamao promedio de estrofas stanza_sizes = [] current_size = 0 for verse in verses: if verse.strip(): current_size += 1 elif current_size > 0: stanza_sizes.append(current_size) current_size = 0 if current_size > 0: stanza_sizes.append(current_size) # Regularidad de tamao de estrofas if stanza_sizes and len(stanza_sizes) > 1: avg_stanza = sum(stanza_sizes) / len(stanza_sizes) stanza_variance = sum((s - avg_stanza) ** 2 for s in stanza_sizes) / len(stanza_sizes) stanza_regularity = max(0, 1 - min(math.sqrt(stanza_variance) / avg_stanza, 1.0)) # Ms estrofas y ms regulares = estructura ms clara = ms confianza stanza_factor = min(0.15, (stanzas * 0.03) * (1 + stanza_regularity)) factors["stanza_structure"] = stanza_factor # 3. Anlisis de patrones de rima if len(non_empty) >= 4: # Extraer ltimas palabras y finales fonticos estimados endings = [] for verse in non_empty: words = verse.strip().split() if words: # Tomar ltima palabra y sus ltimos 2-3 caracteres como aproximacin fontica last_word = words[-1].lower().rstrip(',.;:!?') if len(last_word) >= 3: endings.append(last_word[-3:]) elif last_word: endings.append(last_word) # Contar repeticiones de terminaciones if endings: ending_counts = {} for e in endings: ending_counts[e] = ending_counts.get(e, 0) + 1 # Calcular frecuencia de repeticiones repeated = sum(count - 1 for count in ending_counts.values() if count > 1) # Normalizar por nmero total de finales rhyme_score = min(repeated / len(endings), 1.0) # Identificar patrones de rima potenciales rhyme_patterns = self._identify_rhyme_patterns(endings) if rhyme_patterns: rhyme_score = max(rhyme_score, 0.7) # Si hay patrn evidente, aumentar score factors["rhyme_patterns"] = rhyme_score * 0.25 # 4. Anlisis de caractersticas lingsticas poticas poetic_expressions = self._analyze_poetic_expressions(non_empty) factors["linguistic_features"] = poetic_expressions * 0.1 # 5. Patrones mtricos (simplificado) if len(non_empty) >= 3: metric_score = self._analyze_metric_patterns(non_empty) factors["metric_patterns"] = metric_score * 0.15 # Calcular confianza total confidence = sum(factors.values()) # Normalizar a rango 0.0-1.0 return min(1.0, max(0.0, confidence)) def _identify_rhyme_patterns(self, endings: List[str]) -> bool: """ Identifica patrones potenciales de rima (ABAB, AABB, etc.). Args: endings: Lista de terminaciones fonticas Returns: True si se identifica un patrn, False en caso contrario """ if len(endings) < 4: return False # Convertir terminaciones a patrones (A, B, C, etc.) pattern = [] ending_to_letter = {} next_letter = 'A' for ending in endings: if ending not in ending_to_letter: ending_to_letter[ending] = next_letter next_letter = chr(ord(next_letter) + 1) pattern.append(ending_to_letter[ending]) # Buscar patrones comunes de rima pattern_str = ''.join(pattern) common_patterns = [ 'ABAB', 'AABB', 'ABBA', 'AAAA', 'ABCB', 'AABCCB' ] # Verificar si existe alguno de los patrones comunes en segmentos for i in range(len(pattern_str) - 3): segment = pattern_str[i:i+4] for p in common_patterns: if segment in p: return True return False def _analyze_poetic_expressions(self, verses: List[str]) -> float: """ Analiza el texto buscando expresiones tpicas de poesa. Args: verses: Lista de versos no vacos Returns: Puntuacin de 0.0 a 1.0 """ text = ' '.join(verses).lower() # Palabras y expresiones comunes en poesa poetic_words = [ 'alma', 'amor', 'corazn', 'sueo', 'cielo', 'estrella', 'noche', 'mar', 'sol', 'luna', 'luz', 'sombra', 'silencio', 'tiempo', 'suspiro', 'lgrima', 'rosa', 'flor', 'vida', 'muerte' ] # Figuras retricas comunes rhetorical_patterns = [ r'como si', r'cual', r'semejante a', r'parece', # smiles r'oh!', r'ah!', r'!', r'\?', # exclamaciones e interrogaciones retricas r'ni', r'no', # negaciones repetidas (frecuentes en poesa) r'siempre', r'nunca', r'jams', # absolutos (comunes en poesa) r'tan', r'qu', # intensificadores ] # Contar palabras poticas word_matches = sum(1 for word in poetic_words if word in text) max_possible_words = min(len(poetic_words), len(verses) * 2) word_score = min(word_matches / max_possible_words, 1.0) if max_possible_words > 0 else 0 # Buscar patrones retricos pattern_matches = sum(len(re.findall(pattern, text)) for pattern in rhetorical_patterns) max_possible_patterns = len(verses) * 2 pattern_score = min(pattern_matches / max_possible_patterns, 1.0) if max_possible_patterns > 0 else 0 # Combinar scores return (word_score * 0.6) + (pattern_score * 0.4) def _analyze_metric_patterns(self, verses: List[str]) -> float: """ Analiza patrones mtricos simplificados (basados en slabas aproximadas). Args: verses: Lista de versos no vacos Returns: Puntuacin de 0.0 a 1.0 """ # Funcin simplificada para estimar slabas en espaol def estimate_syllables(text): # Contar vocales como aproximacin bsica vowels = 'aeiou' count = sum(1 for c in text.lower() if c in vowels) # Ajustar por diptongos comunes (aproximacin) diphthongs = ['ia', 'ie', 'io', 'iu', 'ua', 'ue', 'ui', 'uo', 'ai', 'ei', 'oi', 'ui'] for d in diphthongs: count -= text.lower().count(d) return max(count, 1) # Calcular slabas estimadas por verso syllables = [estimate_syllables(verse) for verse in verses] if not syllables: return 0.0 # Analizar regularidad en nmero de slabas avg_syllables = sum(syllables) / len(syllables) variance = sum((s - avg_syllables) ** 2 for s in syllables) / len(syllables) std_dev = math.sqrt(variance) # Si hay poca variacin, hay un patrn mtrico fuerte regularity = max(0, 1 - (std_dev / avg_syllables)) # Para versos con longitud comn en poesa espaola (7-11 slabas), dar bonus common_length_verses = sum(1 for s in syllables if 7 <= s <= 14) common_ratio = common_length_verses / len(syllables) # Combinar ambos factores return (regularity * 0.7) + (common_ratio * 0.3) def get_full_text_from_numbered_verses(self, numbered_verses: List[Dict[str, Any]]) -> str: """ Reconstruye el texto completo a partir de los versos numerados. Args: numbered_verses: Lista de versos numerados Returns: Texto completo del poema """ return "\n".join(verse["text"] for verse in numbered_verses)
---

# txt loader

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** loaders

# txt loader

from datetime import datetime import re from pathlib import Path from typing import Dict, Any, Optional, List import logging from datetime import datetime, timezone # Asegurar timezone from .base_loader import BaseLoader logger = logging.getLogger(__name__) class txtLoader(BaseLoader): """Loader para archivos de texto plano (.txt).""" def __init__(self, file_path: str | Path, tipo: str = 'escritos', encoding: str = 'utf-8'): """ Inicializa el loader de texto plano. Args: file_path: Ruta al archivo de texto tipo: Tipo de contenido (se guarda en metadatos pero no afecta la carga) encoding: Codificacin del archivo """ super().__init__(file_path) self.tipo = tipo.lower() self.encoding = encoding logger.debug(f"txtLoader inicializado para: {self.file_path} con tipo: {self.tipo}, encoding: {self.encoding}") def load(self) -> Dict[str, Any]: """ Carga y procesa el archivo de texto, agrupando lneas en prrafos. Returns: Dict[str, Any]: Un diccionario con 'blocks' y 'document_metadata'. """ logger.info(f"Iniciando carga de archivo TXT (txtLoader): {self.file_path}") blocks: List[Dict[str, Any]] = [] original_fuente, original_contexto = self.get_source_info() # --- Obtener Timestamps del Sistema de Archivos --- fs_creation_time_iso: Optional[str] = None fs_modified_time_iso: Optional[str] = None fallback_publication_date: Optional[str] = None publication_date_source: Optional[str] = None try: stat_info = self.file_path.stat() # Fecha de Modificacin (ms robusta como fallback para publicacin) m_timestamp = stat_info.st_mtime fs_modified_time_iso = datetime.fromtimestamp(m_timestamp, timezone.utc).isoformat() fallback_publication_date = datetime.fromtimestamp(m_timestamp, timezone.utc).strftime('%Y-%m-%d') publication_date_source = 'file_system_modification_time' # Fecha de Creacin (puede variar su disponibilidad/significado por OS) try: c_timestamp = stat_info.st_birthtime # Idealmente st_birthtime except AttributeError: c_timestamp = stat_info.st_ctime # Fallback a st_ctime fs_creation_time_iso = datetime.fromtimestamp(c_timestamp, timezone.utc).isoformat() except Exception as e: logger.warning(f"No se pudieron obtener las marcas de tiempo del sistema de archivos para {self.file_path}: {e}") document_metadata = { 'ruta_archivo_original': str(self.file_path.resolve()), 'file_format': self.file_path.suffix.lstrip('.').lower() or 'txt', 'titulo_documento': self.file_path.stem, # Fallback: nombre de archivo sin extensin 'autor_documento': None, 'fecha_publicacion_documento': fallback_publication_date, # Fallback: fecha de modificacin del archivo 'idioma_documento': 'und', # Indeterminado 'metadatos_adicionales_fuente': { 'loader_used': self.__class__.__name__, 'loader_config': { 'tipo': self.tipo, 'encoding': self.encoding }, 'original_fuente': original_fuente, 'original_contexto': original_contexto, 'fs_creation_time_iso': fs_creation_time_iso, 'fs_modified_time_iso': fs_modified_time_iso, 'publication_date_source': publication_date_source }, 'error': None, 'warning': None } try: with self.file_path.open('r', encoding=self.encoding) as f: content = f.read() if not content.strip(): warning_message = f"El archivo TXT '{self.file_path.name}' (usando txtLoader) est vaco o solo contiene espacios en blanco." logger.warning(warning_message) document_metadata['warning'] = warning_message return { 'blocks': [], 'document_metadata': document_metadata } # Lgica de agrupacin por prrafos (similar a la de TxtLoader refactorizado) normalized_content = content.replace('\r\n', '\n').replace('\r', '\n') order_idx = 0 current_paragraph_lines = [] for line in normalized_content.split('\n'): if not line.strip(): # Lnea efectivamente vaca if current_paragraph_lines: paragraph_text = "\n".join(current_paragraph_lines).strip() if paragraph_text: blocks.append({ 'text': paragraph_text, 'order_in_document': order_idx }) order_idx += 1 current_paragraph_lines = [] else: current_paragraph_lines.append(line) # Aadir el ltimo prrafo si existe if current_paragraph_lines: paragraph_text = "\n".join(current_paragraph_lines).strip() if paragraph_text: blocks.append({ 'text': paragraph_text, 'order_in_document': order_idx }) logger.info(f"Archivo TXT cargado con txtLoader: {self.file_path}. Bloques encontrados: {len(blocks)}") except FileNotFoundError: error_message = f"Archivo TXT no encontrado (txtLoader): {self.file_path.name}" logger.error(error_message) document_metadata['error'] = error_message except UnicodeDecodeError as e: error_message = f"Error de decodificacin para el archivo TXT '{self.file_path.name}' (txtLoader) con encoding '{self.encoding}': {e}" logger.error(error_message) document_metadata['error'] = error_message except Exception as e: error_message = f"Error general al procesar archivo TXT {self.file_path.name} (txtLoader): {e}" logger.error(error_message, exc_info=True) document_metadata['error'] = error_message return { 'blocks': blocks, 'document_metadata': document_metadata } # _parece_poema ha sido eliminado.
---

# pdf loader

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** loaders

# pdf loader

from datetime import datetime, timezone import re from pathlib import Path from typing import Dict, Any, Optional, List, Tuple # import PyPDF2 # Eliminado PyPDF2 import fitz # Importacin de PyMuPDF import logging from .base_loader import BaseLoader logger = logging.getLogger(__name__) class PDFLoader(BaseLoader): """Loader para archivos PDF (.pdf) usando PyMuPDF (fitz) para mayor rendimiento.""" def __init__(self, file_path: str | Path, tipo: str = 'escritos', encoding: str = 'utf-8'): """ Inicializa el loader para archivos PDF. Args: file_path: Ruta al archivo PDF tipo: Tipo de contenido (no se usa directamente aqu pero podra ser til para perfiles) encoding: Codificacin (no se usa directamente en PDFs pero se mantiene por consistencia) """ super().__init__(file_path) self.tipo = tipo.lower() # self.encoding = encoding # Encoding no es tan relevante para fitz de esta manera logger.debug(f"PDFLoader (fitz) inicializado para: {self.file_path} con tipo: {self.tipo}") @staticmethod def _parse_pdf_datetime_str(date_str: Optional[str]) -> Tuple[Optional[str], Optional[str]]: """Parsea una cadena de fecha/hora de metadatos PDF (ej. 'D:YYYYMMDDHHMMSS[Z|+-HH\'MM\']'). Returns: Tuple[Optional[str], Optional[str]]: (fecha YYYY-MM-DD, fecha ISO completa) o (None, None). """ if not date_str or not isinstance(date_str, str) or not date_str.startswith('D:'): return None, None raw_dt_part = date_str[2:] # Remover 'D:' # Extraer la parte principal de la fecha/hora (YYYYMMDDHHMMSS) dt_core = raw_dt_part[:14] try: parsed_dt = datetime.strptime(dt_core, '%Y%m%d%H%M%S') # Manejar la informacin de la zona horaria si existe tz_part = raw_dt_part[14:] if tz_part: if tz_part == 'Z': parsed_dt = parsed_dt.replace(tzinfo=timezone.utc) elif tz_part.startswith(('+', '-')) and len(tz_part) >= 3: try: offset_hours = int(tz_part[1:3]) offset_minutes = int(tz_part[4:6]) if len(tz_part) >= 6 and tz_part[3] == "'" else 0 delta = timezone(datetime.timedelta(hours=offset_hours, minutes=offset_minutes) * (1 if tz_part[0] == '+' else -1)) parsed_dt = parsed_dt.replace(tzinfo=delta) except ValueError: # Si la zona horaria es invlida, mantener como naive pero loguear logger.debug(f"No se pudo parsear la zona horaria de PDF: {tz_part} para fecha {date_str}") # Devolver la fecha parseada sin tzinfo explcito, pero s como ISO return parsed_dt.strftime('%Y-%m-%d'), parsed_dt.isoformat() return parsed_dt.strftime('%Y-%m-%d'), parsed_dt.isoformat() except ValueError as e: logger.warning(f"No se pudo parsear la parte central de la fecha PDF '{dt_core}' de '{date_str}': {e}") return None, date_str # Devolver la cadena original como fecha ISO si falla el parseo def load(self) -> Dict[str, Any]: """ Carga y procesa el archivo PDF. Returns: Dict[str, Any]: Un diccionario con 'blocks' y 'document_metadata'. """ logger.info(f"Iniciando carga de archivo PDF con fitz: {self.file_path}") blocks = [] # Construir document_metadata desde cero aqu original_fuente, original_contexto = self.get_source_info() document_metadata: Dict[str, Any] = { 'ruta_archivo_original': str(self.file_path.resolve()), 'file_format': 'pdf', 'titulo_documento': None, 'autor_documento': None, 'fecha_publicacion_documento': None, # Se llenar ms abajo 'idioma_documento': None, # Generalmente no disponible en metadatos PDF estndar 'metadatos_adicionales_fuente': { 'loader_used': self.__class__.__name__, 'loader_config': {'tipo': self.tipo}, 'original_fuente': original_fuente, 'original_contexto': original_contexto, 'blocks_are_fitz_native': True, # Indica que los bloques son nativos de fitz # Otros campos especficos del PDF se aadirn aqu }, 'error': None, 'warning': None } doc = None # Definir doc aqu para que est en el scope del finally try: doc = fitz.open(self.file_path) # Restaurar extraccin de metadatos del PDF pdf_meta = doc.metadata if pdf_meta: document_metadata['titulo_documento'] = pdf_meta.get('title') if pdf_meta.get('title') else None document_metadata['autor_documento'] = pdf_meta.get('author') if pdf_meta.get('author') else None additional_meta = document_metadata['metadatos_adicionales_fuente'] additional_meta['pdf_subject'] = pdf_meta.get('subject') if pdf_meta.get('subject') else None additional_meta['pdf_keywords'] = pdf_meta.get('keywords') if pdf_meta.get('keywords') else None additional_meta['pdf_creator'] = pdf_meta.get('creator') if pdf_meta.get('creator') else None additional_meta['pdf_producer'] = pdf_meta.get('producer') if pdf_meta.get('producer') else None creation_date_str = pdf_meta.get('creationDate') mod_date_str = pdf_meta.get('modDate') parsed_creation_date_simple, parsed_creation_date_iso = self._parse_pdf_datetime_str(creation_date_str) parsed_mod_date_simple, parsed_mod_date_iso = self._parse_pdf_datetime_str(mod_date_str) additional_meta['pdf_creation_date_iso'] = parsed_creation_date_iso additional_meta['pdf_modified_date_iso'] = parsed_mod_date_iso # Priorizar fecha de creacin, luego modificacin para 'fecha_publicacion_documento' document_metadata['fecha_publicacion_documento'] = parsed_creation_date_simple or parsed_mod_date_simple additional_meta['pdf_page_count'] = doc.page_count current_order = 0 if doc.page_count == 0: warning_message = f"El archivo PDF '{self.file_path.name}' (fitz) no contiene pginas o est vaco." logger.warning(warning_message) document_metadata['warning'] = warning_message # No es necesario retornar aqu explcitamente si el bucle de pginas no se ejecuta. # doc.close() se llamar en el finally. else: for page_num in range(doc.page_count): page = doc.load_page(page_num) page_blocks = page.get_text("blocks", sort=True) logger.debug(f"Pgina {page_num + 1}/{doc.page_count} - get_text(\"blocks\") devolvi {len(page_blocks)} bloques.") for i, b in enumerate(page_blocks): x0, y0, x1, y1, block_text, block_no, block_type = b # Descomentar para depuracin muy detallada: # logger.debug(f" Bloque {i} (fitz_block_no {block_no}) en pg {page_num+1}: Type={block_type}, Pos=({x0:.2f},{y0:.2f}-{x1:.2f},{y1:.2f}), Texto (primeros 50): '{block_text[:50].replace('\\n', ' ')}'") if block_type == 0: # Solo procesar bloques de texto if block_text and block_text.strip(): blocks.append({ 'type': 'text_block', 'text': block_text.strip(), 'order_in_document': current_order, 'source_page_number': page_num + 1, 'source_block_number': block_no, 'coordinates': {'x0': x0, 'y0': y0, 'x1': x1, 'y1': y1} }) current_order += 1 logger.info(f"Archivo PDF cargado con fitz: {self.file_path}. Bloques de texto extrados: {len(blocks)}") except FileNotFoundError: error_message = f"Archivo no encontrado: {self.file_path}" logger.error(error_message) document_metadata['error'] = error_message except fitz.FitzUnableToOpenError: # Corregido: FitzUnableToOpenError dentro del try/except error_message = f"Error al abrir el archivo PDF (fitz FitzUnableToOpenError) '{self.file_path.name}': El archivo no se puede abrir." logger.error(error_message) document_metadata['error'] = error_message except Exception as e: error_message = f"Error general al abrir o procesar PDF '{self.file_path.name}' (fitz): {e}" logger.error(error_message) document_metadata['error'] = error_message logger.exception(f"Detalles de la excepcin en PDFLoader para {self.file_path.name}:") # Loguear stacktrace finally: if 'doc' in locals() and doc: # Asegurarse de que doc exista y no sea None try: doc.close() except Exception as e: logger.error(f"Error al cerrar el documento PDF {self.file_path.name}: {e}") return { 'blocks': blocks, 'document_metadata': document_metadata }
---

# excel loader

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** loaders

# excel loader

from datetime import datetime import re from pathlib import Path from typing import Iterator, Dict, Any, Optional import pandas as pd from .base_loader import BaseLoader class ExcelLoader(BaseLoader): """Loader para archivos Excel (.xlsx, .xls).""" def __init__(self, file_path: str | Path, tipo: str = 'escritos', encoding: str = 'utf-8'): """ Inicializa el loader para archivos Excel. Args: file_path: Ruta al archivo Excel tipo: Tipo de contenido ('escritos', 'poemas', 'canciones') encoding: Codificacin para las cadenas de texto """ super().__init__(file_path) self.tipo = tipo.lower() self.encoding = encoding def _extract_date_from_filename(self) -> Optional[str]: """Intenta extraer una fecha del nombre del archivo.""" # Patrones comunes de fecha en nombres de archivo patterns = [ r'(\d{4})[_-](\d{1,2})[_-](\d{1,2})', # YYYY-MM-DD o YYYY_MM_DD r'(\d{1,2})[_-](\d{1,2})[_-](\d{4})', # DD-MM-YYYY o DD_MM_YYYY r'(\d{4})(\d{2})(\d{2})' # YYYYMMDD ] filename = self.file_path.stem for pattern in patterns: match = re.search(pattern, filename) if match: # Asegurar que sea una fecha vlida try: if len(match.groups()) == 3: if pattern == patterns[0]: # YYYY-MM-DD year, month, day = match.groups() elif pattern == patterns[1]: # DD-MM-YYYY day, month, year = match.groups() else: # YYYYMMDD year, month, day = match.groups() # Asegurar que los valores son numricos year, month, day = int(year), int(month), int(day) # Validar rango if 1900 <= year <= datetime.now().year and 1 <= month <= 12 and 1 <= day <= 31: return f"{year:04d}-{month:02d}-{day:02d}" except (ValueError, IndexError): continue return None def load(self) -> Iterator[Dict[str, Any]]: """ Carga y procesa el archivo Excel. Returns: Iterator[Dict[str, Any]]: Documentos procesados como bloques de texto """ fuente, contexto = self.get_source_info() fecha = self._extract_date_from_filename() try: # Detectar automticamente si es xls o xlsx excel_file = pd.ExcelFile(self.file_path) # Procesar cada hoja for sheet_name in excel_file.sheet_names: # Leer la hoja df = pd.read_excel(excel_file, sheet_name=sheet_name) # Pasar nombre de la hoja como ttulo yield { 'text': sheet_name, 'is_heading': True, 'heading_level': 1, 'fuente': fuente, 'contexto': contexto, 'fecha': fecha } # Procesar las columnas como cabeceras headers = df.columns.tolist() if headers: yield { 'text': ', '.join(str(h) for h in headers), 'is_heading': True, 'heading_level': 2, 'fuente': fuente, 'contexto': contexto, 'fecha': fecha } # Procesar cada fila for _, row in df.iterrows(): # Convertir la fila a texto row_text = ' | '.join(str(v) for v in row.values if pd.notna(v)) if row_text: yield { 'text': row_text, 'is_heading': False, 'sheet_name': sheet_name, 'fuente': fuente, 'contexto': contexto, 'fecha': fecha } except Exception as e: print(f"Error al procesar archivo Excel {self.file_path}: {e}") raise def _process_table_format(self, df: pd.DataFrame) -> Iterator[Dict[str, Any]]: """ Procesa un DataFrame como una tabla estructurada (alternativa). Args: df: DataFrame de pandas con los datos Returns: Iterator con los elementos procesados """ fuente, contexto = self.get_source_info() # Crear una representacin ms elaborada de la tabla for i, row in df.iterrows(): row_dict = { 'row_number': i + 1, 'is_heading': False, 'fuente': fuente, 'contexto': contexto, 'columns': {} } # Aadir cada columna for col in df.columns: value = row[col] if pd.notna(value): row_dict['columns'][str(col)] = str(value) # Si hay algn contenido, yield if row_dict['columns']: yield row_dict
---

# base loader

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** loaders

# base loader

from abc import ABC, abstractmethod from pathlib import Path from typing import Iterator, Dict, Any, List, Optional class BaseLoader(ABC): """Clase base abstracta para todos los loaders de documentos.""" def __init__(self, file_path: str | Path): self.file_path = Path(file_path) if not self.file_path.exists(): raise FileNotFoundError(f"El archivo {file_path} no existe") @abstractmethod def load(self) -> Dict[str, Any]: """ Carga y procesa el archivo, devolviendo los bloques de contenido y metadatos del documento. Returns: Dict[str, Any]: Un diccionario con las siguientes claves: 'blocks': List[Dict[str, Any]], donde cada diccionario de bloque tiene: 'text': str, # El contenido textual del bloque. 'order_in_document': int, # El ndice secuencial del bloque. # ... (otros metadatos especficos del bloque pueden incluirse aqu) 'document_metadata': Dict[str, Any], con informacin como: 'source_file_path': str, 'file_format': str, # ej. 'txt', 'docx' 'detected_date': Optional[str], # YYYY-MM-DD 'original_fuente': str, # Carpeta principal (de get_source_info) 'original_contexto': str # Ruta completa (de get_source_info) # ... (otros metadatos a nivel de documento) """ pass def get_source_info(self) -> tuple[str, str]: """ Extrae la informacin de fuente y contexto del archivo. Returns: tuple[str, str]: (fuente, contexto) """ # El contexto es la ruta completa contexto = str(self.file_path.absolute()) # La fuente es la carpeta principal (primer nivel despus de 'fuentes') try: # Busca la carpeta 'fuentes' en la ruta parts = self.file_path.parts fuentes_idx = parts.index('fuentes') fuente = parts[fuentes_idx + 1] if fuentes_idx + 1 < len(parts) else 'desconocido' except ValueError: # Si no encuentra 'fuentes', usa el nombre de la carpeta padre fuente = self.file_path.parent.name return fuente, contexto
---

# docx loader

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** loaders

# docx loader

from pathlib import Path from typing import Iterator, Dict, Any, Optional from datetime import datetime import docx import re import logging from docx.opc.exceptions import PackageNotFoundError from .base_loader import BaseLoader logger = logging.getLogger(__name__) class DocxLoader(BaseLoader): """Loader para archivos DOCX (Microsoft Word).""" def __init__(self, file_path: str | Path, tipo: str = 'escritos', encoding: str = 'utf-8'): """ Inicializa el loader de DOCX. Args: file_path: Ruta al archivo DOCX tipo: Tipo de contenido ('escritos', 'poemas', 'canciones') encoding: Codificacin para leer el texto (por defecto utf-8) """ super().__init__(file_path) self.tipo = tipo.lower() self.encoding = encoding logger.debug(f"Inicializando DocxLoader para archivo: {file_path}") def _extract_date_from_core_properties(self, doc) -> Optional[str]: """Intenta extraer la fecha de las propiedades del documento.""" try: # Intenta obtener la fecha de creacin if doc.core_properties.created: return doc.core_properties.created.strftime('%Y-%m-%d') # Si no hay fecha de creacin, intenta la de modificacin if doc.core_properties.modified: return doc.core_properties.modified.strftime('%Y-%m-%d') except Exception as e: logger.warning(f"Error al extraer fecha de propiedades: {str(e)}") return None def _extract_date_from_filename(self) -> Optional[str]: """Intenta extraer una fecha del nombre del archivo.""" # Patrones comunes de fecha en nombres de archivo patterns = [ r'(\d{4})[_-](\d{2})[_-](\d{2})', # 2024-01-30, 2024_01_30 r'(\d{2})[_-](\d{2})[_-](\d{4})', # 30-01-2024, 30_01_2024 r'(\d{4})(\d{2})(\d{2})' # 20240130 ] filename = self.file_path.stem for pattern in patterns: match = re.search(pattern, filename) if match: try: if len(match.group(1)) == 4: # Ao primero year, month, day = match.groups() else: # Da primero day, month, year = match.groups() # Validar fecha date = datetime(int(year), int(month), int(day)) return date.strftime('%Y-%m-%d') except ValueError: continue return None def _clean_text(self, text: str) -> str: """Limpia el texto de caracteres no deseados y espacios extra.""" try: # Intenta decodificar si es bytes if isinstance(text, bytes): text = text.decode(self.encoding, errors='replace') # Elimina caracteres de control excepto saltos de lnea text = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f]', '', text) # Normaliza saltos de lnea text = text.replace('\r\n', '\n').replace('\r', '\n') # Elimina espacios mltiples y al inicio/final text = re.sub(r' +', ' ', text).strip() return text except Exception as e: logger.warning(f"Error al limpiar texto: {str(e)}") # Retorna una versin segura del texto return str(text).encode('ascii', errors='replace').decode('ascii') def _process_paragraph(self, paragraph) -> Dict[str, Any]: """ Procesa un prrafo del documento. Args: paragraph: Prrafo de python-docx Returns: Dict con la informacin del prrafo """ # Extraer texto y propiedades text = paragraph.text.strip() if not text: return None # Detectar si es un encabezado por el estilo is_heading = False heading_level = 0 style_name = "Normal" try: if paragraph.style and paragraph.style.name: style_name = paragraph.style.name is_heading = paragraph.style.name.lower().startswith('heading') or 'ttulo' in paragraph.style.name.lower() if is_heading: # Extraer nivel del nombre del estilo (e.g., 'Heading 1' -> 1) try: level_match = re.search(r'\d+', paragraph.style.name) if level_match: heading_level = int(level_match.group()) else: heading_level = 1 except (ValueError, IndexError): heading_level = 1 # Verificar otros estilos que podran indicar encabezados if not is_heading and ( 'title' in paragraph.style.name.lower() or 'subtitle' in paragraph.style.name.lower() or 'caption' in paragraph.style.name.lower() ): is_heading = True heading_level = 1 if 'title' in paragraph.style.name.lower() else 2 except Exception as e: logger.debug(f"Error al procesar estilo: {str(e)}") # Detectar alineacin alignment = None is_centered = False try: if paragraph.alignment: alignment = str(paragraph.alignment) is_centered = paragraph.alignment == docx.enum.text.WD_ALIGN_PARAGRAPH.CENTER except Exception as e: logger.debug(f"Error al procesar alineacin: {str(e)}") # Analizar formato de los runs is_bold = False is_italic = False is_caps = False fonts = [] try: runs_with_text = [run for run in paragraph.runs if run.text.strip()] if runs_with_text: is_bold = all(run.bold for run in runs_with_text) is_italic = all(run.italic for run in runs_with_text) # Verificar si todo est en maysculas is_caps = text.isupper() and len(text) > 3 # Recopilar informacin de fuentes for run in runs_with_text: if run.font.name: fonts.append(run.font.name) except Exception as e: logger.debug(f"Error al procesar formato de texto: {str(e)}") # Verificaciones adicionales para detectar encabezados if not is_heading: # Los textos en maysculas cortos suelen ser ttulos if is_caps and len(text) < 100: is_heading = True heading_level = 1 # Textos en negrita cortos tambin pueden ser ttulos elif is_bold and len(text) < 80: is_heading = True heading_level = 2 # Textos centrados cortos son candidatos a ttulos elif is_centered and len(text) < 80: is_heading = True heading_level = 2 # Textos que terminan con puntos suspensivos pueden ser ttulos elif text.endswith('...') and len(text) < 80: is_heading = True heading_level = 2 # Verificar por patrones comunes de ttulos en el texto heading_patterns = [ r'^(La|El|Los|Las|Un|Una) [A-Z-][a-z-]+(\.{3})?$', # La Permanencia... r'^[A-Z-]{4,}$', # INTRODUCCIN r'^Captulo \d+', # Captulo 1 r'^[A-Z-][a-z-]+(:|\.{3})' # Naturaleza divina: ] if not is_heading and any(re.match(pattern, text) for pattern in heading_patterns): is_heading = True heading_level = 2 return { 'text': text, 'is_heading': is_heading, 'heading_level': heading_level, 'is_bold': is_bold, 'is_italic': is_italic, 'is_caps': is_caps, 'is_centered': is_centered, 'style': style_name, 'alignment': alignment, 'fonts': ','.join(fonts) if fonts else None } def load(self) -> Iterator[Dict[str, Any]]: """ Carga y procesa el archivo DOCX. Returns: Iterator[Dict[str, Any]]: Documentos procesados """ logger.info(f"Iniciando carga de archivo DOCX: {self.file_path}") try: # Abre el documento doc = docx.Document(self.file_path) # Obtiene informacin de fuente y contexto fuente, contexto = self.get_source_info() logger.debug(f"Fuente: {fuente}, Contexto: {contexto}") # Extrae fecha de las propiedades del documento fecha = self._extract_date_from_core_properties(doc) or self._extract_date_from_filename() logger.debug(f"Fecha extrada: {fecha}") # Procesa el contenido segn el tipo if self.tipo in ['poemas', 'canciones']: # Recorre todos los prrafos y extrae informacin adicional paragraphs_with_metadata = [] for paragraph in doc.paragraphs: metadata = self._process_paragraph(paragraph) if metadata: paragraphs_with_metadata.append(metadata) # Detecta secciones o poemas separados por lneas vacas o ttulos current_poem = [] poem_groups = [] for i, metadata in enumerate(paragraphs_with_metadata): # Si es un posible ttulo y no es la primera lnea, podra ser inicio de nuevo poema if metadata['is_heading'] and i > 0 and current_poem: poem_groups.append(current_poem) current_poem = [metadata] # Lnea normal, la agregamos al poema actual else: current_poem.append(metadata) # Agregamos el ltimo poema si existe if current_poem: poem_groups.append(current_poem) # Procesamos cada grupo como un documento separado for poem_group in poem_groups: # Separamos posible ttulo del resto title = None verses = [] numbered_verses = [] for i, metadata in enumerate(poem_group): if title is None and metadata['is_heading']: title = metadata['text'] else: verses.append(metadata['text']) numbered_verses.append({ 'number': i + 1, 'text': metadata['text'], 'metadata': metadata }) # Generamos el documento final yield { 'texto': '\n'.join(verses), 'titulo': title, 'tipo': self.tipo, 'fuente': fuente, 'contexto': contexto, 'fecha': fecha, 'versos': verses, 'numbered_verses': numbered_verses, } else: # Para documentos regulares, extraemos los bloques de texto con metadatos blocks = [] logger.debug(f"Total paragraphs found in DOCX: {len(doc.paragraphs)}") for i, paragraph in enumerate(doc.paragraphs): logger.debug(f"Processing paragraph {i+1}/{len(doc.paragraphs)} - Raw text: '{paragraph.text[:100]}...'" ) # Log first 100 chars metadata = self._process_paragraph(paragraph) if metadata: logger.debug(f"Paragraph {i+1} processed. Metadata: {metadata}") blocks.append(metadata) else: logger.debug(f"Paragraph {i+1} skipped (empty or no metadata after processing).") logger.debug(f"Final _blocks content before yielding: {blocks}") # Enviamos todos los bloques con metadatos al segmentador yield { 'tipo': self.tipo, 'fuente': fuente, 'contexto': contexto, 'fecha': fecha, '_blocks': blocks, # Para uso del segmentador } except PackageNotFoundError: logger.error(f"Error al abrir archivo: {self.file_path} (no es un archivo DOCX vlido)") raise ValueError(f"No se pudo abrir el archivo: {self.file_path}") except Exception as e: logger.error(f"Error al procesar documento DOCX: {str(e)}") raise
---

# md loader

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** loaders

# md loader

from datetime import datetime, timezone from pathlib import Path from typing import Dict, Any, Optional, List, Tuple import logging import os # Para stat try: import frontmatter FRONTMATTER_AVAILABLE = True except ImportError: FRONTMATTER_AVAILABLE = False logging.warning("python-frontmatter no est instalado. El parsing de Frontmatter en archivos .md se omitir.") from .base_loader import BaseLoader logger = logging.getLogger(__name__) class MarkdownLoader(BaseLoader): """Loader para archivos Markdown (.md).""" def __init__(self, file_path: str | Path, tipo: str = 'escritos', encoding: str = 'utf-8'): """ Inicializa el loader de Markdown. Args: file_path: Ruta al archivo Markdown. tipo: Tipo de contenido (se guarda en metadatos). encoding: Codificacin del archivo. """ super().__init__(file_path) self.tipo = tipo.lower() self.encoding = encoding logger.debug(f"MarkdownLoader inicializado para: {self.file_path} con tipo: {self.tipo}, encoding: {self.encoding}") @staticmethod def _parse_frontmatter_date(date_value: Any) -> Optional[str]: """Parsea un valor de fecha del frontmatter a YYYY-MM-DD.""" if isinstance(date_value, datetime): return date_value.strftime('%Y-%m-%d') if isinstance(date_value, str): try: # Intentar parsear si es una cadena con formato ISO reconocible por datetime parsed_dt = datetime.fromisoformat(date_value.replace('Z', '+00:00')) return parsed_dt.strftime('%Y-%m-%d') except ValueError: # Podra ser solo YYYY o YYYY-MM, intentar otros parseos si es necesario # Por ahora, si no es ISO completo, se devuelve None o se podra intentar con dateutil logger.debug(f"No se pudo parsear la cadena de fecha del frontmatter '{date_value}' directamente a YYYY-MM-DD.") return None # O manejar formatos ms simples aqu return None def load(self) -> Dict[str, Any]: """ Carga y procesa el archivo Markdown. Extrae metadatos del frontmatter y el contenido principal. Returns: Dict[str, Any]: Un diccionario con 'blocks' y 'document_metadata'. """ logger.info(f"Iniciando carga de archivo Markdown: {self.file_path}") blocks: List[Dict[str, Any]] = [] original_fuente, original_contexto = self.get_source_info() fm_data = {} main_content = "" file_content_read_error = None try: with self.file_path.open('r', encoding=self.encoding) as f: file_raw_content = f.read() if FRONTMATTER_AVAILABLE: post = frontmatter.loads(file_raw_content) fm_data = post.metadata main_content = post.content else: main_content = file_raw_content # Sin frontmatter, todo es contenido except FileNotFoundError: file_content_read_error = f"Archivo MD no encontrado: {self.file_path.name}" logger.error(file_content_read_error) except UnicodeDecodeError as e: file_content_read_error = f"Error de decodificacin para el archivo MD '{self.file_path.name}' con encoding '{self.encoding}': {e}" logger.error(file_content_read_error) except Exception as e: file_content_read_error = f"Error general al leer archivo MD {self.file_path.name}: {e}" logger.error(file_content_read_error, exc_info=True) # --- Obtener Timestamps del Sistema de Archivos --- fs_creation_time_iso: Optional[str] = None fs_modified_time_iso: Optional[str] = None fs_fallback_publication_date: Optional[str] = None if not file_content_read_error: # Solo si se pudo leer el archivo, o al menos no hubo error fatal al inicio try: stat_info = self.file_path.stat() m_timestamp = stat_info.st_mtime fs_modified_time_iso = datetime.fromtimestamp(m_timestamp, timezone.utc).isoformat() fs_fallback_publication_date = datetime.fromtimestamp(m_timestamp, timezone.utc).strftime('%Y-%m-%d') try: c_timestamp = stat_info.st_birthtime except AttributeError: c_timestamp = stat_info.st_ctime fs_creation_time_iso = datetime.fromtimestamp(c_timestamp, timezone.utc).isoformat() except Exception as e: logger.warning(f"No se pudieron obtener las marcas de tiempo del sistema de archivos para {self.file_path}: {e}") # --- Determinar metadatos principales --- title_from_fm = fm_data.get('title') if isinstance(fm_data.get('title'), str) else None author_from_fm = fm_data.get('author') if isinstance(fm_data.get('author'), str) else None date_from_fm_val = fm_data.get('date') or fm_data.get('published') or fm_data.get('publish_date') parsed_date_from_fm = self._parse_frontmatter_date(date_from_fm_val) lang_from_fm = fm_data.get('lang') or fm_data.get('language') publication_date = parsed_date_from_fm or fs_fallback_publication_date publication_date_source = 'frontmatter' if parsed_date_from_fm else ('file_system_modification_time' if fs_fallback_publication_date else None) document_metadata = { 'ruta_archivo_original': str(self.file_path.resolve()), 'file_format': self.file_path.suffix.lstrip('.').lower() or 'md', 'titulo_documento': title_from_fm or self.file_path.stem, # Prioridad: FM, luego nombre de archivo 'autor_documento': author_from_fm, 'fecha_publicacion_documento': publication_date, 'idioma_documento': lang_from_fm if isinstance(lang_from_fm, str) else 'und', 'metadatos_adicionales_fuente': { 'loader_used': self.__class__.__name__, 'loader_config': {'tipo': self.tipo, 'encoding': self.encoding}, 'original_fuente': original_fuente, 'original_contexto': original_contexto, 'fs_creation_time_iso': fs_creation_time_iso, 'fs_modified_time_iso': fs_modified_time_iso, 'publication_date_source': publication_date_source }, 'error': file_content_read_error, 'warning': None } # Aadir el resto de campos del frontmatter a metadatos_adicionales_fuente for key, value in fm_data.items(): if key not in ['title', 'author', 'date', 'published', 'publish_date', 'lang', 'language']: document_metadata['metadatos_adicionales_fuente'][f'fm_{key}'] = value if file_content_read_error: return { 'blocks': [], 'document_metadata': document_metadata } if not main_content.strip(): warning_message = f"El contenido principal del archivo MD '{self.file_path.name}' est vaco o solo contiene espacios en blanco." logger.warning(warning_message) document_metadata['warning'] = warning_message # No retornar inmediatamente, an puede haber metadatos tiles # Procesar main_content en bloques (similar a TxtLoader) normalized_content = main_content.replace('\r\n', '\n').replace('\r', '\n') order_idx = 0 current_paragraph_lines = [] for line in normalized_content.split('\n'): if not line.strip(): if current_paragraph_lines: paragraph_text = "\n".join(current_paragraph_lines).strip() if paragraph_text: blocks.append({'text': paragraph_text, 'order_in_document': order_idx}) order_idx += 1 current_paragraph_lines = [] else: current_paragraph_lines.append(line) if current_paragraph_lines: paragraph_text = "\n".join(current_paragraph_lines).strip() if paragraph_text: blocks.append({'text': paragraph_text, 'order_in_document': order_idx}) logger.info(f"Archivo MD cargado: {self.file_path}. Bloques encontrados: {len(blocks)}. Metadatos FM: {bool(fm_data)}") return { 'blocks': blocks, 'document_metadata': document_metadata }
---

#   init  

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** loaders

#   init  

"""Paquete de loaders para diferentes formatos de archivos.""" from .base_loader import BaseLoader from .markdown_loader import MarkdownLoader from .ndjson_loader import NDJSONLoader from .docx_loader import DocxLoader from .txt_loader import txtLoader from .pdf_loader import PDFLoader from .excel_loader import ExcelLoader from .csv_loader import CSVLoader __all__ = [ 'BaseLoader', 'MarkdownLoader', 'NDJSONLoader', 'DocxLoader', 'txtLoader', 'PDFLoader', 'ExcelLoader', 'CSVLoader' ] # A medida que se implementen loaders, se importarn y agregarn aqu # Por ejemplo: # from .text_loader import txtLoader # from .docx_loader import DocxLoader # __all__ = ['txtLoader', 'DocxLoader']
---

# csv loader

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** loaders

# csv loader

from datetime import datetime import re import csv from pathlib import Path from typing import Iterator, Dict, Any, Optional from .base_loader import BaseLoader class CSVLoader(BaseLoader): """Loader para archivos CSV (.csv).""" def __init__(self, file_path: str | Path, tipo: str = 'escritos', encoding: str = 'utf-8', delimiter: str = ',', quotechar: str = '"'): """ Inicializa el loader para archivos CSV. Args: file_path: Ruta al archivo CSV tipo: Tipo de contenido ('escritos', 'poemas', 'canciones') encoding: Codificacin del archivo delimiter: Separador de campos (por defecto ',') quotechar: Carcter para encerrar campos con espacios (por defecto '"') """ super().__init__(file_path) self.tipo = tipo.lower() self.encoding = encoding self.delimiter = delimiter self.quotechar = quotechar def _extract_date_from_filename(self) -> Optional[str]: """Intenta extraer una fecha del nombre del archivo.""" # Patrones comunes de fecha en nombres de archivo patterns = [ r'(\d{4})[_-](\d{1,2})[_-](\d{1,2})', # YYYY-MM-DD o YYYY_MM_DD r'(\d{1,2})[_-](\d{1,2})[_-](\d{4})', # DD-MM-YYYY o DD_MM_YYYY r'(\d{4})(\d{2})(\d{2})' # YYYYMMDD ] filename = self.file_path.stem for pattern in patterns: match = re.search(pattern, filename) if match: # Asegurar que sea una fecha vlida try: if len(match.groups()) == 3: if pattern == patterns[0]: # YYYY-MM-DD year, month, day = match.groups() elif pattern == patterns[1]: # DD-MM-YYYY day, month, year = match.groups() else: # YYYYMMDD year, month, day = match.groups() # Asegurar que los valores son numricos year, month, day = int(year), int(month), int(day) # Validar rango if 1900 <= year <= datetime.now().year and 1 <= month <= 12 and 1 <= day <= 31: return f"{year:04d}-{month:02d}-{day:02d}" except (ValueError, IndexError): continue return None def _detect_delimiter(self) -> str: """ Detecta automticamente el delimitador del archivo CSV. Returns: Delimitador detectado o el valor por defecto """ try: with open(self.file_path, 'r', encoding=self.encoding) as f: sample = f.read(1024) # Leer una muestra pequea # Contar posibles delimitadores counts = { ',': sample.count(','), ';': sample.count(';'), '\t': sample.count('\t'), '|': sample.count('|') } # Elegir el ms comn if max(counts.values()) > 0: return max(counts.items(), key=lambda x: x[1])[0] except Exception: pass return self.delimiter # Usar valor por defecto si falla la deteccin def load(self) -> Dict[str, Any]: """ Carga y procesa el archivo CSV. Returns: Dict[str, Any]: Un diccionario con bloques de contenido y metadatos del documento. """ fuente, contexto = self.get_source_info() fecha = self._extract_date_from_filename() delimiter = self._detect_delimiter() blocks = [] order_in_document = 0 document_metadata = { 'source_file_path': str(self.file_path.absolute()), 'file_format': self.file_path.suffix, 'detected_date': fecha, 'original_fuente': fuente, 'original_contexto': contexto, 'csv_delimiter_used': delimiter, 'content_type_provided_to_loader': self.tipo } try: with open(self.file_path, 'r', encoding=self.encoding, newline='') as f: reader = csv.reader(f, delimiter=delimiter, quotechar=self.quotechar) headers = [] try: headers = next(reader) blocks.append({ 'text': ', '.join(headers), 'order_in_document': order_in_document, 'block_type': 'csv_header' }) order_in_document += 1 except StopIteration: document_metadata['csv_has_header'] = False return {'blocks': [], 'document_metadata': document_metadata} document_metadata['csv_has_header'] = True document_metadata['csv_headers'] = headers for row_num, row in enumerate(reader, 1): if not any(field.strip() for field in row): continue row_texts = [] for i, value in enumerate(row): cell_text = value.strip() if i < len(headers): field_name = headers[i].strip() if field_name and cell_text: row_texts.append(f"{field_name}: {cell_text}") elif cell_text: row_texts.append(cell_text) elif cell_text: row_texts.append(cell_text) if row_texts: blocks.append({ 'text': '; '.join(row_texts), 'order_in_document': order_in_document, 'block_type': 'csv_row', 'csv_row_number': row_num }) order_in_document += 1 except Exception as e: error_message = f"Error al procesar CSV {self.file_path}: {str(e)}" document_metadata['loader_error'] = error_message return {'blocks': blocks, 'document_metadata': document_metadata} return {'blocks': blocks, 'document_metadata': document_metadata}
---

# markdown loader

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** loaders

# markdown loader

from datetime import datetime import re from pathlib import Path from typing import Iterator, Dict, Any, Optional from .base_loader import BaseLoader class MarkdownLoader(BaseLoader): """Loader para archivos Markdown.""" def __init__(self, file_path: str | Path, tipo: str = 'escritos', encoding: str = 'utf-8'): """ Inicializa el loader de Markdown. Args: file_path: Ruta al archivo Markdown tipo: Tipo de contenido ('escritos', 'poemas', 'canciones') encoding: Codificacin del archivo (por defecto utf-8) """ super().__init__(file_path) self.tipo = tipo.lower() self.encoding = encoding def _extract_date_from_filename(self) -> Optional[str]: """Intenta extraer una fecha del nombre del archivo.""" # Patrones comunes de fecha en nombres de archivo patterns = [ r'(\d{4})-(\d{2})-(\d{2})', # YYYY-MM-DD r'(\d{4})-(\d{2})', # YYYY-MM r'(\d{4})', # YYYY ] filename = self.file_path.stem for pattern in patterns: if match := re.search(pattern, filename): return match.group(0) # Si no encuentra fecha en el nombre, usa la fecha de modificacin del archivo mtime = datetime.fromtimestamp(self.file_path.stat().st_mtime) return mtime.strftime('%Y-%m-%d') def _segment_content(self, content: str) -> Iterator[str]: """ Segmenta el contenido segn el tipo de documento. Para escritos: segmenta por prrafos (doble salto de lnea) Para poemas/canciones: devuelve el contenido completo """ if self.tipo in ['poemas', 'canciones']: yield content.strip() else: # escritos # Elimina lneas vacas mltiples y espacios extra content = re.sub(r'\n{3,}', '\n\n', content.strip()) # Segmenta por prrafos for parrafo in content.split('\n\n'): if parrafo.strip(): yield parrafo.strip() def load(self) -> Dict[str, Any]: """ Carga y procesa el archivo Markdown. Returns: Dict[str, Any]: Un diccionario con bloques de contenido y metadatos del documento. """ fuente, contexto = self.get_source_info() fecha = self._extract_date_from_filename() content = self.file_path.read_text(encoding=self.encoding) blocks = [] order_in_document = 0 for texto_bloque in self._segment_content(content): blocks.append({ 'text': texto_bloque, 'order_in_document': order_in_document # Aqu puedes aadir otros metadatos especficos del bloque si los tienes, # como 'tipo_bloque': 'parrafo' o similar si _segment_content lo diferencia. }) order_in_document += 1 document_metadata = { 'source_file_path': str(self.file_path.absolute()), 'file_format': self.file_path.suffix, 'detected_date': fecha, 'original_fuente': fuente, 'original_contexto': contexto, 'content_type_provided_to_loader': self.tipo # Para depuracin } return { 'blocks': blocks, 'document_metadata': document_metadata }
---

# ndjson loader

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** loaders

# ndjson loader

import json from datetime import datetime import re from pathlib import Path from typing import Iterator, Dict, Any, Optional from .base_loader import BaseLoader class NDJSONLoader(BaseLoader): """Loader para archivos NDJSON (JSON por lnea).""" def __init__(self, file_path: str | Path, tipo: str = 'escritos', encoding: str = 'utf-8'): """ Inicializa el loader de NDJSON. Args: file_path: Ruta al archivo NDJSON tipo: Tipo de contenido ('escritos', 'poemas', 'canciones') encoding: Codificacin del archivo (por defecto utf-8) """ super().__init__(file_path) self.tipo = tipo.lower() self.encoding = encoding def _extract_date_from_filename(self) -> Optional[str]: """Intenta extraer una fecha del nombre del archivo.""" # Patrones comunes de fecha en nombres de archivo patterns = [ r'(\d{4})-(\d{2})-(\d{2})', # YYYY-MM-DD r'(\d{4})-(\d{2})', # YYYY-MM r'(\d{4})', # YYYY ] filename = self.file_path.stem for pattern in patterns: if match := re.search(pattern, filename): return match.group(0) # Si no hay fecha en el nombre, devolver None return None def load(self) -> Iterator[Dict[str, Any]]: """ Carga y procesa el archivo NDJSON. Returns: Iterator[Dict[str, Any]]: Documentos procesados """ fuente, contexto = self.get_source_info() fecha_archivo = self._extract_date_from_filename() with self.file_path.open('r', encoding=self.encoding) as f: for line in f: if line.strip(): try: # Intenta parsear el JSON de la lnea data = json.loads(line.strip()) # Asegura que tenga los campos requeridos if not isinstance(data, dict): continue # Extrae o establece valores por defecto texto = data.get('texto', data.get('text', data.get('content', ''))) fecha = data.get('fecha', data.get('date', fecha_archivo)) # Solo procesa si hay texto if texto: yield { 'texto': texto, 'fecha': fecha, 'fuente': fuente, 'contexto': contexto } except json.JSONDecodeError: # Ignora lneas que no son JSON vlido continue
---

# common block preprocessor

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** pre_processors

# common block preprocessor

from typing import List, Dict, Tuple, Optional, Union, Any from pathlib import Path import re import logging from datetime import datetime logger = logging.getLogger(__name__) class CommonBlockPreprocessor: """Pre-procesador comn para bloques de texto y metadatos. Este pre-procesador realiza operaciones agnsticas al formato de archivo despus de que los datos han sido cargados por un Loader y antes de ser pasados a un Segmenter. Operaciones configurables: - Extraccin de fecha desde el nombre del archivo. - Limpieza de texto (normalizacin de saltos de lnea, eliminacin de NULs). """ DEFAULT_CONFIG = { 'remove_nul_bytes': True, 'normalize_line_endings': True, 'extract_date_from_filename': True, 'split_blocks_into_paragraphs': True, 'min_chars_for_paragraph_split': 100, # Umbral principal para dividir por \\n\\n 'max_vertical_gap_for_merge_pt': 10, 'min_block_area_for_split': 1000, 'try_single_newline_split_if_block_longer_than': 500, 'min_chars_for_single_newline_paragraph': 75 } def __init__(self, config: Optional[Dict] = None): """ Inicializa el CommonBlockPreprocessor. Args: config: Configuracin opcional para el pre-procesador. Sobrescribe los valores de DEFAULT_CONFIG. """ self.config = {**CommonBlockPreprocessor.DEFAULT_CONFIG, **(config if config else {})} logger.debug(f"CommonBlockPreprocessor inicializado con config: {self.config}") def _extract_date_from_filename(self, filename: str) -> Optional[str]: # Implementacin placeholder, ya que la lgica principal se movi # pero la opcin de config existe. if self.config.get('extract_date_from_filename', True): # Ejemplo: Intenta encontrar YYYY-MM-DD o YYYYMMDD match = re.search(r'(\d{4}-\d{2}-\d{2}|\d{8})', filename) if match: date_str = match.group(1) try: # Validar y normalizar si es necesario if '-' in date_str: datetime.strptime(date_str, '%Y-%m-%d') else: datetime.strptime(date_str, '%Y%m%d') date_str = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}" return date_str except ValueError: logger.warning(f"Formato de fecha invlido '{date_str}' encontrado en el nombre de archivo '{filename}'.") return None def _clean_block_text(self, text: str) -> str: """Limpia el texto de un bloque de forma conservadora segn la configuracin.""" if text is None: return None text = text.replace('\r\n', '\n').replace('\r', '\n') if self.config.get('remove_nul_bytes', True): text = text.replace('\x00', '') return text.strip() # Strip al final para quitar espacios/saltos al inicio/fin def _split_text_into_paragraphs(self, text: str, base_order: float, original_coordinates: Optional[Dict] = None) -> List[Tuple[str, float, Optional[Dict]]]: paragraphs_data = [] min_chars_for_split = self.config.get('min_chars_for_paragraph_split', 100) try_single_newline_split_threshold = self.config.get('try_single_newline_split_if_block_longer_than', 500) min_chars_for_single_newline_para = self.config.get('min_chars_for_single_newline_paragraph', 75) logger.debug(f"[_split_text_into_paragraphs] Texto original (primeros 300 chars): '{text[:300]}'") logger.debug(f"[_split_text_into_paragraphs] Longitud total del texto original: {len(text)}") logger.debug(f"[_split_text_into_paragraphs] min_chars_for_split (config): {min_chars_for_split}") logger.debug(f"[_split_text_into_paragraphs] try_single_newline_split_threshold (config): {try_single_newline_split_threshold}") logger.debug(f"[_split_text_into_paragraphs] min_chars_for_single_newline_para (config): {min_chars_for_single_newline_para}") # Intento primario: dividir por dos o ms saltos de lnea # Usar una expresin regular para capturar dos o ms saltos de lnea como delimitador raw_paragraphs = re.split(r'\n{2,}', text) logger.debug(f"[_split_text_into_paragraphs] Intento con r'\n{{2,}}': {len(raw_paragraphs)} prrafos crudos.") for i, p_text in enumerate(raw_paragraphs): # Corregido: Escapar la barra invertida en \\n dentro del replace log_text_preview = p_text[:100].replace('\n', '<NL>') logger.debug(f" Raw para (doble \\\\n) #{i}: '{log_text_preview}{'...' if len(p_text) > 100 else ''}' (len: {len(p_text)})") current_paragraphs = [p.strip() for p in raw_paragraphs if p.strip()] logger.debug(f"[_split_text_into_paragraphs] Prrafos despus de strip y filtro de vacos (doble \\\\n): {len(current_paragraphs)}") # Si la divisin por \\n\\n no funcion bien (pocos prrafos o uno muy largo) # Y el texto original es suficientemente largo, intentar dividir por un solo \\n # Condicin para intentar divisin por \\n simple: # 1. El bloque de texto original es ms largo que 'try_single_newline_split_if_block_longer_than' # 2. Y ( (la divisin por \\n\\n result en 1 solo prrafo Y ese prrafo es ms largo que min_chars_for_split) # O (la divisin por \\n\\n result en ms de 1 prrafo PERO la mayora son muy cortos) ) # Esta segunda parte de la condicin es difcil de cuantificar con precisin sin ms heursticas, # por ahora, simplificaremos: si el texto es largo y la divisin por \\n\\n result en <=1 prrafo significativo. should_try_single_newline_split = False if len(text) > try_single_newline_split_threshold: if len(current_paragraphs) == 1 and len(current_paragraphs[0]) > min_chars_for_split: should_try_single_newline_split = True logger.debug(f"[_split_text_into_paragraphs] Condicin para split por \\\\n simple CUMPLIDA (1 prrafo largo despus de \\\\n\\\\n).") elif not current_paragraphs: # Si \\n\\n no produjo nada (raro si hay texto) should_try_single_newline_split = True logger.debug(f"[_split_text_into_paragraphs] Condicin para split por \\\\n simple CUMPLIDA (ningn prrafo despus de \\\\n\\\\n).") if should_try_single_newline_split: logger.info(f"[_split_text_into_paragraphs] Texto largo y r'\n\n' no dividi bien. Intentando con un solo r'\n'.") # Usar una expresin regular para capturar un solo salto de lnea como delimitador raw_paragraphs_single_nl = re.split(r'\n', text) logger.debug(f"[_split_text_into_paragraphs] Intento con r'\n': {len(raw_paragraphs_single_nl)} prrafos crudos.") for i, p_text in enumerate(raw_paragraphs_single_nl): # Corregido: Escapar la barra invertida en \\n dentro del replace log_text_preview_single = p_text[:100].replace('\n', '<NL>') logger.debug(f" Raw para (simple \\\\n) #{i}: '{log_text_preview_single}{'...' if len(p_text) > 100 else ''}' (len: {len(p_text)})") current_paragraphs = [p.strip() for p in raw_paragraphs_single_nl if p.strip()] min_chars_for_split = min_chars_for_single_newline_para # Usar el umbral especfico para \\n logger.debug(f"[_split_text_into_paragraphs] Prrafos despus de strip y filtro de vacos (simple \\\\n): {len(current_paragraphs)}. Usando min_chars_for_split: {min_chars_for_split}") if not current_paragraphs or (len(current_paragraphs) == 1 and len(current_paragraphs[0]) < min_chars_for_split): # Si no hay prrafos o solo uno que es demasiado corto, devolver el texto original como un solo bloque # (siempre que el texto original en s mismo supere el umbral mnimo, de lo contrario se descarta) if len(text.strip()) >= min_chars_for_split: # Comparar con el umbral activo logger.debug(f"[_split_text_into_paragraphs] Devolviendo texto original como nico prrafo (longitud: {len(text.strip())} >= umbral: {min_chars_for_split}).") paragraphs_data.append((text.strip(), base_order, original_coordinates)) else: logger.debug(f"[_split_text_into_paragraphs] Texto original no cumple umbral ({len(text.strip())} < {min_chars_for_split}), no se devuelve nada.") else: sub_order = 0 for i, paragraph_text in enumerate(current_paragraphs): # logger.debug(f" Procesando prrafo candidato #{i}: '{paragraph_text[:100].replace('\\n','<NL>')}{'...' if len(paragraph_text) > 100 else ''}' (len: {len(paragraph_text)})") if len(paragraph_text) >= min_chars_for_split: # Crear nuevas coordenadas si las originales existen (simplificado, podra mejorarse) new_coords = None if original_coordinates: # Esta es una simplificacin. Una mejor aproximacin requerira # recalcular las coordenadas del prrafo dentro del bloque original. # Por ahora, simplemente propagamos las coordenadas del bloque original. new_coords = original_coordinates.copy() # Podramos aadir un offset o un identificador de sub-bloque aqu # new_coords['paragraph_index'] = sub_order final_order = base_order + (sub_order * 0.001) # Asegurar orden incremental paragraphs_data.append((paragraph_text, final_order, new_coords)) # Corregido: Crear variable para el texto del log log_paragraph_preview = paragraph_text[:80].replace('\n','<NL>') logger.debug(f" PRRAFO AADIDO #{sub_order} (orig idx {i}): orden {final_order:.3f}, len {len(paragraph_text)}, texto: '{log_paragraph_preview}{'...' if len(paragraph_text) > 80 else ''}'") sub_order += 1 else: # Corregido: Escapar la barra invertida en \\n dentro del replace log_text_preview_discarded = paragraph_text[:80].replace('\n','<NL>') logger.debug(f" Prrafo DESCARTADO (orig idx {i}) por longitud: {len(paragraph_text)} < {min_chars_for_split}. Texto: '{log_text_preview_discarded}{'...' if len(paragraph_text) > 80 else ''}'") logger.debug(f"[_split_text_into_paragraphs] Finalizado. Total prrafos generados: {len(paragraphs_data)}") return paragraphs_data def _merge_contiguous_fitz_blocks(self, blocks: List[Dict]) -> List[Dict]: if not blocks: return [] merged_blocks: List[Dict] = [] current_merged_block: Optional[Dict] = None max_gap = self.config.get('max_vertical_gap_for_merge_pt', CommonBlockPreprocessor.DEFAULT_CONFIG['max_vertical_gap_for_merge_pt']) for block_idx, block in enumerate(blocks): block_text_content = block.get('text') or block.get('content') block_type = block.get('type', 'unknown_block') if block_type == 'text_block' and block_text_content and 'coordinates' in block and 'source_page_number' in block: cleaned_current_text = self._clean_block_text(block_text_content) # Limpiar texto antes de decidir if not cleaned_current_text: # Saltar si el texto limpiado est vaco if current_merged_block is not None: # Guardar el anterior si existe merged_blocks.append(current_merged_block) current_merged_block = None # No aadir este bloque vaco, pero s mantener su lugar si es un bloque no textual importante # OJO: esta lgica puede necesitar refinar si los bloques vacos deben mantenerse continue if current_merged_block is None: current_merged_block = block.copy() current_merged_block['text'] = cleaned_current_text else: prev_coords = current_merged_block['coordinates'] curr_coords = block['coordinates'] vertical_gap = curr_coords['y0'] - prev_coords['y1'] # logger.debug(f"Merging? Block {block_idx}: prev_y1={prev_coords['y1']:.2f}, curr_y0={curr_coords['y0']:.2f}, gap={vertical_gap:.2f}, max_gap={max_gap}, same_page={current_merged_block.get('source_page_number') == block.get('source_page_number')}") if current_merged_block.get('source_page_number') == block.get('source_page_number') and vertical_gap <= max_gap and vertical_gap >= -max_gap/2: # Permitir pequea superposicin tambin current_merged_block['text'] += f" {cleaned_current_text}" # Unir con espacio current_merged_block['coordinates']['y1'] = max(prev_coords['y1'], curr_coords['y1']) current_merged_block['coordinates']['x0'] = min(prev_coords['x0'], curr_coords['x0']) current_merged_block['coordinates']['x1'] = max(prev_coords['x1'], curr_coords['x1']) # Conservar metadatos del primer bloque del grupo else: merged_blocks.append(current_merged_block) current_merged_block = block.copy() current_merged_block['text'] = cleaned_current_text else: # Bloque no textual o sin info para fusionar if current_merged_block is not None: merged_blocks.append(current_merged_block) current_merged_block = None merged_blocks.append(block) if current_merged_block is not None: merged_blocks.append(current_merged_block) # logger.debug(f"Bloques despus de _merge_contiguous_fitz_blocks: {len(merged_blocks)}") # if merged_blocks and len(merged_blocks) < 15: # for i, mb in enumerate(merged_blocks[:15]): # logger.debug(f" Merged Block {i}: page={mb.get('source_page_number')}, order={mb.get('order_in_document')} text='{mb.get('text', '')[:100]}...'") return merged_blocks def process(self, blocks: List[Dict], document_metadata: Dict[str, Any]) -> List[Dict]: logger.debug(f"CommonBlockPreprocessor: Iniciando procesamiento de {len(blocks)} bloques para {document_metadata.get('source_file_path', 'archivo desconocido')}.") processed_document_metadata = document_metadata.copy() # 1. Fusionar bloques si son nativos de Fitz y la configuracin lo permite should_merge = ( processed_document_metadata.get('blocks_are_fitz_native', False) and \ self.config.get('split_blocks_into_paragraphs', True) ) if should_merge: logger.info(f"Detectados bloques nativos de Fitz ({len(blocks)}), intentando fusionar bloques contiguos.") blocks = self._merge_contiguous_fitz_blocks(blocks) logger.info(f"Nmero de bloques despus de la fusin inicial de Fitz: {len(blocks)}") final_processed_blocks: List[Dict] = [] for i, block_data in enumerate(blocks): current_block_metadata = block_data.copy() block_text_content = current_block_metadata.pop('text', None) or \ current_block_metadata.pop('content', None) original_block_coordinates = current_block_metadata.get('coordinates') if block_text_content is not None: cleaned_text = self._clean_block_text(block_text_content) if not cleaned_text: continue # DEBUG logs que aadimos para cleaned_text (los mantenemos por ahora) if i < 5: logger.debug(f"POST-MERGE Bloque {i} - cleaned_text (primeros 200 chars): '{cleaned_text[:200] if cleaned_text else ''}'") logger.debug(f"POST-MERGE Bloque {i} - len(cleaned_text): {len(cleaned_text) if cleaned_text else 0}") # Aqu est la condicin clave: should_split_paragraphs_config = self.config.get('split_blocks_into_paragraphs', True) min_area_config = self.config.get('min_block_area_for_split', 0) block_has_sufficient_area_eval = True # Asumimos True por ahora si no hay 'coordinates' calculated_area = -1 # Valor por defecto si no hay coords o no se calcula if original_block_coordinates: width = original_block_coordinates['x1'] - original_block_coordinates['x0'] height = original_block_coordinates['y1'] - original_block_coordinates['y0'] calculated_area = width * height if calculated_area < min_area_config: block_has_sufficient_area_eval = False # LOGS DE DEPURACIN PARA LA CONDICIN DE DIVISIN logger.debug(f"PRE-SPLIT Bloque {i} (despus de fusin y limpieza):") logger.debug(f" should_split_paragraphs_config: {should_split_paragraphs_config}") logger.debug(f" original_block_coordinates: {original_block_coordinates is not None}") if original_block_coordinates: logger.debug(f" coords: x0={original_block_coordinates['x0']:.2f}, y0={original_block_coordinates['y0']:.2f}, x1={original_block_coordinates['x1']:.2f}, y1={original_block_coordinates['y1']:.2f}") logger.debug(f" calculated_area: {calculated_area:.2f}") logger.debug(f" min_area_config: {min_area_config}") logger.debug(f" block_has_sufficient_area_eval: {block_has_sufficient_area_eval}") logger.debug(f" Texto (primeros 50 chars): '{cleaned_text[:50] if cleaned_text else ''}'") logger.debug(f" Longitud texto: {len(cleaned_text) if cleaned_text else 0}") if should_split_paragraphs_config and block_has_sufficient_area_eval: logger.debug(f" ENTRANDO a _split_text_into_paragraphs para bloque {i}.") paragraphs_with_order = self._split_text_into_paragraphs( cleaned_text, float(current_block_metadata.get('order_in_document', i)), original_block_coordinates ) for sub_text, sub_order, sub_coords in paragraphs_with_order: new_sub_block = current_block_metadata.copy() new_sub_block['text'] = sub_text new_sub_block['order_in_document'] = sub_order if sub_coords: new_sub_block['coordinates'] = sub_coords new_sub_block['type'] = current_block_metadata.get('type', 'text_block') final_processed_blocks.append(new_sub_block) else: logger.debug(f" SALTANDO _split_text_into_paragraphs para bloque {i}. Razn: should_split_config={should_split_paragraphs_config}, has_sufficient_area={block_has_sufficient_area_eval}") # No se divide, se aade el bloque limpiado tal cual processed_block_metadata = current_block_metadata.copy() processed_block_metadata['text'] = cleaned_text processed_block_metadata['type'] = processed_block_metadata.get('type', 'text_block') final_processed_blocks.append(processed_block_metadata) elif current_block_metadata: current_block_metadata['type'] = current_block_metadata.get('type', 'unknown_empty_block') final_processed_blocks.append(current_block_metadata) logger.info(f"CommonBlockPreprocessor: Finalizado procesamiento, {len(final_processed_blocks)} bloques resultantes.") return final_processed_blocks, processed_document_metadata if __name__ == '__main__': # Ejemplo de uso bsico logging.basicConfig(level=logging.DEBUG) print("--- Ejemplo 1: Configuracin por defecto ---") preprocessor_default = CommonBlockPreprocessor() sample_metadata_1 = { 'source_file_path': '/path/to/my_document_2023-10-25.txt', 'author': 'Test Author' } sample_blocks_1 = [ {'text': 'Bloque 1.\r\nSalto Windows. \x00 NUL char.\n\nEste es otro prrafo en el mismo bloque.', 'order_in_document': 0, 'source_page_number': 1}, {'text': 'Bloque 2\rSalto Mac.', 'order_in_document': 1, 'source_page_number': 2} ] processed_blocks_1, processed_metadata_1 = preprocessor_default.process(sample_blocks_1, sample_metadata_1) print("Metadata 1:", processed_metadata_1) print("Blocks 1:") for b in processed_blocks_1: print(b) print("\n--- Ejemplo 2: Sin extraccin de fecha del nombre de archivo y sin divisin de prrafos ---") config_no_split = { 'extract_filename_date': False, 'normalize_line_endings': True, 'remove_nul_chars': True, 'split_blocks_into_paragraphs': False } preprocessor_no_split = CommonBlockPreprocessor(config=config_no_split) sample_metadata_2 = { 'source_file_path': '/path/to/another_doc_2024_01_15.md', 'detected_date': '2024-01-01' # Fecha preexistente de propiedades del doc } sample_blocks_2 = [ {'text': 'Prrafo nico.\nAunque tenga saltos.\n\nY una lnea vaca en medio.', 'order_in_document': 'A'} ] processed_blocks_2, processed_metadata_2 = preprocessor_no_split.process(sample_blocks_2, sample_metadata_2) print("Metadata 2:", processed_metadata_2) print("Blocks 2:") for b in processed_blocks_2: print(b) print("\n--- Ejemplo 3: Texto corto, no se divide ---") preprocessor_short_text = CommonBlockPreprocessor(config={'min_chars_for_paragraph_split': 100}) sample_blocks_short = [ {'text': 'Texto corto.\nNo se divide.', 'order_in_document': 0} ] processed_blocks_short, _ = preprocessor_short_text.process(sample_blocks_short, {}) print("Blocks Short:") for b in processed_blocks_short: print(b) print("\n--- Ejemplo 4: Bloque sin texto ---") sample_blocks_no_text = [ {'order_in_document': 0, 'source_page_number': 1, 'type': 'image_placeholder'} ] processed_blocks_no_text, _ = preprocessor_default.process(sample_blocks_no_text, {}) print("Blocks No Text:") for b in processed_blocks_no_text: print(b)
---

#   init  

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** pre_processors

#   init  

from .common_block_preprocessor import CommonBlockPreprocessor __all__ = [ 'CommonBlockPreprocessor' ]
---

# content profiles.schema

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** schema

# content profiles.schema

{ "$schema": "http://json-schema.org/draft-07/schema#", "title": "Content Profiles Configuration Schema", "description": "Schema for validating the content_profiles.json file, which defines different profiles for processing various types of content.", "type": "object", "patternProperties": { "^[a-zA-Z0-9_\\-]+$": { "type": "object", "properties": { "description": { "type": "string", "description": "A human-readable description of the content profile." }, "source_format_group": { "type": "string", "enum": ["document", "text_plain", "json_like"], "description": "The general format group of the source files (e.g., DOCX, PDF, TXT are 'document'; JSON, NDJSON are 'json_like')." }, "content_kind": { "type": "string", "enum": ["prose", "messages", "reference_material", "poetry", "structured_data"], "description": "The kind of content this profile handles (e.g., book chapters, chat messages, EGW references)." }, "parser_config": { "type": ["object", "null"], "description": "Configuration for parsing source_format_group: 'json_like'. Null for other types.", "properties": { "json_item_prefix_ijson": { "type": "string", "description": "The ijson path prefix to access individual items in a large JSON array (e.g., 'messages.item')." }, "text_property_paths": { "type": "array", "items": {"type": "string"}, "description": "Ordered list of dot-notation paths to find the main text content within a JSON object." }, "metadata_paths": { "type": "object", "description": "Mapping of ProcessedContentItem fields to dot-notation paths in the JSON object.", "properties": { "publication_date": {"type": "string"}, "author_name": {"type": "string"}, "title": {"type": "string"} }, "additionalProperties": {"type": "string"} }, "filter_rules_schema": { "type": "object", "description": "Optional JSON schema definition for filter_rules that might be provided in a job config." }, "deep_text_config": { "type": "object", "description": "Configuration for deeply searching text within complex JSON structures.", "properties": { "target_keys": {"type": "array", "items": {"type": "string"}}, "ignore_keys": {"type": "array", "items": {"type": "string"}}, "min_length": {"type": "integer"} }, "required": ["target_keys"] } }, "if": { "properties": { "source_format_group": { "const": "json_like" } } }, "then": { "required": ["text_property_paths"] } }, "converter_config": { "type": ["object", "null"], "description": "Configuration for file conversion (e.g., Pandoc options). Null if not applicable.", "properties": { "docx_to_markdown_options": { "type": "array", "items": {"type": "string"} }, "text_encoding": {"type": "string"} } }, "chunking_strategy_name": { "type": "string", "description": "The name of the ChunkingStrategy class to use." }, "chunking_config": { "type": "object", "description": "Specific configuration for the chosen chunking strategy." }, "post_chunk_processors": { "type": "array", "items": {"type": "string"}, "description": "List of names of post-chunk processing functions." } }, "required": [ "description", "source_format_group", "content_kind", "chunking_strategy_name" ], "additionalProperties": false } }, "additionalProperties": false }
---

# jobs config.schema

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** schema

# jobs config.schema

{ "$schema": "http://json-schema.org/draft-07/schema#", "title": "Processing Jobs Configuration Schema", "description": "Schema for validating the jobs_config.json file, which defines a list of processing jobs.", "type": "array", "items": { "type": "object", "properties": { "job_id": { "type": "string", "description": "A unique identifier for the job (e.g., a UUID or a descriptive name)." }, "author_name": { "type": "string", "description": "The name of the author for this job." }, "language_code": { "type": "string", "description": "The primary language code for the content (e.g., 'es', 'en')." }, "source_directory_name": { "type": "string", "description": "The name of the sub-directory within 'dataset/raw_data/' containing the source files for this job." }, "content_profile_name": { "type": "string", "description": "The name of the content profile (from content_profiles.json) to use for this job." }, "origin_type_name": { "type": "string", "description": "A descriptive name for the origin type of the content (e.g., 'Telegram Archive', 'EGW Digital Library')." }, "acquisition_date": { "type": ["string", "null"], "format": "date", "description": "The date when the source material was acquired (YYYY-MM-DD). Optional." }, "force_null_publication_date": { "type": "boolean", "default": false, "description": "If true, forces the publication_date to null, ignoring any extracted dates." }, "filter_rules": { "type": ["array", "null"], "description": "Specific filter rules for 'json_like' content, conforming to the schema defined in the selected content profile's parser_config.filter_rules_schema. Optional.", "items": { "type": "object" } }, "job_specific_metadata": { "type": ["object", "null"], "description": "Any other job-specific metadata that might be useful. Optional.", "additionalProperties": true } }, "required": [ "job_id", "author_name", "language_code", "source_directory_name", "content_profile_name", "origin_type_name" ], "additionalProperties": false } }
---

# book structure

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** profiles

# book structure

name: book_structure # REGLA: Prohibido el uso de palabras clave o semntica para detectar encabezados. Solo seales estructurales. description: "Detecta estructura jerrquica en libros y documentos SOLO con seales estructurales (longitud, formato visual, numeraciones, maysculas, markdown)." segmenter: heading file_types: [".txt", ".md", ".docx", ".pdf"] thresholds: max_heading_length: 200 # Longitud mxima para considerar encabezado max_section_depth: 3 # Profundidad mxima de anidacin min_heading_content: 1 # Mnimo de bloques para considerar seccin max_empty_after_heading: 5 # Mximo de lneas vacas tras encabezado heading_patterns: - "^#{1,6} " # Encabezados Markdown (# Ttulo) - "^\\d+\\.\\d*\\s+" # Numeracin tipo "1.2 Ttulo" - "^[A-Z][A-Z- ]{3,}$" # TTULO EN MAYSCULAS - "^[A-Z][A-Z- ]{3,}\\.?" # TTULO EN MAYSCULAS seguido de punto - "^[A-Z][A-Z ]{0,2}[A-Z- ]{3,}\\.{0,3}" # Maysculas con o sin puntos # PROHIBIDO: No incluir patrones con palabras como Captulo, Artculo, Prlogo, etc. debug: true # Activar modo debug para ver ms informacin post_processor: text_normalizer post_processor_config: min_length: 10 # Longitud mnima para conservar unidad metadata_map: # Mapeo de campos internos a nombres finales titulo: title nivel: level contenido: content subsecciones: subsections exporter: ndjson
---

# chapter heading

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** profiles

# chapter heading

name: chapter_heading description: "Detecta encabezados temticos y secciones en documentos sin numeracin tradicional" segmenter: heading file_types: [".docx", ".txt", ".md", ".pdf"] thresholds: max_heading_length: 200 # Longitud mxima para considerar encabezado (mayor que el perfil book_structure) max_section_depth: 4 # Profundidad mxima de anidacin min_heading_content: 1 # Mnimo de bloques para considerar seccin (menor que el perfil book_structure) max_empty_after_heading: 5 # Mximo de lneas vacas tras encabezado heading_patterns: - "^[A-Z][A-Z- ]{2,}$" # TTULO EN MAYSCULAS - "^[A-Z][A-Z- ]{2,}\\.+" # TTULO EN MAYSCULAS seguido de puntos - "^[A-Z][\\w ]{0,30}\\.\\.\\." # Ttulo con puntos suspensivos - "^[A-Z-][\\w ]{0,40}:" # Ttulos con formato "Palabra: texto" - "^(Naturaleza|Capacidad|Libertad|Impulsos|La|El|Los|Las|Cmo|Gnesis) [\\w ]{2,}" # Patrones comunes en el documento flat: true # Convertir la estructura jerrquica en una lista plana post_processor: text_normalizer post_processor_config: min_length: 10 # Longitud mnima para conservar unidad metadata_map: # Mapeo de campos internos a nombres finales titulo: title nivel: level contenido: content subsecciones: subsections exporter: ndjson
---

# content profiles

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** profiles

# content profiles

{ "perfil_docx_heading": { "name": "perfil_docx_heading", "description": "Procesa archivos DOCX usando HeadingSegmenter.", "segmenter": "heading", "file_types": [".docx"] } }
---

# perfil docx heading

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** profiles

# perfil docx heading

name: perfil_docx_heading description: "Procesa archivos DOCX usando DocxLoader y HeadingSegmenter, generando ProcessedContentItems con jerarqua contextual." segmenter: heading # Indica a ProfileManager que use HeadingSegmenter # No es necesario especificar el loader, ProfileManager lo infiere de la extensin .docx # Puedes aadir configuracin para CommonBlockPreprocessor si es necesario aqu common_preprocessor_config: min_chars_for_paragraph_split: 5 min_chars_for_single_newline_paragraph: 5 split_blocks_into_paragraphs: true remove_nul_bytes: true normalize_line_endings: true # Configuracin especfica para HeadingSegmenter (si fuera necesaria, como title_patterns) # Por ahora, HeadingSegmenter usa su lgica interna basada en niveles de heading del DOCX # thresholds: # min_heading_level: 1 # Ejemplo
---

# poem or lyrics

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** profiles

# poem or lyrics

name: poem_or_lyrics description: "Detecta poemas y canciones en archivos de texto" segmenter: verse file_types: [".txt", ".md", ".docx", ".pdf"] thresholds: max_verse_length: 120 # Longitud mxima para considerar verso max_title_length: 80 # Longitud mxima para considerar ttulo max_space_ratio: 0.35 # Proporcin mxima de espacios/caracteres min_consecutive_verses: 3 # Mnimo de versos consecutivos para detectar poema min_stanza_verses: 2 # Mnimo de versos para formar estrofa max_empty_in_stanza: 2 # Mximo de lneas vacas en misma estrofa min_empty_between_stanzas: 2 # Mnimo de lneas vacas entre estrofas max_empty_between_stanzas: 3 # Mximo de lneas vacas entre estrofas max_empty_end_poem: 3 # Mximo de lneas vacas antes de fin de poema title_patterns: # Patrones para detectar ttulos - "^# " # Markdown H1 - "^\\* " # Asterisco inicial - "^> " # Cita - "^[A-Z ]{4,}:$" # TTULO: (todo maysculas seguido de dos puntos) post_processor: text_normalizer post_processor_config: min_length: 30 # Longitud mnima para conservar unidad min_verses: 2 # Mnimo de versos para conservar poema metadata_map: # Mapeo de campos internos a nombres finales titulo: title versos: verses_count estrofas: stanzas_count exporter: ndjson
---

# levantar meilisearch

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# levantar meilisearch

import subprocess import os import sys from pathlib import Path # Ruta al ejecutable de Meilisearch (ajusta si cambias el nombre o carpeta) MEILI_PATH = Path(__file__).parent.parent / 'meilisearch' / 'meilisearch-windows-amd64.exe' if not MEILI_PATH.exists(): print(f"No se encontr el ejecutable de Meilisearch en: {MEILI_PATH}") sys.exit(1) # Opcional: puedes agregar argumentos como --master-key o --db-path si lo necesitas cmd = [str(MEILI_PATH)] print(f"Levantando Meilisearch desde: {MEILI_PATH}") subprocess.Popen(cmd, cwd=MEILI_PATH.parent) print("Meilisearch iniciado en segundo plano. Puedes acceder a http://127.0.0.1:7700")
---

# probar embeddings

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# probar embeddings

#!/usr/bin/env python3 # -*- coding: utf-8 -*- """ Script para probar la generacin de embeddings """ from embedding_service import EmbeddingService def probar_embeddings(): print("Probando servicio de embeddings...") # Inicializar servicio service = EmbeddingService() # Probar con varios textos textos_prueba = [ "amor y fe", "La importancia de la fe en tiempos difciles", "Un texto largo sobre diversos temas filosficos y existenciales que debera funcionar sin problemas", "a", # Texto muy corto "", # Texto vaco "amor", "fe", "cristianismo" ] print("\nProbando textos:") print("----------------") for texto in textos_prueba: print(f"\nProbando texto: '{texto}'") embedding = service.generar_embedding(texto) if embedding: print(f" Embedding generado correctamente - Dimensin: {len(embedding)}") # Mostrar primeros 5 valores para verificar print(f"Primeros valores: {embedding[:5]}") else: print(f" No se pudo generar embedding") print("\nPrueba completada") if __name__ == "__main__": probar_embeddings()
---

# api conexion

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# api conexion

#!/usr/bin/env python3 # -*- coding: utf-8 -*- """ API de conexin para IAs - Biblioteca de Conocimiento Personal Este mdulo proporciona funciones para que diferentes modelos de IA puedan acceder y analizar el contenido de la biblioteca de conocimiento. """ import os import json import sqlite3 import datetime import importlib.util from pathlib import Path from flask import Flask, request, jsonify, send_file from flask_cors import CORS from dotenv import load_dotenv import subprocess import socket import sys import meilisearch # --- Levantar Meilisearch automticamente si no est corriendo --- def is_meilisearch_running(host='127.0.0.1', port=7700): try: with socket.create_connection((host, port), timeout=1): return True except Exception: return False if not is_meilisearch_running(): meili_script = Path(__file__).parent / 'levantar_meilisearch.py' if meili_script.exists(): print("[INFO] Meilisearch no est corriendo. Iniciando...") subprocess.Popen([sys.executable, str(meili_script)]) else: print(f"[ADVERTENCIA] No se encontr {meili_script}. Meilisearch no se levantar automticamente.") # --- Inicializar campos filterable y sort en Meilisearch al arrancar el backend --- def inicializar_meilisearch_indices(): try: client = meilisearch.Client('http://127.0.0.1:7700') # Configuracin para el ndice documentos (embeddings) try: index_doc = client.index('documentos') # Asegura que estos campos sean filtrables y ordenables index_doc.update_filterable_attributes(['id', 'autor', 'fecha_creacion', 'fecha_importacion', 'fuente_id', 'plataforma_id', 'idioma']) index_doc.update_sortable_attributes(['id', 'fecha_creacion', 'fecha_importacion']) index_doc.update_searchable_attributes(['contenido_texto', 'autor', 'contexto', 'url_original']) # Configurar bsqueda vectorial # Es necesario definir el nombre del campo donde estn los embeddings try: # Verificamos si ya existe la configuracin index_config = index_doc.get_settings() print(f"[INFO] Configuracin actual del ndice: {index_config}") # Configuramos los embedders index_doc.update_embedders({ 'default': { 'source': 'embedding', 'dimensions': 768, # Dimensiones para el modelo 'paraphrase-multilingual-mpnet-base-v2' 'distance': 'cosine' # Usar similitud coseno } }) print(f"[INFO] Meilisearch: ndice 'documentos' configurado con bsqueda vectorial") # NUEVO: Aumentar el lmite de paginationTotalHits a 100000 index_doc.update_settings({"paginationTotalHits": 100000}) print(f"[INFO] Meilisearch: paginationTotalHits configurado a 100000 para 'documentos'") # Verificar que se aplic correctamente updated_config = index_doc.get_settings() print(f"[INFO] Nueva configuracin del ndice: {updated_config}") except Exception as e: print(f"[ADVERTENCIA] Error configurando embedders: {e}") except Exception as e: print(f"[ADVERTENCIA] No se pudo configurar el ndice 'documentos': {e}") # Mantener compatibilidad con el ndice 'contenidos' try: index = client.index('contenidos') # Asegura que estos campos sean filtrables y ordenables filterable = ['autor'] sortable = ['fecha'] index.update_filterable_attributes(filterable) index.update_sortable_attributes(sortable) print(f"[INFO] Meilisearch: Campos filterable={filterable}, sortable={sortable} inicializados para 'contenidos'.") except Exception as e: print(f"[ADVERTENCIA] No se pudo inicializar el ndice 'contenidos': {e}") except Exception as e: print(f"[ADVERTENCIA] No se pudo conectar con Meilisearch: {e}") # Cargar variables de entorno load_dotenv() # Configuracin de rutas BASE_DIR = Path(os.getenv("BASE_DIR", Path(__file__).resolve().parent.parent)) DB_PATH = BASE_DIR / 'data' / 'biblioteca.db' CONFIG_DIR = BASE_DIR / 'config' INDICES_DIR = BASE_DIR / 'shared' / 'indices' CONTENIDO_DIR = BASE_DIR / 'shared' / 'documentation' # Imprimir informacin de depuracin print(f"BASE_DIR: {BASE_DIR.absolute()}") print(f"DB_PATH: {DB_PATH.absolute()}") print(f"DB_PATH exists: {DB_PATH.exists()}") # Inicializar aplicacin Flask app = Flask(__name__) CORS(app, origins=[f"http://localhost:{port}" for port in range(5170, 5180)] + [f"http://127.0.0.1:{port}" for port in range(5170, 5180)]) # Inicializar proveedores de LLM segn las claves disponibles llm_providers = {} def cargar_llm_providers(): """Carga los proveedores de LLM disponibles.""" # Verificar Gemini gemini_key = os.getenv("GEMINI_API_KEY") if gemini_key: try: import google.generativeai as genai genai.configure(api_key=gemini_key) def generar_con_gemini(prompt): model = genai.GenerativeModel('gemini-pro') response = model.generate_content(prompt) return response.text llm_providers["gemini"] = generar_con_gemini print("Proveedor LLM: Gemini cargado correctamente") except ImportError: print("No se pudo cargar el proveedor Gemini. Librera no instalada.") # Verificar OpenAI openai_key = os.getenv("OPENAI_API_KEY") if openai_key: try: import openai client = openai.OpenAI(api_key=openai_key) def generar_con_openai(prompt): response = client.chat.completions.create( model="gpt-3.5-turbo", messages=[ {"role": "system", "content": "Eres un asistente que genera contenido basado en los textos proporcionados."}, {"role": "user", "content": prompt} ] ) return response.choices[0].message.content llm_providers["openai"] = generar_con_openai print("Proveedor LLM: OpenAI cargado correctamente") except ImportError: print("No se pudo cargar el proveedor OpenAI. Librera no instalada.") # Intentar cargar los proveedores de LLM cargar_llm_providers() def conectar_db(): """Establece conexin con la base de datos SQLite.""" # Asegurarse que el directorio de datos exista os.makedirs(DB_PATH.parent, exist_ok=True) conn = sqlite3.connect(DB_PATH) conn.row_factory = sqlite3.Row return conn def tabla_existe(conn, nombre_tabla): """Verifica si una tabla existe en la base de datos.""" cursor = conn.cursor() cursor.execute( "SELECT name FROM sqlite_master WHERE type='table' AND name=?", (nombre_tabla,) ) return cursor.fetchone() is not None @app.route('/api/info', methods=['GET']) def get_info(): """Proporciona informacin general sobre la biblioteca.""" conn = conectar_db() cursor = conn.cursor() # Verificar si existe la tabla contenidos total_contenidos = 0 fechas = {'primera': None, 'ultima': None} distribucion = [] if tabla_existe(conn, 'contenidos'): # Obtener estadsticas bsicas cursor.execute('SELECT COUNT(*) as total FROM contenidos') total_contenidos = cursor.fetchone()['total'] # Obtener rango de fechas cursor.execute('SELECT MIN(fecha_creacion) as primera, MAX(fecha_creacion) as ultima FROM contenidos') fechas = cursor.fetchone() # Obtener distribucin por plataforma if tabla_existe(conn, 'plataformas'): cursor.execute(''' SELECT p.nombre, COUNT(*) as cantidad FROM contenidos c JOIN plataformas p ON c.plataforma_id = p.id GROUP BY p.nombre ORDER BY cantidad DESC ''') for row in cursor.fetchall(): distribucion.append({ 'plataforma': row['nombre'], 'cantidad': row['cantidad'] }) # Verificar si la tabla temas existe total_temas = 0 if tabla_existe(conn, 'temas'): cursor.execute('SELECT COUNT(*) as total FROM temas') total_temas = cursor.fetchone()['total'] # Nuevo: contar autores nicos total_autores = 0 cursor.execute('SELECT COUNT(DISTINCT autor) as total FROM contenidos WHERE autor IS NOT NULL AND autor != ""') total_autores = cursor.fetchone()['total'] conn.close() return jsonify({ 'total_contenidos': total_contenidos, 'total_temas': total_temas, 'total_autores': total_autores, 'rango_fechas': { 'primera': fechas['primera'], 'ultima': fechas['ultima'] }, 'distribucion_plataformas': distribucion, 'descripcion': 'Biblioteca de Conocimiento Personal de Ismael Lpez-Silvero Guimarais' }) @app.route('/api/contenido', methods=['GET']) def get_contenido(): """ Busca contenido segn criterios especificados. Parmetros de consulta: - tema: ID o nombre del tema - texto: Texto a buscar - plataforma: Nombre de la plataforma - fecha_inicio: Fecha de inicio (YYYY-MM-DD) - fecha_fin: Fecha de fin (YYYY-MM-DD) - limite: Nmero mximo de resultados (default: 100) """ conn = conectar_db() cursor = conn.cursor() # Verificar si existe la tabla contenidos if not tabla_existe(conn, 'contenidos'): conn.close() return jsonify([]) # Verificar si existen las tablas relacionadas if not tabla_existe(conn, 'plataformas') or not tabla_existe(conn, 'fuentes'): conn.close() return jsonify([]) # Obtener parmetros tema = request.args.get('tema') contenido_texto = request.args.get('contenido_texto') plataforma = request.args.get('plataforma') fecha_inicio = request.args.get('fecha_inicio') fecha_fin = request.args.get('fecha_fin') limite = request.args.get('limite', 100, type=int) autor_param = request.args.get('autor') # Construir consulta base query = ''' SELECT c.id, c.contenido_texto, c.fecha_creacion, p.nombre as plataforma, f.nombre as fuente FROM contenidos c JOIN plataformas p ON c.plataforma_id = p.id JOIN fuentes f ON c.fuente_id = f.id ''' conditions = [] params = [] # Aadir condiciones segn parmetros if tema: # Verificar si existe la tabla temas if tabla_existe(conn, 'temas') and tabla_existe(conn, 'contenido_tema'): # Verificar si es ID o nombre try: tema_id = int(tema) conditions.append(''' c.id IN ( SELECT contenido_id FROM contenido_tema WHERE tema_id = ? ) ''') params.append(tema_id) except ValueError: conditions.append(''' c.id IN ( SELECT ct.contenido_id FROM contenido_tema ct JOIN temas t ON ct.tema_id = t.id WHERE t.nombre LIKE ? ) ''') params.append(f'%{tema}%') else: # Si la tabla temas no existe, ignorar este filtro pass if contenido_texto: # Verificar si existe la tabla fts if tabla_existe(conn, 'contenidos_fts'): conditions.append(''' c.id IN ( SELECT rowid FROM contenidos_fts WHERE contenidos_fts MATCH ? ) ''') params.append(contenido_texto) if plataforma: conditions.append('p.nombre LIKE ?') params.append(f'%{plataforma}%') if fecha_inicio: conditions.append('c.fecha_creacion >= ?') params.append(fecha_inicio) if fecha_fin: conditions.append('c.fecha_creacion <= ?') params.append(fecha_fin) if autor_param: autores = [a.strip() for a in autor_param.split(',') if a.strip()] if autores: conditions.append('(' + ' OR '.join(['c.autor = ?' for _ in autores]) + ')') params.extend(autores) # Aadir condiciones a la consulta if conditions: query += ' WHERE ' + ' AND '.join(conditions) # Aadir orden y lmite query += ' ORDER BY c.fecha_creacion DESC LIMIT ?' params.append(limite) # Ejecutar consulta cursor.execute(query, params) # Procesar resultados resultados = [] for row in cursor.fetchall(): # Obtener temas relacionados temas = [] if tabla_existe(conn, 'temas') and tabla_existe(conn, 'contenido_tema'): cursor2 = conn.cursor() cursor2.execute(''' SELECT t.id, t.nombre, ct.relevancia FROM temas t JOIN contenido_tema ct ON t.id = ct.tema_id WHERE ct.contenido_id = ? ORDER BY ct.relevancia DESC ''', (row['id'],)) for tema_row in cursor2.fetchall(): temas.append({ 'id': tema_row['id'], 'nombre': tema_row['nombre'], 'relevancia': tema_row['relevancia'] }) resultados.append({ 'id': row['id'], 'contenido_texto': row['contenido_texto'], 'fecha': row['fecha_creacion'], 'plataforma': row['plataforma'], 'fuente': row['fuente'], 'idioma': row['idioma'] if 'idioma' in row.keys() else 'es', 'temas': temas }) conn.close() return jsonify(resultados) @app.route('/api/busqueda', methods=['GET']) def busqueda_general(): """ Bsqueda general de texto usando Meilisearch. Parmetros: - texto: texto a buscar (obligatorio) - pagina: nmero de pgina (default: 1) - por_pagina: resultados por pgina (default: 10) - autor: filtrar por autor (opcional) - ordenar: 'relevancia' (default) o 'fecha' """ contenido_texto = request.args.get('contenido_texto', '').strip() or request.args.get('texto', '').strip() if not contenido_texto: return jsonify({'error': 'Debe proporcionar un contenido_texto para buscar.'}), 400 pagina = int(request.args.get('pagina', 1)) por_pagina = int(request.args.get('por_pagina', 10)) autor = request.args.get('autor') ordenar = request.args.get('ordenar', 'relevancia') # Conexin a Meilisearch client = meilisearch.Client('http://127.0.0.1:7700') index = client.index('documentos') offset = (pagina - 1) * por_pagina # Construir opciones para Meilisearch search_options = { 'limit': por_pagina, 'offset': offset, } if autor: search_options['filter'] = f"autor = '{autor}'" if ordenar == 'fecha': search_options['sort'] = ['fecha_creacion:desc'] # Realizar bsqueda res = index.search(contenido_texto, search_options) hits = res.get('hits', []) total = res.get('estimatedTotalHits', 0) # Formatear resultados resultados = [] for doc in hits: resultados.append({ 'id': doc.get('id'), 'contenido_texto': doc.get('contenido_texto') or doc.get('texto'), 'fecha': doc.get('fecha'), 'autor': doc.get('autor'), }) return jsonify({ 'resultados': resultados, 'paginacion': { 'pagina_actual': pagina, 'resultados_por_pagina': por_pagina, 'total_resultados': total, 'total_paginas': (total + por_pagina - 1) // por_pagina }, 'consulta': contenido_texto }) @app.route('/api/generar', methods=['GET']) def get_generacion(): """ Proporciona material para generar nuevo contenido. Parmetros de consulta: - tema: ID o nombre del tema (requerido) - tipo: Tipo de contenido a generar (post, articulo, guion) - longitud: Longitud aproximada deseada (corto, medio, largo) """ tema = request.args.get('tema') tipo = request.args.get('tipo', 'post') longitud = request.args.get('longitud', 'medio') if not tema: return jsonify({'error': 'Se requiere especificar un tema'}), 400 conn = conectar_db() cursor = conn.cursor() # Verificar si existen las tablas necesarias if not tabla_existe(conn, 'temas') or not tabla_existe(conn, 'contenido_tema'): return jsonify({'error': 'No se han definido temas en el sistema'}), 404 # Determinar lmite de contenidos segn longitud if longitud == 'corto': limite = 5 elif longitud == 'largo': limite = 20 else: # medio limite = 10 # Obtener contenidos relevantes try: tema_id = int(tema) cursor.execute(''' SELECT c.id, c.contenido_texto, c.fecha_creacion, ct.relevancia FROM contenidos c JOIN contenido_tema ct ON c.id = ct.contenido_id WHERE ct.tema_id = ? ORDER BY ct.relevancia DESC, c.fecha_creacion DESC LIMIT ? ''', (tema_id, limite)) except ValueError: cursor.execute(''' SELECT c.id, c.contenido_texto, c.fecha_creacion, ct.relevancia FROM contenidos c JOIN contenido_tema ct ON c.id = ct.contenido_id JOIN temas t ON ct.tema_id = t.id WHERE t.nombre LIKE ? ORDER BY ct.relevancia DESC, c.fecha_creacion DESC LIMIT ? ''', (f'%{tema}%', limite)) # Procesar resultados material = [] for row in cursor.fetchall(): material.append({ 'id': row['id'], 'contenido_texto': row['contenido_texto'], 'fecha': row['fecha_creacion'], 'relevancia': row['relevancia'] }) # Obtener nombre del tema if material: try: tema_id = int(tema) cursor.execute('SELECT nombre FROM temas WHERE id = ?', (tema_id,)) except ValueError: cursor.execute('SELECT nombre FROM temas WHERE nombre LIKE ?', (f'%{tema}%',)) tema_row = cursor.fetchone() tema_nombre = tema_row['nombre'] if tema_row else tema else: tema_nombre = tema # Generar estructura segn tipo de contenido estructura = {} if tipo == 'post': estructura = { 'titulo': f'Ideas sobre {tema_nombre}', 'introduccion': 'Breve introduccin al tema', 'puntos_clave': ['Punto 1', 'Punto 2', 'Punto 3'], 'conclusion': 'Conclusin o llamada a la accin' } elif tipo == 'articulo': estructura = { 'titulo': f'Anlisis profundo sobre {tema_nombre}', 'introduccion': 'Contexto y relevancia del tema', 'secciones': [ {'titulo': 'Antecedentes', 'contenido': 'Desarrollo histrico o conceptual'}, {'titulo': 'Anlisis', 'contenido': 'Puntos principales y argumentos'}, {'titulo': 'Implicaciones', 'contenido': 'Consecuencias o aplicaciones'} ], 'conclusion': 'Sntesis y reflexiones finales' } elif tipo == 'guion': estructura = { 'titulo': f'Guion sobre {tema_nombre}', 'introduccion': 'Saludo y presentacin del tema', 'segmentos': [ {'tiempo': '0:00-2:00', 'contenido': 'Introduccin y contexto'}, {'tiempo': '2:00-5:00', 'contenido': 'Desarrollo de ideas principales'}, {'tiempo': '5:00-8:00', 'contenido': 'Anlisis y ejemplos'}, {'tiempo': '8:00-10:00', 'contenido': 'Conclusiones y despedida'} ], 'recursos': ['Referencias visuales', 'Citas importantes', 'Datos clave'] } conn.close() return jsonify({ 'tema': tema_nombre, 'tipo': tipo, 'longitud': longitud, 'material': material, 'estructura_sugerida': estructura }) @app.route('/api/indices/<nombre>', methods=['GET']) def get_indice(nombre): """Proporciona acceso directo a los archivos de ndices.""" archivo = INDICES_DIR / f'{nombre}.json' if not archivo.exists(): return jsonify({'error': f'ndice no encontrado: {nombre}'}), 404 return send_file(archivo) @app.route('/api/ndjson/contenido', methods=['GET']) def get_ndjson(): """Proporciona acceso al archivo NDJSON con todo el contenido.""" archivo = INDICES_DIR / 'contenido_completo.ndjson' if not archivo.exists(): return jsonify({'error': 'Archivo NDJSON no encontrado'}), 404 return send_file(archivo) @app.route('/api/documentacion', methods=['GET']) def get_documentacion(): """Proporciona documentacin sobre la API.""" return jsonify({ 'nombre': 'API de Biblioteca de Conocimiento Personal', 'version': '1.0', 'descripcion': 'API para acceso a la biblioteca de conocimiento personal de Ismael Lpez-Silvero Guimarais', 'endpoints': [ { 'ruta': '/api/info', 'metodo': 'GET', 'descripcion': 'Informacin general sobre la biblioteca', 'parametros': [] }, { 'ruta': '/api/contenido', 'metodo': 'GET', 'descripcion': 'Bsqueda de contenido', 'parametros': [ {'nombre': 'tema', 'tipo': 'string', 'descripcion': 'ID o nombre del tema'}, {'nombre': 'texto', 'tipo': 'string', 'descripcion': 'Texto a buscar'}, {'nombre': 'plataforma', 'tipo': 'string', 'descripcion': 'Nombre de la plataforma'}, {'nombre': 'fecha_inicio', 'tipo': 'string', 'descripcion': 'Fecha de inicio (YYYY-MM-DD)'}, {'nombre': 'fecha_fin', 'tipo': 'string', 'descripcion': 'Fecha de fin (YYYY-MM-DD)'}, {'nombre': 'limite', 'tipo': 'integer', 'descripcion': 'Nmero mximo de resultados (default: 100)'} ] }, { 'ruta': '/api/generar', 'metodo': 'GET', 'descripcion': 'Material para generar nuevo contenido', 'parametros': [ {'nombre': 'tema', 'tipo': 'string', 'descripcion': 'ID o nombre del tema (requerido)'}, {'nombre': 'tipo', 'tipo': 'string', 'descripcion': 'Tipo de contenido (post, articulo, guion)'}, {'nombre': 'longitud', 'tipo': 'string', 'descripcion': 'Longitud deseada (corto, medio, largo)'} ] }, { 'ruta': '/api/indices/<nombre>', 'metodo': 'GET', 'descripcion': 'Acceso a archivos de ndices', 'parametros': [ {'nombre': 'nombre', 'tipo': 'string', 'descripcion': 'Nombre del ndice (cronologico, temas_jerarquia)'} ] }, { 'ruta': '/api/ndjson/contenido', 'metodo': 'GET', 'descripcion': 'Acceso al archivo NDJSON con todo el contenido', 'parametros': [] }, { 'ruta': '/api/busqueda/semantica', 'metodo': 'GET', 'descripcion': 'Bsqueda semntica de contenido similar', 'parametros': [ {'nombre': 'texto', 'tipo': 'string', 'descripcion': 'Texto para buscar similitud semntica (requerido)'}, {'nombre': 'pagina', 'tipo': 'integer', 'descripcion': 'Nmero de pgina (default: 1)'}, {'nombre': 'por_pagina', 'tipo': 'integer', 'descripcion': 'Resultados por pgina (default: 10, max: 50)'} ] }, { 'ruta': '/api/busqueda', 'metodo': 'GET', 'descripcion': 'Bsqueda general de texto usando Meilisearch', 'parametros': [ {'nombre': 'texto', 'tipo': 'string', 'descripcion': 'Texto a buscar (obligatorio)'}, {'nombre': 'pagina', 'tipo': 'integer', 'descripcion': 'Nmero de pgina (default: 1)'}, {'nombre': 'por_pagina', 'tipo': 'integer', 'descripcion': 'Resultados por pgina (default: 10)'}, {'nombre': 'autor', 'tipo': 'string', 'descripcion': 'Filtrar por autor (opcional)'}, {'nombre': 'ordenar', 'tipo': 'string', 'descripcion': "'relevancia' (default) o 'fecha'"} ] } ], 'ejemplos': [ {'descripcion': 'Obtener informacin general', 'url': '/api/info'}, {'descripcion': 'Buscar contenido sobre cristianismo', 'url': '/api/contenido?tema=Cristianismo'}, {'descripcion': 'Obtener material para generar un artculo sobre poltica', 'url': '/api/generar?tema=Poltica&tipo=articulo&longitud=largo'}, {'descripcion': 'Buscar contenido semnticamente similar a un texto', 'url': '/api/busqueda/semantica?texto=La importancia de la fe&pagina=1&por_pagina=20'}, {'descripcion': 'Realizar una bsqueda general', 'url': '/api/busqueda?texto=educacin&pagina=1&por_pagina=10'} ] }) # Aadir importacin del servicio de embeddings from scripts.embedding_service import EmbeddingService import json # Inicializar el servicio embedding_service = EmbeddingService() @app.route('/api/busqueda/semantica', methods=['GET']) def busqueda_semantica(): """ Realiza una bsqueda semntica en el contenido Parmetros: - contenido_texto: Texto de consulta - pagina: Nmero de pgina (default: 1) - por_pagina: Resultados por pgina (default: 10, max: 50) - usar_meilisearch: si es true, usa Meilisearch para bsqueda vectorial (default: true) """ contenido_texto = request.args.get('contenido_texto', '') if not contenido_texto: return jsonify({'error': 'No se proporcion contenido_texto para buscar'}), 400 # Parmetros de paginacin pagina = request.args.get('pagina', 1, type=int) por_pagina = request.args.get('por_pagina', 10, type=int) limite = request.args.get('limite', None, type=int) # Limitar resultados por pgina por_pagina = min(por_pagina, 50) # Parmetros de filtrado filtros = request.args.get('filtros', '') similitud_min = request.args.get('similitud_min', 0.2, type=float) ordenar_por = request.args.get('ordenar_por', 'similitud') # Filtrado por autor autor_param = request.args.get('autor', '') autores = autor_param.split(',') if autor_param else [] # Flag para usar Meilisearch o mtodo tradicional usar_meilisearch_str = request.args.get('usar_meilisearch', 'true').lower() usar_meilisearch = usar_meilisearch_str in ['true', '1', 'yes', 'y'] # Opcin 1: Usar Meilisearch si est habilitado if usar_meilisearch: try: # Conexin a Meilisearch client = meilisearch.Client('http://127.0.0.1:7700') index = client.index('documentos') # Ver la configuracin actual try: index_settings = index.get_settings() print(f"[INFO] Configuracin actual del ndice: {index_settings}") except Exception as e: print(f"[INFO] No se pudo obtener la configuracin del ndice: {e}") # Generar embedding del texto de consulta query_embedding = embedding_service.generar_embedding(contenido_texto) if not query_embedding: return jsonify({'error': 'No se pudo generar embedding para el contenido_texto'}), 400 # Construir opciones para Meilisearch search_options = { 'limit': por_pagina, 'attributesToRetrieve': ['id', 'contenido_texto', 'fecha_creacion', 'autor', 'plataforma_id', 'fuente_id', 'idioma'], 'vector': query_embedding, 'hybrid': { 'semanticRatio': 0.5 # Un valor entre 0 y 1 que determina cunto peso se da a la parte semntica vs texto } } # Aplicar filtro por autor si se especifica if autores: autor_filters = [] for autor in autores: autor_filters.append(f"autor = '{autor}'") search_options['filter'] = ' OR '.join(autor_filters) print(f"Opciones de bsqueda en Meilisearch: {search_options}") # Realizar bsqueda try: res = index.search(contenido_texto, search_options) hits = res.get('hits', []) print(f"[INFO] Resultados de bsqueda: {len(hits)} documentos encontrados") except Exception as e: print(f"[ERROR] Error al realizar bsqueda en Meilisearch: {type(e).__name__}: {e}") print("Revirtiendo al mtodo tradicional de bsqueda...") return busqueda_semantica_tradicional(contenido_texto, pagina, por_pagina, limite, filtros, similitud_min, ordenar_por, autores) # Formatear resultados contenidos_detallados = [] for doc in hits: # Si hay un umbral de similitud, filtrar aqu en el cdigo score = doc.get('_semanticScore', 0) if score >= similitud_min: contenidos_detallados.append({ 'id': doc.get('id'), 'contenido_texto': doc.get('contenido_texto'), 'fecha': doc.get('fecha_creacion'), 'plataforma': doc.get('plataforma_id', ''), 'fuente': doc.get('fuente_id', ''), 'idioma': doc.get('idioma', 'es'), 'similitud': round(score, 4) if score is not None else 0, 'autor': doc.get('autor', '') }) # Calcular paginacin basada en resultados filtrados total_resultados = len(contenidos_detallados) total_paginas = (total_resultados + por_pagina - 1) // por_pagina # Redondeo hacia arriba # Aplicar paginacin manual despus de filtrar por similitud inicio = (pagina - 1) * por_pagina fin = min(inicio + por_pagina, total_resultados) pagina_resultados = contenidos_detallados[inicio:fin] return jsonify({ 'resultados': pagina_resultados, 'paginacion': { 'pagina_actual': pagina, 'resultados_por_pagina': por_pagina, 'total_resultados': total_resultados, 'total_paginas': total_paginas }, 'consulta': contenido_texto, 'metodo': 'meilisearch' }) except Exception as e: print(f"Error al usar Meilisearch para bsqueda: {type(e).__name__}. Error message: {e}") print("Revirtiendo al mtodo tradicional de bsqueda...") # Opcin 2: Mtodo tradicional de bsqueda por similaridad return busqueda_semantica_tradicional(contenido_texto, pagina, por_pagina, limite, filtros, similitud_min, ordenar_por, autores) def busqueda_semantica_tradicional(contenido_texto, pagina, por_pagina, limite, filtros, similitud_min, ordenar_por, autores): """Funcin para realizar bsqueda semntica usando el mtodo tradicional SQLite""" print("[INFO] Ejecutando bsqueda semntica con mtodo tradicional") # Generar embedding del texto de consulta query_embedding = embedding_service.generar_embedding(contenido_texto) if not query_embedding: return jsonify({'error': 'No se pudo generar embedding para el contenido_texto'}), 400 # Conectar a la base de datos conn = conectar_db() cursor = conn.cursor() # Primero verificar si hay embeddings almacenados cursor.execute('SELECT COUNT(*) as total FROM contenido_embeddings') total_embeddings = cursor.fetchone()['total'] if total_embeddings == 0: conn.close() return jsonify({ 'error': 'No hay embeddings generados', 'mensaje': 'Ejecute primero el script de procesamiento semntico' }), 400 # Buscar contenidos con embeddings cursor.execute(''' SELECT ce.contenido_id, ce.embedding FROM contenido_embeddings ce ''') resultados = [] for row in cursor.fetchall(): contenido_id = row['contenido_id'] embedding = json.loads(row['embedding']) # Calcular similitud similitud = embedding_service.calcular_similitud(query_embedding, embedding) if similitud >= similitud_min: resultados.append({ 'contenido_id': contenido_id, 'similitud': similitud }) # Ordenar por similitud resultados.sort(key=lambda x: x['similitud'], reverse=True) # Aplicar lmite total si se especifica if limite is not None and limite > 0: resultados = resultados[:limite] # Calcular paginacin total_resultados = len(resultados) total_paginas = (total_resultados + por_pagina - 1) // por_pagina # Redondeo hacia arriba if pagina < 1: pagina = 1 elif pagina > total_paginas and total_paginas > 0: pagina = total_paginas # Obtener resultados de la pgina actual inicio = (pagina - 1) * por_pagina fin = min(inicio + por_pagina, total_resultados) resultados_pagina = resultados[inicio:fin] # Obtener detalles completos de los contenidos contenidos_detallados = [] for resultado in resultados_pagina: cursor.execute(''' SELECT c.id, c.contenido_texto, c.fecha_creacion, p.nombre as plataforma, f.nombre as fuente, c.autor FROM contenidos c JOIN plataformas p ON c.plataforma_id = p.id JOIN fuentes f ON c.fuente_id = f.id WHERE c.id = ? ''', (resultado['contenido_id'],)) row = cursor.fetchone() if row: contenidos_detallados.append({ 'id': row['id'], 'contenido_texto': row['contenido_texto'], 'fecha': row['fecha_creacion'], 'plataforma': row['plataforma'], 'fuente': row['fuente'], 'idioma': row['idioma'] if 'idioma' in row.keys() else 'es', 'similitud': round(resultado['similitud'], 4), 'autor': row['autor'] }) # Filtrar por autor si se especifica if autores: contenidos_detallados = [c for c in contenidos_detallados if c.get('autor') in autores] conn.close() # Devolver resultados con metadatos de paginacin return jsonify({ 'resultados': contenidos_detallados, 'paginacion': { 'pagina_actual': pagina, 'resultados_por_pagina': por_pagina, 'total_resultados': total_resultados, 'total_paginas': total_paginas }, 'consulta': contenido_texto, 'metodo': 'tradicional' }) @app.route('/api/generar_rag', methods=['POST']) def generar_rag(): """ Genera contenido utilizando el patrn RAG (Retrieval-Augmented Generation). Parmetros JSON: - tema: Tema sobre el que generar contenido (requerido) - tipo: Tipo de contenido (post, articulo, guion, resumen, analisis) - estilo: Estilo de escritura - num_resultados: Nmero de resultados a recuperar - proveedor: Proveedor de LLM (gemini, openai) - solo_prompt: Si solo se quiere obtener el prompt sin generar contenido """ # Verificar que la solicitud contenga datos JSON if not request.is_json: return jsonify({"error": "La solicitud debe ser en formato JSON"}), 400 # Obtener parmetros data = request.json tema = data.get('tema') tipo = data.get('tipo', 'post') estilo = data.get('estilo') num_resultados = data.get('num_resultados', 5) proveedor = data.get('proveedor', 'gemini') solo_prompt = data.get('solo_prompt', False) # Verificar parmetros requeridos if not tema: return jsonify({"error": "El tema es obligatorio"}), 400 # Verificar tipo vlido tipos_validos = ["post", "articulo", "guion", "resumen", "analisis"] if tipo not in tipos_validos: return jsonify({"error": f"Tipo de contenido no vlido. Opciones: {', '.join(tipos_validos)}"}), 400 # Verificar proveedor vlido y disponible if not solo_prompt and (proveedor not in llm_providers or not llm_providers[proveedor]): proveedores_disponibles = list(llm_providers.keys()) if not proveedores_disponibles: return jsonify({"error": "No hay proveedores de LLM disponibles. Configura las claves API en .env"}), 500 else: return jsonify({ "error": f"Proveedor '{proveedor}' no disponible. Opciones: {', '.join(proveedores_disponibles)}" }), 400 # Paso 1: Recuperar contenido relevante try: # Realizar bsqueda semntica conn = conectar_db() cursor = conn.cursor() # Usar la misma lgica de la funcin busqueda_semantica query = data.get('tema', '') page = 1 per_page = num_resultados if not query: return jsonify({"error": "Query parameter is required"}), 400 # Determinar si usar Meilisearch o mtodo tradicional usar_meilisearch_str = data.get('usar_meilisearch', 'true') usar_meilisearch = usar_meilisearch_str.lower() in ('true', '1', 't', 'y', 'yes') # Primero intentar usar Meilisearch if usar_meilisearch and is_meilisearch_running(): print(f"[INFO] RAG: Usando Meilisearch para bsqueda de: {query}") resultados = buscar_contenido_meilisearch(query, conn, per_page) else: # Mtodo tradicional como fallback print(f"[INFO] RAG: Usando mtodo tradicional para bsqueda de: {query}") resultados = buscar_contenido_tradicional(query, conn, per_page) conn.close() if not resultados: return jsonify({ "error": "No se encontraron contenidos relevantes para este tema" }), 404 # Paso 2: Construir prompt # Extraccin de textos textos_contexto = [f"Fragmento {i+1}:\n\"{item['contenido']}\"\n" for i, item in enumerate(resultados)] # Instrucciones segn tipo instrucciones_tipo = { "post": "Escribe un post para redes sociales", "articulo": "Escribe un artculo detallado", "guion": "Escribe un guion para un video o presentacin", "resumen": "Crea un resumen conciso de las ideas principales", "analisis": "Realiza un anlisis profundo del tema" }.get(tipo.lower(), "Escribe un texto") # Instrucciones de estilo instrucciones_estilo = "" if estilo: instrucciones_estilo = f"Utiliza un estilo {estilo}. " # Prompt completo prompt = f""" # TAREA Acta como un asistente de escritura experto. {instrucciones_tipo} sobre el tema: "{tema}". # CONTEXTO Los siguientes son fragmentos de texto autnticos relacionados con el tema. Utiliza EXCLUSIVAMENTE la informacin de estos fragmentos como base para tu respuesta: {chr(10).join(textos_contexto)} # INSTRUCCIONES ESPECFICAS - Utiliza NICAMENTE la informacin proporcionada en los fragmentos. - Mantn el tono y perspectiva que se refleja en los textos originales. - {instrucciones_estilo}S conciso pero informativo. - No aadas informacin que no est presente en los fragmentos proporcionados. - No menciones que ests basndote en fragmentos o que tienes limitaciones. - Finaliza con una reflexin o pregunta que invite a la interaccin. # RESPUESTA """ # Si solo se quiere el prompt if solo_prompt: return jsonify({ "prompt": prompt, "fragmentos_recuperados": len(resultados) }) # Paso 3: Generar contenido con el LLM seleccionado try: contenido_generado = llm_providers[proveedor](prompt) # Paso 4: Devolver resultado return jsonify({ "contenido": contenido_generado, "fragmentos_utilizados": len(resultados), "tema": tema, "tipo": tipo, "estilo": estilo, "proveedor": proveedor }) except Exception as e: return jsonify({"error": f"Error del proveedor LLM: {str(e)}"}), 500 except Exception as e: return jsonify({"error": f"Error en el proceso RAG: {str(e)}"}), 500 @app.route('/api/autores', methods=['GET']) def get_autores(): """Devuelve la lista de autores nicos en la base de datos.""" conn = conectar_db() cursor = conn.cursor() cursor.execute('SELECT DISTINCT autor FROM contenidos WHERE autor IS NOT NULL AND autor != "" ORDER BY autor ASC') autores = [row['autor'] for row in cursor.fetchall()] conn.close() return jsonify({'autores': autores}) @app.route('/api/estadisticas/autores', methods=['GET']) def estadisticas_autores(): """Devuelve la cantidad de contenidos, primer y ltimo registro, y cantidad de temas por autor.""" conn = conectar_db() cursor = conn.cursor() cursor.execute(''' SELECT c.autor, COUNT(*) as cantidad, MIN(c.fecha_creacion) as primer_registro, MAX(c.fecha_creacion) as ultimo_registro, (SELECT COUNT(DISTINCT ct.tema_id) FROM contenido_tema ct WHERE ct.contenido_id IN (SELECT id FROM contenidos WHERE autor = c.autor)) as temas FROM contenidos c WHERE c.autor IS NOT NULL AND c.autor != "" GROUP BY c.autor ORDER BY cantidad DESC ''') data = [ { 'autor': row['autor'], 'cantidad': row['cantidad'], 'primer_registro': row['primer_registro'], 'ultimo_registro': row['ultimo_registro'], 'temas': row['temas'] } for row in cursor.fetchall() ] conn.close() return jsonify(data) def buscar_contenido_meilisearch(query, conn, per_page): """Busca contenido usando Meilisearch y embeddings vectoriales para RAG""" try: # Generar embedding para la consulta query_embedding = embedding_service.generar_embedding(query) if not query_embedding: print(f"[ADVERTENCIA] No se pudo generar embedding para: {query}") return [] # Conexin a Meilisearch client = meilisearch.Client('http://127.0.0.1:7700') index = client.index('documentos') # Construir opciones para Meilisearch search_options = { 'limit': per_page, 'attributesToRetrieve': ['id', 'contenido_texto', 'fecha_creacion', 'autor', 'plataforma_id', 'fuente_id', 'idioma'], 'vector': query_embedding, 'hybrid': { 'semanticRatio': 0.5 # Un valor entre 0 y 1 que determina cunto peso se da a la parte semntica vs texto } } try: # Realizar bsqueda res = index.search(query, search_options) hits = res.get('hits', []) print(f"[INFO] RAG: Resultados de bsqueda: {len(hits)} documentos encontrados") # Formatear resultados resultados = [] for doc in hits: score = doc.get('_semanticScore', 0) resultados.append({ 'id': doc.get('id'), 'contenido': doc.get('contenido_texto'), 'fecha': doc.get('fecha_creacion', ''), 'plataforma': doc.get('plataforma_id', ''), 'fuente': doc.get('fuente_id', ''), 'idioma': doc.get('idioma', 'es'), 'similitud': round(score, 4) if score is not None else 0, 'autor': doc.get('autor', '') }) return resultados except Exception as e: print(f"[ERROR] Error en bsqueda RAG con Meilisearch: {type(e).__name__}: {e}") # Si hay error, devolvemos lista vaca para que pueda continuar con mtodo tradicional return [] except Exception as e: print(f"[ERROR] Error general en bsqueda RAG Meilisearch: {e}") return [] # Funcin auxiliar para buscar contenido con mtodo tradicional (para RAG) def buscar_contenido_tradicional(query, conn, per_page): """Realiza una bsqueda por similitud vectorial usando el mtodo tradicional""" try: cursor = conn.cursor() # Obtener embedding para la consulta query_embedding = embedding_service.generar_embedding(query) if not query_embedding: return [] # Convertir a string para SQL query_embedding_str = json.dumps(query_embedding) # Buscar en la base de datos sql = """ SELECT c.id, c.contenido_texto as contenido, c.fecha_creacion as fecha, p.nombre as plataforma, f.nombre as fuente, ce.embedding FROM contenido_embeddings ce JOIN contenidos c ON ce.contenido_id = c.id JOIN plataformas p ON c.plataforma_id = p.id JOIN fuentes f ON c.fuente_id = f.id LIMIT ? """ cursor.execute(sql, (per_page * 3,)) # Buscamos ms para luego filtrar por similitud resultados = [] for row in cursor.fetchall(): embedding = json.loads(row['embedding']) if row['embedding'] else None if embedding: # Calcular similitud similitud = embedding_service.calcular_similitud(query_embedding, embedding) resultados.append({ 'id': row['id'], 'contenido': row['contenido'], 'fecha': row['fecha'], 'plataforma': row['plataforma'], 'fuente': row['fuente'], 'similitud': similitud, }) # Ordenar por similitud y limitar resultados.sort(key=lambda x: x['similitud'], reverse=True) return resultados[:per_page] except Exception as e: print(f"Error en bsqueda tradicional: {str(e)}") return [] if __name__ == '__main__': # Asegurarse de que existen los directorios necesarios os.makedirs(INDICES_DIR, exist_ok=True) # Inicializar Meilisearch indices inicializar_meilisearch_indices() # Iniciar servidor app.run(host='0.0.0.0', port=5000, debug=True)
---

# inicializar semantica

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# inicializar semantica

#!/usr/bin/env python3 # -*- coding: utf-8 -*- """ Inicializacin de tablas para anlisis semntico - Biblioteca de Conocimiento Personal """ import os import sqlite3 from pathlib import Path # Configuracin de rutas BASE_DIR = Path(__file__).resolve().parent.parent DB_PATH = BASE_DIR / 'data' / 'biblioteca.db' def inicializar_tablas_semanticas(): print(f"Inicializando tablas semnticas en {DB_PATH}...") # Verificar que existe la base de datos if not DB_PATH.exists(): print(f"Error: La base de datos no existe en {DB_PATH}") return False # Conectar a la base de datos conn = sqlite3.connect(DB_PATH) cursor = conn.cursor() # Tabla para embeddings print("Creando tabla de embeddings...") cursor.execute(''' CREATE TABLE IF NOT EXISTS contenido_embeddings ( contenido_id INTEGER PRIMARY KEY, embedding TEXT, fecha_creacion TIMESTAMP DEFAULT CURRENT_TIMESTAMP, FOREIGN KEY (contenido_id) REFERENCES contenidos(id) ) ''') # Tabla para entidades nombradas print("Creando tabla de entidades...") cursor.execute(''' CREATE TABLE IF NOT EXISTS entidades ( id INTEGER PRIMARY KEY, contenido_id INTEGER, tipo TEXT, texto TEXT, FOREIGN KEY (contenido_id) REFERENCES contenidos(id) ) ''') # Crear ndices para optimizar consultas print("Creando ndices...") cursor.execute('CREATE INDEX IF NOT EXISTS idx_contenido_id ON contenido_embeddings (contenido_id)') cursor.execute('CREATE INDEX IF NOT EXISTS idx_entidades_contenido ON entidades (contenido_id)') cursor.execute('CREATE INDEX IF NOT EXISTS idx_entidades_tipo ON entidades (tipo)') cursor.execute('CREATE INDEX IF NOT EXISTS idx_entidades_texto ON entidades (texto)') conn.commit() conn.close() print("Tablas semnticas inicializadas correctamente.") return True if __name__ == "__main__": inicializar_tablas_semanticas()
---

# indexar meilisearch

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# indexar meilisearch

#!/usr/bin/env python # -*- coding: utf-8 -*- import os import time import threading import queue import json import logging import sqlite3 import argparse import concurrent.futures import signal import sys from datetime import datetime from dotenv import load_dotenv import meilisearch import requests # Configuracin del logging logging.basicConfig( level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[ logging.FileHandler("indexacion_meilisearch.log"), logging.StreamHandler() ] ) logger = logging.getLogger(__name__) # Cargar variables de entorno load_dotenv(os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')) # Configuracin de Meilisearch MEILISEARCH_URL = os.getenv("MEILISEARCH_URL", "http://localhost:7700") MEILISEARCH_API_KEY = os.getenv("MEILISEARCH_API_KEY", "") # Por defecto sin clave BATCH_SIZE = int(os.getenv("BATCH_SIZE", "5000")) DOCS_PER_REQUEST = int(os.getenv("DOCS_PER_REQUEST", "1000")) NUM_THREADS = int(os.getenv("NUM_THREADS", "8")) TIMEOUT = int(os.getenv("TIMEOUT", "60")) # timeout en segundos INDEX_NAME = os.getenv("INDEX_NAME", "documentos") # Parmetros de configuracin optimizados para alto rendimiento WAIT_TIME_BETWEEN_BATCHES = 0.5 # Reducido a 0.5 segundos MAX_WAIT_TIME = 30 # 30 segundos mximo de espera MAX_RETRY_ATTEMPTS = 3 # 3 intentos si falla def listar_tablas(db_path): """ Lista todas las tablas disponibles en la base de datos """ try: conn = sqlite3.connect(db_path) cursor = conn.cursor() # Consulta para obtener todas las tablas cursor.execute("SELECT name FROM sqlite_master WHERE type='table';") tablas = cursor.fetchall() cursor.close() conn.close() return [tabla[0] for tabla in tablas] except Exception as e: logger.error(f"Error al listar tablas: {str(e)}") return [] class MeilisearchIndexer: def __init__(self, table_name="contenidos"): # Si la clave API est vaca, no la usamos if MEILISEARCH_API_KEY: self.client = meilisearch.Client(MEILISEARCH_URL, MEILISEARCH_API_KEY, timeout=TIMEOUT) else: self.client = meilisearch.Client(MEILISEARCH_URL, timeout=TIMEOUT) self.index = self.client.index(INDEX_NAME) self.queue = queue.Queue() self.lock = threading.Lock() self.documents_indexed = 0 self.total_documents = 0 self.failed_documents = [] self.table_name = table_name self.running = True # Ruta a la base de datos SQLite self.db_path = os.getenv("DB_PATH", os.path.join(os.path.dirname(os.path.dirname(__file__)), "data", "biblioteca.db")) def obtener_total_documentos(self): """ Consulta cuntos documentos hay que indexar en total desde la base de datos SQLite """ try: # Conectar a la base de datos SQLite conn = sqlite3.connect(self.db_path) cursor = conn.cursor() # Ejecutar consulta para contar documentos cursor.execute(f"SELECT COUNT(*) FROM {self.table_name}") count = cursor.fetchone()[0] # Cerrar conexin cursor.close() conn.close() self.total_documents = count logger.info(f"Total de documentos a indexar (desde tabla '{self.table_name}'): {self.total_documents}") return self.total_documents except Exception as e: logger.error(f"Error al obtener el total de documentos de la tabla '{self.table_name}': {str(e)}") # En caso de error, podemos usar un valor predeterminado logger.warning("Usando valor predeterminado para total de documentos") self.total_documents = 0 return self.total_documents def obtener_documentos(self, offset, limit): """ Obtiene un lote de documentos para indexar desde la base de datos SQLite incluyendo sus embeddings """ try: # Conectar a la base de datos SQLite conn = sqlite3.connect(self.db_path) conn.row_factory = sqlite3.Row # Permite acceder a las columnas por nombre cursor = conn.cursor() # Ejecutar consulta paginada para obtener los documentos cursor.execute( f"SELECT * FROM {self.table_name} ORDER BY id LIMIT ? OFFSET ?", (limit, offset) ) # Convertir resultados a lista de diccionarios documentos = [] for row in cursor.fetchall(): # Convertir objeto Row a diccionario doc = {key: row[key] for key in row.keys()} # Obtener el embedding para este documento si existe cursor_embed = conn.cursor() cursor_embed.execute( "SELECT embedding FROM contenido_embeddings WHERE contenido_id = ?", (doc['id'],) ) embedding_row = cursor_embed.fetchone() if embedding_row and embedding_row['embedding']: # Convertir el embedding a vector si est en formato texto embedding = embedding_row['embedding'] if isinstance(embedding, str): try: # Intentar convertir de texto a lista de nmeros import ast embedding = ast.literal_eval(embedding) except (ValueError, SyntaxError): # Si falla, mantenerlo como string pass doc['embedding'] = embedding documentos.append(doc) # Cerrar conexin cursor.close() conn.close() logger.debug(f"Obtenidos {len(documentos)} documentos desde la tabla '{self.table_name}' (offset {offset})") return documentos except Exception as e: logger.error(f"Error al obtener documentos de la tabla '{self.table_name}' (offset={offset}, limit={limit}): {str(e)}") return [] def configurar_indice(self): """ Configura el ndice de Meilisearch con la configuracin correcta """ try: # Crear ndice si no existe try: self.client.create_index(INDEX_NAME, {"primaryKey": "id"}) logger.info(f"ndice '{INDEX_NAME}' creado con clave primaria 'id'") except Exception as e: if "index_already_exists" in str(e): logger.info(f"El ndice '{INDEX_NAME}' ya existe") else: raise # Configurar ajustes del ndice self.index.update_settings({ "searchableAttributes": [ "contenido_texto", "autor", "contexto", "url_original" ], "filterableAttributes": [ "id", "autor", "fecha_creacion", "fecha_importacion", "fuente_id", "plataforma_id", "idioma" ], "sortableAttributes": [ "id", "fecha_creacion", "fecha_importacion" ], "displayedAttributes": [ "id", "contenido_texto", "fecha_creacion", "fecha_importacion", "fuente_id", "plataforma_id", "url_original", "contexto", "autor", "idioma" ], "typoTolerance": { "enabled": True } }) logger.info(f"Configuracin del ndice '{INDEX_NAME}' actualizada") # Configurar bsqueda vectorial try: self.index.update_vector_config({ 'embeddings': { 'dimensions': 768, # Dimensiones para el modelo 'paraphrase-multilingual-mpnet-base-v2' 'distance': 'cosine' # Usar similitud coseno } }) logger.info(f"Configuracin vectorial actualizada") except Exception as e: logger.warning(f"Error al configurar vectores (esto es normal en versiones antiguas): {str(e)}") # Esperar a que la tarea se complete time.sleep(1) return True except Exception as e: logger.error(f"Error al configurar el ndice: {str(e)}") return False def transform_document(self, doc): """ Transforma un documento para que sea compatible con Meilisearch """ # Crear una copia para no modificar el original transformed = {} # Copiar todos los campos for key, value in doc.items(): # Gestionar el campo embedding especialmente para vectores if key == 'embedding': if value is not None: # Renombrar embedding para cumplir con el formato de Meilisearch transformed['_vectors'] = { 'embeddings': value } # Convertir campos None a cadenas vacas o valores predeterminados elif value is None: if key in ['contenido_texto', 'url_original', 'contexto', 'autor']: transformed[key] = "" elif key == 'idioma': transformed[key] = "es" else: transformed[key] = None else: transformed[key] = value # Asegurar que el id sea entero if 'id' in transformed: try: transformed['id'] = int(transformed['id']) except (ValueError, TypeError): # Si no se puede convertir, dejarlo como est pass return transformed def indexar_lote_documentos(self, documentos): """ Indexa un lote de documentos en Meilisearch con reintentos """ if not documentos: return 0 # Transformar documentos para Meilisearch documentos_transformados = [self.transform_document(doc) for doc in documentos] for intento in range(MAX_RETRY_ATTEMPTS): try: # Enviar el lote completo a Meilisearch de una vez inicio_request = time.time() resultado = self.index.add_documents(documentos_transformados) tiempo_request = time.time() - inicio_request num_docs = len(documentos) with self.lock: self.documents_indexed += num_docs progreso = (self.documents_indexed / self.total_documents) * 100 logger.info(f"Lote de {num_docs} documentos indexado en {tiempo_request:.2f}s. Progreso: {self.documents_indexed}/{self.total_documents} ({progreso:.2f}%)") # Esperar un poco entre lotes (no entre documentos individuales) time.sleep(WAIT_TIME_BETWEEN_BATCHES) return num_docs except Exception as e: logger.warning(f"Error al indexar lote de {len(documentos)} documentos (intento {intento+1}/{MAX_RETRY_ATTEMPTS}): {str(e)}") # Si es el ltimo intento, dividir el lote a la mitad y reintentar if intento == MAX_RETRY_ATTEMPTS - 1 and len(documentos) > 10: mitad = len(documentos) // 2 logger.warning(f"Dividiendo lote de {len(documentos)} en dos partes de {mitad} y {len(documentos)-mitad}") # Procesar la primera mitad primera_mitad = documentos[:mitad] self.indexar_lote_documentos(primera_mitad) # Procesar la segunda mitad segunda_mitad = documentos[mitad:] return self.indexar_lote_documentos(segunda_mitad) else: # Esperar antes de reintentar, con tiempo exponencial wait_time = min(2 ** intento, MAX_WAIT_TIME) time.sleep(wait_time) # Si llegamos aqu, fallaron todos los intentos with self.lock: self.failed_documents.extend([doc.get('id', 'unknown') for doc in documentos]) logger.error(f"Fall la indexacin del lote de {len(documentos)} documentos despus de {MAX_RETRY_ATTEMPTS} intentos") return 0 def watchdog(self): """ Funcin de vigilancia para detectar y reportar bloqueos """ last_progress = 0 last_time = time.time() while self.running: time.sleep(30) # Verificar cada 30 segundos current_progress = self.documents_indexed current_time = time.time() elapsed = current_time - last_time if current_progress == last_progress and elapsed > 120: # Sin progreso por 2 minutos logger.warning(f" POSIBLE BLOQUEO DETECTADO: Sin progreso en {elapsed:.1f} segundos") logger.warning(f"ltimo progreso: {last_progress}/{self.total_documents}") last_progress = current_progress last_time = current_time def indexar_todos(self): """ Indexa todos los documentos utilizando mltiples hilos """ try: # Iniciar el watchdog watchdog_thread = threading.Thread(target=self.watchdog) watchdog_thread.daemon = True watchdog_thread.start() # Obtener el total de documentos self.obtener_total_documentos() # Verificar si hay documentos para indexar if self.total_documents == 0: logger.info("No hay documentos para indexar") return # Configurar el ndice de Meilisearch if not self.configurar_indice(): logger.error("No se pudo configurar el ndice de Meilisearch. Abortando.") return # Indexar documentos en lotes grandes start_time = time.time() try: for offset in range(0, self.total_documents, BATCH_SIZE): # Imprimir marcador de tiempo para monitorear posibles bloqueos logger.info(f"Iniciando procesamiento del lote con offset {offset} a las {datetime.now().strftime('%H:%M:%S')}") documentos = self.obtener_documentos(offset, BATCH_SIZE) if not documentos: logger.warning(f"No se pudieron obtener documentos para el lote con offset {offset}") continue logger.info(f"Procesando lote de {len(documentos)} documentos (offset {offset})") # Dividir documentos en sublotes ms pequeos para los hilos sublotes = [] for i in range(0, len(documentos), DOCS_PER_REQUEST): sublote = documentos[i:i+DOCS_PER_REQUEST] if sublote: sublotes.append(sublote) # Procesar sublotes con un pool de hilos para mejor gestin with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_THREADS) as executor: futures = [executor.submit(self.indexar_lote_documentos, sublote) for sublote in sublotes] # Esperar a que se completen todos o hasta que haya timeout for future in concurrent.futures.as_completed(futures, timeout=300): # 5 minutos mximo try: result = future.result() except concurrent.futures.TimeoutError: logger.error("Timeout al procesar sublote") except Exception as e: logger.error(f"Error al procesar sublote: {str(e)}") # Calcular estadsticas de rendimiento elapsed = time.time() - start_time documents_per_second = self.documents_indexed / elapsed if elapsed > 0 else 0 logger.info(f"Lote completado (offset {offset}). Progreso: {self.documents_indexed}/{self.total_documents} ({(self.documents_indexed/self.total_documents)*100:.2f}%)") logger.info(f"Velocidad: {documents_per_second:.2f} documentos/segundo. Tiempo transcurrido: {elapsed:.2f} segundos") except KeyboardInterrupt: logger.warning("Interrupcin del usuario. Finalizando proceso...") # Resultados finales self.running = False # Detener el watchdog total_elapsed = time.time() - start_time final_speed = self.documents_indexed / total_elapsed if total_elapsed > 0 else 0 logger.info(f"Tiempo total de indexacin: {total_elapsed:.2f} segundos") logger.info(f"Velocidad promedio: {final_speed:.2f} documentos/segundo") if self.failed_documents: logger.error(f"Indexacin completada con errores. {len(self.failed_documents)} documentos fallaron.") logger.error(f"Primeros 10 documentos fallidos: {self.failed_documents[:10]}") else: logger.info(f"Indexacin completada exitosamente. {self.documents_indexed} documentos indexados.") except Exception as e: logger.error(f"Error durante la indexacin: {str(e)}") raise finally: self.running = False # Asegurar que el watchdog se detenga def signal_handler(sig, frame): print("Ctrl+C detectado. Finalizando proceso...") sys.exit(0) if __name__ == "__main__": try: # Registrar manejador de seales para Ctrl+C signal.signal(signal.SIGINT, signal_handler) # Configurar parser de argumentos parser = argparse.ArgumentParser(description='Indexa documentos en Meilisearch desde una base de datos SQLite') parser.add_argument('--tabla', type=str, help='Nombre de la tabla que contiene los documentos') parser.add_argument('--listar', action='store_true', help='Listar todas las tablas disponibles') parser.add_argument('--db-path', type=str, default="E:\\dev-projects\\biblioperson\\backend\\data\\biblioteca.db", help='Ruta a la base de datos SQLite') parser.add_argument('--hilos', type=int, default=NUM_THREADS, help='Nmero de hilos a utilizar') parser.add_argument('--batch-size', type=int, default=BATCH_SIZE, help='Tamao del lote de la base de datos') parser.add_argument('--docs-per-request', type=int, default=DOCS_PER_REQUEST, help='Documentos por solicitud a Meilisearch') parser.add_argument('--timeout', type=int, default=TIMEOUT, help='Timeout para solicitudes HTTP en segundos') args = parser.parse_args() # Actualizar configuracin desde argumentos if args.hilos: NUM_THREADS = args.hilos if args.batch_size: BATCH_SIZE = args.batch_size if args.docs_per_request: DOCS_PER_REQUEST = args.docs_per_request # Usar la ruta proporcionada por el usuario db_path = args.db_path print(f"\nConfiguracin:") print(f"- Base de datos: {db_path}") print(f"- Hilos: {NUM_THREADS}") print(f"- Tamao de lote DB: {BATCH_SIZE}") print(f"- Documentos por solicitud: {DOCS_PER_REQUEST}") # Verificar si la base de datos existe if not os.path.exists(db_path): print(f" La base de datos no existe en la ruta: {db_path}") exit(1) # Intentar abrir la base de datos para ver si funciona try: conn = sqlite3.connect(db_path) cursor = conn.cursor() # Consulta para obtener todas las tablas cursor.execute("SELECT name FROM sqlite_master WHERE type='table';") tablas = cursor.fetchall() tablas = [tabla[0] for tabla in tablas] # Cerrar conexin cursor.close() conn.close() # Mostrar todas las tablas independientemente del parmetro --listar print("\nTablas disponibles en la base de datos:") if tablas: for tabla in tablas: print(f" - {tabla}") print("\nPara indexar una tabla especfica, ejecuta:") print(f" python {os.path.basename(__file__)} --tabla NOMBRE_TABLA\n") else: print(" No se encontraron tablas en la base de datos.") # Si se pidi explcitamente listar, terminar aqu if args.listar: exit(0) # Si no hay tablas, no podemos continuar if not tablas: print("No hay tablas disponibles en la base de datos. No se puede continuar.") exit(1) # Si no se especific una tabla, usar la primera disponible tabla_elegida = args.tabla or tablas[0] print(f"Usando tabla: {tabla_elegida}") except Exception as e: print(f"\n Error al intentar abrir la base de datos: {str(e)}") print("Asegrate de que la ruta es correcta y que la base de datos existe.") logger.error(f"Error al abrir la base de datos: {str(e)}") exit(1) # Iniciar indexacin logger.info(f"Iniciando proceso de indexacin en Meilisearch usando base de datos: {db_path}") indexer = MeilisearchIndexer(table_name=tabla_elegida) indexer.db_path = db_path # Actualizar la ruta de la base de datos indexer.indexar_todos() logger.info("Proceso de indexacin finalizado") except Exception as e: logger.error(f"Error fatal durante la indexacin: {str(e)}")
---

# limpiar datos duplicados

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# limpiar datos duplicados

#!/usr/bin/env python3 # -*- coding: utf-8 -*- """ Script para limpiar duplicados exactos por texto en la tabla 'contenidos' de la base de datos. Uso: python limpiar_duplicados.py --db-path <ruta_a_biblioteca.db> """ import sqlite3 import argparse from pathlib import Path def eliminar_duplicados_por_texto(conn): cursor = conn.cursor() print("Buscando duplicados exactos por texto en la tabla 'contenidos'...") # Buscar duplicados cursor.execute(''' SELECT contenido_texto, COUNT(*) as cantidad, GROUP_CONCAT(id) as ids FROM contenidos GROUP BY contenido_texto HAVING cantidad > 1 ''') duplicados = cursor.fetchall() if not duplicados: print("No se encontraron duplicados. No se eliminar nada.") return print(f"Se encontraron {len(duplicados)} textos duplicados.") total_entradas_eliminadas = 0 for dup in duplicados: contenido_texto = dup[0] ids = [int(x) for x in dup[2].split(',')] ids.sort() ids_a_borrar = ids[1:] # Mantener el de menor id total_entradas_eliminadas += len(ids_a_borrar) primeras_palabras = ' '.join(contenido_texto.strip().split()[:3]) print(f"DUPLICADO: '{primeras_palabras}...' | IDs eliminados: {ids_a_borrar}") # Eliminar los duplicados (excepto el primero) cursor.executemany('DELETE FROM contenidos WHERE id = ?', [(i,) for i in ids_a_borrar]) conn.commit() print(f"Total de entradas eliminadas: {total_entradas_eliminadas}") def main(): parser = argparse.ArgumentParser(description='Eliminar duplicados exactos por texto en la tabla contenidos') parser.add_argument('--db-path', required=True, help='Ruta a la base de datos SQLite (biblioteca.db)') args = parser.parse_args() db_path = Path(args.db_path) if not db_path.exists(): print(f"Error: No se encontr la base de datos en {db_path}") return conn = sqlite3.connect(str(db_path)) try: eliminar_duplicados_por_texto(conn) finally: conn.close() if __name__ == "__main__": main()
---

# importar datos

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# importar datos

#!/usr/bin/env python3 # -*- coding: utf-8 -*- """ Script para importar datos desde los archivos existentes a la base de datos de la Biblioteca de Conocimiento Personal. """ import os import json import sqlite3 import datetime import re from pathlib import Path from dotenv import load_dotenv import argparse load_dotenv() # Configuracin de rutas BASE_DIR = Path(__file__).resolve().parent.parent DB_PATH = BASE_DIR / 'data' / 'biblioteca.db' CONTENIDO_DIR = Path(os.getenv("CONTENIDO_DIR", BASE_DIR / "contenido")) if not CONTENIDO_DIR.is_absolute(): CONTENIDO_DIR = BASE_DIR / CONTENIDO_DIR def conectar_db(): """Establece conexin con la base de datos SQLite.""" conn = sqlite3.connect(DB_PATH) conn.row_factory = sqlite3.Row return conn def inicializar_datos_base(conn): """Inicializa datos bsicos en las tablas de referencia.""" cursor = conn.cursor() # Insertar plataformas plataformas = [ (1, 'Facebook', 'red_social'), (2, 'Twitter', 'red_social'), (3, 'Telegram', 'mensajeria'), (4, 'Blog', 'web'), (5, 'Escritos', 'documento') ] cursor.executemany( 'INSERT OR IGNORE INTO plataformas (id, nombre, tipo) VALUES (?, ?, ?)', plataformas ) # Insertar fuentes fuentes = [ (1, 'Perfil personal Facebook', 'Publicaciones en perfil personal de Facebook'), (2, 'Cuenta Twitter', 'Tweets desde cuenta personal'), (3, 'Grupos Telegram', 'Mensajes en grupos de Telegram'), (4, 'Canal Telegram', 'Publicaciones en canal de Telegram'), (5, 'Escritos personales', 'Documentos y ensayos personales'), (6, 'Poesas', 'Textos poticos'), (7, 'Canciones', 'Letras de canciones') ] cursor.executemany( 'INSERT OR IGNORE INTO fuentes (id, nombre, descripcion) VALUES (?, ?, ?)', fuentes ) # Verificar si la tabla temas existe cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='temas'") if cursor.fetchone(): try: # Insertar temas principales solo si la tabla existe temas = [ (1, 'Religin', 'Temas relacionados con religin y espiritualidad', None), (2, 'Cristianismo', 'Temas especficos del cristianismo', 1), (3, 'Judasmo', 'Temas relacionados con el judasmo', 1), (4, 'Poltica', 'Temas polticos y de gobierno', None), (5, 'Libertarianismo', 'Filosofa poltica libertaria', 4), (6, 'Filosofa', 'Temas filosficos generales', None), (7, 'Arte', 'Expresiones artsticas', None), (8, 'Poesa', 'Poesa y expresin lrica', 7), (9, 'Msica', 'Temas relacionados con msica', 7), (10, 'Sociedad', 'Temas sociales y culturales', None) ] cursor.executemany( 'INSERT OR IGNORE INTO temas (id, nombre, descripcion, tema_padre_id) VALUES (?, ?, ?, ?)', temas ) except sqlite3.Error as e: print(f"Aviso: No se pudieron inicializar temas: {e}") else: print("Tabla de temas no encontrada, omitiendo inicializacin de temas") conn.commit() def eliminar_duplicados_por_texto(conn): """Elimina duplicados exactos en la tabla contenidos, dejando solo la primera aparicin de cada texto. Imprime las primeras palabras de los duplicados eliminados.""" cursor = conn.cursor() print("Eliminando duplicados exactos por texto en la tabla 'contenidos'...") # Buscar duplicados cursor.execute(''' SELECT contenido_texto, COUNT(*) as cantidad, GROUP_CONCAT(id) as ids FROM contenidos GROUP BY contenido_texto HAVING cantidad > 1 ''') duplicados = cursor.fetchall() if not duplicados: print("No se encontraron duplicados. No se eliminar nada.") else: print(f"Se encontraron {len(duplicados)} textos duplicados.") total_entradas_eliminadas = 0 for dup in duplicados: texto = dup[0] ids = [int(x) for x in dup[2].split(',')] ids.sort() ids_a_borrar = ids[1:] # Mantener el de menor id total_entradas_eliminadas += len(ids_a_borrar) primeras_palabras = ' '.join(texto.strip().split()[:3]) print(f"DUPLICADO: '{primeras_palabras}...' | IDs eliminados: {ids_a_borrar}") # Eliminar los duplicados (excepto el primero) cursor.executemany('DELETE FROM contenidos WHERE id = ?', [(i,) for i in ids_a_borrar]) print(f"Total de entradas eliminadas: {total_entradas_eliminadas}") conn.commit() print("Duplicados eliminados.") def clasificar_por_temas(conn, contenido_id, contenido_texto): """Clasifica un contenido por temas basado en palabras clave. Si las tablas de temas no existen, esta funcin no hace nada.""" try: cursor = conn.cursor() # Verificar si la tabla temas existe cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='temas'") if not cursor.fetchone(): # Si la tabla no existe, salir silenciosamente return # Definir palabras clave para cada tema temas_keywords = { 1: ['religin', 'religioso', 'espiritual', 'fe', 'creencia', 'dios'], 2: ['cristo', 'jess', 'biblia', 'evangelio', 'cristiano', 'iglesia'], 3: ['judasmo', 'judo', 'tor', 'israel', 'rabino'], 4: ['poltica', 'gobierno', 'estado', 'nacin', 'democracia'], 5: ['libertad', 'libertario', 'libre mercado', 'individualismo'], 6: ['filosofa', 'filsofo', 'pensamiento', 'razn', 'lgica'], 7: ['arte', 'artstico', 'esttica', 'belleza', 'expresin'], 8: ['poesa', 'poema', 'verso', 'estrofa', 'lrica'], 9: ['msica', 'cancin', 'meloda', 'ritmo', 'armona'], 10: ['sociedad', 'social', 'cultura', 'comunidad', 'pueblo'] } # Texto normalizado para bsqueda texto_norm = contenido_texto.lower() # Buscar coincidencias y calcular relevancia for tema_id, keywords in temas_keywords.items(): relevancia = 0 for keyword in keywords: count = texto_norm.count(keyword) if count > 0: relevancia += count * 0.1 # Ajustar factor segn necesidad if relevancia > 0: # Limitar relevancia a un mximo de 1.0 relevancia = min(relevancia, 1.0) # Insertar relacin contenido-tema cursor.execute( '''INSERT INTO contenido_tema (contenido_id, tema_id, relevancia) VALUES (?, ?, ?)''', (contenido_id, tema_id, relevancia) ) except sqlite3.Error as e: print(f"Aviso: No se pudo clasificar por temas: {e}") except Exception as e: print(f"Error en clasificacin por temas: {e}") def generar_indices(conn, base_dir): """Genera archivos de ndices para facilitar el acceso.""" cursor = conn.cursor() indices_dir = base_dir / 'shared' / 'indices' # Crear directorio si no existe os.makedirs(indices_dir, exist_ok=True) # Generar ndice cronolgico cursor.execute(''' SELECT c.id, c.contenido_texto, c.fecha_creacion, p.nombre as plataforma, f.nombre as fuente FROM contenidos c JOIN plataformas p ON c.plataforma_id = p.id JOIN fuentes f ON c.fuente_id = f.id ORDER BY c.fecha_creacion ''') cronologia = [] for row in cursor.fetchall(): cronologia.append({ 'id': row['id'], 'texto': row['contenido_texto'][:200] + '...' if len(row['contenido_texto']) > 200 else row['contenido_texto'], 'fecha': row['fecha_creacion'], 'plataforma': row['plataforma'], 'fuente': row['fuente'] }) with open(indices_dir / 'cronologico.json', 'w', encoding='utf-8') as f: json.dump(cronologia, f, ensure_ascii=False, indent=2) # Verificar si la tabla temas existe cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='temas'") if cursor.fetchone(): # Generar ndice temtico solo si existe la tabla temas try: cursor.execute(''' SELECT t.id, t.nombre, t.descripcion, t.tema_padre_id FROM temas t ORDER BY t.tema_padre_id NULLS FIRST, t.nombre ''') temas = {} for row in cursor.fetchall(): tema_id = row['id'] temas[tema_id] = { 'id': tema_id, 'nombre': row['nombre'], 'descripcion': row['descripcion'], 'padre_id': row['tema_padre_id'], 'contenidos': [] } # Aadir contenidos a cada tema for tema_id in temas: cursor.execute(''' SELECT c.id, c.contenido_texto, c.fecha_creacion, ct.relevancia FROM contenidos c JOIN contenido_tema ct ON c.id = ct.contenido_id WHERE ct.tema_id = ? ORDER BY ct.relevancia DESC ''', (tema_id,)) for row in cursor.fetchall(): temas[tema_id]['contenidos'].append({ 'id': row['id'], 'texto': row['contenido_texto'][:200] + '...' if len(row['contenido_texto']) > 200 else row['contenido_texto'], 'fecha': row['fecha_creacion'], 'relevancia': row['relevancia'] }) # Convertir a estructura jerrquica jerarquia_temas = [] for tema_id, tema in temas.items(): if tema['padre_id'] is None: # Es un tema raz tema_jerarquico = { 'id': tema['id'], 'nombre': tema['nombre'], 'descripcion': tema['descripcion'], 'contenidos': tema['contenidos'], 'subtemas': [] } # Aadir subtemas for subtema_id, subtema in temas.items(): if subtema['padre_id'] == tema_id: tema_jerarquico['subtemas'].append({ 'id': subtema['id'], 'nombre': subtema['nombre'], 'descripcion': subtema['descripcion'], 'contenidos': subtema['contenidos'] }) jerarquia_temas.append(tema_jerarquico) with open(indices_dir / 'temas_jerarquia.json', 'w', encoding='utf-8') as f: json.dump(jerarquia_temas, f, ensure_ascii=False, indent=2) except sqlite3.Error: print("No se pudo generar el ndice temtico (las tablas de temas podran estar incompletas)") else: print("No se gener el ndice temtico porque no existe la tabla de temas") # Generar archivo NDJSON con todo el contenido cursor.execute(''' SELECT c.id, c.contenido_texto, c.fecha_creacion, p.nombre as plataforma, f.nombre as fuente, c.url_original, c.contexto, c.autor FROM contenidos c JOIN plataformas p ON c.plataforma_id = p.id JOIN fuentes f ON c.fuente_id = f.id ''') with open(indices_dir / 'contenido_completo.ndjson', 'w', encoding='utf-8') as f: for row in cursor.fetchall(): # Crear objeto con los datos bsicos contenido_obj = { 'id': row['id'], 'texto': row['contenido_texto'], 'fecha': row['fecha_creacion'], 'plataforma': row['plataforma'], 'fuente': row['fuente'], 'url': row['url_original'], 'contexto': row['contexto'], 'autor': row['autor'], 'temas': [] } # Intentar obtener temas relacionados solo si la tabla existe try: cursor2 = conn.cursor() cursor2.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='temas'") if cursor2.fetchone(): cursor2.execute(''' SELECT t.nombre, ct.relevancia FROM temas t JOIN contenido_tema ct ON t.id = ct.tema_id WHERE ct.contenido_id = ? ''', (row['id'],)) temas_contenido = [] for tema_row in cursor2.fetchall(): temas_contenido.append({ 'nombre': tema_row['nombre'], 'relevancia': tema_row['relevancia'] }) contenido_obj['temas'] = temas_contenido except: pass # Si hay error, simplemente dejamos la lista de temas vaca # Escribir como lnea NDJSON f.write(json.dumps(contenido_obj, ensure_ascii=False) + '\n') print("Generacin de ndices completada.") def importar_contenido(conn, contenido_dir): """Importa todo el contenido de forma recursiva, sin importar la estructura de carpetas.""" cursor = conn.cursor() print(f"Iniciando importacin recursiva desde: {contenido_dir}") # Recorrer todos los archivos en todas las subcarpetas for file_path in Path(contenido_dir).rglob('*'): try: if file_path.is_file(): # Solo procesar archivos, no carpetas # Procesar archivos segn su extensin if file_path.suffix.lower() == '.md': with open(file_path, 'r', encoding='utf-8') as f: contenido = f.read() # Determinar fuente_id y plataforma_id segn el contexto fuente_id = 5 # Por defecto, escritos personales plataforma_id = 5 # Por defecto, Escritos # Determinar tipo segn la ruta path_str = str(file_path).lower() if 'poesias' in path_str or 'poesa' in path_str or 'poemas' in path_str: fuente_id = 6 # Poesas elif 'canciones' in path_str or 'letras' in path_str: fuente_id = 7 # Canciones # Insertar en la base de datos cursor.execute( '''INSERT INTO contenidos (contenido_texto, fecha_creacion, fecha_importacion, fuente_id, plataforma_id, url_original, contexto) VALUES (?, ?, CURRENT_TIMESTAMP, ?, ?, ?, ?)''', (contenido, datetime.datetime.fromtimestamp(os.path.getmtime(file_path)).strftime('%Y-%m-%d %H:%M:%S'), fuente_id, plataforma_id, "", f"Archivo: {file_path.relative_to(contenido_dir)}") ) contenido_id = cursor.lastrowid print(f"Importado: {file_path.relative_to(contenido_dir)} (ID: {contenido_id})") # Clasificar por temas clasificar_por_temas(conn, contenido_id, contenido) elif file_path.suffix.lower() == '.json': # Procesar archivos JSON (Facebook, Twitter, Telegram) if 'facebook' in str(file_path).lower(): with open(file_path, 'r', encoding='utf-8') as f: data = json.load(f) if isinstance(data, list): for item in data: if isinstance(item, dict): texto = json.dumps(item.get('data', item)) cursor.execute( '''INSERT INTO contenidos (contenido_texto, fecha_creacion, fecha_importacion, fuente_id, plataforma_id, url_original, contexto) VALUES (?, ?, CURRENT_TIMESTAMP, ?, ?, ?, ?)''', (texto, datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 1, 1, "", "Importado de Facebook") ) elif 'twitter' in str(file_path).lower(): with open(file_path, 'r', encoding='utf-8') as f: data = json.load(f) if isinstance(data, list): for tweet in data: if isinstance(tweet, dict) and 'text' in tweet: cursor.execute( '''INSERT INTO contenidos (contenido_texto, fecha_creacion, fecha_importacion, fuente_id, plataforma_id, url_original, contexto) VALUES (?, ?, CURRENT_TIMESTAMP, ?, ?, ?, ?)''', (tweet['text'], datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 2, 2, "", "Importado de Twitter") ) elif 'telegram' in str(file_path).lower(): with open(file_path, 'r', encoding='utf-8') as f: data = json.load(f) if isinstance(data, list): for msg in data: if isinstance(msg, dict) and 'text' in msg: cursor.execute( '''INSERT INTO contenidos (contenido_texto, fecha_creacion, fecha_importacion, fuente_id, plataforma_id, url_original, contexto) VALUES (?, ?, CURRENT_TIMESTAMP, ?, ?, ?, ?)''', (msg['text'], datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 3, 3, "", "Importado de Telegram") ) elif file_path.suffix.lower() == '.ndjson': # Procesar archivos NDJSON with open(file_path, 'r', encoding='utf-8') as f: for line in f: try: msg = json.loads(line) if isinstance(msg, dict) and 'text' in msg: cursor.execute( '''INSERT INTO contenidos (contenido_texto, fecha_creacion, fecha_importacion, fuente_id, plataforma_id, url_original, contexto) VALUES (?, ?, CURRENT_TIMESTAMP, ?, ?, ?, ?)''', (msg['text'], datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 3, 3, "", "Importado de Telegram NDJSON") ) except json.JSONDecodeError: continue # Hacer commit cada cierto nmero de archivos para evitar transacciones muy largas conn.commit() except Exception as e: print(f"Error al procesar {file_path}: {e}") continue conn.commit() print("Importacin de contenido completada.") def importar_desde_ndjson(conn, archivo_ndjson): """Importa datos desde un archivo NDJSON unificado.""" cursor = conn.cursor() if not Path(archivo_ndjson).exists(): print(f"Error: El archivo {archivo_ndjson} no existe.") return print(f"Importando desde archivo NDJSON: {archivo_ndjson}") # Contador de entradas total_entradas = 0 entradas_importadas = 0 try: with open(archivo_ndjson, 'r', encoding='utf-8') as f: for line_num, line in enumerate(f, 1): total_entradas += 1 try: # Convertir la lnea JSON a un objeto Python entrada = json.loads(line) if not isinstance(entrada, dict): continue # Extraer datos relevantes contenido_texto = entrada.get('texto', '') fecha = entrada.get('fecha', '') plataforma = entrada.get('plataforma', 'Desconocida') fuente = entrada.get('fuente', 'Desconocida') autor = entrada.get('autor', 'Ismael') # Valor por defecto: 'Ismael' # Normalizar la fecha if fecha: try: # Verificar si es un entero (timestamp Unix) if isinstance(fecha, int): dt = datetime.datetime.fromtimestamp(fecha) fecha_db = dt.strftime('%Y-%m-%d %H:%M:%S') else: # Intentar parsear la fecha (formato flexible) dt = datetime.datetime.strptime(fecha, '%Y-%m-%d') fecha_db = dt.strftime('%Y-%m-%d %H:%M:%S') except ValueError: try: # Segundo intento con formato ms flexible dt = datetime.datetime.strptime(fecha, '%Y-%m') fecha_db = dt.strftime('%Y-%m-%d %H:%M:%S') except ValueError: # Si no se puede parsear, usar fecha actual fecha_db = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') else: fecha_db = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') # Determinar ID de plataforma cursor.execute('SELECT id FROM plataformas WHERE nombre LIKE ?', (f'%{plataforma}%',)) plataforma_row = cursor.fetchone() if plataforma_row: plataforma_id = plataforma_row['id'] else: # Insertar nueva plataforma cursor.execute( 'INSERT INTO plataformas (nombre, tipo) VALUES (?, ?)', (plataforma, 'desconocido') ) plataforma_id = cursor.lastrowid # Determinar ID de fuente cursor.execute('SELECT id FROM fuentes WHERE nombre LIKE ?', (f'%{fuente}%',)) fuente_row = cursor.fetchone() if fuente_row: fuente_id = fuente_row['id'] else: # Insertar nueva fuente cursor.execute( 'INSERT INTO fuentes (nombre, descripcion) VALUES (?, ?)', (fuente, f'Fuente importada: {fuente}') ) fuente_id = cursor.lastrowid # Insertar en la base de datos (aadiendo el campo autor) cursor.execute( '''INSERT INTO contenidos (contenido_texto, fecha_creacion, fecha_importacion, fuente_id, plataforma_id, url_original, contexto, autor) VALUES (?, ?, CURRENT_TIMESTAMP, ?, ?, ?, ?, ?)''', (contenido_texto, fecha_db, fuente_id, plataforma_id, entrada.get('url', ''), entrada.get('contexto', f'Importado de {archivo_ndjson}'), autor) ) contenido_id = cursor.lastrowid # Clasificar por temas clasificar_por_temas(conn, contenido_id, contenido_texto) entradas_importadas += 1 # Informar progreso cada 1000 entradas if entradas_importadas % 1000 == 0: print(f"Importadas {entradas_importadas} entradas de {total_entradas} procesadas.") except json.JSONDecodeError: print(f"Error: Lnea {line_num} no es un JSON vlido. Omitiendo.") continue except Exception as e: print(f"Error al procesar entrada {line_num}: {e}") continue # Hacer commit cada cierto nmero de entradas if entradas_importadas % 5000 == 0: conn.commit() # Hacer commit final conn.commit() print(f"Importacin completada: {entradas_importadas} entradas importadas de {total_entradas} procesadas.") except Exception as e: print(f"Error durante la importacin desde NDJSON: {e}") def agregar_contenido(conn, contenido_texto, fecha_creacion, fuente_id=None, plataforma_id=None, url_original=None, contexto=None, autor=None, idioma='es'): """Agrega un nuevo contenido a la base de datos, incluyendo el idioma.""" cursor = conn.cursor() cursor.execute(''' INSERT INTO contenidos (contenido_texto, fecha_creacion, fuente_id, plataforma_id, url_original, contexto, autor, idioma) VALUES (?, ?, ?, ?, ?, ?, ?, ?) ''', (contenido_texto, fecha_creacion, fuente_id, plataforma_id, url_original, contexto, autor, idioma)) return cursor.lastrowid def main(): """Funcin principal para ejecutar la importacin de datos.""" # Aadir parser de argumentos parser = argparse.ArgumentParser(description='Importar datos a la Biblioteca de Conocimiento Personal') parser.add_argument('--base-dir', default=str(BASE_DIR), help='Directorio base del proyecto') parser.add_argument('--contenido-dir', default=None, help='Directorio que contiene los archivos a importar') parser.add_argument('--db-path', default=str(DB_PATH), help='Ruta a la base de datos SQLite') parser.add_argument('--ndjson-file', default=None, help='Ruta al archivo NDJSON unificado para importar (opcional)') args = parser.parse_args() # Actualizar rutas con argumentos base_dir = Path(args.base_dir) contenido_dir = Path(args.contenido_dir) if args.contenido_dir else None db_path = Path(args.db_path) # Directorio de importacin import_dir = base_dir / 'data' / 'import' # Determinar los archivos NDJSON a utilizar ndjson_files = [] if args.ndjson_file: # Si se especific un archivo, usar solo ese ndjson_files = [args.ndjson_file] else: # Buscar TODOS los archivos NDJSON en el directorio de importacin ndjson_files_found = list(import_dir.glob('*.ndjson')) if ndjson_files_found: ndjson_files = [str(f) for f in ndjson_files_found] print(f"Se encontraron {len(ndjson_files)} archivos NDJSON en {import_dir}:") for i, f in enumerate(ndjson_files_found, 1): print(f" {i}. {f.name}") else: print(f"Error: No se encontraron archivos NDJSON en {import_dir}") print("Por favor, copie sus archivos NDJSON a esta ubicacin antes de importar.") return print(f"Iniciando importacin de datos a la Biblioteca de Conocimiento Personal...") print(f"Base Dir: {base_dir}") if contenido_dir: print(f"Contenido Dir: {contenido_dir}") print(f"DB Path: {db_path}") # Verificar existencia de los archivos NDJSON for ndjson_file in ndjson_files: if not Path(ndjson_file).exists(): print(f"Error: El archivo NDJSON especificado no existe: {ndjson_file}") print(f"Por favor, asegrese de que el archivo existe en la ubicacin correcta.") return # Conectar a la base de datos conn = sqlite3.connect(db_path) conn.row_factory = sqlite3.Row try: # Inicializar datos base inicializar_datos_base(conn) # Eliminar duplicados antes de importar (opcional, puedes moverlo despus de importar si prefieres) eliminar_duplicados_por_texto(conn) # Importar desde todos los archivos NDJSON encontrados for ndjson_file in ndjson_files: print(f"\n=== Procesando archivo: {Path(ndjson_file).name} ===") importar_desde_ndjson(conn, ndjson_file) # Eliminar duplicados despus de importar (ms seguro) eliminar_duplicados_por_texto(conn) # Generar ndices generar_indices(conn, base_dir) print("Importacin de datos completada con xito.") except Exception as e: print(f"Error durante la importacin: {e}") finally: conn.close() if __name__ == "__main__": main()
---

# procesar semantica

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# procesar semantica

#!/usr/bin/env python3 # -*- coding: utf-8 -*- """ Procesador semntico para la Biblioteca de Conocimiento Personal """ import os import sqlite3 import time from pathlib import Path from embedding_service import EmbeddingService from tqdm import tqdm # Configuracin BASE_DIR = Path(__file__).resolve().parent.parent DB_PATH = BASE_DIR / 'data' / 'biblioteca.db' def procesar_embeddings(): print("Iniciando generacin de embeddings...") # Verificar que existe la base de datos if not DB_PATH.exists(): print(f"Error: La base de datos no existe en {DB_PATH}") return False # Conectar a la base de datos conn = sqlite3.connect(DB_PATH) conn.row_factory = sqlite3.Row cursor = conn.cursor() # Verificar que existen las tablas necesarias cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='contenidos'") if not cursor.fetchone(): print("Error: La tabla 'contenidos' no existe") conn.close() return False # Verificar que existe la tabla de embeddings cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='contenido_embeddings'") if not cursor.fetchone(): print("Error: La tabla 'contenido_embeddings' no existe. Ejecute primero 'inicializar_semantica.py'") conn.close() return False # Obtener contenidos sin embeddings cursor.execute(''' SELECT c.id, c.contenido_texto FROM contenidos c LEFT JOIN contenido_embeddings ce ON c.id = ce.contenido_id WHERE ce.contenido_id IS NULL ''') contenidos = cursor.fetchall() total = len(contenidos) if total == 0: print("No hay nuevos contenidos para procesar") conn.close() return True print(f"Procesando {total} contenidos...") # Inicializar servicio de embeddings embedding_service = EmbeddingService() # Procesar cada contenido procesados = 0 inicio = time.time() for contenido in tqdm(contenidos): contenido_id = contenido['id'] contenido_texto = contenido['contenido_texto'] # Generar y guardar embedding embedding = embedding_service.generar_embedding(contenido_texto) if embedding: embedding_service.guardar_embedding(conn, contenido_id, embedding) procesados += 1 # Commit cada 100 documentos if procesados % 100 == 0: conn.commit() tiempo_transcurrido = time.time() - inicio tiempo_por_doc = tiempo_transcurrido / procesados tiempo_restante = tiempo_por_doc * (total - procesados) print(f"Procesados {procesados}/{total} - Tiempo restante estimado: {int(tiempo_restante/60)} minutos") # Commit final conn.commit() conn.close() tiempo_total = time.time() - inicio print(f"Procesamiento completado. {procesados} documentos procesados en {int(tiempo_total/60)} minutos") return True if __name__ == "__main__": procesar_embeddings()
---

# embedding service

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# embedding service

#!/usr/bin/env python3 # -*- coding: utf-8 -*- """ Servicio de embeddings para la Biblioteca de Conocimiento Personal """ from sentence_transformers import SentenceTransformer import numpy as np import json import sqlite3 from pathlib import Path from tqdm import tqdm class EmbeddingService: def __init__(self, model_name='paraphrase-multilingual-mpnet-base-v2'): print(f"Inicializando servicio de embeddings con modelo: {model_name}") try: self.model = SentenceTransformer(model_name) print(f"Modelo cargado correctamente: {model_name}") except Exception as e: print(f"ERROR al cargar el modelo {model_name}: {str(e)}") # Intentar con un modelo alternativo ms bsico try: backup_model = 'distiluse-base-multilingual-cased-v1' print(f"Intentando con modelo alternativo: {backup_model}") self.model = SentenceTransformer(backup_model) print(f"Modelo alternativo cargado correctamente") except Exception as e2: print(f"ERROR crtico al cargar modelos de embedding: {str(e2)}") raise RuntimeError(f"No se pudo inicializar ningn modelo de embedding: {str(e2)}") def generar_embedding(self, contenido_texto): """Genera el vector de embedding para un contenido_texto""" try: if not contenido_texto: print(f"Error: contenido_texto vaco") return None if len(contenido_texto) < 3: print(f"Error: contenido_texto demasiado corto ({len(contenido_texto)} caracteres): {contenido_texto}") return None return self.model.encode(contenido_texto).tolist() except Exception as e: print(f"Error al generar embedding: {str(e)}") print(f"Texto problemtico: '{contenido_texto}'") return None def guardar_embedding(self, conn, contenido_id, embedding): """Guarda el embedding en la base de datos""" if not embedding: return False cursor = conn.cursor() embedding_json = json.dumps(embedding) # Verificar si ya existe cursor.execute('SELECT contenido_id FROM contenido_embeddings WHERE contenido_id = ?', (contenido_id,)) if cursor.fetchone(): cursor.execute('UPDATE contenido_embeddings SET embedding = ? WHERE contenido_id = ?', (embedding_json, contenido_id)) else: cursor.execute('INSERT INTO contenido_embeddings (contenido_id, embedding) VALUES (?, ?)', (contenido_id, embedding_json)) return True def calcular_similitud(self, vec1, vec2): """Calcula la similitud coseno entre dos vectores""" np_vec1 = np.array(vec1) np_vec2 = np.array(vec2) return np.dot(np_vec1, np_vec2) / (np.linalg.norm(np_vec1) * np.linalg.norm(np_vec2))
---

# schema

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# schema

{ "contenidos": { "columns": [ { "id": 0, "name": "id", "type": "INTEGER", "notnull": false, "default": null, "primary_key": true }, { "id": 1, "name": "contenido_texto", "type": "TEXT", "notnull": true, "default": null, "primary_key": false }, { "id": 2, "name": "fecha_creacion", "type": "DATETIME", "notnull": true, "default": null, "primary_key": false }, { "id": 3, "name": "fecha_importacion", "type": "DATETIME", "notnull": true, "default": "CURRENT_TIMESTAMP", "primary_key": false }, { "id": 4, "name": "fuente_id", "type": "INTEGER", "notnull": false, "default": null, "primary_key": false }, { "id": 5, "name": "plataforma_id", "type": "INTEGER", "notnull": false, "default": null, "primary_key": false }, { "id": 6, "name": "url_original", "type": "TEXT", "notnull": false, "default": null, "primary_key": false }, { "id": 7, "name": "contexto", "type": "TEXT", "notnull": false, "default": null, "primary_key": false }, { "id": 8, "name": "autor", "type": "TEXT", "notnull": false, "default": null, "primary_key": false }, { "id": 9, "name": "idioma", "type": "TEXT", "notnull": false, "default": "'es'", "primary_key": false } ], "indices": [ { "name": "idx_contenidos_autor", "unique": false, "columns": [ "autor" ] }, { "name": "idx_contenidos_plataforma", "unique": false, "columns": [ "plataforma_id" ] }, { "name": "idx_contenidos_fuente", "unique": false, "columns": [ "fuente_id" ] }, { "name": "idx_contenidos_fecha", "unique": false, "columns": [ "fecha_creacion" ] } ], "foreign_keys": [ { "id": 0, "seq": 0, "table": "plataformas", "from": "plataforma_id", "to": "id", "on_update": "NO ACTION", "on_delete": "NO ACTION", "match": "NONE" }, { "id": 1, "seq": 0, "table": "fuentes", "from": "fuente_id", "to": "id", "on_update": "NO ACTION", "on_delete": "NO ACTION", "match": "NONE" } ] }, "fuentes": { "columns": [ { "id": 0, "name": "id", "type": "INTEGER", "notnull": false, "default": null, "primary_key": true }, { "id": 1, "name": "nombre", "type": "TEXT", "notnull": true, "default": null, "primary_key": false }, { "id": 2, "name": "descripcion", "type": "TEXT", "notnull": false, "default": null, "primary_key": false } ], "indices": [], "foreign_keys": [] }, "plataformas": { "columns": [ { "id": 0, "name": "id", "type": "INTEGER", "notnull": false, "default": null, "primary_key": true }, { "id": 1, "name": "nombre", "type": "TEXT", "notnull": true, "default": null, "primary_key": false }, { "id": 2, "name": "tipo", "type": "TEXT", "notnull": true, "default": null, "primary_key": false } ], "indices": [], "foreign_keys": [] }, "metadatos": { "columns": [ { "id": 0, "name": "contenido_id", "type": "INTEGER", "notnull": false, "default": null, "primary_key": true }, { "id": 1, "name": "clave", "type": "TEXT", "notnull": false, "default": null, "primary_key": true }, { "id": 2, "name": "valor", "type": "TEXT", "notnull": false, "default": null, "primary_key": false } ], "indices": [ { "name": "sqlite_autoindex_metadatos_1", "unique": true, "columns": [ "contenido_id", "clave" ] } ], "foreign_keys": [ { "id": 0, "seq": 0, "table": "contenidos", "from": "contenido_id", "to": "id", "on_update": "NO ACTION", "on_delete": "NO ACTION", "match": "NONE" } ] }, "contenido_relacion": { "columns": [ { "id": 0, "name": "contenido_id1", "type": "INTEGER", "notnull": false, "default": null, "primary_key": true }, { "id": 1, "name": "contenido_id2", "type": "INTEGER", "notnull": false, "default": null, "primary_key": true }, { "id": 2, "name": "tipo_relacion", "type": "TEXT", "notnull": false, "default": null, "primary_key": true }, { "id": 3, "name": "fuerza_relacion", "type": "FLOAT", "notnull": false, "default": null, "primary_key": false } ], "indices": [ { "name": "sqlite_autoindex_contenido_relacion_1", "unique": true, "columns": [ "contenido_id1", "contenido_id2", "tipo_relacion" ] } ], "foreign_keys": [ { "id": 0, "seq": 0, "table": "contenidos", "from": "contenido_id2", "to": "id", "on_update": "NO ACTION", "on_delete": "NO ACTION", "match": "NONE" }, { "id": 1, "seq": 0, "table": "contenidos", "from": "contenido_id1", "to": "id", "on_update": "NO ACTION", "on_delete": "NO ACTION", "match": "NONE" } ] }, "analisis": { "columns": [ { "id": 0, "name": "id", "type": "INTEGER", "notnull": false, "default": null, "primary_key": true }, { "id": 1, "name": "contenido_id", "type": "INTEGER", "notnull": false, "default": null, "primary_key": false }, { "id": 2, "name": "tipo_analisis", "type": "TEXT", "notnull": true, "default": null, "primary_key": false }, { "id": 3, "name": "resultado", "type": "TEXT", "notnull": true, "default": null, "primary_key": false }, { "id": 4, "name": "fecha_analisis", "type": "DATETIME", "notnull": true, "default": "CURRENT_TIMESTAMP", "primary_key": false } ], "indices": [ { "name": "idx_analisis_tipo", "unique": false, "columns": [ "tipo_analisis" ] }, { "name": "idx_analisis_contenido", "unique": false, "columns": [ "contenido_id" ] } ], "foreign_keys": [ { "id": 0, "seq": 0, "table": "contenidos", "from": "contenido_id", "to": "id", "on_update": "NO ACTION", "on_delete": "NO ACTION", "match": "NONE" } ] }, "estadisticas": { "columns": [ { "id": 0, "name": "id", "type": "INTEGER", "notnull": false, "default": null, "primary_key": true }, { "id": 1, "name": "entidad_id", "type": "INTEGER", "notnull": false, "default": null, "primary_key": false }, { "id": 2, "name": "tipo_entidad", "type": "TEXT", "notnull": true, "default": null, "primary_key": false }, { "id": 3, "name": "tipo_estadistica", "type": "TEXT", "notnull": true, "default": null, "primary_key": false }, { "id": 4, "name": "valor", "type": "TEXT", "notnull": true, "default": null, "primary_key": false }, { "id": 5, "name": "fecha_calculo", "type": "DATETIME", "notnull": true, "default": "CURRENT_TIMESTAMP", "primary_key": false } ], "indices": [ { "name": "idx_estadisticas_entidad", "unique": false, "columns": [ "entidad_id", "tipo_entidad" ] } ], "foreign_keys": [] }, "contenidos_fts": { "columns": [ { "id": 0, "name": "contenido_texto", "type": "", "notnull": false, "default": null, "primary_key": false } ], "indices": [], "foreign_keys": [] }, "contenidos_fts_data": { "columns": [ { "id": 0, "name": "id", "type": "INTEGER", "notnull": false, "default": null, "primary_key": true }, { "id": 1, "name": "block", "type": "BLOB", "notnull": false, "default": null, "primary_key": false } ], "indices": [], "foreign_keys": [] }, "contenidos_fts_idx": { "columns": [ { "id": 0, "name": "segid", "type": "", "notnull": true, "default": null, "primary_key": true }, { "id": 1, "name": "term", "type": "", "notnull": true, "default": null, "primary_key": true }, { "id": 2, "name": "pgno", "type": "", "notnull": false, "default": null, "primary_key": false } ], "indices": [ { "name": "sqlite_autoindex_contenidos_fts_idx_1", "unique": true, "columns": [ "segid", "term" ] } ], "foreign_keys": [] }, "contenidos_fts_docsize": { "columns": [ { "id": 0, "name": "id", "type": "INTEGER", "notnull": false, "default": null, "primary_key": true }, { "id": 1, "name": "sz", "type": "BLOB", "notnull": false, "default": null, "primary_key": false } ], "indices": [], "foreign_keys": [] }, "contenidos_fts_config": { "columns": [ { "id": 0, "name": "k", "type": "", "notnull": true, "default": null, "primary_key": true }, { "id": 1, "name": "v", "type": "", "notnull": false, "default": null, "primary_key": false } ], "indices": [ { "name": "sqlite_autoindex_contenidos_fts_config_1", "unique": true, "columns": [ "k" ] } ], "foreign_keys": [] }, "entidades": { "columns": [ { "id": 0, "name": "id", "type": "INTEGER", "notnull": false, "default": null, "primary_key": true }, { "id": 1, "name": "contenido_id", "type": "INTEGER", "notnull": false, "default": null, "primary_key": false }, { "id": 2, "name": "tipo", "type": "TEXT", "notnull": false, "default": null, "primary_key": false }, { "id": 3, "name": "texto", "type": "TEXT", "notnull": false, "default": null, "primary_key": false } ], "indices": [ { "name": "idx_entidades_texto", "unique": false, "columns": [ "texto" ] }, { "name": "idx_entidades_tipo", "unique": false, "columns": [ "tipo" ] }, { "name": "idx_entidades_contenido", "unique": false, "columns": [ "contenido_id" ] } ], "foreign_keys": [ { "id": 0, "seq": 0, "table": "contenidos", "from": "contenido_id", "to": "id", "on_update": "NO ACTION", "on_delete": "NO ACTION", "match": "NONE" } ] }, "contenido_tema": { "columns": [ { "id": 0, "name": "id", "type": "INTEGER", "notnull": false, "default": null, "primary_key": true }, { "id": 1, "name": "contenido_id", "type": "INTEGER", "notnull": false, "default": null, "primary_key": false }, { "id": 2, "name": "tema_id", "type": "INTEGER", "notnull": false, "default": null, "primary_key": false }, { "id": 3, "name": "relevancia", "type": "REAL", "notnull": false, "default": "1.0", "primary_key": false } ], "indices": [], "foreign_keys": [ { "id": 0, "seq": 0, "table": "temas", "from": "tema_id", "to": "id", "on_update": "NO ACTION", "on_delete": "NO ACTION", "match": "NONE" }, { "id": 1, "seq": 0, "table": "contenidos", "from": "contenido_id", "to": "id", "on_update": "NO ACTION", "on_delete": "NO ACTION", "match": "NONE" } ] }, "contenido_embeddings": { "columns": [ { "id": 0, "name": "contenido_id", "type": "INTEGER", "notnull": false, "default": null, "primary_key": true }, { "id": 1, "name": "embedding", "type": "TEXT", "notnull": false, "default": null, "primary_key": false }, { "id": 2, "name": "fecha_creacion", "type": "TIMESTAMP", "notnull": false, "default": "CURRENT_TIMESTAMP", "primary_key": false } ], "indices": [ { "name": "idx_contenido_id", "unique": false, "columns": [ "contenido_id" ] } ], "foreign_keys": [ { "id": 0, "seq": 0, "table": "contenidos", "from": "contenido_id", "to": "id", "on_update": "NO ACTION", "on_delete": "NO ACTION", "match": "NONE" } ] } }
---

# limpiar base datos

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# limpiar base datos

#!/usr/bin/env python3 # -*- coding: utf-8 -*- """ Script para limpiar la base de datos y recrearla desde cero. """ import os import sys import sqlite3 from pathlib import Path # Configuracin de rutas BASE_DIR = Path(__file__).resolve().parent.parent DB_PATH = BASE_DIR / 'data' / 'biblioteca.db' SCHEMA_PATH = BASE_DIR / 'data' / 'db_schema.sql' def limpiar_base_datos(): """Elimina la base de datos existente y la recrea con el esquema definido.""" # Verificar si existe la base de datos if DB_PATH.exists(): print(f"Eliminando base de datos existente: {DB_PATH}") try: os.remove(DB_PATH) print("Base de datos eliminada correctamente.") except Exception as e: print(f"Error al eliminar la base de datos: {e}") return False else: print("No existe una base de datos previa.") # Verificar si existe el esquema if not SCHEMA_PATH.exists(): print(f"Error: No se encontr el archivo de esquema: {SCHEMA_PATH}") return False # Crear directorio de datos si no existe DB_PATH.parent.mkdir(parents=True, exist_ok=True) # Crear nueva base de datos con el esquema try: # Leer el archivo de esquema with open(SCHEMA_PATH, 'r', encoding='utf-8') as f: schema_sql = f.read() # Conectar a la nueva base de datos y ejecutar el esquema conn = sqlite3.connect(DB_PATH) conn.executescript(schema_sql) conn.commit() conn.close() print(f"Base de datos recreada correctamente: {DB_PATH}") return True except Exception as e: print(f"Error al recrear la base de datos: {e}") return False def main(): """Funcin principal.""" print("=== Iniciando limpieza de base de datos ===") resultado = limpiar_base_datos() if resultado: print("=== Limpieza completada con xito ===") return 0 else: print("=== ERROR: La limpieza fall ===") return 1 if __name__ == "__main__": sys.exit(main())
---

# mostrar esquema db

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# mostrar esquema db

#!/usr/bin/env python # -*- coding: utf-8 -*- import os import sqlite3 import json from dotenv import load_dotenv # Cargar variables de entorno load_dotenv(os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')) # Ruta a la base de datos SQLite DB_PATH = "E:\\dev-projects\\biblioperson\\backend\\data\\biblioteca.db" def print_table_schema(conn, table_name): """Muestra el esquema de una tabla""" cursor = conn.cursor() # Obtener informacin del esquema cursor.execute(f"PRAGMA table_info({table_name})") columns = cursor.fetchall() print(f"\n=== ESQUEMA DE LA TABLA '{table_name}' ===") print("ID | Nombre | Tipo | NotNull | Default | PK") print("-" * 60) for col in columns: print(f"{col[0]} | {col[1]} | {col[2]} | {col[3]} | {col[4]} | {col[5]}") # Obtener informacin de ndices cursor.execute(f"PRAGMA index_list({table_name})") indices = cursor.fetchall() if indices: print("\n--- NDICES ---") for idx in indices: print(f"ndice: {idx[1]}, nico: {idx[2]}") # Detalles del ndice cursor.execute(f"PRAGMA index_info({idx[1]})") idx_info = cursor.fetchall() for info in idx_info: print(f" Columna: {info[2]}") # Verificar si hay claves forneas cursor.execute(f"PRAGMA foreign_key_list({table_name})") fks = cursor.fetchall() if fks: print("\n--- CLAVES FORNEAS ---") for fk in fks: print(f"ID: {fk[0]}, Tabla: {fk[2]}, Desde: {fk[3]}, Hacia: {fk[4]}") def show_sample_data(conn, table_name, limit=5): """Muestra una muestra de datos de la tabla""" cursor = conn.cursor() try: # Obtener columnas cursor.execute(f"PRAGMA table_info({table_name})") columns = [col[1] for col in cursor.fetchall()] # Obtener datos de muestra cursor.execute(f"SELECT * FROM {table_name} LIMIT {limit}") rows = cursor.fetchall() print(f"\n=== MUESTRA DE DATOS DE '{table_name}' (Primeros {limit}) ===") # Imprimir encabezados print(" | ".join(columns)) print("-" * 100) # Imprimir filas de datos for row in rows: # Limitar longitud de campos largos para mejor visualizacin formatted_row = [] for item in row: if item is None: formatted_row.append("NULL") elif isinstance(item, str) and len(item) > 50: formatted_row.append(f"{item[:47]}...") else: formatted_row.append(str(item)) print(" | ".join(formatted_row)) except sqlite3.Error as e: print(f"Error al obtener datos de muestra: {e}") def export_schema_to_json(conn, output_file="schema.json"): """Exporta el esquema completo de la base de datos a un archivo JSON""" cursor = conn.cursor() # Obtener todas las tablas cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'") tables = cursor.fetchall() schema = {} for table in tables: table_name = table[0] schema[table_name] = { "columns": [], "indices": [], "foreign_keys": [] } # Obtener columnas cursor.execute(f"PRAGMA table_info({table_name})") columns = cursor.fetchall() for col in columns: schema[table_name]["columns"].append({ "id": col[0], "name": col[1], "type": col[2], "notnull": bool(col[3]), "default": col[4], "primary_key": bool(col[5]) }) # Obtener ndices cursor.execute(f"PRAGMA index_list({table_name})") indices = cursor.fetchall() for idx in indices: index_info = { "name": idx[1], "unique": bool(idx[2]), "columns": [] } # Detalles del ndice cursor.execute(f"PRAGMA index_info({idx[1]})") idx_columns = cursor.fetchall() for info in idx_columns: index_info["columns"].append(info[2]) schema[table_name]["indices"].append(index_info) # Obtener claves forneas cursor.execute(f"PRAGMA foreign_key_list({table_name})") fks = cursor.fetchall() for fk in fks: schema[table_name]["foreign_keys"].append({ "id": fk[0], "seq": fk[1], "table": fk[2], "from": fk[3], "to": fk[4], "on_update": fk[5], "on_delete": fk[6], "match": fk[7] }) # Guardar a archivo with open(output_file, 'w', encoding='utf-8') as f: json.dump(schema, f, indent=2, ensure_ascii=False) print(f"\nEsquema completo exportado a {output_file}") def main(): try: # Conectar a la base de datos conn = sqlite3.connect(DB_PATH) # Obtener todas las tablas cursor = conn.cursor() cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'") tables = cursor.fetchall() print("=== TABLAS EN LA BASE DE DATOS ===") for i, table in enumerate(tables): print(f"{i+1}. {table[0]}") # Mostrar esquema de la tabla 'contenidos' if ('contenidos',) in tables: print_table_schema(conn, 'contenidos') show_sample_data(conn, 'contenidos') # Exportar esquema completo a JSON export_schema_to_json(conn) # Cerrar conexin conn.close() except sqlite3.Error as e: print(f"Error de SQLite: {e}") except Exception as e: print(f"Error: {e}") if __name__ == "__main__": main()
---

# generador asistido

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# generador asistido

import os import sys import json import requests import argparse from dotenv import load_dotenv from typing import List, Dict, Any, Optional # Cargar variables de entorno desde .env load_dotenv() # Configuracin API_LOCAL_URL = "http://localhost:5000/api" NUM_RESULTADOS_DEFAULT = 5 # Clase base para los LLMs class LLMProvider: def generate_content(self, prompt: str) -> str: raise NotImplementedError("Las subclases deben implementar este mtodo") # Implementacin para Google Gemini class GeminiProvider(LLMProvider): def __init__(self, api_key: str): try: import google.generativeai as genai self.genai = genai self.genai.configure(api_key=api_key) except ImportError: print("Error: No se pudo importar la librera 'google.generativeai'.") print("Por favor, instlala con: pip install google-generativeai") sys.exit(1) def generate_content(self, prompt: str) -> str: model = self.genai.GenerativeModel('gemini-pro') response = model.generate_content(prompt) return response.text # Implementacin para OpenAI class OpenAIProvider(LLMProvider): def __init__(self, api_key: str): try: import openai self.client = openai.OpenAI(api_key=api_key) except ImportError: print("Error: No se pudo importar la librera 'openai'.") print("Por favor, instlala con: pip install openai") sys.exit(1) def generate_content(self, prompt: str) -> str: response = self.client.chat.completions.create( model="gpt-3.5-turbo", messages=[ {"role": "system", "content": "Eres un asistente que genera contenido basado en los textos proporcionados."}, {"role": "user", "content": prompt} ] ) return response.choices[0].message.content # Funcin para buscar contenido semnticamente relevante def buscar_contenido_semantico(query: str, num_resultados: int = 5) -> List[Dict[str, Any]]: url = f"{API_LOCAL_URL}/busqueda/semantica" params = { "texto": query, "por_pagina": num_resultados } try: response = requests.get(url, params=params) response.raise_for_status() # Lanzar excepcin si hay error HTTP data = response.json() return data.get("resultados", []) except requests.exceptions.RequestException as e: print(f"Error al conectar con la API local: {e}") print("Est ejecutndose el servidor Flask en http://localhost:5000?") sys.exit(1) # Funcin para construir el prompt def construir_prompt(tema: str, contenidos: List[Dict[str, Any]], tipo_contenido: str, estilo: Optional[str] = None) -> str: # Extraer los textos de los resultados textos_contexto = [f"Fragmento {i+1}:\n\"{item['contenido']}\"\n" for i, item in enumerate(contenidos)] # Seleccionar instrucciones segn tipo de contenido instrucciones_tipo = { "post": "Escribe un post para redes sociales", "articulo": "Escribe un artculo detallado", "guion": "Escribe un guion para un video o presentacin", "resumen": "Crea un resumen conciso de las ideas principales", "analisis": "Realiza un anlisis profundo del tema" }.get(tipo_contenido.lower(), "Escribe un texto") # Instrucciones de estilo instrucciones_estilo = "" if estilo: instrucciones_estilo = f"Utiliza un estilo {estilo}. " # Construir el prompt completo prompt = f""" # TAREA Acta como un asistente de escritura experto. {instrucciones_tipo} sobre el tema: "{tema}". # CONTEXTO Los siguientes son fragmentos de texto autnticos relacionados con el tema. Utiliza EXCLUSIVAMENTE la informacin de estos fragmentos como base para tu respuesta: {chr(10).join(textos_contexto)} # INSTRUCCIONES ESPECFICAS - Utiliza NICAMENTE la informacin proporcionada en los fragmentos. - Mantn el tono y perspectiva que se refleja en los textos originales. - {instrucciones_estilo}S conciso pero informativo. - No aadas informacin que no est presente en los fragmentos proporcionados. - No menciones que ests basndote en fragmentos o que tienes limitaciones. - Finaliza con una reflexin o pregunta que invite a la interaccin. # RESPUESTA """ return prompt def main(): parser = argparse.ArgumentParser(description="Generador de contenido basado en RAG para Biblioteca Personal") parser.add_argument("tema", help="Tema sobre el que generar contenido") parser.add_argument("--tipo", "-t", default="post", choices=["post", "articulo", "guion", "resumen", "analisis"], help="Tipo de contenido a generar (por defecto: post)") parser.add_argument("--estilo", "-e", help="Estilo de escritura (ej: formal, conversacional, acadmico)") parser.add_argument("--llm", "-l", default="gemini", choices=["gemini", "openai"], help="Proveedor de LLM a utilizar (por defecto: gemini)") parser.add_argument("--resultados", "-r", type=int, default=NUM_RESULTADOS_DEFAULT, help=f"Nmero de resultados a recuperar (por defecto: {NUM_RESULTADOS_DEFAULT})") parser.add_argument("--solo-prompt", "-p", action="store_true", help="Mostrar solo el prompt sin realizar la generacin") args = parser.parse_args() # Verificar las claves API llm_provider = None if not args.solo_prompt: if args.llm == "gemini": api_key = os.getenv("GEMINI_API_KEY") if not api_key: print("Error: No se encontr la clave API para Gemini.") print("Por favor, agrega GEMINI_API_KEY=tu_clave en el archivo .env") sys.exit(1) llm_provider = GeminiProvider(api_key) elif args.llm == "openai": api_key = os.getenv("OPENAI_API_KEY") if not api_key: print("Error: No se encontr la clave API para OpenAI.") print("Por favor, agrega OPENAI_API_KEY=tu_clave en el archivo .env") sys.exit(1) llm_provider = OpenAIProvider(api_key) # Paso 1: Recuperar contenido relevante print(f"Buscando informacin sobre: '{args.tema}'...") resultados = buscar_contenido_semantico(args.tema, args.resultados) if not resultados: print("No se encontraron resultados relevantes. Intenta con otro tema o ampla tu bsqueda.") sys.exit(1) print(f"Se encontraron {len(resultados)} fragmentos relevantes.\n") # Paso 2: Construir el prompt prompt = construir_prompt(args.tema, resultados, args.tipo, args.estilo) if args.solo_prompt: print("\n=== PROMPT GENERADO ===\n") print(prompt) sys.exit(0) # Paso 3: Generar contenido print(f"Generando {args.tipo} con {args.llm.upper()}...") contenido_generado = llm_provider.generate_content(prompt) # Paso 4: Mostrar resultados print("\n=== CONTENIDO GENERADO ===\n") print(contenido_generado) print("\n===========================\n") if __name__ == "__main__": main()
---

# importar completo

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# importar completo

#!/usr/bin/env python3 # -*- coding: utf-8 -*- """ Script que ejecuta todo el proceso de importacin en un solo paso. Copia el archivo NDJSON y luego lo importa a la base de datos. """ import os import sys import argparse import subprocess from pathlib import Path # Configuracin de rutas BASE_DIR = Path(__file__).resolve().parent.parent SCRIPTS_DIR = Path(__file__).resolve().parent def ejecutar_comando(comando): """Ejecuta un comando y muestra la salida en tiempo real.""" proceso = subprocess.Popen( comando, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1 ) # Mostrar la salida en tiempo real for linea in proceso.stdout: print(linea, end='') # Esperar a que termine proceso.wait() return proceso.returncode def main(): """Funcin principal que ejecuta todo el proceso de importacin.""" parser = argparse.ArgumentParser(description='Ejecutar todo el proceso de importacin en un solo paso') parser.add_argument('archivo_ndjson', nargs='?', default=None, help='Ruta al archivo NDJSON unificado a importar (opcional si ya hay archivos en la carpeta de importacin)') parser.add_argument('--reiniciar-db', action='store_true', help='Eliminar la base de datos existente y crearla de nuevo') parser.add_argument('--solo-importar', action='store_true', help='Solo importar desde los archivos ya existentes en la carpeta de importacin') args = parser.parse_args() archivo_ndjson = Path(args.archivo_ndjson) if args.archivo_ndjson else None # Verificar si se proporcion un archivo y si existe if archivo_ndjson and not archivo_ndjson.exists() and not args.solo_importar: print(f"Error: El archivo NDJSON especificado no existe: {archivo_ndjson}") return 1 # Si no se especific archivo y se requiere solo importar, verificar si hay archivos en la carpeta if args.solo_importar or not archivo_ndjson: import_dir = BASE_DIR / 'data' / 'import' archivos_existentes = list(import_dir.glob('*.ndjson')) if not archivos_existentes: print(f"Error: No se encontraron archivos NDJSON en {import_dir}") print("Por favor, especifique un archivo para importar o copie archivos NDJSON a la carpeta de importacin.") return 1 else: print(f"Se utilizarn los archivos existentes en la carpeta de importacin:") for i, f in enumerate(archivos_existentes, 1): print(f" {i}. {f.name}") # Reiniciar base de datos si se solicita if args.reiniciar_db: print("\n=== PASO 0: Limpiando base de datos ===\n") comando_limpiar = [ sys.executable, str(SCRIPTS_DIR / 'limpiar_base_datos.py') ] if ejecutar_comando(comando_limpiar) != 0: print("Error al limpiar la base de datos.") return 1 # Paso 1: Copiar el archivo NDJSON (si se proporcion uno) if archivo_ndjson and not args.solo_importar: print("\n=== PASO 1: Copiando archivo NDJSON ===\n") comando_preparar = [ sys.executable, str(SCRIPTS_DIR / 'preparar_importacion.py'), str(archivo_ndjson) ] if ejecutar_comando(comando_preparar) != 0: print("Error al copiar el archivo NDJSON.") return 1 else: print("\n=== PASO 1: Omitiendo copia de archivo (usando archivos existentes) ===\n") # Paso 2: Importar datos a la base de datos print("\n=== PASO 2: Importando datos a la base de datos ===\n") comando_importar = [ sys.executable, str(SCRIPTS_DIR / 'importar_datos.py') ] if ejecutar_comando(comando_importar) != 0: print("Error al importar datos a la base de datos.") return 1 print("\nImportacin completada con xito!") if archivo_ndjson and not args.solo_importar: print(f"Los datos han sido importados desde: {archivo_ndjson}") else: print(f"Los datos han sido importados desde los archivos en: {BASE_DIR / 'data' / 'import'}") print(f"Base de datos: {BASE_DIR / 'data' / 'biblioteca.db'}") return 0 if __name__ == "__main__": sys.exit(main())
---

# preparar importacion

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** scripts

# preparar importacion

#!/usr/bin/env python3 # -*- coding: utf-8 -*- """ Script para preparar la importacin copiando el archivo NDJSON unificado a la ubicacin correcta dentro del proyecto. """ import os import shutil import argparse from pathlib import Path # Configuracin de rutas BASE_DIR = Path(__file__).resolve().parent.parent IMPORT_DIR = BASE_DIR / 'data' / 'import' def main(): """Funcin principal para copiar el archivo NDJSON.""" parser = argparse.ArgumentParser(description='Preparar la importacin de datos') parser.add_argument('archivo_ndjson', help='Ruta al archivo NDJSON unificado a importar') args = parser.parse_args() archivo_origen = Path(args.archivo_ndjson) if not archivo_origen.exists(): print(f"Error: El archivo especificado no existe: {archivo_origen}") return False # Crear directorio si no existe os.makedirs(IMPORT_DIR, exist_ok=True) # Mantener el nombre original del archivo archivo_destino = IMPORT_DIR / archivo_origen.name try: # Copiar archivo a la ubicacin de importacin shutil.copy2(archivo_origen, archivo_destino) print(f"Archivo copiado con xito a: {archivo_destino}") print("Ahora puede ejecutar 'python importar_datos.py' para comenzar la importacin.") return True except Exception as e: print(f"Error al copiar el archivo: {e}") return False if __name__ == "__main__": main()
---

# README

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** profiles

# README

# Sistema de Perfiles de Procesamiento de Documentos Este mdulo implementa un sistema modular y extensible para procesar diversos tipos de documentos (texto, JSON, DOCX, PDF, etc.) en unidades semnticas significativas mediante perfiles configurables. ## Arquitectura La arquitectura sigue un patrn de pipeline con cuatro componentes principales: ```Documento  [LOADER]  bloques  [SEGMENTER]  unidades  [POST-PROCESSOR]  [EXPORTER]  NDJSON/DB ``` ### 1. Loader Responsable de cargar cualquier formato de archivo y convertirlo en una secuencia de bloques de texto con metadatos de formato preservados: - Convierte documentos estructurados en bloques uniformes - Preserva metadatos crticos (estilo, nivel de encabezado, formato, etc.) - Hace las transformaciones formato-especficas necesarias **Loaders disponibles:** - `DocxLoader`: Archivos Microsoft Word (.docx) - `txtLoader`: Archivos de texto plano (.txt, .md) - *Prximamente:* `PdfLoader`, `JsonLoader`, etc. ### 2. Segmenter Define qu constituye una "unidad semntica" segn el perfil, aplicando reglas especficas para: - Detectar lmites de unidades (poemas, prrafos, captulos, mensajes) - Procesar bloques agrupndolos en unidades coherentes - Generar metadatos adicionales (estrofas, versos, nivel jerrquico) Implementa algoritmos como mquinas de estado para seguir los patrones definidos en `docs/ALGORITMOS_PROPUESTOS.md`. **Segmenters disponibles:** - `VerseSegmenter`: Detecta poemas y canciones - `HeadingSegmenter`: Detecta estructura de libros y documentos jerrquicos - `ParagraphSegmenter`: Detecta prrafos en artculos y textos generales - `MessageSegmenter`: Procesa mensajes en formato JSON ### 3. Post-Processor Aplica filtros y transformaciones genricas a las unidades detectadas: - Filtrado por longitud mnima - Normalizacin de texto (Unicode, espacios, etc.) - Deteccin de idioma - Deduplicacin - Enriquecimiento de metadatos ### 4. Exporter Exporta las unidades procesadas a diversos formatos: - NDJSON (formato histrico de Biblioperson) - SQLite - Meilisearch - Otros destinos configurables ## Perfiles y Configuracin Los perfiles de procesamiento se definen en archivos YAML en `profiles/`. Cada perfil especifica: ```yaml name: poem_or_lyrics description: "Detecta poemas y canciones en archivos de texto" segmenter: verse file_types: [".txt", ".md", ".docx", ".pdf"] thresholds: max_verse_length: 120 # Otros umbrales configurables title_patterns: - "^# " # Patrones regex para deteccin post_processor: text_normalizer post_processor_config: min_length: 30 metadata_map: titulo: title versos: verses_count exporter: ndjson ``` ## Mquina de Estados Los segmentadores utilizan mquinas de estado claras para modelar el procesamiento lnea a lnea. Por ejemplo, el `VerseSegmenter` usa los siguientes estados: - `SEARCH_TITLE`: Buscando ttulo de poema - `TITLE_FOUND`: Ttulo encontrado, esperando versos - `COLLECTING_VERSE`: Recolectando versos - `STANZA_GAP`: En hueco entre estrofas - `END_POEM`: Finalizando poema actual - `OUTSIDE_POEM`: Procesando otro tipo de contenido ## Uso Para procesar un archivo con un perfil: ```python from backend.shared.profiles.profile_manager import ProfileManager # Inicializar gestor de perfiles profile_manager = ProfileManager("/ruta/a/perfiles") # Procesar archivo units = profile_manager.process_file( file_path="mi_documento.docx", profile_name="poem_or_lyrics", output_path="resultados.ndjson" # Opcional ) # Acceder a unidades procesadas for unit in units: print(f"Tipo: {unit['type']}") if unit['type'] == 'poem': print(f"Ttulo: {unit.get('title')}") print(f"Estrofas: {unit.get('stanzas')}") ``` ## Extensibilidad El sistema est diseado para ser fcilmente extensible: 1. **Agregar un nuevo loader**: Crear una subclase de `BaseLoader` e implementar `load()` 2. **Agregar un nuevo segmenter**: Crear una subclase de `BaseSegmenter` e implementar `segment()` 3. **Agregar un nuevo perfil**: Crear un archivo YAML en `profiles/` ## Ventajas sobre el sistema anterior - Separacin completa entre I/O, lgica y configuracin - Umbrales 100% parametrizables en YAML (no hard-code) - Mquinas de estado explcitas en lugar de ifs anidados - Preservacin de metadatos de formato desde el origen - Mtricas de confianza para cada unidad detectada - Pipeline modular que permite intercambiar componentes - Fcil extensin con nuevos perfiles sin modificar cdigo ## Prximos pasos - Implementar `PdfLoader` con extraccin de formato - Aadir soporte para documentos HTML - Implementar deteccin de idioma en post-procesador - Crear interfaz de usuario para ajuste de perfiles
---

# verse segmenter

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** segmenters

# verse segmenter

import re from enum import Enum from typing import List, Dict, Any, Optional from .base import BaseSegmenter class VerseState(Enum): """Estados posibles durante el procesamiento de poemas.""" SEARCH_TITLE = 1 # Buscando ttulo de poema TITLE_FOUND = 2 # Ttulo encontrado, esperando versos iniciales COLLECTING_VERSE = 3 # Recolectando versos de un poema STANZA_GAP = 4 # En hueco entre estrofas END_POEM = 5 # Finalizando poema actual OUTSIDE_POEM = 6 # Fuera de poema (procesando otro tipo de contenido) class VerseSegmenter(BaseSegmenter): """ Segmentador para poemas y canciones basado en una mquina de estados. Este segmentador implementa las reglas descritas en ALGORITMOS_PROPUESTOS.md para la deteccin de poemas, versos y estrofas. """ def __init__(self, config: Optional[Dict[str, Any]] = None): super().__init__(config or {}) # Configurar thresholds con valores por defecto si no estn en config self.thresholds = self.config.get('thresholds', {}) self.max_verse_length = self.thresholds.get('max_verse_length', 120) self.max_title_length = self.thresholds.get('max_title_length', 80) self.min_consecutive_verses = self.thresholds.get('min_consecutive_verses', 3) self.min_stanza_verses = self.thresholds.get('min_stanza_verses', 2) self.max_empty_in_stanza = self.thresholds.get('max_empty_in_stanza', 2) self.min_empty_between_stanzas = self.thresholds.get('min_empty_between_stanzas', 2) self.max_empty_between_stanzas = self.thresholds.get('max_empty_between_stanzas', 3) self.max_empty_end_poem = self.thresholds.get('max_empty_end_poem', 3) self.max_space_ratio = self.thresholds.get('max_space_ratio', 0.35) # Patrones para deteccin de ttulo y estilos self.title_patterns = self.config.get('title_patterns', [ r'^# ', # Markdown H1 r'^\* ', # Asterisco inicial r'^> ', # Cita r'^[A-Z ]{4,}:$' # TTULO: (todo maysculas seguido de dos puntos) ]) self.title_regex = re.compile('|'.join(self.title_patterns)) # Patrn para acordes self.chord_pattern = re.compile(r'^\s*\[[A-G][#b]?m?[0-9]?(\([0-9]\))?\]\s*$') def is_verse(self, text: str) -> bool: """ Determina si una lnea es un verso segn heursticas. Args: text: Texto de la lnea Returns: True si parece un verso, False en caso contrario """ stripped = text.strip() if not stripped: return False # Chequear longitud if len(stripped) <= self.max_verse_length: # Si es una lnea de acordes, no es un verso if self.chord_pattern.match(stripped): return False # Si termina en puntuacin final, podra ser dilogo, no verso if stripped[-1] in '.!?' and not self._is_verse_by_context(stripped): return False return True # Chequear ratio de espacios (para versos con formato especial) space_count = stripped.count(' ') if space_count / len(stripped) >= self.max_space_ratio: return True return False def _is_verse_by_context(self, text: str) -> bool: """ Anlisis adicional para lneas que podran ser dilogo. (Implementacin simplificada, se expandira con anlisis de contexto) """ # Si empieza con guin de dilogo y termina en puntuacin, probablemente es dilogo if text.startswith('') and text[-1] in '.!?': return False return True def has_title_format(self, block: Dict[str, Any]) -> bool: """ Determina si un bloque tiene formato de ttulo. Args: block: Bloque de texto con metadatos Returns: True si parece un ttulo, False en caso contrario """ # Si ya viene marcado como heading desde el loader, es ttulo if block.get('is_heading', False): return True text = block.get('text', '').strip() if not text: return False # Verificar longitud mxima if len(text) > self.max_title_length: return False # Verificar patrones de formato if self.title_regex.search(text): return True # Verificar maysculas (para ttulos en maysculas) if len(text) >= 4: # Solo para textos no muy cortos uppercase_ratio = sum(1 for c in text if c.isupper()) / len(text) if uppercase_ratio > 0.7: # >70% maysculas return True # Verificar si est en negrita o centrado (si hay metadatos disponibles) if block.get('is_bold', False) or block.get('is_centered', False): return True return False def count_stanzas(self, verses: List[str]) -> int: """ Cuenta el nmero de estrofas en un conjunto de versos. Args: verses: Lista de versos incluyendo lneas vacas Returns: Nmero de estrofas detectadas """ stanzas = 1 empty_count = 0 for verse in verses: if not verse.strip(): empty_count += 1 if empty_count >= self.min_empty_between_stanzas: stanzas += 1 empty_count = 0 else: empty_count = 0 return stanzas def segment(self, blocks: List[Dict[str, Any]]) -> List[Dict[str, Any]]: """ Segmenta bloques en poemas usando una mquina de estados. Args: blocks: Lista de bloques de texto con metadatos Returns: Lista de unidades semnticas (poemas, prrafos, etc.) """ segments = [] # Variables para el poema actual current_title = None current_verses = [] consecutive_empty = 0 consecutive_verses = 0 # Estado inicial state = VerseState.SEARCH_TITLE # Procesamiento basado en estados for i, block in enumerate(blocks): text = block.get('text', '').strip() is_empty = not text # Actualizar contador de lneas vacas if is_empty: consecutive_empty += 1 # No procesar ms lgica para lneas vacas, solo actualizar contador continue else: # Lnea no vaca, evaluar segn estado actual is_potential_title = self.has_title_format(block) is_verse = self.is_verse(text) # Transiciones de estado y acciones if state == VerseState.SEARCH_TITLE: if is_potential_title: # Posible ttulo encontrado, cambiar estado y verificar lo que sigue current_title = text consecutive_empty = 0 state = VerseState.TITLE_FOUND elif is_verse: # Verso sin ttulo previo, podra ser poema sin ttulo consecutive_verses = 1 current_verses.append(text) if self._peek_ahead_for_verses(blocks, i, 2): # Si vienen ms versos, tratarlo como inicio de poema sin ttulo state = VerseState.COLLECTING_VERSE else: # Lnea suelta, no es poema segments.append({ "type": "paragraph", "text": text }) current_verses = [] consecutive_verses = 0 else: # Texto normal, no poema segments.append({ "type": "paragraph", "text": text }) elif state == VerseState.TITLE_FOUND: # Despus de un ttulo, esperamos versos if is_verse: current_verses.append(text) consecutive_verses = 1 consecutive_empty = 0 # Si vienen ms versos, confirmar poema if self._peek_ahead_for_verses(blocks, i, self.min_consecutive_verses - 1): state = VerseState.COLLECTING_VERSE elif is_potential_title: # Otro ttulo despus de un ttulo, sin versos intermedios # El ttulo anterior era falso positivo segments.append({ "type": "heading", "text": current_title, "level": block.get('heading_level', 1) }) current_title = text consecutive_empty = 0 # Seguimos en estado TITLE_FOUND con el nuevo ttulo else: # Texto normal despus de ttulo, no es poema segments.append({ "type": "heading", "text": current_title, "level": block.get('heading_level', 1) }) segments.append({ "type": "paragraph", "text": text }) current_title = None state = VerseState.SEARCH_TITLE elif state == VerseState.COLLECTING_VERSE: if is_verse: # Continuar recogiendo versos current_verses.append(text) consecutive_verses += 1 consecutive_empty = 0 elif is_potential_title: # Ttulo despus de recoger algunos versos = fin de poema state = VerseState.END_POEM # Procesaremos el ttulo en la siguiente iteracin, tras aadir el poema i -= 1 # Retroceder para reprocesar este bloque else: # Texto normal despus de versos = fin de poema state = VerseState.END_POEM # Retroceder para procesar este bloque despus i -= 1 elif state == VerseState.STANZA_GAP: if is_verse: # Verso despus de hueco = nueva estrofa del mismo poema current_verses.append("") # Aadir marcador de separacin de estrofa current_verses.append(text) consecutive_verses += 1 consecutive_empty = 0 state = VerseState.COLLECTING_VERSE elif is_potential_title: # Ttulo despus de hueco = fin del poema anterior state = VerseState.END_POEM # Retroceder para procesar ttulo en siguiente iteracin i -= 1 else: # Texto normal despus de hueco = fin de poema state = VerseState.END_POEM # Retroceder para procesar texto en siguiente iteracin i -= 1 elif state == VerseState.END_POEM: # Finalizar poema actual if current_verses: if consecutive_verses >= self.min_consecutive_verses: # Solo agregar como poema si hay suficientes versos segments.append({ "type": "poem", "title": current_title, "verses": current_verses, "verses_count": len([v for v in current_verses if v.strip()]), "stanzas": self.count_stanzas(current_verses), "confidence": self._calculate_confidence(current_verses) }) else: # Pocos versos, tratar como prrafos normales if current_title: segments.append({ "type": "heading", "text": current_title, "level": 1 }) for verse in current_verses: if verse.strip(): segments.append({ "type": "paragraph", "text": verse }) # Reiniciar variables para siguiente poema current_title = None current_verses = [] consecutive_verses = 0 consecutive_empty = 0 # Volver a estado inicial y reprocesar el bloque actual state = VerseState.SEARCH_TITLE i -= 1 # Retroceder para procesar este bloque desde inicio # Transiciones basadas en contador de lneas vacas if consecutive_empty > 0: if state == VerseState.COLLECTING_VERSE: if consecutive_empty <= self.max_empty_in_stanza: # Pocas vacas: sigue siendo misma estrofa pass elif consecutive_empty <= self.max_empty_between_stanzas: # Vacas intermedias: posible separacin de estrofas state = VerseState.STANZA_GAP else: # Demasiadas vacas: fin de poema state = VerseState.END_POEM elif state == VerseState.TITLE_FOUND: if consecutive_empty > self.max_empty_in_stanza: # Demasiadas vacas tras ttulo, no parece inicio de poema segments.append({ "type": "heading", "text": current_title, "level": block.get('heading_level', 1) }) current_title = None state = VerseState.SEARCH_TITLE # Procesar estado final - si qued un poema sin cerrar if state in (VerseState.COLLECTING_VERSE, VerseState.STANZA_GAP) and current_verses: if consecutive_verses >= self.min_consecutive_verses: segments.append({ "type": "poem", "title": current_title, "verses": current_verses, "verses_count": len([v for v in current_verses if v.strip()]), "stanzas": self.count_stanzas(current_verses), "confidence": self._calculate_confidence(current_verses) }) else: # Pocos versos, tratar como prrafos normales if current_title: segments.append({ "type": "heading", "text": current_title, "level": 1 }) for verse in current_verses: if verse.strip(): segments.append({ "type": "paragraph", "text": verse }) return segments def _peek_ahead_for_verses(self, blocks: List[Dict[str, Any]], current_idx: int, count: int) -> bool: """ Mira adelante en los bloques para verificar si hay suficientes versos. Args: blocks: Lista completa de bloques current_idx: ndice actual count: Nmero de versos a buscar Returns: True si hay al menos 'count' versos en los bloques siguientes """ if current_idx >= len(blocks) - 1: return False verse_count = 0 empty_count = 0 for i in range(current_idx + 1, min(len(blocks), current_idx + count*2 + 5)): text = blocks[i].get('text', '').strip() if not text: empty_count += 1 if empty_count > self.max_empty_in_stanza: # Demasiadas lneas vacas, corta la bsqueda break continue # Reiniciar contador de vacas si encontramos texto empty_count = 0 # Si es verso, incrementar contador if self.is_verse(text): verse_count += 1 if verse_count >= count: return True else: # Si encontramos algo que no es verso, cortar bsqueda break return False def _calculate_confidence(self, verses: List[str]) -> float: """ Calcula un valor de confianza para el poema detectado. Args: verses: Lista de versos incluyendo lneas vacas Returns: Valor de confianza de 0.0 a 1.0 """ if not verses: return 0.0 # Factores que aumentan la confianza factors = { "min_verses": 0.3, # Base por tener versos suficientes "verse_regularity": 0.0, # Consistencia en longitud de versos "stanza_structure": 0.0, # Estructura clara de estrofas "rhyme_presence": 0.0, # Posibles rimas (simplificado) } # 1. Calcular regularidad de versos (longitudes similares) non_empty = [v for v in verses if v.strip()] if len(non_empty) >= self.min_consecutive_verses: lengths = [len(v) for v in non_empty] if lengths: avg_len = sum(lengths) / len(lengths) # Desviacin estndar simplificada variance = sum((l - avg_len) ** 2 for l in lengths) / len(lengths) std_dev = variance ** 0.5 # Menor desviacin = ms regular = ms confianza if avg_len > 0: regularity = max(0, 1 - (std_dev / avg_len)) factors["verse_regularity"] = regularity * 0.3 # 2. Estructura de estrofas stanzas = self.count_stanzas(verses) if stanzas > 1: # Ms estrofas = estructura ms clara = ms confianza factors["stanza_structure"] = min(0.2, stanzas * 0.05) # 3. Rimas (simplificado - solo detecta terminaciones similares) if len(non_empty) >= 4: endings = [v[-2:] for v in non_empty if len(v) > 2] unique_endings = set(endings) if len(endings) > 0: repetition_ratio = 1 - (len(unique_endings) / len(endings)) factors["rhyme_presence"] = repetition_ratio * 0.2 # Calcular confianza total confidence = sum(factors.values()) # Normalizar a rango 0.0-1.0 return min(1.0, max(0.0, confidence))
---

# vite

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** public

# vite

<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="iconify iconify--logos" width="31.88" height="32" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 257"><defs><linearGradient id="IconifyId1813088fe1fbc01fb466" x1="-.828%" x2="57.636%" y1="7.652%" y2="78.411%"><stop offset="0%" stop-color="#41D1FF"></stop><stop offset="100%" stop-color="#BD34FE"></stop></linearGradient><linearGradient id="IconifyId1813088fe1fbc01fb467" x1="43.376%" x2="50.316%" y1="2.242%" y2="89.03%"><stop offset="0%" stop-color="#FFEA83"></stop><stop offset="8.333%" stop-color="#FFDD35"></stop><stop offset="100%" stop-color="#FFA800"></stop></linearGradient></defs><path fill="url(#IconifyId1813088fe1fbc01fb466)" d="M255.153 37.938L134.897 252.976c-2.483 4.44-8.862 4.466-11.382.048L.875 37.958c-2.746-4.814 1.371-10.646 6.827-9.67l120.385 21.517a6.537 6.537 0 0 0 2.322-.004l117.867-21.483c5.438-.991 9.574 4.796 6.877 9.62Z"></path><path fill="url(#IconifyId1813088fe1fbc01fb467)" d="M185.432.063L96.44 17.501a3.268 3.268 0 0 0-2.634 3.014l-5.474 92.456a3.268 3.268 0 0 0 3.997 3.378l24.777-5.718c2.318-.535 4.413 1.507 3.936 3.838l-7.361 36.047c-.495 2.426 1.782 4.5 4.151 3.78l15.304-4.649c2.372-.72 4.652 1.36 4.15 3.788l-11.698 56.621c-.732 3.542 3.979 5.473 5.943 2.437l1.313-2.028l72.516-144.72c1.215-2.423-.88-5.186-3.54-4.672l-25.505 4.922c-2.396.462-4.435-1.77-3.759-4.114l16.646-57.705c.677-2.35-1.37-4.583-3.769-4.113Z"></path></svg>
---

# vite env.d

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** src

# vite env.d

/// <reference types="vite/client" />
---

# main

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** src

# main

import { StrictMode } from 'react' import { createRoot } from 'react-dom/client' import './index.css' import App from './App.tsx' createRoot(document.getElementById('root')!).render( <StrictMode> <App /> </StrictMode>, )
---

# App

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** src

# App

import { BrowserRouter as Router, Routes, Route } from 'react-router-dom'; import { QueryClient, QueryClientProvider } from '@tanstack/react-query'; import Navbar from './components/Navbar'; import Home from './pages/Home'; import Search from './pages/Search'; import SemanticSearch from './pages/SemanticSearch'; import Explore from './pages/Explore'; import Generate from './pages/Generate'; import './App.css'; // Crear cliente de React Query const queryClient = new QueryClient(); function App() { return ( <QueryClientProvider client={queryClient}> <Router> <div className="flex flex-col min-h-screen bg-gray-100"> <Navbar /> <main className="flex-grow container mx-auto px-4 py-6"> <Routes> <Route path="/" element={<Home />} /> <Route path="/search" element={<Search />} /> <Route path="/semantic-search" element={<SemanticSearch />} /> <Route path="/explore" element={<Explore />} /> <Route path="/generate" element={<Generate />} /> </Routes> </main> <footer className="bg-blue-600 text-white py-4"> <div className="container mx-auto px-4 text-center"> <p>Biblioteca de Conocimiento Personal  {new Date().getFullYear()}</p> </div> </footer> </div> </Router> </QueryClientProvider> ); } export default App;
---

# App

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** src

# App

#root { max-width: 1280px; margin: 0 auto; padding: 2rem; text-align: center; } .logo { height: 6em; padding: 1.5em; will-change: filter; transition: filter 300ms; } .logo:hover { filter: drop-shadow(0 0 2em #646cffaa); } .logo.react:hover { filter: drop-shadow(0 0 2em #61dafbaa); } @keyframes logo-spin { from { transform: rotate(0deg); } to { transform: rotate(360deg); } } @media (prefers-reduced-motion: no-preference) { a:nth-of-type(2) .logo { animation: logo-spin infinite 20s linear; } } .card { padding: 2em; } .read-the-docs { color: #888; }
---

# index

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** src

# index

@tailwind base; @tailwind components; @tailwind utilities; :root { --primary-color: #4f46e5; --primary-hover: #4338ca; --bg-color: #f9fafb; --card-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06); --card-hover-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05); font-family: system-ui, Avenir, Helvetica, Arial, sans-serif; line-height: 1.5; font-weight: 400; color-scheme: light dark; color: rgba(255, 255, 255, 0.87); background-color: #242424; font-synthesis: none; text-rendering: optimizeLegibility; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } a { font-weight: 500; color: #646cff; text-decoration: inherit; } a:hover { color: #535bf2; } body { margin: 0; display: flex; place-items: center; min-width: 320px; min-height: 100vh; background-color: var(--bg-color); font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif; color: #1f2937; } h1 { font-size: 3.2em; line-height: 1.1; } button { border-radius: 8px; border: 1px solid transparent; padding: 0.6em 1.2em; font-size: 1em; font-weight: 500; font-family: inherit; background-color: #1a1a1a; cursor: pointer; transition: border-color 0.25s; } button:hover { border-color: #646cff; } button:focus, button:focus-visible { outline: 4px auto -webkit-focus-ring-color; } @media (prefers-color-scheme: light) { :root { color: #213547; background-color: #ffffff; } a:hover { color: #747bff; } button { background-color: #f9f9f9; } } /* Animaciones y efectos */ @keyframes fadeIn { from { opacity: 0; } to { opacity: 1; } } @keyframes slideUp { from { transform: translateY(10px); opacity: 0; } to { transform: translateY(0); opacity: 1; } } .animate-fadeIn { animation: fadeIn 0.5s ease forwards; } .animate-slideUp { animation: slideUp 0.3s ease forwards; } /* Estilos para tarjetas */ .card { @apply bg-white rounded-lg shadow-md overflow-hidden border border-gray-200; transition: all 0.2s ease; } .card:hover { @apply shadow-lg border-gray-300; transform: translateY(-2px); } /* Mejora de botones */ .btn { @apply font-medium rounded-lg px-4 py-2 transition-colors; } .btn-primary { @apply bg-blue-600 text-white hover:bg-blue-700; } .btn-secondary { @apply bg-gray-200 text-gray-800 hover:bg-gray-300; } /* Efectos de entrada para resultados */ .result-item { opacity: 0; transform: translateY(10px); animation: slideUp 0.3s ease forwards; } .result-item:nth-child(1) { animation-delay: 0.1s; } .result-item:nth-child(2) { animation-delay: 0.2s; } .result-item:nth-child(3) { animation-delay: 0.3s; } .result-item:nth-child(4) { animation-delay: 0.4s; } .result-item:nth-child(5) { animation-delay: 0.5s; }
---

# ResultCard

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** components

# ResultCard

import { ContentItem } from '../services/api'; interface ResultCardProps { item: ContentItem; searchTerm?: string; } const ResultCard = ({ item, searchTerm }: ResultCardProps) => { // Funcin para resaltar trminos de bsqueda en el texto const highlightText = (text: string, term: string) => { if (!term || term.trim() === '') return text; const parts = text.split(new RegExp(`(${term})`, 'gi')); return ( <> {parts.map((part, i) => part.toLowerCase() === term.toLowerCase() ? <mark key={i} className="bg-yellow-200 px-1 rounded">{part}</mark> : part )} </> ); }; // Calcular color basado en porcentaje de similitud const getSimiWidthAndColor = (similitud: number | undefined) => { if (similitud === undefined) return { width: '0%', color: 'bg-gray-300' }; const percentage = Math.round(similitud * 100); let color = 'bg-green-500'; if (percentage < 50) color = 'bg-red-500'; else if (percentage < 75) color = 'bg-yellow-500'; return { width: `${percentage}%`, color }; }; const { width, color } = getSimiWidthAndColor(item.similitud); // Formatear fecha elegantemente const formatDate = (dateString: string) => { const date = new Date(dateString); return new Intl.DateTimeFormat('es-ES', { year: 'numeric', month: 'long', day: 'numeric' }).format(date); }; // Log de depuracin si falta el contenido if (!item.contenido_texto) { console.error('El resultado no tiene contenido_texto:', item); } return ( <div className="card hover:shadow-xl transition-all duration-300"> {/* Barra de similitud en la parte superior */} <div className="h-1.5 bg-gray-100 w-full"> <div className={`h-full ${color} transition-all duration-500`} style={{ width }} /> </div> <div className="p-5"> {/* Metadatos y porcentaje */} <div className="flex flex-wrap justify-between items-center mb-4"> <div className="flex flex-wrap items-center text-sm text-gray-600 mb-2 sm:mb-0"> <div className="flex items-center mr-3"> <svg className="w-4 h-4 mr-1 text-gray-500" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"> <path fillRule="evenodd" d="M6 2a1 1 0 00-1 1v1H4a2 2 0 00-2 2v10a2 2 0 002 2h12a2 2 0 002-2V6a2 2 0 00-2-2h-1V3a1 1 0 10-2 0v1H7V3a1 1 0 00-1-1zm0 5a1 1 0 000 2h8a1 1 0 100-2H6z" clipRule="evenodd" /> </svg> <span>{formatDate(item.fecha)}</span> </div> <div className="flex items-center mr-3"> <span className="px-2 py-1 bg-gray-100 text-xs rounded-full">{item.plataforma}</span> </div> <div className="flex items-center"> <span className="text-blue-600">{item.fuente}</span> </div> </div> {item.similitud !== undefined && ( <div className={`text-sm font-medium px-3 py-1 rounded-full ${ item.similitud > 0.75 ? 'bg-green-100 text-green-800' : item.similitud > 0.5 ? 'bg-blue-100 text-blue-800' : 'bg-yellow-100 text-yellow-800' }`}> {Math.round(item.similitud * 100)}% similar </div> )} </div> {/* Contenido principal */} <div className="mb-4 text-gray-700 whitespace-pre-line"> {searchTerm ? highlightText(item.contenido_texto || '', searchTerm) : (item.contenido_texto || <span className="text-red-500">[Sin contenido]</span>)} </div> {/* Temas */} {item.temas && item.temas.length > 0 && ( <div className="mt-4"> <div className="flex flex-wrap gap-2"> {item.temas.map(tema => ( <div key={tema.id} className="flex items-center text-xs px-3 py-1 bg-indigo-50 text-indigo-700 rounded-full border border-indigo-100 hover:bg-indigo-100 transition-colors cursor-pointer" > <svg className="w-3 h-3 mr-1 text-indigo-500" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"> <path fillRule="evenodd" d="M17.707 9.293a1 1 0 010 1.414l-7 7a1 1 0 01-1.414 0l-7-7A.997.997 0 012 10V5a3 3 0 013-3h5c.256 0 .512.098.707.293l7 7zM5 6a1 1 0 100-2 1 1 0 000 2z" clipRule="evenodd" /> </svg> {tema.nombre} </div> ))} </div> </div> )} </div> </div> ); }; export default ResultCard;
---

# SearchBar

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** components

# SearchBar

import { useState } from 'react'; import { FaSearch, FaTimes } from 'react-icons/fa'; export interface SearchFilter { id: string; label: string; } interface SearchBarProps { value: string; onChange: (value: string) => void; onSearch: () => void; placeholder?: string; filters?: SearchFilter[]; selectedFilters?: string[]; onFilterToggle?: (filterId: string) => void; } const SearchBar = ({ value, onChange, onSearch, placeholder = "Escribe una idea, concepto o pregunta...", filters = [], selectedFilters = [], onFilterToggle }: SearchBarProps) => { const [isFocused, setIsFocused] = useState(false); const handleSubmit = (e: React.FormEvent) => { e.preventDefault(); onSearch(); }; const handleKeyDown = (e: React.KeyboardEvent) => { if (e.key === 'Enter') { onSearch(); } }; return ( <div className="w-full"> <form onSubmit={handleSubmit}> <div className={`relative flex items-center bg-white rounded-lg overflow-hidden transition-all ${ isFocused ? 'ring-2 ring-blue-500 shadow-lg' : 'border border-gray-300' }`}> <div className="pl-3 text-gray-400"> <FaSearch size={16} /> </div> <input type="text" value={value} onChange={(e) => onChange(e.target.value)} onFocus={() => setIsFocused(true)} onBlur={() => setIsFocused(false)} onKeyDown={handleKeyDown} placeholder={placeholder} className="w-full px-3 py-3 outline-none text-gray-700" /> {value && ( <button type="button" onClick={() => onChange('')} className="p-2 text-gray-400 hover:text-gray-600" > <FaTimes size={12} /> </button> )} <button type="submit" className="h-full px-5 py-3 bg-blue-600 text-white font-medium transition hover:bg-blue-700" > Buscar </button> </div> </form> {filters.length > 0 && ( <div className="flex flex-wrap gap-2 mt-3"> {filters.map(filter => ( <button key={filter.id} onClick={() => onFilterToggle?.(filter.id)} className={`px-3 py-1 rounded-full text-sm font-medium transition-colors ${ selectedFilters.includes(filter.id) ? 'bg-blue-100 text-blue-800 hover:bg-blue-200' : 'bg-gray-100 text-gray-700 hover:bg-gray-200' }`} > {filter.label} </button> ))} </div> )} </div> ); }; export default SearchBar;
---

# Pagination

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** components

# Pagination

import { PaginationData } from '../services/api'; interface PaginationProps { pagination: PaginationData; onPageChange: (page: number) => void; } const Pagination = ({ pagination, onPageChange }: PaginationProps) => { const { pagina_actual, total_paginas } = pagination; if (total_paginas <= 1) return null; // Crear array de pginas a mostrar const getVisiblePageNumbers = () => { const pages = []; const maxVisiblePages = 5; let startPage = Math.max(1, pagina_actual - Math.floor(maxVisiblePages / 2)); const endPage = Math.min(total_paginas, startPage + maxVisiblePages - 1); if (endPage - startPage + 1 < maxVisiblePages) { startPage = Math.max(1, endPage - maxVisiblePages + 1); } // Aadir primera pgina si es necesario if (startPage > 1) { pages.push( <button key="first" onClick={() => onPageChange(1)} className="relative inline-flex items-center px-3 py-2 rounded-md text-sm font-medium text-gray-700 hover:bg-blue-50 focus:z-20 focus:outline-none focus:ring-2 focus:ring-blue-400" aria-label="Primera pgina" > 1 </button> ); if (startPage > 2) { pages.push( <span key="ellipsis1" className="relative inline-flex items-center px-3 py-2 text-sm font-medium text-gray-700"> ... </span> ); } } // Aadir pginas intermedias for (let i = startPage; i <= endPage; i++) { pages.push( <button key={i} onClick={() => onPageChange(i)} className={`relative inline-flex items-center px-3 py-2 rounded-md text-sm font-medium ${ i === pagina_actual ? 'z-10 bg-blue-50 border-blue-500 text-blue-600 focus:z-20 focus:outline-none focus:ring-2 focus:ring-blue-500' : 'text-gray-700 hover:bg-blue-50 focus:z-20 focus:outline-none focus:ring-2 focus:ring-blue-400' }`} aria-current={i === pagina_actual ? 'page' : undefined} > {i} </button> ); } // Aadir ltima pgina si es necesario if (endPage < total_paginas) { if (endPage < total_paginas - 1) { pages.push( <span key="ellipsis2" className="relative inline-flex items-center px-3 py-2 text-sm font-medium text-gray-700"> ... </span> ); } pages.push( <button key="last" onClick={() => onPageChange(total_paginas)} className="relative inline-flex items-center px-3 py-2 rounded-md text-sm font-medium text-gray-700 hover:bg-blue-50 focus:z-20 focus:outline-none focus:ring-2 focus:ring-blue-400" aria-label="ltima pgina" > {total_paginas} </button> ); } return pages; }; return ( <nav className="flex justify-center mt-8"> <div className="inline-flex rounded-md shadow-sm -space-x-px items-center" aria-label="Paginacin"> <button onClick={() => onPageChange(pagina_actual - 1)} disabled={pagina_actual === 1} className={`relative inline-flex items-center px-3 py-2 rounded-l-md border border-gray-300 text-sm font-medium ${ pagina_actual === 1 ? 'bg-gray-100 text-gray-400 cursor-not-allowed' : 'bg-white text-gray-700 hover:bg-blue-50 focus:z-20 focus:outline-none focus:ring-2 focus:ring-blue-400' }`} aria-label="Pgina anterior" > <svg className="h-5 w-5" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true"> <path fillRule="evenodd" d="M12.707 5.293a1 1 0 010 1.414L9.414 10l3.293 3.293a1 1 0 01-1.414 1.414l-4-4a1 1 0 010-1.414l4-4a1 1 0 011.414 0z" clipRule="evenodd" /> </svg> </button> {getVisiblePageNumbers()} <button onClick={() => onPageChange(pagina_actual + 1)} disabled={pagina_actual === total_paginas} className={`relative inline-flex items-center px-3 py-2 rounded-r-md border border-gray-300 text-sm font-medium ${ pagina_actual === total_paginas ? 'bg-gray-100 text-gray-400 cursor-not-allowed' : 'bg-white text-gray-700 hover:bg-blue-50 focus:z-20 focus:outline-none focus:ring-2 focus:ring-blue-400' }`} aria-label="Pgina siguiente" > <svg className="h-5 w-5" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true"> <path fillRule="evenodd" d="M7.293 14.707a1 1 0 010-1.414L10.586 10 7.293 6.707a1 1 0 011.414-1.414l4 4a1 1 0 010 1.414l-4 4a1 1 0 01-1.414 0z" clipRule="evenodd" /> </svg> </button> </div> </nav> ); }; export default Pagination;
---

# Skeleton

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** components

# Skeleton

interface SkeletonProps { count?: number; } const SkeletonItem = () => { return ( <div className="bg-white border border-gray-200 rounded-lg shadow-md overflow-hidden animate-pulse"> {/* Barra superior */} <div className="h-1.5 bg-gray-200 w-full" /> <div className="p-5"> {/* Header */} <div className="flex flex-wrap justify-between items-center mb-4"> <div className="flex items-center"> <div className="h-4 bg-gray-200 rounded w-32 mb-2 sm:mb-0" /> </div> <div className="h-6 bg-gray-200 rounded-full w-20" /> </div> {/* Contenido */} <div className="space-y-2 mb-4"> <div className="h-4 bg-gray-200 rounded w-full" /> <div className="h-4 bg-gray-200 rounded w-full" /> <div className="h-4 bg-gray-200 rounded w-full" /> <div className="h-4 bg-gray-200 rounded w-3/4" /> </div> {/* Etiquetas */} <div className="mt-4 flex flex-wrap gap-2"> <div className="h-6 bg-gray-200 rounded-full w-16" /> <div className="h-6 bg-gray-200 rounded-full w-20" /> <div className="h-6 bg-gray-200 rounded-full w-24" /> </div> </div> </div> ); }; const Skeleton = ({ count = 3 }: SkeletonProps) => { return ( <div className="space-y-6"> {Array.from({ length: count }).map((_, index) => ( <SkeletonItem key={index} /> ))} </div> ); }; export default Skeleton;
---

# Navbar

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** components

# Navbar

import { useState } from 'react'; import { Link, useLocation } from 'react-router-dom'; const Navbar = () => { const location = useLocation(); const [isMobileMenuOpen, setIsMobileMenuOpen] = useState(false); // Verificar qu ruta est activa const isActive = (path: string) => { return location.pathname === path; }; // Alternar men mvil const toggleMobileMenu = () => { setIsMobileMenuOpen(!isMobileMenuOpen); }; return ( <nav className="bg-gradient-to-r from-blue-600 to-indigo-700 text-white shadow-lg sticky top-0 z-50"> <div className="container mx-auto px-4"> <div className="flex justify-between items-center py-3"> {/* Logo */} <Link to="/" className="text-xl font-bold flex items-center group"> <div className="mr-3 p-2 bg-white rounded-full transform transition-transform group-hover:rotate-12 shadow-md"> <svg xmlns="http://www.w3.org/2000/svg" className="h-6 w-6 text-blue-600" fill="none" viewBox="0 0 24 24" stroke="currentColor"> <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M12 6.253v13m0-13C10.832 5.477 9.246 5 7.5 5S4.168 5.477 3 6.253v13C4.168 18.477 5.754 18 7.5 18s3.332.477 4.5 1.253m0-13C13.168 5.477 14.754 5 16.5 5c1.747 0 3.332.477 4.5 1.253v13C19.832 18.477 18.247 18 16.5 18c-1.746 0-3.332.477-4.5 1.253" /> </svg> </div> <span className="font-semibold tracking-wide text-white">Biblioteca Personal</span> </Link> {/* Navegacin escritorio */} <div className="hidden md:flex items-center space-x-1"> <NavLink to="/" active={isActive('/')}> Inicio </NavLink> <NavLink to="/search" active={isActive('/search')}> Bsqueda </NavLink> <NavLink to="/semantic-search" active={isActive('/semantic-search')}> Bsqueda Semntica </NavLink> <NavLink to="/explore" active={isActive('/explore')}> Explorar </NavLink> <NavLink to="/generate" active={isActive('/generate')}> Generar </NavLink> </div> {/* Botn men mvil */} <div className="md:hidden"> <button onClick={toggleMobileMenu} className="focus:outline-none p-2 rounded-md bg-black bg-opacity-30 hover:bg-opacity-50 transition" aria-expanded={isMobileMenuOpen} aria-label="Abrir men" > {isMobileMenuOpen ? ( <svg xmlns="http://www.w3.org/2000/svg" className="h-6 w-6 text-white" fill="none" viewBox="0 0 24 24" stroke="currentColor"> <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M6 18L18 6M6 6l12 12" /> </svg> ) : ( <svg xmlns="http://www.w3.org/2000/svg" className="h-6 w-6 text-white" fill="none" viewBox="0 0 24 24" stroke="currentColor"> <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M4 6h16M4 12h16M4 18h16" /> </svg> )} </button> </div> </div> {/* Men mvil desplegable */} <div className={`md:hidden ${isMobileMenuOpen ? 'block' : 'hidden'} animate-fadeIn`}> <div className="py-2 space-y-1"> <MobileNavLink to="/" active={isActive('/')}> Inicio </MobileNavLink> <MobileNavLink to="/search" active={isActive('/search')}> Bsqueda </MobileNavLink> <MobileNavLink to="/semantic-search" active={isActive('/semantic-search')}> Bsqueda Semntica </MobileNavLink> <MobileNavLink to="/explore" active={isActive('/explore')}> Explorar </MobileNavLink> <MobileNavLink to="/generate" active={isActive('/generate')}> Generar </MobileNavLink> </div> </div> </div> </nav> ); }; // Componente de enlace para escritorio interface NavLinkProps { to: string; active: boolean; children: React.ReactNode; } const NavLink = ({ to, active, children }: NavLinkProps) => ( <Link to={to} className={`px-3 py-2 rounded-md text-sm font-medium transition-all ${ active ? 'bg-white bg-opacity-20 text-white' : 'text-blue-100 hover:bg-white hover:bg-opacity-10 hover:text-white' }`} > {children} </Link> ); // Componente de enlace para mvil const MobileNavLink = ({ to, active, children }: NavLinkProps) => ( <Link to={to} className={`block px-3 py-2 rounded-md text-base font-medium ${ active ? 'bg-white bg-opacity-20 text-white' : 'text-blue-100 hover:bg-white hover:bg-opacity-10 hover:text-white' }`} > {children} </Link> ); export default Navbar;
---

# api

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** services

# api

import axios from 'axios'; // Determinar URL base segn el entorno const BASE_URL = import.meta.env.DEV ? 'http://localhost:5000/api' : '/api'; // Cliente Axios configurado const apiClient = axios.create({ baseURL: BASE_URL, headers: { 'Content-Type': 'application/json', }, }); // Interfaces para tipado export interface SearchParams { contenido_texto?: string; tema?: string; plataforma?: string; fecha_inicio?: string; fecha_fin?: string; limite?: number; } export interface SemanticSearchParams { contenido_texto: string; pagina?: number; por_pagina?: number; filtros?: string; limite?: number; similitud_min?: number; ordenar_por?: string; autor?: string; usar_meilisearch?: boolean; } export interface PaginationData { pagina_actual: number; resultados_por_pagina: number; total_resultados: number; total_paginas: number; } export interface SemanticSearchResponse { resultados: ContentItem[]; paginacion: PaginationData; consulta: string; } export interface ContentItem { id: number; contenido_texto: string; fecha: string; plataforma: string; fuente: string; similitud?: number; temas?: Array<{ id: number; nombre: string; relevancia: number; }>; } export interface GenerateParams { tema: string; tipo?: 'post' | 'articulo' | 'guion'; longitud?: 'corto' | 'medio' | 'largo'; } export interface RagParams { tema: string; tipo?: 'post' | 'articulo' | 'guion' | 'resumen' | 'analisis'; estilo?: string; num_resultados?: number; proveedor?: 'gemini' | 'openai'; solo_prompt?: boolean; } export interface RagResponse { contenido: string; fragmentos_utilizados: number; tema: string; tipo: string; estilo?: string; proveedor: string; } export interface RagPromptResponse { prompt: string; fragmentos_recuperados: number; } // API para informacin general export const getInfo = async () => { const response = await apiClient.get('/info'); return response.data; }; // API para bsqueda bsica export const searchContent = async (params: SearchParams) => { const response = await apiClient.get('/contenido', { params }); return response.data; }; // API para bsqueda semntica export const semanticSearch = async (params: SemanticSearchParams): Promise<SemanticSearchResponse> => { const searchParams = { ...params, usar_meilisearch: params.usar_meilisearch !== false }; const response = await apiClient.get('/busqueda/semantica', { params: searchParams }); return response.data; }; // API para generar contenido export const generateContent = async (params: GenerateParams) => { const response = await apiClient.get('/generar', { params }); return response.data; }; // API para generacin con RAG export const generateWithRag = async (params: RagParams): Promise<RagResponse> => { const response = await apiClient.post('/generar_rag', params); return response.data; }; // API para obtener solo el prompt RAG export const getRagPrompt = async (params: RagParams): Promise<RagPromptResponse> => { const response = await apiClient.post('/generar_rag', {...params, solo_prompt: true}); return response.data; }; // API para estadstica de autores (grfico de pastel) export const getAutoresStats = async () => { const response = await apiClient.get('/estadisticas/autores'); return response.data; }; export { apiClient }; export default { getInfo, searchContent, semanticSearch, generateContent, generateWithRag, getRagPrompt, getAutoresStats, };
---

# Home

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** pages

# Home

import { useEffect, useState } from 'react'; import { Link } from 'react-router-dom'; import { getInfo, getAutoresStats } from '../services/api'; import { PieChart, Pie, Cell, Tooltip, Legend, ResponsiveContainer } from 'recharts'; interface LibraryInfo { total_contenidos: number; total_temas: number; rango_fechas: { primera: string; ultima: string; }; distribucion_plataformas: Array<{ plataforma: string; cantidad: number; }>; descripcion: string; total_autores: number; } interface AutorStat { autor: string; cantidad: number; primer_registro?: string; ultimo_registro?: string; temas?: number; } const Home = () => { const [info, setInfo] = useState<LibraryInfo | null>(null); const [loading, setLoading] = useState(true); const [error, setError] = useState<string | null>(null); const [autoresStats, setAutoresStats] = useState<AutorStat[]>([]); useEffect(() => { const fetchInfo = async () => { try { setLoading(true); const data = await getInfo(); setInfo(data); const autoresData = await getAutoresStats(); setAutoresStats(autoresData); } catch (err) { console.error('Error al cargar informacin:', err); setError('No se pudo cargar la informacin de la biblioteca'); } finally { setLoading(false); } }; fetchInfo(); }, []); if (loading) { return ( <div className="flex justify-center items-center h-64"> <div className="text-blue-600 text-center"> <svg className="animate-spin h-10 w-10 mx-auto mb-4" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"> <circle className="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" strokeWidth="4"></circle> <path className="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path> </svg> <p>Cargando informacin...</p> </div> </div> ); } if (error) { return ( <div className="bg-red-50 border-l-4 border-red-500 p-4 my-4"> <div className="flex"> <div className="flex-shrink-0"> <svg className="h-5 w-5 text-red-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor"> <path fillRule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zM8.707 7.293a1 1 0 00-1.414 1.414L8.586 10l-1.293 1.293a1 1 0 101.414 1.414L10 11.414l1.293 1.293a1 1 0 001.414-1.414L11.414 10l1.293-1.293a1 1 0 00-1.414-1.414L10 8.586 8.707 7.293z" clipRule="evenodd" /> </svg> </div> <div className="ml-3"> <p className="text-sm text-red-700">{error}</p> </div> </div> </div> ); } return ( <div> <div className="mb-10 text-center"> <h1 className="text-3xl font-bold text-blue-800 mb-4">Biblioteca de Conocimiento Personal</h1> <p className="text-lg text-gray-600 max-w-3xl mx-auto"> Bienvenido a tu biblioteca personal de conocimiento, un repositorio centralizado de todos tus comentarios, debates y reflexiones publicados en redes sociales y otros medios. </p> </div> {info && ( <div className="grid md:grid-cols-2 gap-8"> <div className="bg-white rounded-lg shadow-md p-6"> <h2 className="text-xl font-semibold text-blue-700 mb-4">Estadsticas de la Biblioteca</h2> <div className="space-y-4"> <div className="flex justify-between border-b pb-2"> <span className="text-gray-600">Contenidos:</span> <span className="font-semibold">{info.total_contenidos}</span> </div> <div className="flex justify-between border-b pb-2"> <span className="text-gray-600">Temas:</span> <span className="font-semibold">{info.total_temas}</span> </div> <div className="flex justify-between border-b pb-2"> <span className="text-gray-600">Primer registro:</span> <span className="font-semibold"> {info.rango_fechas.primera ? new Date(info.rango_fechas.primera).toLocaleDateString() : 'N/A'} </span> </div> <div className="flex justify-between"> <span className="text-gray-600">ltimo registro:</span> <span className="font-semibold"> {info.rango_fechas.ultima ? new Date(info.rango_fechas.ultima).toLocaleDateString() : 'N/A'} </span> </div> <div className="flex justify-between border-b pb-2"> <span className="text-gray-600">Autores:</span> <span className="font-semibold">{info.total_autores}</span> </div> </div> </div> <div className="bg-white rounded-lg shadow-md p-6"> <h2 className="text-xl font-semibold text-blue-700 mb-4">Explorar Contenido</h2> <div className="space-y-4"> <Link to="/search" className="block bg-blue-50 hover:bg-blue-100 p-4 rounded transition"> <h3 className="font-semibold text-blue-700">Bsqueda Bsica</h3> <p className="text-sm text-gray-600">Busca contenido por palabras clave, temas o plataformas</p> </Link> <Link to="/semantic-search" className="block bg-indigo-50 hover:bg-indigo-100 p-4 rounded transition"> <h3 className="font-semibold text-indigo-700">Bsqueda Semntica</h3> <p className="text-sm text-gray-600">Encuentra contenido conceptualmente relacionado con tu consulta</p> </Link> <Link to="/explore" className="block bg-green-50 hover:bg-green-100 p-4 rounded transition"> <h3 className="font-semibold text-green-700">Explorar por Categoras</h3> <p className="text-sm text-gray-600">Navega por temas, fechas y plataformas</p> </Link> <Link to="/generate" className="block bg-purple-50 hover:bg-purple-100 p-4 rounded transition"> <h3 className="font-semibold text-purple-700">Generar Contenido</h3> <p className="text-sm text-gray-600">Crea nuevo contenido basado en tu biblioteca personal</p> </Link> </div> </div> </div> )} {autoresStats.length > 0 && ( <div className="bg-white rounded-lg shadow-md p-6 mt-8"> <h2 className="text-xl font-semibold text-blue-700 mb-4">Distribucin por Autor</h2> <div style={{ width: '100%', height: 300 }}> <ResponsiveContainer> <PieChart> <Pie data={autoresStats} dataKey="cantidad" nameKey="autor" cx="50%" cy="50%" outerRadius={100} fill="#2563eb" label={({ name, percent }) => `${name} (${(percent * 100).toFixed(1)}%)`} > {autoresStats.map((entry, index) => ( <Cell key={`cell-${index}`} fill={['#2563eb', '#10b981', '#f59e42', '#f43f5e', '#6366f1', '#fbbf24', '#14b8a6', '#a21caf'][index % 8]} /> ))} </Pie> <Tooltip formatter={(value: number) => `${value} contenidos`} /> <Legend /> </PieChart> </ResponsiveContainer> </div> <div className="mt-8 overflow-x-auto"> <table className="min-w-full text-sm text-left border border-gray-200 rounded-lg"> <thead className="bg-gray-50"> <tr> <th className="px-4 py-2 font-semibold text-gray-700">Autor</th> <th className="px-4 py-2 font-semibold text-gray-700">Contenidos</th> <th className="px-4 py-2 font-semibold text-gray-700">Primer registro</th> <th className="px-4 py-2 font-semibold text-gray-700">ltimo registro</th> <th className="px-4 py-2 font-semibold text-gray-700">Temas</th> </tr> </thead> <tbody> {autoresStats.map((a, i) => ( <tr key={a.autor} className={i % 2 === 0 ? 'bg-white' : 'bg-gray-50'}> <td className="px-4 py-2">{a.autor}</td> <td className="px-4 py-2">{a.cantidad}</td> <td className="px-4 py-2">{a.primer_registro ? new Date(a.primer_registro).toLocaleDateString() : 'N/A'}</td> <td className="px-4 py-2">{a.ultimo_registro ? new Date(a.ultimo_registro).toLocaleDateString() : 'N/A'}</td> <td className="px-4 py-2">{a.temas ?? 0}</td> </tr> ))} </tbody> </table> </div> </div> )} </div> ); }; export default Home;
---

# Explore

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** pages

# Explore

import { Link } from 'react-router-dom'; const Explore = () => { return ( <div className="text-center py-10"> <h1 className="text-2xl font-bold text-blue-800 mb-4">Explorar por Categoras</h1> <p className="mb-6 text-gray-600">Esta funcin estar disponible prximamente.</p> <div className="mt-8"> <Link to="/semantic-search" className="px-6 py-3 bg-blue-600 text-white rounded-lg hover:bg-blue-700 transition" > Ir a Bsqueda Semntica </Link> </div> </div> ); }; export default Explore;
---

# Search

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** pages

# Search

import { useState, useEffect } from 'react'; import { useQuery } from '@tanstack/react-query'; import { apiClient } from '../services/api'; import SearchBar, { SearchFilter } from '../components/SearchBar'; import ResultCard from '../components/ResultCard'; import Pagination from '../components/Pagination'; import Skeleton from '../components/Skeleton'; import Select from 'react-select'; const AVAILABLE_FILTERS: SearchFilter[] = [ { id: 'recientes', label: 'Ms recientes' }, { id: 'notas', label: 'Notas' }, { id: 'articulos', label: 'Artculos' }, { id: 'redes', label: 'Redes sociales' } ]; const Search = () => { const [searchTerm, setSearchTerm] = useState(''); const [queryParams, setQueryParams] = useState<any>(null); const [resultsPerPage, setResultsPerPage] = useState(10); const [selectedFilters, setSelectedFilters] = useState<string[]>([]); const [orderBy, setOrderBy] = useState<'relevancia' | 'fecha'>('relevancia'); const [autores, setAutores] = useState<{ label: string; value: string }[]>([]); const [selectedAutores, setSelectedAutores] = useState<{ label: string; value: string }[]>([]); // Consulta a la API de bsqueda general const { data, isLoading, isError, error } = useQuery({ queryKey: ['busquedaGeneral', queryParams, selectedFilters], queryFn: () => queryParams ? apiClient.get('/busqueda', { params: queryParams }).then(r => r.data) : Promise.resolve(null), enabled: !!queryParams, }); useEffect(() => { apiClient.get<{ autores: string[] }>('/autores') .then(response => { const data = response.data; if (data && data.autores) { setAutores(data.autores.map((a: string) => ({ label: a, value: a }))); } }); }, []); const handleFilterToggle = (filterId: string) => { setSelectedFilters(prev => prev.includes(filterId) ? prev.filter(id => id !== filterId) : [...prev, filterId]); if (queryParams) handleSearch(); }; const handleSearch = () => { if (searchTerm.trim() === '') return; setQueryParams({ contenido_texto: searchTerm, pagina: 1, por_pagina: resultsPerPage, filtros: selectedFilters.length > 0 ? selectedFilters.join(',') : undefined, ordenar: orderBy, autor: selectedAutores.length > 0 ? selectedAutores.map(a => a.value).join(',') : undefined, }); }; const handlePageChange = (page: number) => { if (!queryParams) return; setQueryParams({ ...queryParams, pagina: page }); window.scrollTo({ top: 0, behavior: 'smooth' }); }; const handleResultsPerPageChange = (perPage: number) => { setResultsPerPage(perPage); if (queryParams) { setQueryParams({ ...queryParams, pagina: 1, por_pagina: perPage }); } }; const handleOrderByChange = (e: React.ChangeEvent<HTMLSelectElement>) => { setOrderBy(e.target.value as 'relevancia' | 'fecha'); }; return ( <div className="max-w-screen-xl mx-auto px-4 sm:px-6 py-8 animate-fadeIn"> <div className="mb-8 bg-white rounded-lg shadow-lg p-6 border border-gray-200"> <h1 className="text-3xl font-bold text-blue-800 mb-3">Bsqueda General</h1> <p className="text-gray-600 mb-6"> Busca en toda la biblioteca por palabras clave, frases o autores. Los resultados se ordenan por relevancia o fecha. </p> <div className="mt-6 space-y-4"> <SearchBar value={searchTerm} onChange={setSearchTerm} onSearch={handleSearch} filters={AVAILABLE_FILTERS} selectedFilters={selectedFilters} onFilterToggle={handleFilterToggle} /> <div className="flex flex-wrap justify-between items-center pt-2"> <div className="flex items-center space-x-4"> <div className="flex items-center space-x-2"> <span className="text-sm text-gray-600">Cantidad por pgina:</span> <select value={resultsPerPage} onChange={(e) => handleResultsPerPageChange(Number(e.target.value))} className="text-sm border-gray-300 rounded-md shadow-sm focus:border-blue-500 focus:ring-blue-500" > <option value={10}>10</option> <option value={20}>20</option> <option value={50}>50</option> <option value={100}>100</option> <option value={200}>200</option> <option value={500}>500</option> </select> </div> <div className="flex items-center space-x-2"> <span className="text-sm text-gray-600">Ordenar por:</span> <select value={orderBy} onChange={handleOrderByChange} className="text-sm border-gray-300 rounded-md shadow-sm focus:border-blue-500 focus:ring-blue-500" > <option value="relevancia">Relevancia</option> <option value="fecha">Fecha</option> </select> </div> <div className="flex items-center space-x-2"> <span className="text-sm text-gray-600">Autor/es:</span> <div style={{ minWidth: 200, maxWidth: 350 }}> <Select isMulti options={autores} value={selectedAutores} onChange={(vals) => setSelectedAutores(vals as { label: string; value: string }[])} placeholder="Selecciona autor/es..." isClearable closeMenuOnSelect={false} noOptionsMessage={() => "No hay ms autores"} filterOption={(option, inputValue) => option.label.toLowerCase().includes(inputValue.toLowerCase()) } /> </div> </div> </div> </div> </div> </div> {isLoading && <Skeleton count={resultsPerPage > 5 ? 5 : resultsPerPage} />} {isError && ( <div className="bg-red-50 border-l-4 border-red-500 p-4 my-4 rounded-r-lg shadow animate-fadeIn"> <div className="flex"> <div className="flex-shrink-0"> <svg className="h-5 w-5 text-red-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor"> <path fillRule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zM8.707 7.293a1 1 0 00-1.414 1.414L8.586 10l-1.293 1.293a1 1 0 101.414 1.414L10 11.414l1.293 1.293a1 1 0 001.414-1.414L11.414 10l1.293-1.293a1 1 0 00-1.414-1.414L10 8.586 8.707 7.293z" clipRule="evenodd" /> </svg> </div> <div className="ml-3"> <p className="text-sm text-red-700">Error al realizar la bsqueda: {error instanceof Error ? error.message : 'Desconocido'}</p> </div> </div> </div> )} {data && data.resultados && !isLoading && ( <div className="animate-fadeIn"> {data.paginacion && data.resultados.length > 0 && ( <div className="mb-6 text-gray-600 bg-gray-50 p-3 rounded-lg border border-gray-100 shadow-sm"> <span className="font-medium"> {(data.paginacion.pagina_actual - 1) * data.paginacion.resultados_por_pagina + 1}- {Math.min( data.paginacion.pagina_actual * data.paginacion.resultados_por_pagina, data.paginacion.total_resultados )} </span> de {data.paginacion.total_resultados} resultados para "<span className="italic">{data.consulta}</span>" </div> )} {data.resultados.length > 0 ? ( <div className="space-y-6"> {data.resultados.map((item: any) => ( <div key={item.id} className="result-item"> <ResultCard item={item} searchTerm={searchTerm} /> </div> ))} {data.paginacion && ( <Pagination pagination={data.paginacion} onPageChange={handlePageChange} /> )} </div> ) : ( <div className="text-center p-10 bg-white border rounded-lg shadow animate-fadeIn"> <svg xmlns="http://www.w3.org/2000/svg" className="h-16 w-16 mx-auto text-gray-400 mb-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"> <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M9.172 16.172a4 4 0 015.656 0M9 10h.01M15 10h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" /> </svg> <h3 className="text-lg font-medium text-gray-900">No se encontraron resultados</h3> <p className="mt-2 text-gray-500">Prueba con otra consulta o diferentes trminos.</p> <button onClick={() => setSearchTerm('')} className="mt-4 btn btn-primary">Nueva bsqueda</button> </div> )} </div> )} </div> ); }; export default Search;
---

# SemanticSearch

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** pages

# SemanticSearch

import { useState, useEffect } from 'react'; import { useQuery } from '@tanstack/react-query'; import { semanticSearch, SemanticSearchParams, apiClient } from '../services/api'; import SearchBar, { SearchFilter } from '../components/SearchBar'; import ResultCard from '../components/ResultCard'; import Pagination from '../components/Pagination'; import Skeleton from '../components/Skeleton'; import Select from 'react-select'; // Filtros disponibles para la bsqueda const AVAILABLE_FILTERS: SearchFilter[] = [ { id: 'recientes', label: 'Ms recientes' }, { id: 'relevantes', label: 'Ms relevantes' }, { id: 'notas', label: 'Notas' }, { id: 'articulos', label: 'Artculos' }, { id: 'redes', label: 'Redes sociales' } ]; /* Aadir estos estilos para el toggle switch */ const toggleStyles = ` .toggle-checkbox { right: 0; border-color: #ccc; transition: all 0.3s; } .toggle-checkbox:checked { right: 100%; transform: translateX(100%); border-color: #3b82f6; } .toggle-label { transition: background-color 0.3s ease; } `; const SemanticSearch = () => { const [searchTerm, setSearchTerm] = useState(''); const [queryParams, setQueryParams] = useState<SemanticSearchParams | null>(null); const [resultsPerPage, setResultsPerPage] = useState(10); const [selectedFilters, setSelectedFilters] = useState<string[]>([]); const [maxResults, setMaxResults] = useState(1000); const [minSimilitud, setMinSimilitud] = useState(0.2); const [orderBy, setOrderBy] = useState<'similitud' | 'fecha'>('similitud'); const [autores, setAutores] = useState<{ label: string; value: string }[]>([]); const [selectedAutores, setSelectedAutores] = useState<{ label: string; value: string }[]>([]); const [usarMeilisearch, setUsarMeilisearch] = useState(true); // Consulta a la API const { data, isLoading, isError, error } = useQuery({ queryKey: ['semanticSearch', queryParams, selectedFilters], queryFn: () => queryParams ? semanticSearch(queryParams) : Promise.resolve(null), enabled: !!queryParams, }); // Cargar autores al montar useEffect(() => { apiClient.get<{ autores: string[] }>('/autores') .then(response => { const data = response.data; if (data && data.autores) { const autoresMapeados = data.autores.map((a: string) => ({ label: a, value: a })); setAutores(autoresMapeados); console.log('Autores en estado:', autoresMapeados); } else { console.error('La respuesta de /api/autores no tiene el formato esperado:', data); } }) .catch(err => { console.error('Error al hacer fetch a /api/autores:', err); }); }, []); // Manejar filtros const handleFilterToggle = (filterId: string) => { setSelectedFilters(prev => prev.includes(filterId) ? prev.filter(id => id !== filterId) : [...prev, filterId] ); if (queryParams) { handleSearch(); } }; // Ejecutar bsqueda const handleSearch = () => { if (searchTerm.trim() === '') return; setQueryParams({ contenido_texto: searchTerm, pagina: 1, por_pagina: resultsPerPage, filtros: selectedFilters.length > 0 ? selectedFilters.join(',') : undefined, limite: maxResults, similitud_min: minSimilitud, ordenar_por: orderBy, autor: selectedAutores.length > 0 ? selectedAutores.map(a => a.value).join(',') : undefined, usar_meilisearch: usarMeilisearch, }); }; // Cambiar de pgina const handlePageChange = (page: number) => { if (!queryParams) return; setQueryParams({ ...queryParams, pagina: page }); // Scroll hacia arriba window.scrollTo({ top: 0, behavior: 'smooth' }); }; // Cambiar resultados por pgina const handleResultsPerPageChange = (perPage: number) => { setResultsPerPage(perPage); if (queryParams) { setQueryParams({ ...queryParams, pagina: 1, por_pagina: perPage }); } }; // Nuevo: Cambiar similitud mnima const handleMinSimilitudChange = (e: React.ChangeEvent<HTMLInputElement>) => { const value = Math.max(0, Math.min(1, Number(e.target.value))); setMinSimilitud(value); }; // Nuevo: Cambiar orden const handleOrderByChange = (e: React.ChangeEvent<HTMLSelectElement>) => { setOrderBy(e.target.value as 'similitud' | 'fecha'); }; // Nuevo: Cambiar modo de bsqueda const toggleSearchMode = () => { setUsarMeilisearch(!usarMeilisearch); }; return ( <div className="max-w-screen-xl mx-auto px-4 sm:px-6 py-8 animate-fadeIn"> {/* Estilos CSS para el toggle switch */} <style>{toggleStyles}</style> <div className="mb-8 bg-white rounded-lg shadow-lg p-6 border border-gray-200"> <h1 className="text-3xl font-bold text-blue-800 mb-3">Bsqueda Semntica</h1> <p className="text-gray-600 mb-6"> Encuentra contenido conceptualmente relacionado con tu consulta, incluso si no contiene las mismas palabras exactas. </p> <div className="mt-6 space-y-4"> <SearchBar value={searchTerm} onChange={setSearchTerm} onSearch={handleSearch} filters={AVAILABLE_FILTERS} selectedFilters={selectedFilters} onFilterToggle={handleFilterToggle} /> <div className="flex flex-wrap justify-between items-center pt-2"> <div className="text-sm text-gray-600"> {data?.paginacion && ( <span> Total: <span className="font-medium">{data.paginacion.total_resultados}</span> resultados </span> )} </div> <div className="flex items-center space-x-4"> {/* Agregar toggle para usar Meilisearch */} <div className="flex items-center space-x-2"> <span className="text-sm text-gray-600">Modo:</span> <div className="relative inline-block w-12 mr-2 align-middle select-none"> <input type="checkbox" checked={usarMeilisearch} onChange={toggleSearchMode} id="meilisearch-toggle" className="toggle-checkbox absolute block w-6 h-6 rounded-full bg-white border-4 appearance-none cursor-pointer" /> <label htmlFor="meilisearch-toggle" className={`toggle-label block overflow-hidden h-6 rounded-full cursor-pointer ${usarMeilisearch ? 'bg-blue-500' : 'bg-gray-300'}`} ></label> </div> <span className="text-sm font-medium text-gray-700">{usarMeilisearch ? 'Meilisearch' : 'Clsico'}</span> </div> <div className="flex items-center space-x-4"> <div className="flex items-center space-x-2"> <span className="text-sm text-gray-600">Cantidad por pgina:</span> <select value={resultsPerPage} onChange={(e) => handleResultsPerPageChange(Number(e.target.value))} className="text-sm border-gray-300 rounded-md shadow-sm focus:border-blue-500 focus:ring-blue-500" > <option value={10}>10</option> <option value={20}>20</option> <option value={50}>50</option> <option value={100}>100</option> <option value={200}>200</option> <option value={500}>500</option> <option value={1000}>1,000</option> <option value={5000}>5,000</option> <option value={10000}>10,000</option> <option value={50000}>50,000</option> <option value={100000}>100,000</option> <option value={1000000}>1,000,000</option> </select> </div> <div className="flex items-center space-x-2"> <span className="text-sm text-gray-600">Mximo total:</span> <input type="number" min={1} max={1000000} value={maxResults} onChange={(e) => setMaxResults(Number(e.target.value))} className="w-24 px-2 py-1 border border-gray-300 rounded-md text-sm" /> </div> <div className="flex items-center space-x-2"> {/* Tooltip de informacin mejorado */} <div className="relative group flex items-center"> {/* cono moderno con fondo */} <span className="inline-flex items-center justify-center w-7 h-7 rounded-full bg-blue-100 text-blue-600 shadow-md cursor-pointer mr-1 border border-blue-200 hover:bg-blue-200 transition"> <svg className="w-4 h-4" fill="none" stroke="currentColor" strokeWidth="2" viewBox="0 0 24 24"> <circle cx="12" cy="12" r="10" stroke="#2563eb" strokeWidth="2" fill="#e0edff" /> <text x="12" y="16" textAnchor="middle" fontSize="12" fill="#2563eb" fontFamily="Arial">i</text> </svg> </span> {/* Tooltip con animacin, flecha y mejor UI */} <div className="absolute left-1/2 z-30 hidden group-hover:flex flex-col items-center w-80 -translate-x-1/2 mt-4 animate-fadeIn"> <div className="px-5 py-4 bg-white text-gray-900 text-sm rounded-xl shadow-2xl border border-blue-200 font-medium leading-relaxed transition-opacity duration-200 backdrop-blur-md"> <span className="font-semibold text-blue-700 block mb-1">Qu es la similitud mnima?</span> La <b>similitud mnima</b> es un valor entre <b>0</b> y <b>1</b>.<br /> Resultados ms cercanos a <b>1</b> son ms parecidos a tu consulta.<br /> Por ejemplo, <b>0.8</b> mostrar solo resultados muy similares, mientras que <b>0.2</b> incluir resultados ms lejanos. </div> {/* Flecha decorativa mejorada */} <div className="w-4 h-4 bg-white border-l border-t border-blue-200 rotate-45 -mt-2 shadow-xl"></div> </div> </div> <span className="text-sm text-gray-600">Similitud mnima:</span> <input type="number" min={0} max={1} step={0.01} value={minSimilitud} onChange={handleMinSimilitudChange} className="w-20 px-2 py-1 border border-gray-300 rounded-md text-sm focus:ring-2 focus:ring-blue-500 focus:border-blue-500" /> </div> <div className="flex items-center space-x-2"> <span className="text-sm text-gray-600">Ordenar por:</span> <select value={orderBy} onChange={handleOrderByChange} className="text-sm border-gray-300 rounded-md shadow-sm focus:border-blue-500 focus:ring-blue-500" > <option value="similitud">Similitud</option> <option value="fecha">Fecha</option> </select> </div> <div className="flex items-center space-x-2"> <span className="text-sm text-gray-600">Autor/es:</span> <div style={{ minWidth: 200, maxWidth: 350 }}> <Select isMulti options={autores} value={selectedAutores} onChange={(vals: readonly { label: string; value: string }[] | null) => setSelectedAutores(vals as { label: string; value: string }[])} placeholder="Selecciona autor/es..." isClearable closeMenuOnSelect={false} noOptionsMessage={() => "No hay ms autores"} filterOption={(option, inputValue) => option.label.toLowerCase().includes(inputValue.toLowerCase()) } /> </div> </div> </div> </div> </div> </div> </div> {/* Estado de carga */} {isLoading && <Skeleton count={resultsPerPage > 5 ? 5 : resultsPerPage} />} {/* Error */} {isError && ( <div className="bg-red-50 border-l-4 border-red-500 p-4 my-4 rounded-r-lg shadow animate-fadeIn"> <div className="flex"> <div className="flex-shrink-0"> <svg className="h-5 w-5 text-red-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor"> <path fillRule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zM8.707 7.293a1 1 0 00-1.414 1.414L8.586 10l-1.293 1.293a1 1 0 101.414 1.414L10 11.414l1.293 1.293a1 1 0 001.414-1.414L11.414 10l1.293-1.293a1 1 0 00-1.414-1.414L10 8.586 8.707 7.293z" clipRule="evenodd" /> </svg> </div> <div className="ml-3"> <p className="text-sm text-red-700">Error al realizar la bsqueda: {error instanceof Error ? error.message : 'Desconocido'}</p> </div> </div> </div> )} {/* Resultados */} {data && data.resultados && !isLoading && ( <div className="animate-fadeIn"> {/* Info de resultados */} {data.paginacion && data.resultados.length > 0 && ( <div className="mb-6 text-gray-600 bg-gray-50 p-3 rounded-lg border border-gray-100 shadow-sm"> <span className="font-medium"> {(data.paginacion.pagina_actual - 1) * data.paginacion.resultados_por_pagina + 1}- {Math.min( data.paginacion.pagina_actual * data.paginacion.resultados_por_pagina, data.paginacion.total_resultados )} </span> de {data.paginacion.total_resultados} resultados para "<span className="italic">{data.consulta}</span>" </div> )} {/* Lista de resultados */} {data.resultados.length > 0 ? ( <div className="space-y-6"> {data.resultados.map((item) => ( <div key={item.id} className="result-item"> <ResultCard item={item} searchTerm={searchTerm} /> </div> ))} {data.paginacion && ( <Pagination pagination={data.paginacion} onPageChange={handlePageChange} /> )} </div> ) : ( <div className="text-center p-10 bg-white border rounded-lg shadow animate-fadeIn"> <svg xmlns="http://www.w3.org/2000/svg" className="h-16 w-16 mx-auto text-gray-400 mb-3" fill="none" viewBox="0 0 24 24" stroke="currentColor"> <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M9.172 16.172a4 4 0 015.656 0M9 10h.01M15 10h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" /> </svg> <h3 className="text-lg font-medium text-gray-900">No se encontraron resultados</h3> <p className="mt-2 text-gray-500">Prueba con otra consulta o diferentes trminos.</p> <button onClick={() => setSearchTerm('')} className="mt-4 btn btn-primary" > Nueva bsqueda </button> </div> )} </div> )} </div> ); }; export default SemanticSearch;
---

# Generate

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** pages

# Generate

import { useState } from 'react'; import { useQuery, useMutation } from '@tanstack/react-query'; import { RagParams, generateWithRag, getRagPrompt } from '../services/api'; const GeneratePage = () => { const [formData, setFormData] = useState<RagParams>({ tema: '', tipo: 'post', estilo: '', num_resultados: 5, proveedor: 'gemini' }); const [showPrompt, setShowPrompt] = useState(false); // Consulta para obtener solo el prompt const promptQuery = useQuery({ queryKey: ['ragPrompt', formData], queryFn: () => getRagPrompt(formData), enabled: showPrompt && !!formData.tema, }); // Mutacin para la generacin de contenido const generateMutation = useMutation({ mutationFn: generateWithRag, onError: (error) => { console.error('Error al generar contenido:', error); } }); // Manejar cambios en el formulario const handleChange = (e: React.ChangeEvent<HTMLInputElement | HTMLSelectElement | HTMLTextAreaElement>) => { const { name, value } = e.target; setFormData(prev => ({ ...prev, [name]: name === 'num_resultados' ? parseInt(value) : value })); }; // Manejar envo del formulario const handleSubmit = (e: React.FormEvent) => { e.preventDefault(); if (!formData.tema) return; if (showPrompt) { promptQuery.refetch(); } else { generateMutation.mutate(formData); } }; return ( <div className="max-w-screen-xl mx-auto px-4 sm:px-6 py-8 animate-fadeIn"> <div className="bg-white rounded-lg shadow-lg p-6 border border-gray-200 mb-8"> <h1 className="text-3xl font-bold text-blue-800 mb-3">Generacin asistida por biblioteca</h1> <p className="text-gray-600 mb-6"> Genera contenido basado en tu biblioteca personal utilizando tecnologa RAG (Retrieval-Augmented Generation). </p> <form onSubmit={handleSubmit} className="space-y-6"> <div> <label htmlFor="tema" className="block text-sm font-medium text-gray-700 mb-1"> Tema<span className="text-red-500">*</span> </label> <input id="tema" name="tema" type="text" value={formData.tema} onChange={handleChange} placeholder="Escribe el tema sobre el que quieres generar contenido..." className="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-blue-500 focus:border-blue-500" required /> </div> <div className="grid grid-cols-1 md:grid-cols-3 gap-4"> <div> <label htmlFor="tipo" className="block text-sm font-medium text-gray-700 mb-1"> Tipo de contenido </label> <select id="tipo" name="tipo" value={formData.tipo} onChange={handleChange} className="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-blue-500 focus:border-blue-500" > <option value="post">Post para redes</option> <option value="articulo">Artculo</option> <option value="guion">Guion</option> <option value="resumen">Resumen</option> <option value="analisis">Anlisis</option> </select> </div> <div> <label htmlFor="estilo" className="block text-sm font-medium text-gray-700 mb-1"> Estilo (opcional) </label> <input id="estilo" name="estilo" type="text" value={formData.estilo} onChange={handleChange} placeholder="Formal, conversacional, acadmico..." className="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-blue-500 focus:border-blue-500" /> </div> <div> <label htmlFor="proveedor" className="block text-sm font-medium text-gray-700 mb-1"> Modelo IA </label> <select id="proveedor" name="proveedor" value={formData.proveedor} onChange={handleChange} className="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-blue-500 focus:border-blue-500" > <option value="gemini">Google Gemini</option> <option value="openai">OpenAI GPT</option> </select> </div> </div> <div> <label htmlFor="num_resultados" className="block text-sm font-medium text-gray-700 mb-1"> Fragmentos a recuperar: {formData.num_resultados} </label> <input id="num_resultados" name="num_resultados" type="range" min="1" max="10" value={formData.num_resultados} onChange={handleChange} className="w-full h-2 bg-gray-200 rounded-lg appearance-none cursor-pointer" /> <div className="flex justify-between text-xs text-gray-500"> <span>Pocos (mayor foco)</span> <span>Muchos (mayor contexto)</span> </div> </div> <div className="flex items-center"> <input id="showPrompt" type="checkbox" checked={showPrompt} onChange={() => setShowPrompt(!showPrompt)} className="h-4 w-4 text-blue-600 focus:ring-blue-500 border-gray-300 rounded" /> <label htmlFor="showPrompt" className="ml-2 block text-sm text-gray-700"> Solo mostrar prompt (no generar contenido) </label> </div> <div className="flex justify-end space-x-3"> <button type="button" onClick={() => setFormData({ tema: '', tipo: 'post', estilo: '', num_resultados: 5, proveedor: 'gemini' })} className="btn btn-secondary" > Limpiar </button> <button type="submit" className="btn btn-primary" disabled={promptQuery.isFetching || generateMutation.isPending || !formData.tema} > {promptQuery.isFetching || generateMutation.isPending ? 'Procesando...' : showPrompt ? 'Mostrar Prompt' : 'Generar'} </button> </div> </form> </div> {/* Mostrar el prompt */} {promptQuery.isSuccess && ( <div className="bg-white rounded-lg shadow-lg p-6 border border-gray-200 mb-8"> <h2 className="text-xl font-semibold text-gray-800 mb-3">Prompt generado</h2> <p className="text-sm text-gray-600 mb-2"> Se encontraron {promptQuery.data.fragmentos_recuperados} fragmentos relevantes para el tema. </p> <div className="bg-gray-50 p-4 rounded-lg border border-gray-200 whitespace-pre-wrap font-mono text-sm"> {promptQuery.data.prompt} </div> </div> )} {/* Mostrar resultado de la generacin */} {generateMutation.isSuccess && ( <div className="bg-white rounded-lg shadow-lg p-6 border border-gray-200"> <div className="flex justify-between items-start mb-6"> <h2 className="text-xl font-semibold text-gray-800">Contenido generado</h2> <span className="bg-green-100 text-green-800 text-xs font-medium px-2.5 py-0.5 rounded"> {generateMutation.data.proveedor.toUpperCase()} </span> </div> <div className="mb-4"> <span className="text-sm text-gray-500"> Se utilizaron {generateMutation.data.fragmentos_utilizados} fragmentos relevantes de la biblioteca. </span> </div> <div className="prose prose-blue max-w-none"> <div className="bg-gray-50 p-6 rounded-lg border border-gray-200 whitespace-pre-wrap"> {generateMutation.data.contenido} </div> </div> <div className="mt-6 flex justify-end space-x-3"> <button onClick={() => navigator.clipboard.writeText(generateMutation.data.contenido)} className="btn btn-secondary" > Copiar al portapapeles </button> <button onClick={() => window.print()} className="btn btn-primary" > Imprimir / Guardar PDF </button> </div> </div> )} {/* Mostrar errores */} {(promptQuery.isError || generateMutation.isError) && ( <div className="bg-red-50 border-l-4 border-red-500 p-4 rounded-r-lg shadow"> <div className="flex"> <div className="flex-shrink-0"> <svg className="h-5 w-5 text-red-500" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor"> <path fillRule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zM8.707 7.293a1 1 0 00-1.414 1.414L8.586 10l-1.293 1.293a1 1 0 101.414 1.414L10 11.414l1.293 1.293a1 1 0 001.414-1.414L11.414 10l1.293-1.293a1 1 0 00-1.414-1.414L10 8.586 8.707 7.293z" clipRule="evenodd" /> </svg> </div> <div className="ml-3"> <p className="text-sm text-red-700"> {promptQuery.error instanceof Error ? promptQuery.error.message : generateMutation.error instanceof Error ? generateMutation.error.message : 'Ocurri un error desconocido'} </p> </div> </div> </div> )} </div> ); }; export default GeneratePage;
---

# react

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** assets

# react

<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="iconify iconify--logos" width="35.93" height="32" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 228"><path fill="#00D8FF" d="M210.483 73.824a171.49 171.49 0 0 0-8.24-2.597c.465-1.9.893-3.777 1.273-5.621c6.238-30.281 2.16-54.676-11.769-62.708c-13.355-7.7-35.196.329-57.254 19.526a171.23 171.23 0 0 0-6.375 5.848a155.866 155.866 0 0 0-4.241-3.917C100.759 3.829 77.587-4.822 63.673 3.233C50.33 10.957 46.379 33.89 51.995 62.588a170.974 170.974 0 0 0 1.892 8.48c-3.28.932-6.445 1.924-9.474 2.98C17.309 83.498 0 98.307 0 113.668c0 15.865 18.582 31.778 46.812 41.427a145.52 145.52 0 0 0 6.921 2.165a167.467 167.467 0 0 0-2.01 9.138c-5.354 28.2-1.173 50.591 12.134 58.266c13.744 7.926 36.812-.22 59.273-19.855a145.567 145.567 0 0 0 5.342-4.923a168.064 168.064 0 0 0 6.92 6.314c21.758 18.722 43.246 26.282 56.54 18.586c13.731-7.949 18.194-32.003 12.4-61.268a145.016 145.016 0 0 0-1.535-6.842c1.62-.48 3.21-.974 4.76-1.488c29.348-9.723 48.443-25.443 48.443-41.52c0-15.417-17.868-30.326-45.517-39.844Zm-6.365 70.984c-1.4.463-2.836.91-4.3 1.345c-3.24-10.257-7.612-21.163-12.963-32.432c5.106-11 9.31-21.767 12.459-31.957c2.619.758 5.16 1.557 7.61 2.4c23.69 8.156 38.14 20.213 38.14 29.504c0 9.896-15.606 22.743-40.946 31.14Zm-10.514 20.834c2.562 12.94 2.927 24.64 1.23 33.787c-1.524 8.219-4.59 13.698-8.382 15.893c-8.067 4.67-25.32-1.4-43.927-17.412a156.726 156.726 0 0 1-6.437-5.87c7.214-7.889 14.423-17.06 21.459-27.246c12.376-1.098 24.068-2.894 34.671-5.345a134.17 134.17 0 0 1 1.386 6.193ZM87.276 214.515c-7.882 2.783-14.16 2.863-17.955.675c-8.075-4.657-11.432-22.636-6.853-46.752a156.923 156.923 0 0 1 1.869-8.499c10.486 2.32 22.093 3.988 34.498 4.994c7.084 9.967 14.501 19.128 21.976 27.15a134.668 134.668 0 0 1-4.877 4.492c-9.933 8.682-19.886 14.842-28.658 17.94ZM50.35 144.747c-12.483-4.267-22.792-9.812-29.858-15.863c-6.35-5.437-9.555-10.836-9.555-15.216c0-9.322 13.897-21.212 37.076-29.293c2.813-.98 5.757-1.905 8.812-2.773c3.204 10.42 7.406 21.315 12.477 32.332c-5.137 11.18-9.399 22.249-12.634 32.792a134.718 134.718 0 0 1-6.318-1.979Zm12.378-84.26c-4.811-24.587-1.616-43.134 6.425-47.789c8.564-4.958 27.502 2.111 47.463 19.835a144.318 144.318 0 0 1 3.841 3.545c-7.438 7.987-14.787 17.08-21.808 26.988c-12.04 1.116-23.565 2.908-34.161 5.309a160.342 160.342 0 0 1-1.76-7.887Zm110.427 27.268a347.8 347.8 0 0 0-7.785-12.803c8.168 1.033 15.994 2.404 23.343 4.08c-2.206 7.072-4.956 14.465-8.193 22.045a381.151 381.151 0 0 0-7.365-13.322Zm-45.032-43.861c5.044 5.465 10.096 11.566 15.065 18.186a322.04 322.04 0 0 0-30.257-.006c4.974-6.559 10.069-12.652 15.192-18.18ZM82.802 87.83a323.167 323.167 0 0 0-7.227 13.238c-3.184-7.553-5.909-14.98-8.134-22.152c7.304-1.634 15.093-2.97 23.209-3.984a321.524 321.524 0 0 0-7.848 12.897Zm8.081 65.352c-8.385-.936-16.291-2.203-23.593-3.793c2.26-7.3 5.045-14.885 8.298-22.6a321.187 321.187 0 0 0 7.257 13.246c2.594 4.48 5.28 8.868 8.038 13.147Zm37.542 31.03c-5.184-5.592-10.354-11.779-15.403-18.433c4.902.192 9.899.29 14.978.29c5.218 0 10.376-.117 15.453-.343c-4.985 6.774-10.018 12.97-15.028 18.486Zm52.198-57.817c3.422 7.8 6.306 15.345 8.596 22.52c-7.422 1.694-15.436 3.058-23.88 4.071a382.417 382.417 0 0 0 7.859-13.026a347.403 347.403 0 0 0 7.425-13.565Zm-16.898 8.101a358.557 358.557 0 0 1-12.281 19.815a329.4 329.4 0 0 1-23.444.823c-7.967 0-15.716-.248-23.178-.732a310.202 310.202 0 0 1-12.513-19.846h.001a307.41 307.41 0 0 1-10.923-20.627a310.278 310.278 0 0 1 10.89-20.637l-.001.001a307.318 307.318 0 0 1 12.413-19.761c7.613-.576 15.42-.876 23.31-.876H128c7.926 0 15.743.303 23.354.883a329.357 329.357 0 0 1 12.335 19.695a358.489 358.489 0 0 1 11.036 20.54a329.472 329.472 0 0 1-11 20.722Zm22.56-122.124c8.572 4.944 11.906 24.881 6.52 51.026c-.344 1.668-.73 3.367-1.15 5.09c-10.622-2.452-22.155-4.275-34.23-5.408c-7.034-10.017-14.323-19.124-21.64-27.008a160.789 160.789 0 0 1 5.888-5.4c18.9-16.447 36.564-22.941 44.612-18.3ZM128 90.808c12.625 0 22.86 10.235 22.86 22.86s-10.235 22.86-22.86 22.86s-22.86-10.235-22.86-22.86s10.235-22.86 22.86-22.86Z"></path></svg>
---

# self improve

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** rules

# self improve

--- description: Guidelines for continuously improving Roo Code rules based on emerging code patterns and best practices. globs: **/* alwaysApply: true --- - **Rule Improvement Triggers:** - New code patterns not covered by existing rules - Repeated similar implementations across files - Common error patterns that could be prevented - New libraries or tools being used consistently - Emerging best practices in the codebase - **Analysis Process:** - Compare new code with existing rules - Identify patterns that should be standardized - Look for references to external documentation - Check for consistent error handling patterns - Monitor test patterns and coverage - **Rule Updates:** - **Add New Rules When:** - A new technology/pattern is used in 3+ files - Common bugs could be prevented by a rule - Code reviews repeatedly mention the same feedback - New security or performance patterns emerge - **Modify Existing Rules When:** - Better examples exist in the codebase - Additional edge cases are discovered - Related rules have been updated - Implementation details have changed - **Example Pattern Recognition:** ```typescript // If you see repeated patterns like: const data = await prisma.user.findMany({ select: { id: true, email: true }, where: { status: 'ACTIVE' } }); // Consider adding to [prisma.md](mdc:.roo/rules/prisma.md): // - Standard select fields // - Common where conditions // - Performance optimization patterns ``` - **Rule Quality Checks:** - Rules should be actionable and specific - Examples should come from actual code - References should be up to date - Patterns should be consistently enforced - **Continuous Improvement:** - Monitor code review comments - Track common development questions - Update rules after major refactors - Add links to relevant documentation - Cross-reference related rules - **Rule Deprecation:** - Mark outdated patterns as deprecated - Remove rules that no longer apply - Update references to deprecated rules - Document migration paths for old patterns - **Documentation Updates:** - Keep examples synchronized with code - Update references to external docs - Maintain links between related rules - Document breaking changes Follow [cursor_rules.md](mdc:.roo/rules/cursor_rules.md) for proper rule formatting and structure.
---

# roo rules

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** rules

# roo rules

--- description: Guidelines for creating and maintaining Roo Code rules to ensure consistency and effectiveness. globs: .roo/rules/*.md alwaysApply: true --- - **Required Rule Structure:** ```markdown --- description: Clear, one-line description of what the rule enforces globs: path/to/files/*.ext, other/path/**/* alwaysApply: boolean --- - **Main Points in Bold** - Sub-points with details - Examples and explanations ``` - **File References:** - Use `[filename](mdc:path/to/file)` ([filename](mdc:filename)) to reference files - Example: [prisma.md](mdc:.roo/rules/prisma.md) for rule references - Example: [schema.prisma](mdc:prisma/schema.prisma) for code references - **Code Examples:** - Use language-specific code blocks ```typescript //  DO: Show good examples const goodExample = true; //  DON'T: Show anti-patterns const badExample = false; ``` - **Rule Content Guidelines:** - Start with high-level overview - Include specific, actionable requirements - Show examples of correct implementation - Reference existing code when possible - Keep rules DRY by referencing other rules - **Rule Maintenance:** - Update rules when new patterns emerge - Add examples from actual codebase - Remove outdated patterns - Cross-reference related rules - **Best Practices:** - Use bullet points for clarity - Keep descriptions concise - Include both DO and DON'T examples - Reference actual code over theoretical examples - Use consistent formatting across rules
---

# dev workflow

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** rules

# dev workflow

--- description: Guide for using Task Master to manage task-driven development workflows globs: **/* alwaysApply: true --- # Task Master Development Workflow This guide outlines the typical process for using Task Master to manage software development projects. ## Primary Interaction: MCP Server vs. CLI Task Master offers two primary ways to interact: 1. **MCP Server (Recommended for Integrated Tools)**: - For AI agents and integrated development environments (like Roo Code), interacting via the **MCP server is the preferred method**. - The MCP server exposes Task Master functionality through a set of tools (e.g., `get_tasks`, `add_subtask`). - This method offers better performance, structured data exchange, and richer error handling compared to CLI parsing. - Refer to [`mcp.md`](mdc:.roo/rules/mcp.md) for details on the MCP architecture and available tools. - A comprehensive list and description of MCP tools and their corresponding CLI commands can be found in [`taskmaster.md`](mdc:.roo/rules/taskmaster.md). - **Restart the MCP server** if core logic in `scripts/modules` or MCP tool/direct function definitions change. 2. **`task-master` CLI (For Users & Fallback)**: - The global `task-master` command provides a user-friendly interface for direct terminal interaction. - It can also serve as a fallback if the MCP server is inaccessible or a specific function isn't exposed via MCP. - Install globally with `npm install -g task-master-ai` or use locally via `npx task-master-ai ...`. - The CLI commands often mirror the MCP tools (e.g., `task-master list` corresponds to `get_tasks`). - Refer to [`taskmaster.md`](mdc:.roo/rules/taskmaster.md) for a detailed command reference. ## Standard Development Workflow Process - Start new projects by running `initialize_project` tool / `task-master init` or `parse_prd` / `task-master parse-prd --input='<prd-file.txt>'` (see [`taskmaster.md`](mdc:.roo/rules/taskmaster.md)) to generate initial tasks.json - Begin coding sessions with `get_tasks` / `task-master list` (see [`taskmaster.md`](mdc:.roo/rules/taskmaster.md)) to see current tasks, status, and IDs - Determine the next task to work on using `next_task` / `task-master next` (see [`taskmaster.md`](mdc:.roo/rules/taskmaster.md)). - Analyze task complexity with `analyze_project_complexity` / `task-master analyze-complexity --research` (see [`taskmaster.md`](mdc:.roo/rules/taskmaster.md)) before breaking down tasks - Review complexity report using `complexity_report` / `task-master complexity-report` (see [`taskmaster.md`](mdc:.roo/rules/taskmaster.md)). - Select tasks based on dependencies (all marked 'done'), priority level, and ID order - Clarify tasks by checking task files in tasks/ directory or asking for user input - View specific task details using `get_task` / `task-master show <id>` (see [`taskmaster.md`](mdc:.roo/rules/taskmaster.md)) to understand implementation requirements - Break down complex tasks using `expand_task` / `task-master expand --id=<id> --force --research` (see [`taskmaster.md`](mdc:.roo/rules/taskmaster.md)) with appropriate flags like `--force` (to replace existing subtasks) and `--research`. - Clear existing subtasks if needed using `clear_subtasks` / `task-master clear-subtasks --id=<id>` (see [`taskmaster.md`](mdc:.roo/rules/taskmaster.md)) before regenerating - Implement code following task details, dependencies, and project standards - Verify tasks according to test strategies before marking as complete (See [`tests.md`](mdc:.roo/rules/tests.md)) - Mark completed tasks with `set_task_status` / `task-master set-status --id=<id> --status=done` (see [`taskmaster.md`](mdc:.roo/rules/taskmaster.md)) - Update dependent tasks when implementation differs from original plan using `update` / `task-master update --from=<id> --prompt="..."` or `update_task` / `task-master update-task --id=<id> --prompt="..."` (see [`taskmaster.md`](mdc:.roo/rules/taskmaster.md)) - Add new tasks discovered during implementation using `add_task` / `task-master add-task --prompt="..." --research` (see [`taskmaster.md`](mdc:.roo/rules/taskmaster.md)). - Add new subtasks as needed using `add_subtask` / `task-master add-subtask --parent=<id> --title="..."` (see [`taskmaster.md`](mdc:.roo/rules/taskmaster.md)). - Append notes or details to subtasks using `update_subtask` / `task-master update-subtask --id=<subtaskId> --prompt='Add implementation notes here...\nMore details...'` (see [`taskmaster.md`](mdc:.roo/rules/taskmaster.md)). - Generate task files with `generate` / `task-master generate` (see [`taskmaster.md`](mdc:.roo/rules/taskmaster.md)) after updating tasks.json - Maintain valid dependency structure with `add_dependency`/`remove_dependency` tools or `task-master add-dependency`/`remove-dependency` commands, `validate_dependencies` / `task-master validate-dependencies`, and `fix_dependencies` / `task-master fix-dependencies` (see [`taskmaster.md`](mdc:.roo/rules/taskmaster.md)) when needed - Respect dependency chains and task priorities when selecting work - Report progress regularly using `get_tasks` / `task-master list` ## Task Complexity Analysis - Run `analyze_project_complexity` / `task-master analyze-complexity --research` (see [`taskmaster.md`](mdc:.roo/rules/taskmaster.md)) for comprehensive analysis - Review complexity report via `complexity_report` / `task-master complexity-report` (see [`taskmaster.md`](mdc:.roo/rules/taskmaster.md)) for a formatted, readable version. - Focus on tasks with highest complexity scores (8-10) for detailed breakdown - Use analysis results to determine appropriate subtask allocation - Note that reports are automatically used by the `expand_task` tool/command ## Task Breakdown Process - Use `expand_task` / `task-master expand --id=<id>`. It automatically uses the complexity report if found, otherwise generates default number of subtasks. - Use `--num=<number>` to specify an explicit number of subtasks, overriding defaults or complexity report recommendations. - Add `--research` flag to leverage Perplexity AI for research-backed expansion. - Add `--force` flag to clear existing subtasks before generating new ones (default is to append). - Use `--prompt="<context>"` to provide additional context when needed. - Review and adjust generated subtasks as necessary. - Use `expand_all` tool or `task-master expand --all` to expand multiple pending tasks at once, respecting flags like `--force` and `--research`. - If subtasks need complete replacement (regardless of the `--force` flag on `expand`), clear them first with `clear_subtasks` / `task-master clear-subtasks --id=<id>`. ## Implementation Drift Handling - When implementation differs significantly from planned approach - When future tasks need modification due to current implementation choices - When new dependencies or requirements emerge - Use `update` / `task-master update --from=<futureTaskId> --prompt='<explanation>\nUpdate context...' --research` to update multiple future tasks. - Use `update_task` / `task-master update-task --id=<taskId> --prompt='<explanation>\nUpdate context...' --research` to update a single specific task. ## Task Status Management - Use 'pending' for tasks ready to be worked on - Use 'done' for completed and verified tasks - Use 'deferred' for postponed tasks - Add custom status values as needed for project-specific workflows ## Task Structure Fields - **id**: Unique identifier for the task (Example: `1`, `1.1`) - **title**: Brief, descriptive title (Example: `"Initialize Repo"`) - **description**: Concise summary of what the task involves (Example: `"Create a new repository, set up initial structure."`) - **status**: Current state of the task (Example: `"pending"`, `"done"`, `"deferred"`) - **dependencies**: IDs of prerequisite tasks (Example: `[1, 2.1]`) - Dependencies are displayed with status indicators ( for completed,  for pending) - This helps quickly identify which prerequisite tasks are blocking work - **priority**: Importance level (Example: `"high"`, `"medium"`, `"low"`) - **details**: In-depth implementation instructions (Example: `"Use GitHub client ID/secret, handle callback, set session token."`) - **testStrategy**: Verification approach (Example: `"Deploy and call endpoint to confirm 'Hello World' response."`) - **subtasks**: List of smaller, more specific tasks (Example: `[{"id": 1, "title": "Configure OAuth", ...}]`) - Refer to task structure details (previously linked to `tasks.md`). ## Configuration Management (Updated) Taskmaster configuration is managed through two main mechanisms: 1. **`.taskmasterconfig` File (Primary):** * Located in the project root directory. * Stores most configuration settings: AI model selections (main, research, fallback), parameters (max tokens, temperature), logging level, default subtasks/priority, project name, etc. * **Managed via `task-master models --setup` command.** Do not edit manually unless you know what you are doing. * **View/Set specific models via `task-master models` command or `models` MCP tool.** * Created automatically when you run `task-master models --setup` for the first time. 2. **Environment Variables (`.env` / `mcp.json`):** * Used **only** for sensitive API keys and specific endpoint URLs. * Place API keys (one per provider) in a `.env` file in the project root for CLI usage. * For MCP/Roo Code integration, configure these keys in the `env` section of `.roo/mcp.json`. * Available keys/variables: See `assets/env.example` or the Configuration section in the command reference (previously linked to `taskmaster.md`). **Important:** Non-API key settings (like model selections, `MAX_TOKENS`, `LOG_LEVEL`) are **no longer configured via environment variables**. Use the `task-master models` command (or `--setup` for interactive configuration) or the `models` MCP tool. **If AI commands FAIL in MCP** verify that the API key for the selected provider is present in the `env` section of `.roo/mcp.json`. **If AI commands FAIL in CLI** verify that the API key for the selected provider is present in the `.env` file in the root of the project. ## Determining the Next Task - Run `next_task` / `task-master next` to show the next task to work on. - The command identifies tasks with all dependencies satisfied - Tasks are prioritized by priority level, dependency count, and ID - The command shows comprehensive task information including: - Basic task details and description - Implementation details - Subtasks (if they exist) - Contextual suggested actions - Recommended before starting any new development work - Respects your project's dependency structure - Ensures tasks are completed in the appropriate sequence - Provides ready-to-use commands for common task actions ## Viewing Specific Task Details - Run `get_task` / `task-master show <id>` to view a specific task. - Use dot notation for subtasks: `task-master show 1.2` (shows subtask 2 of task 1) - Displays comprehensive information similar to the next command, but for a specific task - For parent tasks, shows all subtasks and their current status - For subtasks, shows parent task information and relationship - Provides contextual suggested actions appropriate for the specific task - Useful for examining task details before implementation or checking status ## Managing Task Dependencies - Use `add_dependency` / `task-master add-dependency --id=<id> --depends-on=<id>` to add a dependency. - Use `remove_dependency` / `task-master remove-dependency --id=<id> --depends-on=<id>` to remove a dependency. - The system prevents circular dependencies and duplicate dependency entries - Dependencies are checked for existence before being added or removed - Task files are automatically regenerated after dependency changes - Dependencies are visualized with status indicators in task listings and files ## Iterative Subtask Implementation Once a task has been broken down into subtasks using `expand_task` or similar methods, follow this iterative process for implementation: 1. **Understand the Goal (Preparation):** * Use `get_task` / `task-master show <subtaskId>` (see [`taskmaster.md`](mdc:.roo/rules/taskmaster.md)) to thoroughly understand the specific goals and requirements of the subtask. 2. **Initial Exploration & Planning (Iteration 1):** * This is the first attempt at creating a concrete implementation plan. * Explore the codebase to identify the precise files, functions, and even specific lines of code that will need modification. * Determine the intended code changes (diffs) and their locations. * Gather *all* relevant details from this exploration phase. 3. **Log the Plan:** * Run `update_subtask` / `task-master update-subtask --id=<subtaskId> --prompt='<detailed plan>'`. * Provide the *complete and detailed* findings from the exploration phase in the prompt. Include file paths, line numbers, proposed diffs, reasoning, and any potential challenges identified. Do not omit details. The goal is to create a rich, timestamped log within the subtask's `details`. 4. **Verify the Plan:** * Run `get_task` / `task-master show <subtaskId>` again to confirm that the detailed implementation plan has been successfully appended to the subtask's details. 5. **Begin Implementation:** * Set the subtask status using `set_task_status` / `task-master set-status --id=<subtaskId> --status=in-progress`. * Start coding based on the logged plan. 6. **Refine and Log Progress (Iteration 2+):** * As implementation progresses, you will encounter challenges, discover nuances, or confirm successful approaches. * **Before appending new information**: Briefly review the *existing* details logged in the subtask (using `get_task` or recalling from context) to ensure the update adds fresh insights and avoids redundancy. * **Regularly** use `update_subtask` / `task-master update-subtask --id=<subtaskId> --prompt='<update details>\n- What worked...\n- What didn't work...'` to append new findings. * **Crucially, log:** * What worked ("fundamental truths" discovered). * What didn't work and why (to avoid repeating mistakes). * Specific code snippets or configurations that were successful. * Decisions made, especially if confirmed with user input. * Any deviations from the initial plan and the reasoning. * The objective is to continuously enrich the subtask's details, creating a log of the implementation journey that helps the AI (and human developers) learn, adapt, and avoid repeating errors. 7. **Review & Update Rules (Post-Implementation):** * Once the implementation for the subtask is functionally complete, review all code changes and the relevant chat history. * Identify any new or modified code patterns, conventions, or best practices established during the implementation. * Create new or update existing rules following internal guidelines (previously linked to `cursor_rules.md` and `self_improve.md`). 8. **Mark Task Complete:** * After verifying the implementation and updating any necessary rules, mark the subtask as completed: `set_task_status` / `task-master set-status --id=<subtaskId> --status=done`. 9. **Commit Changes (If using Git):** * Stage the relevant code changes and any updated/new rule files (`git add .`). * Craft a comprehensive Git commit message summarizing the work done for the subtask, including both code implementation and any rule adjustments. * Execute the commit command directly in the terminal (e.g., `git commit -m 'feat(module): Implement feature X for subtask <subtaskId>\n\n- Details about changes...\n- Updated rule Y for pattern Z'`). * Consider if a Changeset is needed according to internal versioning guidelines (previously linked to `changeset.md`). If so, run `npm run changeset`, stage the generated file, and amend the commit or create a new one. 10. **Proceed to Next Subtask:** * Identify the next subtask (e.g., using `next_task` / `task-master next`). ## Code Analysis & Refactoring Techniques - **Top-Level Function Search**: - Useful for understanding module structure or planning refactors. - Use grep/ripgrep to find exported functions/constants: `rg "export (async function|function|const) \w+"` or similar patterns. - Can help compare functions between files during migrations or identify potential naming conflicts. --- *This workflow provides a general guideline. Adapt it based on your specific project needs and team practices.*
---

# taskmaster

## Metadata
- **Generated on:** 2025-05-24 21:38:08
- **Source:** rules

# taskmaster

--- description: Comprehensive reference for Taskmaster MCP tools and CLI commands. globs: **/* alwaysApply: true --- # Taskmaster Tool & Command Reference This document provides a detailed reference for interacting with Taskmaster, covering both the recommended MCP tools, suitable for integrations like Roo Code, and the corresponding `task-master` CLI commands, designed for direct user interaction or fallback. **Note:** For interacting with Taskmaster programmatically or via integrated tools, using the **MCP tools is strongly recommended** due to better performance, structured data, and error handling. The CLI commands serve as a user-friendly alternative and fallback. **Important:** Several MCP tools involve AI processing... The AI-powered tools include `parse_prd`, `analyze_project_complexity`, `update_subtask`, `update_task`, `update`, `expand_all`, `expand_task`, and `add_task`. --- ## Initialization & Setup ### 1. Initialize Project (`init`) * **MCP Tool:** `initialize_project` * **CLI Command:** `task-master init [options]` * **Description:** `Set up the basic Taskmaster file structure and configuration in the current directory for a new project.` * **Key CLI Options:** * `--name <name>`: `Set the name for your project in Taskmaster's configuration.` * `--description <text>`: `Provide a brief description for your project.` * `--version <version>`: `Set the initial version for your project, e.g., '0.1.0'.` * `-y, --yes`: `Initialize Taskmaster quickly using default settings without interactive prompts.` * **Usage:** Run this once at the beginning of a new project. * **MCP Variant Description:** `Set up the basic Taskmaster file structure and configuration in the current directory for a new project by running the 'task-master init' command.` * **Key MCP Parameters/Options:** * `projectName`: `Set the name for your project.` (CLI: `--name <name>`) * `projectDescription`: `Provide a brief description for your project.` (CLI: `--description <text>`) * `projectVersion`: `Set the initial version for your project, e.g., '0.1.0'.` (CLI: `--version <version>`) * `authorName`: `Author name.` (CLI: `--author <author>`) * `skipInstall`: `Skip installing dependencies. Default is false.` (CLI: `--skip-install`) * `addAliases`: `Add shell aliases tm and taskmaster. Default is false.` (CLI: `--aliases`) * `yes`: `Skip prompts and use defaults/provided arguments. Default is false.` (CLI: `-y, --yes`) * **Usage:** Run this once at the beginning of a new project, typically via an integrated tool like Roo Code. Operates on the current working directory of the MCP server. * **Important:** Once complete, you *MUST* parse a prd in order to generate tasks. There will be no tasks files until then. The next step after initializing should be to create a PRD using the example PRD in scripts/example_prd.txt. ### 2. Parse PRD (`parse_prd`) * **MCP Tool:** `parse_prd` * **CLI Command:** `task-master parse-prd [file] [options]` * **Description:** `Parse a Product Requirements Document, PRD, or text file with Taskmaster to automatically generate an initial set of tasks in tasks.json.` * **Key Parameters/Options:** * `input`: `Path to your PRD or requirements text file that Taskmaster should parse for tasks.` (CLI: `[file]` positional or `-i, --input <file>`) * `output`: `Specify where Taskmaster should save the generated 'tasks.json' file. Defaults to 'tasks/tasks.json'.` (CLI: `-o, --output <file>`) * `numTasks`: `Approximate number of top-level tasks Taskmaster should aim to generate from the document.` (CLI: `-n, --num-tasks <number>`) * `force`: `Use this to allow Taskmaster to overwrite an existing 'tasks.json' without asking for confirmation.` (CLI: `-f, --force`) * **Usage:** Useful for bootstrapping a project from an existing requirements document. * **Notes:** Task Master will strictly adhere to any specific requirements mentioned in the PRD, such as libraries, database schemas, frameworks, tech stacks, etc., while filling in any gaps where the PRD isn't fully specified. Tasks are designed to provide the most direct implementation path while avoiding over-engineering. * **Important:** This MCP tool makes AI calls and can take up to a minute to complete. Please inform users to hang tight while the operation is in progress. If the user does not have a PRD, suggest discussing their idea and then use the example PRD in `scripts/example_prd.txt` as a template for creating the PRD based on their idea, for use with `parse-prd`. --- ## AI Model Configuration ### 2. Manage Models (`models`) * **MCP Tool:** `models` * **CLI Command:** `task-master models [options]` * **Description:** `View the current AI model configuration or set specific models for different roles (main, research, fallback). Allows setting custom model IDs for Ollama and OpenRouter.` * **Key MCP Parameters/Options:** * `setMain <model_id>`: `Set the primary model ID for task generation/updates.` (CLI: `--set-main <model_id>`) * `setResearch <model_id>`: `Set the model ID for research-backed operations.` (CLI: `--set-research <model_id>`) * `setFallback <model_id>`: `Set the model ID to use if the primary fails.` (CLI: `--set-fallback <model_id>`) * `ollama <boolean>`: `Indicates the set model ID is a custom Ollama model.` (CLI: `--ollama`) * `openrouter <boolean>`: `Indicates the set model ID is a custom OpenRouter model.` (CLI: `--openrouter`) * `listAvailableModels <boolean>`: `If true, lists available models not currently assigned to a role.` (CLI: No direct equivalent; CLI lists available automatically) * `projectRoot <string>`: `Optional. Absolute path to the project root directory.` (CLI: Determined automatically) * **Key CLI Options:** * `--set-main <model_id>`: `Set the primary model.` * `--set-research <model_id>`: `Set the research model.` * `--set-fallback <model_id>`: `Set the fallback model.` * `--ollama`: `Specify that the provided model ID is for Ollama (use with --set-*).` * `--openrouter`: `Specify that the provided model ID is for OpenRouter (use with --set-*). Validates against OpenRouter API.` * `--setup`: `Run interactive setup to configure models, including custom Ollama/OpenRouter IDs.` * **Usage (MCP):** Call without set flags to get current config. Use `setMain`, `setResearch`, or `setFallback` with a valid model ID to update the configuration. Use `listAvailableModels: true` to get a list of unassigned models. To set a custom model, provide the model ID and set `ollama: true` or `openrouter: true`. * **Usage (CLI):** Run without flags to view current configuration and available models. Use set flags to update specific roles. Use `--setup` for guided configuration, including custom models. To set a custom model via flags, use `--set-<role>=<model_id>` along with either `--ollama` or `--openrouter`. * **Notes:** Configuration is stored in `.taskmasterconfig` in the project root. This command/tool modifies that file. Use `listAvailableModels` or `task-master models` to see internally supported models. OpenRouter custom models are validated against their live API. Ollama custom models are not validated live. * **API note:** API keys for selected AI providers (based on their model) need to exist in the mcp.json file to be accessible in MCP context. The API keys must be present in the local .env file for the CLI to be able to read them. * **Model costs:** The costs in supported models are expressed in dollars. An input/output value of 3 is $3.00. A value of 0.8 is $0.80. * **Warning:** DO NOT MANUALLY EDIT THE .taskmasterconfig FILE. Use the included commands either in the MCP or CLI format as needed. Always prioritize MCP tools when available and use the CLI as a fallback. --- ## Task Listing & Viewing ### 3. Get Tasks (`get_tasks`) * **MCP Tool:** `get_tasks` * **CLI Command:** `task-master list [options]` * **Description:** `List your Taskmaster tasks, optionally filtering by status and showing subtasks.` * **Key Parameters/Options:** * `status`: `Show only Taskmaster tasks matching this status, e.g., 'pending' or 'done'.` (CLI: `-s, --status <status>`) * `withSubtasks`: `Include subtasks indented under their parent tasks in the list.` (CLI: `--with-subtasks`) * `file`: `Path to your Taskmaster 'tasks.json' file. Default relies on auto-detection.` (CLI: `-f, --file <file>`) * **Usage:** Get an overview of the project status, often used at the start of a work session. ### 4. Get Next Task (`next_task`) * **MCP Tool:** `next_task` * **CLI Command:** `task-master next [options]` * **Description:** `Ask Taskmaster to show the next available task you can work on, based on status and completed dependencies.` * **Key Parameters/Options:** * `file`: `Path to your Taskmaster 'tasks.json' file. Default relies on auto-detection.` (CLI: `-f, --file <file>`) * **Usage:** Identify what to work on next according to the plan. ### 5. Get Task Details (`get_task`) * **MCP Tool:** `get_task` * **CLI Command:** `task-master show [id] [options]` * **Description:** `Display detailed information for a specific Taskmaster task or subtask by its ID.` * **Key Parameters/Options:** * `id`: `Required. The ID of the Taskmaster task, e.g., '15', or subtask, e.g., '15.2', you want to view.` (CLI: `[id]` positional or `-i, --id <id>`) * `file`: `Path to your Taskmaster 'tasks.json' file. Default relies on auto-detection.` (CLI: `-f, --file <file>`) * **Usage:** Understand the full details, implementation notes, and test strategy for a specific task before starting work. --- ## Task Creation & Modification ### 6. Add Task (`add_task`) * **MCP Tool:** `add_task` * **CLI Command:** `task-master add-task [options]` * **Description:** `Add a new task to Taskmaster by describing it; AI will structure it.` * **Key Parameters/Options:** * `prompt`: `Required. Describe the new task you want Taskmaster to create, e.g., "Implement user authentication using JWT".` (CLI: `-p, --prompt <text>`) * `dependencies`: `Specify the IDs of any Taskmaster tasks that must be completed before this new one can start, e.g., '12,14'.` (CLI: `-d, --dependencies <ids>`) * `priority`: `Set the priority for the new task: 'high', 'medium', or 'low'. Default is 'medium'.` (CLI: `--priority <priority>`) * `research`: `Enable Taskmaster to use the research role for potentially more informed task creation.` (CLI: `-r, --research`) * `file`: `Path to your Taskmaster 'tasks.json' file. Default relies on auto-detection.` (CLI: `-f, --file <file>`) * **Usage:** Quickly add newly identified tasks during development. * **Important:** This MCP tool makes AI calls and can take up to a minute to complete. Please inform users to hang tight while the operation is in progress. ### 7. Add Subtask (`add_subtask`) * **MCP Tool:** `add_subtask` * **CLI Command:** `task-master add-subtask [options]` * **Description:** `Add a new subtask to a Taskmaster parent task, or convert an existing task into a subtask.` * **Key Parameters/Options:** * `id` / `parent`: `Required. The ID of the Taskmaster task that will be the parent.` (MCP: `id`, CLI: `-p, --parent <id>`) * `taskId`: `Use this if you want to convert an existing top-level Taskmaster task into a subtask of the specified parent.` (CLI: `-i, --task-id <id>`) * `title`: `Required if not using taskId. The title for the new subtask Taskmaster should create.` (CLI: `-t, --title <title>`) * `description`: `A brief description for the new subtask.` (CLI: `-d, --description <text>`) * `details`: `Provide implementation notes or details for the new subtask.` (CLI: `--details <text>`) * `dependencies`: `Specify IDs of other tasks or subtasks, e.g., '15' or '16.1', that must be done before this new subtask.` (CLI: `--dependencies <ids>`) * `status`: `Set the initial status for the new subtask. Default is 'pending'.` (CLI: `-s, --status <status>`) * `skipGenerate`: `Prevent Taskmaster from automatically regenerating markdown task files after adding the subtask.` (CLI: `--skip-generate`) * `file`: `Path to your Taskmaster 'tasks.json' file. Default relies on auto-detection.` (CLI: `-f, --file <file>`) * **Usage:** Break down tasks manually or reorganize existing tasks. ### 8. Update Tasks (`update`) * **MCP Tool:** `update` * **CLI Command:** `task-master update [options]` * **Description:** `Update multiple upcoming tasks in Taskmaster based on new context or changes, starting from a specific task ID.` * **Key Parameters/Options:** * `from`: `Required. The ID of the first task Taskmaster should update. All tasks with this ID or higher that are not 'done' will be considered.` (CLI: `--from <id>`) * `prompt`: `Required. Explain the change or new context for Taskmaster to apply to the tasks, e.g., "We are now using React Query instead of Redux Toolkit for data fetching".` (CLI: `-p, --prompt <text>`) * `research`: `Enable Taskmaster to use the research role for more informed updates. Requires appropriate API key.` (CLI: `-r, --research`) * `file`: `Path to your Taskmaster 'tasks.json' file. Default relies on auto-detection.` (CLI: `-f, --file <file>`) * **Usage:** Handle significant implementation changes or pivots that affect multiple future tasks. Example CLI: `task-master update --from='18' --prompt='Switching to React Query.\nNeed to refactor data fetching...'` * **Important:** This MCP tool makes AI calls and can take up to a minute to complete. Please inform users to hang tight while the operation is in progress. ### 9. Update Task (`update_task`) * **MCP Tool:** `update_task` * **CLI Command:** `task-master update-task [options]` * **Description:** `Modify a specific Taskmaster task or subtask by its ID, incorporating new information or changes.` * **Key Parameters/Options:** * `id`: `Required. The specific ID of the Taskmaster task, e.g., '15', or subtask, e.g., '15.2', you want to update.` (CLI: `-i, --id <id>`) * `prompt`: `Required. Explain the specific changes or provide the new information Taskmaster should incorporate into this task.` (CLI: `-p, --prompt <text>`) * `research`: `Enable Taskmaster to use the research role for more informed updates. Requires appropriate API key.` (CLI: `-r, --research`) * `file`: `Path to your Taskmaster 'tasks.json' file. Default relies on auto-detection.` (CLI: `-f, --file <file>`) * **Usage:** Refine a specific task based on new understanding or feedback. Example CLI: `task-master update-task --id='15' --prompt='Clarification: Use PostgreSQL instead of MySQL.\nUpdate schema details...'` * **Important:** This MCP tool makes AI calls and can take up to a minute to complete. Please inform users to hang tight while the operation is in progress. ### 10. Update Subtask (`update_subtask`) * **MCP Tool:** `update_subtask` * **CLI Command:** `task-master update-subtask [options]` * **Description:** `Append timestamped notes or details to a specific Taskmaster subtask without overwriting existing content. Intended for iterative implementation logging.` * **Key Parameters/Options:** * `id`: `Required. The specific ID of the Taskmaster subtask, e.g., '15.2', you want to add information to.` (CLI: `-i, --id <id>`) * `prompt`: `Required. Provide the information or notes Taskmaster should append to the subtask's details. Ensure this adds *new* information not already present.` (CLI: `-p, --prompt <text>`) * `research`: `Enable Taskmaster to use the research role for more informed updates. Requires appropriate API key.` (CLI: `-r, --research`) * `file`: `Path to your Taskmaster 'tasks.json' file. Default relies on auto-detection.` (CLI: `-f, --file <file>`) * **Usage:** Add implementation notes, code snippets, or clarifications to a subtask during development. Before calling, review the subtask's current details to append only fresh insights, helping to build a detailed log of the implementation journey and avoid redundancy. Example CLI: `task-master update-subtask --id='15.2' --prompt='Discovered that the API requires header X.\nImplementation needs adjustment...'` * **Important:** This MCP tool makes AI calls and can take up to a minute to complete. Please inform users to hang tight while the operation is in progress. ### 11. Set Task Status (`set_task_status`) * **MCP Tool:** `set_task_status` * **CLI Command:** `task-master set-status [options]` * **Description:** `Update the status of one or more Taskmaster tasks or subtasks, e.g., 'pending', 'in-progress', 'done'.` * **Key Parameters/Options:** * `id`: `Required. The ID(s) of the Taskmaster task(s) or subtask(s), e.g., '15', '15.2', or '16,17.1', to update.` (CLI: `-i, --id <id>`) * `status`: `Required. The new status to set, e.g., 'done', 'pending', 'in-progress', 'review', 'cancelled'.` (CLI: `-s, --status <status>`) * `file`: `Path to your Taskmaster 'tasks.json' file. Default relies on auto-detection.` (CLI: `-f, --file <file>`) * **Usage:** Mark progress as tasks move through the development cycle. ### 12. Remove Task (`remove_task`) * **MCP Tool:** `remove_task` * **CLI Command:** `task-master remove-task [options]` * **Description:** `Permanently remove a task or subtask from the Taskmaster tasks list.` * **Key Parameters/Options:** * `id`: `Required. The ID of the Taskmaster task, e.g., '5', or subtask, e.g., '5.2', to permanently remove.` (CLI: `-i, --id <id>`) * `yes`: `Skip the confirmation prompt and immediately delete the task.` (CLI: `-y, --yes`) * `file`: `Path to your Taskmaster 'tasks.json' file. Default relies on auto-detection.` (CLI: `-f, --file <file>`) * **Usage:** Permanently delete tasks or subtasks that are no longer needed in the project. * **Notes:** Use with caution as this operation cannot be undone. Consider using 'blocked', 'cancelled', or 'deferred' status instead if you just want to exclude a task from active planning but keep it for reference. The command automatically cleans up dependency references in other tasks. --- ## Task Structure & Breakdown ### 13. Expand Task (`expand_task`) * **MCP Tool:** `expand_task` * **CLI Command:** `task-master expand [options]` * **Description:** `Use Taskmaster's AI to break down a complex task into smaller, manageable subtasks. Appends subtasks by default.` * **Key Parameters/Options:** * `id`: `The ID of the specific Taskmaster task you want to break down into subtasks.` (CLI: `-i, --id <id>`) * `num`: `Optional: Suggests how many subtasks Taskmaster should aim to create. Uses complexity analysis/defaults otherwise.` (CLI: `-n, --num <number>`) * `research`: `Enable Taskmaster to use the research role for more informed subtask generation. Requires appropriate API key.` (CLI: `-r, --research`) * `prompt`: `Optional: Provide extra context or specific instructions to Taskmaster for generating the subtasks.` (CLI: `-p, --prompt <text>`) * `force`: `Optional: If true, clear existing subtasks before generating new ones. Default is false (append).` (CLI: `--force`) * `file`: `Path to your Taskmaster 'tasks.json' file. Default relies on auto-detection.` (CLI: `-f, --file <file>`) * **Usage:** Generate a detailed implementation plan for a complex task before starting coding. Automatically uses complexity report recommendations if available and `num` is not specified. * **Important:** This MCP tool makes AI calls and can take up to a minute to complete. Please inform users to hang tight while the operation is in progress. ### 14. Expand All Tasks (`expand_all`) * **MCP Tool:** `expand_all` * **CLI Command:** `task-master expand --all [options]` (Note: CLI uses the `expand` command with the `--all` flag) * **Description:** `Tell Taskmaster to automatically expand all eligible pending/in-progress tasks based on complexity analysis or defaults. Appends subtasks by default.` * **Key Parameters/Options:** * `num`: `Optional: Suggests how many subtasks Taskmaster should aim to create per task.` (CLI: `-n, --num <number>`) * `research`: `Enable research role for more informed subtask generation. Requires appropriate API key.` (CLI: `-r, --research`) * `prompt`: `Optional: Provide extra context for Taskmaster to apply generally during expansion.` (CLI: `-p, --prompt <text>`) * `force`: `Optional: If true, clear existing subtasks before generating new ones for each eligible task. Default is false (append).` (CLI: `--force`) * `file`: `Path to your Taskmaster 'tasks.json' file. Default relies on auto-detection.` (CLI: `-f, --file <file>`) * **Usage:** Useful after initial task generation or complexity analysis to break down multiple tasks at once. * **Important:** This MCP tool makes AI calls and can take up to a minute to complete. Please inform users to hang tight while the operation is in progress. ### 15. Clear Subtasks (`clear_subtasks`) * **MCP Tool:** `clear_subtasks` * **CLI Command:** `task-master clear-subtasks [options]` * **Description:** `Remove all subtasks from one or more specified Taskmaster parent tasks.` * **Key Parameters/Options:** * `id`: `The ID(s) of the Taskmaster parent task(s) whose subtasks you want to remove, e.g., '15' or '16,18'. Required unless using `all`.) (CLI: `-i, --id <ids>`) * `all`: `Tell Taskmaster to remove subtasks from all parent tasks.` (CLI: `--all`) * `file`: `Path to your Taskmaster 'tasks.json' file. Default relies on auto-detection.` (CLI: `-f, --file <file>`) * **Usage:** Used before regenerating subtasks with `expand_task` if the previous breakdown needs replacement. ### 16. Remove Subtask (`remove_subtask`) * **MCP Tool:** `remove_subtask` * **CLI Command:** `task-master remove-subtask [options]` * **Description:** `Remove a subtask from its Taskmaster parent, optionally converting it into a standalone task.` * **Key Parameters/Options:** * `id`: `Required. The ID(s) of the Taskmaster subtask(s) to remove, e.g., '15.2' or '16.1,16.3'.` (CLI: `-i, --id <id>`) * `convert`: `If used, Taskmaster will turn the subtask into a regular top-level task instead of deleting it.` (CLI: `-c, --convert`) * `skipGenerate`: `Prevent Taskmaster from automatically regenerating markdown task files after removing the subtask.` (CLI: `--skip-generate`) * `file`: `Path to your Taskmaster 'tasks.json' file. Default relies on auto-detection.` (CLI: `-f, --file <file>`) * **Usage:** Delete unnecessary subtasks or promote a subtask to a top-level task. --- ## Dependency Management ### 17. Add Dependency (`add_dependency`) * **MCP Tool:** `add_dependency` * **CLI Command:** `task-master add-dependency [options]` * **Description:** `Define a dependency in Taskmaster, making one task a prerequisite for another.` * **Key Parameters/Options:** * `id`: `Required. The ID of the Taskmaster task that will depend on another.` (CLI: `-i, --id <id>`) * `dependsOn`: `Required. The ID of the Taskmaster task that must be completed first, the prerequisite.` (CLI: `-d, --depends-on <id>`) * `file`: `Path to your Taskmaster 'tasks.json' file. Default relies on auto-detection.` (CLI: `-f, --file <path>`) * **Usage:** Establish the correct order of execution between tasks. ### 18. Remove Dependency (`remove_dependency`) * **MCP Tool:** `remove_dependency` * **CLI Command:** `task-master remove-dependency [options]` * **Description:** `Remove a dependency relationship between two Taskmaster tasks.` * **Key Parameters/Options:** * `id`: `Required. The ID of the Taskmaster task you want to remove a prerequisite from.` (CLI: `-i, --id <id>`) * `dependsOn`: `Required. The ID of the Taskmaster task that should no longer be a prerequisite.` (CLI: `-d, --depends-on <id>`) * `file`: `Path to your Taskmaster 'tasks.json' file. Default relies on auto-detection.` (CLI: `-f, --file <file>`) * **Usage:** Update task relationships when the order of execution changes. ### 19. Validate Dependencies (`validate_dependencies`) * **MCP Tool:** `validate_dependencies` * **CLI Command:** `task-master validate-dependencies [options]` * **Description:** `Check your Taskmaster tasks for dependency issues (like circular references or links to non-existent tasks) without making changes.` * **Key Parameters/Options:** * `file`: `Path to your Taskmaster 'tasks.json' file. Default relies on auto-detection.` (CLI: `-f, --file <file>`) * **Usage:** Audit the integrity of your task dependencies. ### 20. Fix Dependencies (`fix_dependencies`) * **MCP Tool:** `fix_dependencies` * **CLI Command:** `task-master fix-dependencies [options]` * **Description:** `Automatically fix dependency issues (like circular references or links to non-existent tasks) in your Taskmaster tasks.` * **Key Parameters/Options:** * `file`: `Path to your Taskmaster 'tasks.json' file. Default relies on auto-detection.` (CLI: `-f, --file <file>`) * **Usage:** Clean up dependency errors automatically. --- ## Analysis & Reporting ### 21. Analyze Project Complexity (`analyze_project_complexity`) * **MCP Tool:** `analyze_project_complexity` * **CLI Command:** `task-master analyze-complexity [options]` * **Description:** `Have Taskmaster analyze your tasks to determine their complexity and suggest which ones need to be broken down further.` * **Key Parameters/Options:** * `output`: `Where to save the complexity analysis report (default: 'scripts/task-complexity-report.json').` (CLI: `-o, --output <file>`) * `threshold`: `The minimum complexity score (1-10) that should trigger a recommendation to expand a task.` (CLI: `-t, --threshold <number>`) * `research`: `Enable research role for more accurate complexity analysis. Requires appropriate API key.` (CLI: `-r, --research`) * `file`: `Path to your Taskmaster 'tasks.json' file. Default relies on auto-detection.` (CLI: `-f, --file <file>`) * **Usage:** Used before breaking down tasks to identify which ones need the most attention. * **Important:** This MCP tool makes AI calls and can take up to a minute to complete. Please inform users to hang tight while the operation is in progress. ### 22. View Complexity Report (`complexity_report`) * **MCP Tool:** `complexity_report` * **CLI Command:** `task-master complexity-report [options]` * **Description:** `Display the task complexity analysis report in a readable format.` * **Key Parameters/Options:** * `file`: `Path to the complexity report (default: 'scripts/task-complexity-report.json').` (CLI: `-f, --file <file>`) * **Usage:** Review and understand the complexity analysis results after running analyze-complexity. --- ## File Management ### 23. Generate Task Files (`generate`) * **MCP Tool:** `generate` * **CLI Command:** `task-master generate [options]` * **Description:** `Create or update individual Markdown files for each task based on your tasks.json.` * **Key Parameters/Options:** * `output`: `The directory where Taskmaster should save the task files (default: in a 'tasks' directory).` (CLI: `-o, --output <directory>`) * `file`: `Path to your Taskmaster 'tasks.json' file. Default relies on auto-detection.` (CLI: `-f, --file <file>`) * **Usage:** Run this after making changes to tasks.json to keep individual task files up to date. --- ## Environment Variables Configuration (Updated) Taskmaster primarily uses the **`.taskmasterconfig`** file (in project root) for configuration (models, parameters, logging level, etc.), managed via `task-master models --setup`. Environment variables are used **only** for sensitive API keys related to AI providers and specific overrides like the Ollama base URL: * **API Keys (Required for corresponding provider):** * `ANTHROPIC_API_KEY` * `PERPLEXITY_API_KEY` * `OPENAI_API_KEY` * `GOOGLE_API_KEY` * `MISTRAL_API_KEY` * `AZURE_OPENAI_API_KEY` (Requires `AZURE_OPENAI_ENDPOINT` too) * `OPENROUTER_API_KEY` * `XAI_API_KEY` * `OLLANA_API_KEY` (Requires `OLLAMA_BASE_URL` too) * **Endpoints (Optional/Provider Specific inside .taskmasterconfig):** * `AZURE_OPENAI_ENDPOINT` * `OLLAMA_BASE_URL` (Default: `http://localhost:11434/api`) **Set API keys** in your **`.env`** file in the project root (for CLI use) or within the `env` section of your **`.roo/mcp.json`** file (for MCP/Roo Code integration). All other settings (model choice, max tokens, temperature, log level, custom endpoints) are managed in `.taskmasterconfig` via `task-master models` command or `models` MCP tool. --- For details on how these commands fit into the development process, see the [Development Workflow Guide](mdc:.roo/rules/dev_workflow.md).